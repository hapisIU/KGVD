[
  {
    "vulcode": "static int semctl_setval(struct ipc_namespace *ns, int semid, int semnum,\n\t\tunsigned long arg)\n{\n\tstruct sem_undo *un;\n\tstruct sem_array *sma;\n\tstruct sem* curr;\n\tint err;\n\tint nsems;\n\tstruct list_head tasks;\n\tint val;\n#if defined(CONFIG_64BIT) && defined(__BIG_ENDIAN)\n\t/* big-endian 64bit */\n\tval = arg >> 32;\n#else\n\t/* 32bit or little-endian 64bit */\n\tval = arg;\n#endif\n\n\tsma = sem_lock_check(ns, semid);\n\tif (IS_ERR(sma))\n\t\treturn PTR_ERR(sma);\n\n\tINIT_LIST_HEAD(&tasks);\n\tnsems = sma->sem_nsems;\n\n\terr = -EACCES;\n\tif (ipcperms(ns, &sma->sem_perm, S_IWUGO))\n\t\tgoto out_unlock;\n\n\terr = security_sem_semctl(sma, SETVAL);\n\tif (err)\n\t\tgoto out_unlock;\n\n\terr = -EINVAL;\n\tif(semnum < 0 || semnum >= nsems)\n\t\tgoto out_unlock;\n\n\tcurr = &sma->sem_base[semnum];\n\n\terr = -ERANGE;\n\tif (val > SEMVMX || val < 0)\n\t\tgoto out_unlock;\n\n\tassert_spin_locked(&sma->sem_perm.lock);\n\tlist_for_each_entry(un, &sma->list_id, list_id)\n\t\tun->semadj[semnum] = 0;\n\n\tcurr->semval = val;\n\tcurr->sempid = task_tgid_vnr(current);\n\tsma->sem_ctime = get_seconds();\n\t/* maybe some queued-up processes were waiting for this */\n\tdo_smart_update(sma, NULL, 0, 0, &tasks);\n\terr = 0;\nout_unlock:\n\tsem_unlock(sma);\n\twake_up_sem_queue_do(&tasks);\n\treturn err;\n}",
    "diff": "@@ -94,6 +94,7 @@\n struct sem {\n \tint\tsemval;\t\t/* current value */\n \tint\tsempid;\t\t/* pid of last operation */\n+\tspinlock_t\tlock;\t/* spinlock for fine-grained semtimedop */\n \tstruct list_head sem_pending; /* pending single-sop operations */\n };\n \n@@ -137,7 +138,6 @@ struct sem_undo_list {\n \n #define sem_ids(ns)\t((ns)->ids[IPC_SEM_IDS])\n \n-#define sem_unlock(sma)\t\tipc_unlock(&(sma)->sem_perm)\n #define sem_checkid(sma, semid)\tipc_checkid(&sma->sem_perm, semid)\n \n static int newary(struct ipc_namespace *, struct ipc_params *);\n@@ -189,11 +189,90 @@ void __init sem_init (void)\n \t\t\t\tIPC_SEM_IDS, sysvipc_sem_proc_show);\n }\n \n+/*\n+ * If the request contains only one semaphore operation, and there are\n+ * no complex transactions pending, lock only the semaphore involved.\n+ * Otherwise, lock the entire semaphore array, since we either have\n+ * multiple semaphores in our own semops, or we need to look at\n+ * semaphores from other pending complex operations.\n+ *\n+ * Carefully guard against sma->complex_count changing between zero\n+ * and non-zero while we are spinning for the lock. The value of\n+ * sma->complex_count cannot change while we are holding the lock,\n+ * so sem_unlock should be fine.\n+ *\n+ * The global lock path checks that all the local locks have been released,\n+ * checking each local lock once. This means that the local lock paths\n+ * cannot start their critical sections while the global lock is held.\n+ */\n+static inline int sem_lock(struct sem_array *sma, struct sembuf *sops,\n+\t\t\t      int nsops)\n+{\n+\tint locknum;\n+ again:\n+\tif (nsops == 1 && !sma->complex_count) {\n+\t\tstruct sem *sem = sma->sem_base + sops->sem_num;\n+\n+\t\t/* Lock just the semaphore we are interested in. */\n+\t\tspin_lock(&sem->lock);\n+\n+\t\t/*\n+\t\t * If sma->complex_count was set while we were spinning,\n+\t\t * we may need to look at things we did not lock here.\n+\t\t */\n+\t\tif (unlikely(sma->complex_count)) {\n+\t\t\tspin_unlock(&sem->lock);\n+\t\t\tgoto lock_array;\n+\t\t}\n+\n+\t\t/*\n+\t\t * Another process is holding the global lock on the\n+\t\t * sem_array; we cannot enter our critical section,\n+\t\t * but have to wait for the global lock to be released.\n+\t\t */\n+\t\tif (unlikely(spin_is_locked(&sma->sem_perm.lock))) {\n+\t\t\tspin_unlock(&sem->lock);\n+\t\t\tspin_unlock_wait(&sma->sem_perm.lock);\n+\t\t\tgoto again;\n+\t\t}\n+\n+\t\tlocknum = sops->sem_num;\n+\t} else {\n+\t\tint i;\n+\t\t/*\n+\t\t * Lock the semaphore array, and wait for all of the\n+\t\t * individual semaphore locks to go away.  The code\n+\t\t * above ensures no new single-lock holders will enter\n+\t\t * their critical section while the array lock is held.\n+\t\t */\n+ lock_array:\n+\t\tspin_lock(&sma->sem_perm.lock);\n+\t\tfor (i = 0; i < sma->sem_nsems; i++) {\n+\t\t\tstruct sem *sem = sma->sem_base + i;\n+\t\t\tspin_unlock_wait(&sem->lock);\n+\t\t}\n+\t\tlocknum = -1;\n+\t}\n+\treturn locknum;\n+}\n+\n+static inline void sem_unlock(struct sem_array *sma, int locknum)\n+{\n+\tif (locknum == -1) {\n+\t\tspin_unlock(&sma->sem_perm.lock);\n+\t} else {\n+\t\tstruct sem *sem = sma->sem_base + locknum;\n+\t\tspin_unlock(&sem->lock);\n+\t}\n+\trcu_read_unlock();\n+}\n+\n /*\n  * sem_lock_(check_) routines are called in the paths where the rw_mutex\n  * is not held.\n  */\n-static inline struct sem_array *sem_obtain_lock(struct ipc_namespace *ns, int id)\n+static inline struct sem_array *sem_obtain_lock(struct ipc_namespace *ns,\n+\t\t\tint id, struct sembuf *sops, int nsops, int *locknum)\n {\n \tstruct kern_ipc_perm *ipcp;\n \tstruct sem_array *sma;\n@@ -205,7 +284,8 @@ static inline struct sem_array *sem_obtain_lock(struct ipc_namespace *ns, int id\n \t\tgoto err;\n \t}\n \n-\tspin_lock(&ipcp->lock);\n+\tsma = container_of(ipcp, struct sem_array, sem_perm);\n+\t*locknum = sem_lock(sma, sops, nsops);\n \n \t/* ipc_rmid() may have already freed the ID while sem_lock\n \t * was spinning: verify that the structure is still valid\n@@ -213,7 +293,7 @@ static inline struct sem_array *sem_obtain_lock(struct ipc_namespace *ns, int id\n \tif (!ipcp->deleted)\n \t\treturn container_of(ipcp, struct sem_array, sem_perm);\n \n-\tspin_unlock(&ipcp->lock);\n+\tsem_unlock(sma, *locknum);\n \tsma = ERR_PTR(-EINVAL);\n err:\n \trcu_read_unlock();\n@@ -230,17 +310,6 @@ static inline struct sem_array *sem_obtain_object(struct ipc_namespace *ns, int\n \treturn container_of(ipcp, struct sem_array, sem_perm);\n }\n \n-static inline struct sem_array *sem_lock_check(struct ipc_namespace *ns,\n-\t\t\t\t\t\tint id)\n-{\n-\tstruct kern_ipc_perm *ipcp = ipc_lock_check(&sem_ids(ns), id);\n-\n-\tif (IS_ERR(ipcp))\n-\t\treturn ERR_CAST(ipcp);\n-\n-\treturn container_of(ipcp, struct sem_array, sem_perm);\n-}\n-\n static inline struct sem_array *sem_obtain_object_check(struct ipc_namespace *ns,\n \t\t\t\t\t\t\tint id)\n {\n@@ -254,21 +323,21 @@ static inline struct sem_array *sem_obtain_object_check(struct ipc_namespace *ns\n \n static inline void sem_lock_and_putref(struct sem_array *sma)\n {\n-\tipc_lock_by_ptr(&sma->sem_perm);\n+\trcu_read_lock();\n+\tsem_lock(sma, NULL, -1);\n \tipc_rcu_putref(sma);\n }\n \n static inline void sem_getref_and_unlock(struct sem_array *sma)\n {\n-\tipc_rcu_getref(sma);\n-\tipc_unlock(&(sma)->sem_perm);\n+\tWARN_ON_ONCE(!ipc_rcu_getref(sma));\n+\tsem_unlock(sma, -1);\n }\n \n static inline void sem_putref(struct sem_array *sma)\n {\n-\tipc_lock_by_ptr(&sma->sem_perm);\n-\tipc_rcu_putref(sma);\n-\tipc_unlock(&(sma)->sem_perm);\n+\tsem_lock_and_putref(sma);\n+\tsem_unlock(sma, -1);\n }\n \n /*\n@@ -276,9 +345,9 @@ static inline void sem_putref(struct sem_array *sma)\n  */\n static inline void sem_getref(struct sem_array *sma)\n {\n-\tspin_lock(&(sma)->sem_perm.lock);\n-\tipc_rcu_getref(sma);\n-\tipc_unlock(&(sma)->sem_perm);\n+\tsem_lock(sma, NULL, -1);\n+\tWARN_ON_ONCE(!ipc_rcu_getref(sma));\n+\tsem_unlock(sma, -1);\n }\n \n static inline void sem_rmid(struct ipc_namespace *ns, struct sem_array *s)\n@@ -371,15 +440,17 @@ static int newary(struct ipc_namespace *ns, struct ipc_params *params)\n \n \tsma->sem_base = (struct sem *) &sma[1];\n \n-\tfor (i = 0; i < nsems; i++)\n+\tfor (i = 0; i < nsems; i++) {\n \t\tINIT_LIST_HEAD(&sma->sem_base[i].sem_pending);\n+\t\tspin_lock_init(&sma->sem_base[i].lock);\n+\t}\n \n \tsma->complex_count = 0;\n \tINIT_LIST_HEAD(&sma->sem_pending);\n \tINIT_LIST_HEAD(&sma->list_id);\n \tsma->sem_nsems = nsems;\n \tsma->sem_ctime = get_seconds();\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, -1);\n \n \treturn sma->sem_perm.id;\n }\n@@ -818,7 +889,7 @@ static void freeary(struct ipc_namespace *ns, struct kern_ipc_perm *ipcp)\n \n \t/* Remove the semaphore set from the IDR */\n \tsem_rmid(ns, sma);\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, -1);\n \n \twake_up_sem_queue_do(&tasks);\n \tns->used_sems -= sma->sem_nsems;\n@@ -947,7 +1018,6 @@ static int semctl_setval(struct ipc_namespace *ns, int semid, int semnum,\n \tstruct sem_array *sma;\n \tstruct sem* curr;\n \tint err;\n-\tint nsems;\n \tstruct list_head tasks;\n \tint val;\n #if defined(CONFIG_64BIT) && defined(__BIG_ENDIAN)\n@@ -958,31 +1028,39 @@ static int semctl_setval(struct ipc_namespace *ns, int semid, int semnum,\n \tval = arg;\n #endif\n \n-\tsma = sem_lock_check(ns, semid);\n-\tif (IS_ERR(sma))\n-\t\treturn PTR_ERR(sma);\n+\tif (val > SEMVMX || val < 0)\n+\t\treturn -ERANGE;\n \n \tINIT_LIST_HEAD(&tasks);\n-\tnsems = sma->sem_nsems;\n \n-\terr = -EACCES;\n-\tif (ipcperms(ns, &sma->sem_perm, S_IWUGO))\n-\t\tgoto out_unlock;\n+\trcu_read_lock();\n+\tsma = sem_obtain_object_check(ns, semid);\n+\tif (IS_ERR(sma)) {\n+\t\trcu_read_unlock();\n+\t\treturn PTR_ERR(sma);\n+\t}\n+\n+\tif (semnum < 0 || semnum >= sma->sem_nsems) {\n+\t\trcu_read_unlock();\n+\t\treturn -EINVAL;\n+\t}\n+\n+\n+\tif (ipcperms(ns, &sma->sem_perm, S_IWUGO)) {\n+\t\trcu_read_unlock();\n+\t\treturn -EACCES;\n+\t}\n \n \terr = security_sem_semctl(sma, SETVAL);\n-\tif (err)\n-\t\tgoto out_unlock;\n+\tif (err) {\n+\t\trcu_read_unlock();\n+\t\treturn -EACCES;\n+\t}\n \n-\terr = -EINVAL;\n-\tif(semnum < 0 || semnum >= nsems)\n-\t\tgoto out_unlock;\n+\tsem_lock(sma, NULL, -1);\n \n \tcurr = &sma->sem_base[semnum];\n \n-\terr = -ERANGE;\n-\tif (val > SEMVMX || val < 0)\n-\t\tgoto out_unlock;\n-\n \tassert_spin_locked(&sma->sem_perm.lock);\n \tlist_for_each_entry(un, &sma->list_id, list_id)\n \t\tun->semadj[semnum] = 0;\n@@ -992,11 +1070,9 @@ static int semctl_setval(struct ipc_namespace *ns, int semid, int semnum,\n \tsma->sem_ctime = get_seconds();\n \t/* maybe some queued-up processes were waiting for this */\n \tdo_smart_update(sma, NULL, 0, 0, &tasks);\n-\terr = 0;\n-out_unlock:\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, -1);\n \twake_up_sem_queue_do(&tasks);\n-\treturn err;\n+\treturn 0;\n }\n \n static int semctl_main(struct ipc_namespace *ns, int semid, int semnum,\n@@ -1051,16 +1127,16 @@ static int semctl_main(struct ipc_namespace *ns, int semid, int semnum,\n \n \t\t\tsem_lock_and_putref(sma);\n \t\t\tif (sma->sem_perm.deleted) {\n-\t\t\t\tsem_unlock(sma);\n+\t\t\t\tsem_unlock(sma, -1);\n \t\t\t\terr = -EIDRM;\n \t\t\t\tgoto out_free;\n \t\t\t}\n-\t\t}\n+\t\t} else\n+\t\t\tsem_lock(sma, NULL, -1);\n \n-\t\tspin_lock(&sma->sem_perm.lock);\n \t\tfor (i = 0; i < sma->sem_nsems; i++)\n \t\t\tsem_io[i] = sma->sem_base[i].semval;\n-\t\tsem_unlock(sma);\n+\t\tsem_unlock(sma, -1);\n \t\terr = 0;\n \t\tif(copy_to_user(array, sem_io, nsems*sizeof(ushort)))\n \t\t\terr = -EFAULT;\n@@ -1071,7 +1147,10 @@ static int semctl_main(struct ipc_namespace *ns, int semid, int semnum,\n \t\tint i;\n \t\tstruct sem_undo *un;\n \n-\t\tipc_rcu_getref(sma);\n+\t\tif (!ipc_rcu_getref(sma)) {\n+\t\t\trcu_read_unlock();\n+\t\t\treturn -EIDRM;\n+\t\t}\n \t\trcu_read_unlock();\n \n \t\tif(nsems > SEMMSL_FAST) {\n@@ -1097,7 +1176,7 @@ static int semctl_main(struct ipc_namespace *ns, int semid, int semnum,\n \t\t}\n \t\tsem_lock_and_putref(sma);\n \t\tif (sma->sem_perm.deleted) {\n-\t\t\tsem_unlock(sma);\n+\t\t\tsem_unlock(sma, -1);\n \t\t\terr = -EIDRM;\n \t\t\tgoto out_free;\n \t\t}\n@@ -1124,7 +1203,7 @@ static int semctl_main(struct ipc_namespace *ns, int semid, int semnum,\n \t\tgoto out_wakeup;\n \t}\n \n-\tspin_lock(&sma->sem_perm.lock);\n+\tsem_lock(sma, NULL, -1);\n \tcurr = &sma->sem_base[semnum];\n \n \tswitch (cmd) {\n@@ -1143,7 +1222,7 @@ static int semctl_main(struct ipc_namespace *ns, int semid, int semnum,\n \t}\n \n out_unlock:\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, -1);\n out_wakeup:\n \twake_up_sem_queue_do(&tasks);\n out_free:\n@@ -1211,11 +1290,11 @@ static int semctl_down(struct ipc_namespace *ns, int semid,\n \n \tswitch(cmd){\n \tcase IPC_RMID:\n-\t\tipc_lock_object(&sma->sem_perm);\n+\t\tsem_lock(sma, NULL, -1);\n \t\tfreeary(ns, ipcp);\n \t\tgoto out_up;\n \tcase IPC_SET:\n-\t\tipc_lock_object(&sma->sem_perm);\n+\t\tsem_lock(sma, NULL, -1);\n \t\terr = ipc_update_perm(&semid64.sem_perm, ipcp);\n \t\tif (err)\n \t\t\tgoto out_unlock;\n@@ -1228,7 +1307,7 @@ static int semctl_down(struct ipc_namespace *ns, int semid,\n \t}\n \n out_unlock:\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, -1);\n out_up:\n \tup_write(&sem_ids(ns).rw_mutex);\n \treturn err;\n@@ -1340,8 +1419,7 @@ static struct sem_undo *find_alloc_undo(struct ipc_namespace *ns, int semid)\n \tstruct sem_array *sma;\n \tstruct sem_undo_list *ulp;\n \tstruct sem_undo *un, *new;\n-\tint nsems;\n-\tint error;\n+\tint nsems, error;\n \n \terror = get_undo_list(&ulp);\n \tif (error)\n@@ -1363,7 +1441,11 @@ static struct sem_undo *find_alloc_undo(struct ipc_namespace *ns, int semid)\n \t}\n \n \tnsems = sma->sem_nsems;\n-\tipc_rcu_getref(sma);\n+\tif (!ipc_rcu_getref(sma)) {\n+\t\trcu_read_unlock();\n+\t\tun = ERR_PTR(-EIDRM);\n+\t\tgoto out;\n+\t}\n \trcu_read_unlock();\n \n \t/* step 2: allocate new undo structure */\n@@ -1376,7 +1458,7 @@ static struct sem_undo *find_alloc_undo(struct ipc_namespace *ns, int semid)\n \t/* step 3: Acquire the lock on semaphore array */\n \tsem_lock_and_putref(sma);\n \tif (sma->sem_perm.deleted) {\n-\t\tsem_unlock(sma);\n+\t\tsem_unlock(sma, -1);\n \t\tkfree(new);\n \t\tun = ERR_PTR(-EIDRM);\n \t\tgoto out;\n@@ -1404,7 +1486,7 @@ static struct sem_undo *find_alloc_undo(struct ipc_namespace *ns, int semid)\n success:\n \tspin_unlock(&ulp->lock);\n \trcu_read_lock();\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, -1);\n out:\n \treturn un;\n }\n@@ -1444,7 +1526,7 @@ SYSCALL_DEFINE4(semtimedop, int, semid, struct sembuf __user *, tsops,\n \tstruct sembuf fast_sops[SEMOPM_FAST];\n \tstruct sembuf* sops = fast_sops, *sop;\n \tstruct sem_undo *un;\n-\tint undos = 0, alter = 0, max;\n+\tint undos = 0, alter = 0, max, locknum;\n \tstruct sem_queue queue;\n \tunsigned long jiffies_left = 0;\n \tstruct ipc_namespace *ns;\n@@ -1488,22 +1570,23 @@ SYSCALL_DEFINE4(semtimedop, int, semid, struct sembuf __user *, tsops,\n \t\t\talter = 1;\n \t}\n \n+\tINIT_LIST_HEAD(&tasks);\n+\n \tif (undos) {\n+\t\t/* On success, find_alloc_undo takes the rcu_read_lock */\n \t\tun = find_alloc_undo(ns, semid);\n \t\tif (IS_ERR(un)) {\n \t\t\terror = PTR_ERR(un);\n \t\t\tgoto out_free;\n \t\t}\n-\t} else\n+\t} else {\n \t\tun = NULL;\n+\t\trcu_read_lock();\n+\t}\n \n-\tINIT_LIST_HEAD(&tasks);\n-\n-\trcu_read_lock();\n \tsma = sem_obtain_object_check(ns, semid);\n \tif (IS_ERR(sma)) {\n-\t\tif (un)\n-\t\t\trcu_read_unlock();\n+\t\trcu_read_unlock();\n \t\terror = PTR_ERR(sma);\n \t\tgoto out_free;\n \t}\n@@ -1534,23 +1617,9 @@ SYSCALL_DEFINE4(semtimedop, int, semid, struct sembuf __user *, tsops,\n \t * \"un\" itself is guaranteed by rcu.\n \t */\n \terror = -EIDRM;\n-\tipc_lock_object(&sma->sem_perm);\n-\tif (un) {\n-\t\tif (un->semid == -1) {\n-\t\t\trcu_read_unlock();\n-\t\t\tgoto out_unlock_free;\n-\t\t} else {\n-\t\t\t/*\n-\t\t\t * rcu lock can be released, \"un\" cannot disappear:\n-\t\t\t * - sem_lock is acquired, thus IPC_RMID is\n-\t\t\t *   impossible.\n-\t\t\t * - exit_sem is impossible, it always operates on\n-\t\t\t *   current (or a dead task).\n-\t\t\t */\n-\n-\t\t\trcu_read_unlock();\n-\t\t}\n-\t}\n+\tlocknum = sem_lock(sma, sops, nsops);\n+\tif (un && un->semid == -1)\n+\t\tgoto out_unlock_free;\n \n \terror = try_atomic_semop (sma, sops, nsops, un, task_tgid_vnr(current));\n \tif (error <= 0) {\n@@ -1591,7 +1660,7 @@ SYSCALL_DEFINE4(semtimedop, int, semid, struct sembuf __user *, tsops,\n \n sleep_again:\n \tcurrent->state = TASK_INTERRUPTIBLE;\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, locknum);\n \n \tif (timeout)\n \t\tjiffies_left = schedule_timeout(jiffies_left);\n@@ -1613,7 +1682,7 @@ SYSCALL_DEFINE4(semtimedop, int, semid, struct sembuf __user *, tsops,\n \t\tgoto out_free;\n \t}\n \n-\tsma = sem_obtain_lock(ns, semid);\n+\tsma = sem_obtain_lock(ns, semid, sops, nsops, &locknum);\n \n \t/*\n \t * Wait until it's guaranteed that no wakeup_sem_queue_do() is ongoing.\n@@ -1652,7 +1721,7 @@ SYSCALL_DEFINE4(semtimedop, int, semid, struct sembuf __user *, tsops,\n \tunlink_queue(sma, &queue);\n \n out_unlock_free:\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, locknum);\n out_wakeup:\n \twake_up_sem_queue_do(&tasks);\n out_free:\n@@ -1716,8 +1785,7 @@ void exit_sem(struct task_struct *tsk)\n \t\tstruct sem_array *sma;\n \t\tstruct sem_undo *un;\n \t\tstruct list_head tasks;\n-\t\tint semid;\n-\t\tint i;\n+\t\tint semid, i;\n \n \t\trcu_read_lock();\n \t\tun = list_entry_rcu(ulp->list_proc.next,\n@@ -1726,23 +1794,26 @@ void exit_sem(struct task_struct *tsk)\n \t\t\tsemid = -1;\n \t\t else\n \t\t\tsemid = un->semid;\n-\t\trcu_read_unlock();\n \n-\t\tif (semid == -1)\n+\t\tif (semid == -1) {\n+\t\t\trcu_read_unlock();\n \t\t\tbreak;\n+\t\t}\n \n-\t\tsma = sem_lock_check(tsk->nsproxy->ipc_ns, un->semid);\n-\n+\t\tsma = sem_obtain_object_check(tsk->nsproxy->ipc_ns, un->semid);\n \t\t/* exit_sem raced with IPC_RMID, nothing to do */\n-\t\tif (IS_ERR(sma))\n+\t\tif (IS_ERR(sma)) {\n+\t\t\trcu_read_unlock();\n \t\t\tcontinue;\n+\t\t}\n \n+\t\tsem_lock(sma, NULL, -1);\n \t\tun = __lookup_undo(ulp, semid);\n \t\tif (un == NULL) {\n \t\t\t/* exit_sem raced with IPC_RMID+semget() that created\n \t\t\t * exactly the same semid. Nothing to do.\n \t\t\t */\n-\t\t\tsem_unlock(sma);\n+\t\t\tsem_unlock(sma, -1);\n \t\t\tcontinue;\n \t\t}\n \n@@ -1782,7 +1853,7 @@ void exit_sem(struct task_struct *tsk)\n \t\t/* maybe some queued-up processes were waiting for this */\n \t\tINIT_LIST_HEAD(&tasks);\n \t\tdo_smart_update(sma, NULL, 0, 1, &tasks);\n-\t\tsem_unlock(sma);\n+\t\tsem_unlock(sma, -1);\n \t\twake_up_sem_queue_do(&tasks);\n \n \t\tkfree_rcu(un, rcu);\n",
    "critical_vars": [
      "val"
    ],
    "variable_definitions": {
      "val": "int val;"
    },
    "variable_types": {
      "val": "integer"
    },
    "type_mapping": {
      "val": "Integer"
    },
    "cve": "CVE-2013-4483",
    "vulnerable_line": "if (val > SEMVMX || val < 0)",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The check for the variable 'val' does not account for the potential of an integer overflow occurring when val is manipulated, leading to unsafe memory operations and denial of service conditions."
  },
  {
    "vulcode": "static int semctl_main(struct ipc_namespace *ns, int semid, int semnum,\n\t\tint cmd, void __user *p)\n{\n\tstruct sem_array *sma;\n\tstruct sem* curr;\n\tint err, nsems;\n\tushort fast_sem_io[SEMMSL_FAST];\n\tushort* sem_io = fast_sem_io;\n\tstruct list_head tasks;\n\n\tINIT_LIST_HEAD(&tasks);\n\n\trcu_read_lock();\n\tsma = sem_obtain_object_check(ns, semid);\n\tif (IS_ERR(sma)) {\n\t\trcu_read_unlock();\n\t\treturn PTR_ERR(sma);\n\t}\n\n\tnsems = sma->sem_nsems;\n\n\terr = -EACCES;\n\tif (ipcperms(ns, &sma->sem_perm,\n\t\t\tcmd == SETALL ? S_IWUGO : S_IRUGO)) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terr = security_sem_semctl(sma, cmd);\n\tif (err) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terr = -EACCES;\n\tswitch (cmd) {\n\tcase GETALL:\n\t{\n\t\tushort __user *array = p;\n\t\tint i;\n\n\t\tif(nsems > SEMMSL_FAST) {\n\t\t\tsem_getref(sma);\n\n\t\t\tsem_io = ipc_alloc(sizeof(ushort)*nsems);\n\t\t\tif(sem_io == NULL) {\n\t\t\t\tsem_putref(sma);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\n\t\t\tsem_lock_and_putref(sma);\n\t\t\tif (sma->sem_perm.deleted) {\n\t\t\t\tsem_unlock(sma);\n\t\t\t\terr = -EIDRM;\n\t\t\t\tgoto out_free;\n\t\t\t}\n\t\t}\n\n\t\tspin_lock(&sma->sem_perm.lock);\n\t\tfor (i = 0; i < sma->sem_nsems; i++)\n\t\t\tsem_io[i] = sma->sem_base[i].semval;\n\t\tsem_unlock(sma);\n\t\terr = 0;\n\t\tif(copy_to_user(array, sem_io, nsems*sizeof(ushort)))\n\t\t\terr = -EFAULT;\n\t\tgoto out_free;\n\t}\n\tcase SETALL:\n\t{\n\t\tint i;\n\t\tstruct sem_undo *un;\n\n\t\tipc_rcu_getref(sma);\n\t\trcu_read_unlock();\n\n\t\tif(nsems > SEMMSL_FAST) {\n\t\t\tsem_io = ipc_alloc(sizeof(ushort)*nsems);\n\t\t\tif(sem_io == NULL) {\n\t\t\t\tsem_putref(sma);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t}\n\n\t\tif (copy_from_user (sem_io, p, nsems*sizeof(ushort))) {\n\t\t\tsem_putref(sma);\n\t\t\terr = -EFAULT;\n\t\t\tgoto out_free;\n\t\t}\n\n\t\tfor (i = 0; i < nsems; i++) {\n\t\t\tif (sem_io[i] > SEMVMX) {\n\t\t\t\tsem_putref(sma);\n\t\t\t\terr = -ERANGE;\n\t\t\t\tgoto out_free;\n\t\t\t}\n\t\t}\n\t\tsem_lock_and_putref(sma);\n\t\tif (sma->sem_perm.deleted) {\n\t\t\tsem_unlock(sma);\n\t\t\terr = -EIDRM;\n\t\t\tgoto out_free;\n\t\t}\n\n\t\tfor (i = 0; i < nsems; i++)\n\t\t\tsma->sem_base[i].semval = sem_io[i];\n\n\t\tassert_spin_locked(&sma->sem_perm.lock);\n\t\tlist_for_each_entry(un, &sma->list_id, list_id) {\n\t\t\tfor (i = 0; i < nsems; i++)\n\t\t\t\tun->semadj[i] = 0;\n\t\t}\n\t\tsma->sem_ctime = get_seconds();\n\t\t/* maybe some queued-up processes were waiting for this */\n\t\tdo_smart_update(sma, NULL, 0, 0, &tasks);\n\t\terr = 0;\n\t\tgoto out_unlock;\n\t}\n\t/* GETVAL, GETPID, GETNCTN, GETZCNT: fall-through */\n\t}\n\terr = -EINVAL;\n\tif (semnum < 0 || semnum >= nsems) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\tspin_lock(&sma->sem_perm.lock);\n\tcurr = &sma->sem_base[semnum];\n\n\tswitch (cmd) {\n\tcase GETVAL:\n\t\terr = curr->semval;\n\t\tgoto out_unlock;\n\tcase GETPID:\n\t\terr = curr->sempid;\n\t\tgoto out_unlock;\n\tcase GETNCNT:\n\t\terr = count_semncnt(sma,semnum);\n\t\tgoto out_unlock;\n\tcase GETZCNT:\n\t\terr = count_semzcnt(sma,semnum);\n\t\tgoto out_unlock;\n\t}\n\nout_unlock:\n\tsem_unlock(sma);\nout_wakeup:\n\twake_up_sem_queue_do(&tasks);\nout_free:\n\tif(sem_io != fast_sem_io)\n\t\tipc_free(sem_io, sizeof(ushort)*nsems);\n\treturn err;\n}",
    "diff": "@@ -94,6 +94,7 @@\n struct sem {\n \tint\tsemval;\t\t/* current value */\n \tint\tsempid;\t\t/* pid of last operation */\n+\tspinlock_t\tlock;\t/* spinlock for fine-grained semtimedop */\n \tstruct list_head sem_pending; /* pending single-sop operations */\n };\n \n@@ -137,7 +138,6 @@ struct sem_undo_list {\n \n #define sem_ids(ns)\t((ns)->ids[IPC_SEM_IDS])\n \n-#define sem_unlock(sma)\t\tipc_unlock(&(sma)->sem_perm)\n #define sem_checkid(sma, semid)\tipc_checkid(&sma->sem_perm, semid)\n \n static int newary(struct ipc_namespace *, struct ipc_params *);\n@@ -189,11 +189,90 @@ void __init sem_init (void)\n \t\t\t\tIPC_SEM_IDS, sysvipc_sem_proc_show);\n }\n \n+/*\n+ * If the request contains only one semaphore operation, and there are\n+ * no complex transactions pending, lock only the semaphore involved.\n+ * Otherwise, lock the entire semaphore array, since we either have\n+ * multiple semaphores in our own semops, or we need to look at\n+ * semaphores from other pending complex operations.\n+ *\n+ * Carefully guard against sma->complex_count changing between zero\n+ * and non-zero while we are spinning for the lock. The value of\n+ * sma->complex_count cannot change while we are holding the lock,\n+ * so sem_unlock should be fine.\n+ *\n+ * The global lock path checks that all the local locks have been released,\n+ * checking each local lock once. This means that the local lock paths\n+ * cannot start their critical sections while the global lock is held.\n+ */\n+static inline int sem_lock(struct sem_array *sma, struct sembuf *sops,\n+\t\t\t      int nsops)\n+{\n+\tint locknum;\n+ again:\n+\tif (nsops == 1 && !sma->complex_count) {\n+\t\tstruct sem *sem = sma->sem_base + sops->sem_num;\n+\n+\t\t/* Lock just the semaphore we are interested in. */\n+\t\tspin_lock(&sem->lock);\n+\n+\t\t/*\n+\t\t * If sma->complex_count was set while we were spinning,\n+\t\t * we may need to look at things we did not lock here.\n+\t\t */\n+\t\tif (unlikely(sma->complex_count)) {\n+\t\t\tspin_unlock(&sem->lock);\n+\t\t\tgoto lock_array;\n+\t\t}\n+\n+\t\t/*\n+\t\t * Another process is holding the global lock on the\n+\t\t * sem_array; we cannot enter our critical section,\n+\t\t * but have to wait for the global lock to be released.\n+\t\t */\n+\t\tif (unlikely(spin_is_locked(&sma->sem_perm.lock))) {\n+\t\t\tspin_unlock(&sem->lock);\n+\t\t\tspin_unlock_wait(&sma->sem_perm.lock);\n+\t\t\tgoto again;\n+\t\t}\n+\n+\t\tlocknum = sops->sem_num;\n+\t} else {\n+\t\tint i;\n+\t\t/*\n+\t\t * Lock the semaphore array, and wait for all of the\n+\t\t * individual semaphore locks to go away.  The code\n+\t\t * above ensures no new single-lock holders will enter\n+\t\t * their critical section while the array lock is held.\n+\t\t */\n+ lock_array:\n+\t\tspin_lock(&sma->sem_perm.lock);\n+\t\tfor (i = 0; i < sma->sem_nsems; i++) {\n+\t\t\tstruct sem *sem = sma->sem_base + i;\n+\t\t\tspin_unlock_wait(&sem->lock);\n+\t\t}\n+\t\tlocknum = -1;\n+\t}\n+\treturn locknum;\n+}\n+\n+static inline void sem_unlock(struct sem_array *sma, int locknum)\n+{\n+\tif (locknum == -1) {\n+\t\tspin_unlock(&sma->sem_perm.lock);\n+\t} else {\n+\t\tstruct sem *sem = sma->sem_base + locknum;\n+\t\tspin_unlock(&sem->lock);\n+\t}\n+\trcu_read_unlock();\n+}\n+\n /*\n  * sem_lock_(check_) routines are called in the paths where the rw_mutex\n  * is not held.\n  */\n-static inline struct sem_array *sem_obtain_lock(struct ipc_namespace *ns, int id)\n+static inline struct sem_array *sem_obtain_lock(struct ipc_namespace *ns,\n+\t\t\tint id, struct sembuf *sops, int nsops, int *locknum)\n {\n \tstruct kern_ipc_perm *ipcp;\n \tstruct sem_array *sma;\n@@ -205,7 +284,8 @@ static inline struct sem_array *sem_obtain_lock(struct ipc_namespace *ns, int id\n \t\tgoto err;\n \t}\n \n-\tspin_lock(&ipcp->lock);\n+\tsma = container_of(ipcp, struct sem_array, sem_perm);\n+\t*locknum = sem_lock(sma, sops, nsops);\n \n \t/* ipc_rmid() may have already freed the ID while sem_lock\n \t * was spinning: verify that the structure is still valid\n@@ -213,7 +293,7 @@ static inline struct sem_array *sem_obtain_lock(struct ipc_namespace *ns, int id\n \tif (!ipcp->deleted)\n \t\treturn container_of(ipcp, struct sem_array, sem_perm);\n \n-\tspin_unlock(&ipcp->lock);\n+\tsem_unlock(sma, *locknum);\n \tsma = ERR_PTR(-EINVAL);\n err:\n \trcu_read_unlock();\n@@ -230,17 +310,6 @@ static inline struct sem_array *sem_obtain_object(struct ipc_namespace *ns, int\n \treturn container_of(ipcp, struct sem_array, sem_perm);\n }\n \n-static inline struct sem_array *sem_lock_check(struct ipc_namespace *ns,\n-\t\t\t\t\t\tint id)\n-{\n-\tstruct kern_ipc_perm *ipcp = ipc_lock_check(&sem_ids(ns), id);\n-\n-\tif (IS_ERR(ipcp))\n-\t\treturn ERR_CAST(ipcp);\n-\n-\treturn container_of(ipcp, struct sem_array, sem_perm);\n-}\n-\n static inline struct sem_array *sem_obtain_object_check(struct ipc_namespace *ns,\n \t\t\t\t\t\t\tint id)\n {\n@@ -254,21 +323,21 @@ static inline struct sem_array *sem_obtain_object_check(struct ipc_namespace *ns\n \n static inline void sem_lock_and_putref(struct sem_array *sma)\n {\n-\tipc_lock_by_ptr(&sma->sem_perm);\n+\trcu_read_lock();\n+\tsem_lock(sma, NULL, -1);\n \tipc_rcu_putref(sma);\n }\n \n static inline void sem_getref_and_unlock(struct sem_array *sma)\n {\n-\tipc_rcu_getref(sma);\n-\tipc_unlock(&(sma)->sem_perm);\n+\tWARN_ON_ONCE(!ipc_rcu_getref(sma));\n+\tsem_unlock(sma, -1);\n }\n \n static inline void sem_putref(struct sem_array *sma)\n {\n-\tipc_lock_by_ptr(&sma->sem_perm);\n-\tipc_rcu_putref(sma);\n-\tipc_unlock(&(sma)->sem_perm);\n+\tsem_lock_and_putref(sma);\n+\tsem_unlock(sma, -1);\n }\n \n /*\n@@ -276,9 +345,9 @@ static inline void sem_putref(struct sem_array *sma)\n  */\n static inline void sem_getref(struct sem_array *sma)\n {\n-\tspin_lock(&(sma)->sem_perm.lock);\n-\tipc_rcu_getref(sma);\n-\tipc_unlock(&(sma)->sem_perm);\n+\tsem_lock(sma, NULL, -1);\n+\tWARN_ON_ONCE(!ipc_rcu_getref(sma));\n+\tsem_unlock(sma, -1);\n }\n \n static inline void sem_rmid(struct ipc_namespace *ns, struct sem_array *s)\n@@ -371,15 +440,17 @@ static int newary(struct ipc_namespace *ns, struct ipc_params *params)\n \n \tsma->sem_base = (struct sem *) &sma[1];\n \n-\tfor (i = 0; i < nsems; i++)\n+\tfor (i = 0; i < nsems; i++) {\n \t\tINIT_LIST_HEAD(&sma->sem_base[i].sem_pending);\n+\t\tspin_lock_init(&sma->sem_base[i].lock);\n+\t}\n \n \tsma->complex_count = 0;\n \tINIT_LIST_HEAD(&sma->sem_pending);\n \tINIT_LIST_HEAD(&sma->list_id);\n \tsma->sem_nsems = nsems;\n \tsma->sem_ctime = get_seconds();\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, -1);\n \n \treturn sma->sem_perm.id;\n }\n@@ -818,7 +889,7 @@ static void freeary(struct ipc_namespace *ns, struct kern_ipc_perm *ipcp)\n \n \t/* Remove the semaphore set from the IDR */\n \tsem_rmid(ns, sma);\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, -1);\n \n \twake_up_sem_queue_do(&tasks);\n \tns->used_sems -= sma->sem_nsems;\n@@ -947,7 +1018,6 @@ static int semctl_setval(struct ipc_namespace *ns, int semid, int semnum,\n \tstruct sem_array *sma;\n \tstruct sem* curr;\n \tint err;\n-\tint nsems;\n \tstruct list_head tasks;\n \tint val;\n #if defined(CONFIG_64BIT) && defined(__BIG_ENDIAN)\n@@ -958,31 +1028,39 @@ static int semctl_setval(struct ipc_namespace *ns, int semid, int semnum,\n \tval = arg;\n #endif\n \n-\tsma = sem_lock_check(ns, semid);\n-\tif (IS_ERR(sma))\n-\t\treturn PTR_ERR(sma);\n+\tif (val > SEMVMX || val < 0)\n+\t\treturn -ERANGE;\n \n \tINIT_LIST_HEAD(&tasks);\n-\tnsems = sma->sem_nsems;\n \n-\terr = -EACCES;\n-\tif (ipcperms(ns, &sma->sem_perm, S_IWUGO))\n-\t\tgoto out_unlock;\n+\trcu_read_lock();\n+\tsma = sem_obtain_object_check(ns, semid);\n+\tif (IS_ERR(sma)) {\n+\t\trcu_read_unlock();\n+\t\treturn PTR_ERR(sma);\n+\t}\n+\n+\tif (semnum < 0 || semnum >= sma->sem_nsems) {\n+\t\trcu_read_unlock();\n+\t\treturn -EINVAL;\n+\t}\n+\n+\n+\tif (ipcperms(ns, &sma->sem_perm, S_IWUGO)) {\n+\t\trcu_read_unlock();\n+\t\treturn -EACCES;\n+\t}\n \n \terr = security_sem_semctl(sma, SETVAL);\n-\tif (err)\n-\t\tgoto out_unlock;\n+\tif (err) {\n+\t\trcu_read_unlock();\n+\t\treturn -EACCES;\n+\t}\n \n-\terr = -EINVAL;\n-\tif(semnum < 0 || semnum >= nsems)\n-\t\tgoto out_unlock;\n+\tsem_lock(sma, NULL, -1);\n \n \tcurr = &sma->sem_base[semnum];\n \n-\terr = -ERANGE;\n-\tif (val > SEMVMX || val < 0)\n-\t\tgoto out_unlock;\n-\n \tassert_spin_locked(&sma->sem_perm.lock);\n \tlist_for_each_entry(un, &sma->list_id, list_id)\n \t\tun->semadj[semnum] = 0;\n@@ -992,11 +1070,9 @@ static int semctl_setval(struct ipc_namespace *ns, int semid, int semnum,\n \tsma->sem_ctime = get_seconds();\n \t/* maybe some queued-up processes were waiting for this */\n \tdo_smart_update(sma, NULL, 0, 0, &tasks);\n-\terr = 0;\n-out_unlock:\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, -1);\n \twake_up_sem_queue_do(&tasks);\n-\treturn err;\n+\treturn 0;\n }\n \n static int semctl_main(struct ipc_namespace *ns, int semid, int semnum,\n@@ -1051,16 +1127,16 @@ static int semctl_main(struct ipc_namespace *ns, int semid, int semnum,\n \n \t\t\tsem_lock_and_putref(sma);\n \t\t\tif (sma->sem_perm.deleted) {\n-\t\t\t\tsem_unlock(sma);\n+\t\t\t\tsem_unlock(sma, -1);\n \t\t\t\terr = -EIDRM;\n \t\t\t\tgoto out_free;\n \t\t\t}\n-\t\t}\n+\t\t} else\n+\t\t\tsem_lock(sma, NULL, -1);\n \n-\t\tspin_lock(&sma->sem_perm.lock);\n \t\tfor (i = 0; i < sma->sem_nsems; i++)\n \t\t\tsem_io[i] = sma->sem_base[i].semval;\n-\t\tsem_unlock(sma);\n+\t\tsem_unlock(sma, -1);\n \t\terr = 0;\n \t\tif(copy_to_user(array, sem_io, nsems*sizeof(ushort)))\n \t\t\terr = -EFAULT;\n@@ -1071,7 +1147,10 @@ static int semctl_main(struct ipc_namespace *ns, int semid, int semnum,\n \t\tint i;\n \t\tstruct sem_undo *un;\n \n-\t\tipc_rcu_getref(sma);\n+\t\tif (!ipc_rcu_getref(sma)) {\n+\t\t\trcu_read_unlock();\n+\t\t\treturn -EIDRM;\n+\t\t}\n \t\trcu_read_unlock();\n \n \t\tif(nsems > SEMMSL_FAST) {\n@@ -1097,7 +1176,7 @@ static int semctl_main(struct ipc_namespace *ns, int semid, int semnum,\n \t\t}\n \t\tsem_lock_and_putref(sma);\n \t\tif (sma->sem_perm.deleted) {\n-\t\t\tsem_unlock(sma);\n+\t\t\tsem_unlock(sma, -1);\n \t\t\terr = -EIDRM;\n \t\t\tgoto out_free;\n \t\t}\n@@ -1124,7 +1203,7 @@ static int semctl_main(struct ipc_namespace *ns, int semid, int semnum,\n \t\tgoto out_wakeup;\n \t}\n \n-\tspin_lock(&sma->sem_perm.lock);\n+\tsem_lock(sma, NULL, -1);\n \tcurr = &sma->sem_base[semnum];\n \n \tswitch (cmd) {\n@@ -1143,7 +1222,7 @@ static int semctl_main(struct ipc_namespace *ns, int semid, int semnum,\n \t}\n \n out_unlock:\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, -1);\n out_wakeup:\n \twake_up_sem_queue_do(&tasks);\n out_free:\n@@ -1211,11 +1290,11 @@ static int semctl_down(struct ipc_namespace *ns, int semid,\n \n \tswitch(cmd){\n \tcase IPC_RMID:\n-\t\tipc_lock_object(&sma->sem_perm);\n+\t\tsem_lock(sma, NULL, -1);\n \t\tfreeary(ns, ipcp);\n \t\tgoto out_up;\n \tcase IPC_SET:\n-\t\tipc_lock_object(&sma->sem_perm);\n+\t\tsem_lock(sma, NULL, -1);\n \t\terr = ipc_update_perm(&semid64.sem_perm, ipcp);\n \t\tif (err)\n \t\t\tgoto out_unlock;\n@@ -1228,7 +1307,7 @@ static int semctl_down(struct ipc_namespace *ns, int semid,\n \t}\n \n out_unlock:\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, -1);\n out_up:\n \tup_write(&sem_ids(ns).rw_mutex);\n \treturn err;\n@@ -1340,8 +1419,7 @@ static struct sem_undo *find_alloc_undo(struct ipc_namespace *ns, int semid)\n \tstruct sem_array *sma;\n \tstruct sem_undo_list *ulp;\n \tstruct sem_undo *un, *new;\n-\tint nsems;\n-\tint error;\n+\tint nsems, error;\n \n \terror = get_undo_list(&ulp);\n \tif (error)\n@@ -1363,7 +1441,11 @@ static struct sem_undo *find_alloc_undo(struct ipc_namespace *ns, int semid)\n \t}\n \n \tnsems = sma->sem_nsems;\n-\tipc_rcu_getref(sma);\n+\tif (!ipc_rcu_getref(sma)) {\n+\t\trcu_read_unlock();\n+\t\tun = ERR_PTR(-EIDRM);\n+\t\tgoto out;\n+\t}\n \trcu_read_unlock();\n \n \t/* step 2: allocate new undo structure */\n@@ -1376,7 +1458,7 @@ static struct sem_undo *find_alloc_undo(struct ipc_namespace *ns, int semid)\n \t/* step 3: Acquire the lock on semaphore array */\n \tsem_lock_and_putref(sma);\n \tif (sma->sem_perm.deleted) {\n-\t\tsem_unlock(sma);\n+\t\tsem_unlock(sma, -1);\n \t\tkfree(new);\n \t\tun = ERR_PTR(-EIDRM);\n \t\tgoto out;\n@@ -1404,7 +1486,7 @@ static struct sem_undo *find_alloc_undo(struct ipc_namespace *ns, int semid)\n success:\n \tspin_unlock(&ulp->lock);\n \trcu_read_lock();\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, -1);\n out:\n \treturn un;\n }\n@@ -1444,7 +1526,7 @@ SYSCALL_DEFINE4(semtimedop, int, semid, struct sembuf __user *, tsops,\n \tstruct sembuf fast_sops[SEMOPM_FAST];\n \tstruct sembuf* sops = fast_sops, *sop;\n \tstruct sem_undo *un;\n-\tint undos = 0, alter = 0, max;\n+\tint undos = 0, alter = 0, max, locknum;\n \tstruct sem_queue queue;\n \tunsigned long jiffies_left = 0;\n \tstruct ipc_namespace *ns;\n@@ -1488,22 +1570,23 @@ SYSCALL_DEFINE4(semtimedop, int, semid, struct sembuf __user *, tsops,\n \t\t\talter = 1;\n \t}\n \n+\tINIT_LIST_HEAD(&tasks);\n+\n \tif (undos) {\n+\t\t/* On success, find_alloc_undo takes the rcu_read_lock */\n \t\tun = find_alloc_undo(ns, semid);\n \t\tif (IS_ERR(un)) {\n \t\t\terror = PTR_ERR(un);\n \t\t\tgoto out_free;\n \t\t}\n-\t} else\n+\t} else {\n \t\tun = NULL;\n+\t\trcu_read_lock();\n+\t}\n \n-\tINIT_LIST_HEAD(&tasks);\n-\n-\trcu_read_lock();\n \tsma = sem_obtain_object_check(ns, semid);\n \tif (IS_ERR(sma)) {\n-\t\tif (un)\n-\t\t\trcu_read_unlock();\n+\t\trcu_read_unlock();\n \t\terror = PTR_ERR(sma);\n \t\tgoto out_free;\n \t}\n@@ -1534,23 +1617,9 @@ SYSCALL_DEFINE4(semtimedop, int, semid, struct sembuf __user *, tsops,\n \t * \"un\" itself is guaranteed by rcu.\n \t */\n \terror = -EIDRM;\n-\tipc_lock_object(&sma->sem_perm);\n-\tif (un) {\n-\t\tif (un->semid == -1) {\n-\t\t\trcu_read_unlock();\n-\t\t\tgoto out_unlock_free;\n-\t\t} else {\n-\t\t\t/*\n-\t\t\t * rcu lock can be released, \"un\" cannot disappear:\n-\t\t\t * - sem_lock is acquired, thus IPC_RMID is\n-\t\t\t *   impossible.\n-\t\t\t * - exit_sem is impossible, it always operates on\n-\t\t\t *   current (or a dead task).\n-\t\t\t */\n-\n-\t\t\trcu_read_unlock();\n-\t\t}\n-\t}\n+\tlocknum = sem_lock(sma, sops, nsops);\n+\tif (un && un->semid == -1)\n+\t\tgoto out_unlock_free;\n \n \terror = try_atomic_semop (sma, sops, nsops, un, task_tgid_vnr(current));\n \tif (error <= 0) {\n@@ -1591,7 +1660,7 @@ SYSCALL_DEFINE4(semtimedop, int, semid, struct sembuf __user *, tsops,\n \n sleep_again:\n \tcurrent->state = TASK_INTERRUPTIBLE;\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, locknum);\n \n \tif (timeout)\n \t\tjiffies_left = schedule_timeout(jiffies_left);\n@@ -1613,7 +1682,7 @@ SYSCALL_DEFINE4(semtimedop, int, semid, struct sembuf __user *, tsops,\n \t\tgoto out_free;\n \t}\n \n-\tsma = sem_obtain_lock(ns, semid);\n+\tsma = sem_obtain_lock(ns, semid, sops, nsops, &locknum);\n \n \t/*\n \t * Wait until it's guaranteed that no wakeup_sem_queue_do() is ongoing.\n@@ -1652,7 +1721,7 @@ SYSCALL_DEFINE4(semtimedop, int, semid, struct sembuf __user *, tsops,\n \tunlink_queue(sma, &queue);\n \n out_unlock_free:\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, locknum);\n out_wakeup:\n \twake_up_sem_queue_do(&tasks);\n out_free:\n@@ -1716,8 +1785,7 @@ void exit_sem(struct task_struct *tsk)\n \t\tstruct sem_array *sma;\n \t\tstruct sem_undo *un;\n \t\tstruct list_head tasks;\n-\t\tint semid;\n-\t\tint i;\n+\t\tint semid, i;\n \n \t\trcu_read_lock();\n \t\tun = list_entry_rcu(ulp->list_proc.next,\n@@ -1726,23 +1794,26 @@ void exit_sem(struct task_struct *tsk)\n \t\t\tsemid = -1;\n \t\t else\n \t\t\tsemid = un->semid;\n-\t\trcu_read_unlock();\n \n-\t\tif (semid == -1)\n+\t\tif (semid == -1) {\n+\t\t\trcu_read_unlock();\n \t\t\tbreak;\n+\t\t}\n \n-\t\tsma = sem_lock_check(tsk->nsproxy->ipc_ns, un->semid);\n-\n+\t\tsma = sem_obtain_object_check(tsk->nsproxy->ipc_ns, un->semid);\n \t\t/* exit_sem raced with IPC_RMID, nothing to do */\n-\t\tif (IS_ERR(sma))\n+\t\tif (IS_ERR(sma)) {\n+\t\t\trcu_read_unlock();\n \t\t\tcontinue;\n+\t\t}\n \n+\t\tsem_lock(sma, NULL, -1);\n \t\tun = __lookup_undo(ulp, semid);\n \t\tif (un == NULL) {\n \t\t\t/* exit_sem raced with IPC_RMID+semget() that created\n \t\t\t * exactly the same semid. Nothing to do.\n \t\t\t */\n-\t\t\tsem_unlock(sma);\n+\t\t\tsem_unlock(sma, -1);\n \t\t\tcontinue;\n \t\t}\n \n@@ -1782,7 +1853,7 @@ void exit_sem(struct task_struct *tsk)\n \t\t/* maybe some queued-up processes were waiting for this */\n \t\tINIT_LIST_HEAD(&tasks);\n \t\tdo_smart_update(sma, NULL, 0, 1, &tasks);\n-\t\tsem_unlock(sma);\n+\t\tsem_unlock(sma, -1);\n \t\twake_up_sem_queue_do(&tasks);\n \n \t\tkfree_rcu(un, rcu);\n",
    "critical_vars": [
      "sma"
    ],
    "variable_definitions": {
      "sma": "struct sem_array *sma;"
    },
    "variable_types": {
      "sma": "struct pointer"
    },
    "type_mapping": {
      "sma": "struct pointer"
    },
    "cve": "CVE-2013-4483",
    "vulnerable_line": "sma = sem_obtain_object_check(ns, semid);",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The function sem_obtain_object_check can lead to memory corruption if traditional integer overflow occurs, allowing an attacker to create a large number of semaphore IDs that push the semaphore's reference count beyond its limits."
  },
  {
    "vulcode": "long do_msgsnd(int msqid, long mtype, void __user *mtext,\n\t\tsize_t msgsz, int msgflg)\n{\n\tstruct msg_queue *msq;\n\tstruct msg_msg *msg;\n\tint err;\n\tstruct ipc_namespace *ns;\n\n\tns = current->nsproxy->ipc_ns;\n\n\tif (msgsz > ns->msg_ctlmax || (long) msgsz < 0 || msqid < 0)\n\t\treturn -EINVAL;\n\tif (mtype < 1)\n\t\treturn -EINVAL;\n\n\tmsg = load_msg(mtext, msgsz);\n\tif (IS_ERR(msg))\n\t\treturn PTR_ERR(msg);\n\n\tmsg->m_type = mtype;\n\tmsg->m_ts = msgsz;\n\n\tmsq = msg_lock_check(ns, msqid);\n\tif (IS_ERR(msq)) {\n\t\terr = PTR_ERR(msq);\n\t\tgoto out_free;\n\t}\n\n\tfor (;;) {\n\t\tstruct msg_sender s;\n\n\t\terr = -EACCES;\n\t\tif (ipcperms(ns, &msq->q_perm, S_IWUGO))\n\t\t\tgoto out_unlock_free;\n\n\t\terr = security_msg_queue_msgsnd(msq, msg, msgflg);\n\t\tif (err)\n\t\t\tgoto out_unlock_free;\n\n\t\tif (msgsz + msq->q_cbytes <= msq->q_qbytes &&\n\t\t\t\t1 + msq->q_qnum <= msq->q_qbytes) {\n\t\t\tbreak;\n\t\t}\n\n\t\t/* queue full, wait: */\n\t\tif (msgflg & IPC_NOWAIT) {\n\t\t\terr = -EAGAIN;\n\t\t\tgoto out_unlock_free;\n\t\t}\n\t\tss_add(msq, &s);\n\t\tipc_rcu_getref(msq);\n\t\tmsg_unlock(msq);\n\t\tschedule();\n\n\t\tipc_lock_by_ptr(&msq->q_perm);\n\t\tipc_rcu_putref(msq);\n\t\tif (msq->q_perm.deleted) {\n\t\t\terr = -EIDRM;\n\t\t\tgoto out_unlock_free;\n\t\t}\n\t\tss_del(&s);\n\n\t\tif (signal_pending(current)) {\n\t\t\terr = -ERESTARTNOHAND;\n\t\t\tgoto out_unlock_free;\n\t\t}\n\t}\n\n\tmsq->q_lspid = task_tgid_vnr(current);\n\tmsq->q_stime = get_seconds();\n\n\tif (!pipelined_send(msq, msg)) {\n\t\t/* no one is waiting for this message, enqueue it */\n\t\tlist_add_tail(&msg->m_list, &msq->q_messages);\n\t\tmsq->q_cbytes += msgsz;\n\t\tmsq->q_qnum++;\n\t\tatomic_add(msgsz, &ns->msg_bytes);\n\t\tatomic_inc(&ns->msg_hdrs);\n\t}\n\n\terr = 0;\n\tmsg = NULL;\n\nout_unlock_free:\n\tmsg_unlock(msq);\nout_free:\n\tif (msg != NULL)\n\t\tfree_msg(msg);\n\treturn err;\n}",
    "diff": "@@ -687,7 +687,12 @@ long do_msgsnd(int msqid, long mtype, void __user *mtext,\n \t\t\tgoto out_unlock_free;\n \t\t}\n \t\tss_add(msq, &s);\n-\t\tipc_rcu_getref(msq);\n+\n+\t\tif (!ipc_rcu_getref(msq)) {\n+\t\t\terr = -EIDRM;\n+\t\t\tgoto out_unlock_free;\n+\t\t}\n+\n \t\tmsg_unlock(msq);\n \t\tschedule();\n \n",
    "critical_vars": [
      "msq"
    ],
    "variable_definitions": {
      "msq": "struct msg_queue *msq;"
    },
    "variable_types": {
      "msq": "struct pointer"
    },
    "type_mapping": {
      "msq": "struct pointer"
    },
    "cve": "CVE-2013-4483",
    "vulnerable_line": "ipc_rcu_getref(msq)",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The reference count for the message queue (msq) is not correctly managed, leading to potential integer overflow when the count exceeds the maximum limit, resulting in denial of service through excessive memory consumption."
  },
  {
    "vulcode": "static void\n_copy_from_pages(char *p, struct page **pages, size_t pgbase, size_t len)\n{\n\tstruct page **pgfrom;\n\tchar *vfrom;\n\tsize_t copy;\n\n\tpgfrom = pages + (pgbase >> PAGE_CACHE_SHIFT);\n\tpgbase &= ~PAGE_CACHE_MASK;\n\n\tdo {\n\t\tcopy = PAGE_CACHE_SIZE - pgbase;\n\t\tif (copy > len)\n\t\t\tcopy = len;\n\n\t\tvfrom = kmap_atomic(*pgfrom, KM_USER0);\n\t\tmemcpy(p, vfrom + pgbase, copy);\n\t\tkunmap_atomic(vfrom, KM_USER0);\n\n\t\tpgbase += copy;\n\t\tif (pgbase == PAGE_CACHE_SIZE) {\n\t\t\tpgbase = 0;\n\t\t\tpgfrom++;\n\t\t}\n\t\tp += copy;\n\n\t} while ((len -= copy) != 0);\n}",
    "diff": "@@ -296,7 +296,7 @@ _copy_to_pages(struct page **pages, size_t pgbase, const char *p, size_t len)\n  * Copies data into an arbitrary memory location from an array of pages\n  * The copy is assumed to be non-overlapping.\n  */\n-static void\n+void\n _copy_from_pages(char *p, struct page **pages, size_t pgbase, size_t len)\n {\n \tstruct page **pgfrom;\n@@ -324,6 +324,7 @@ _copy_from_pages(char *p, struct page **pages, size_t pgbase, size_t len)\n \n \t} while ((len -= copy) != 0);\n }\n+EXPORT_SYMBOL_GPL(_copy_from_pages);\n \n /*\n  * xdr_shrink_bufhead\n",
    "critical_vars": [
      "_copy_from_pages"
    ],
    "variable_definitions": {
      "_copy_from_pages": "Definition not found"
    },
    "variable_types": {
      "_copy_from_pages": "unknown"
    },
    "type_mapping": {
      "_copy_from_pages": "unknown"
    },
    "cve": "CVE-2011-4131",
    "vulnerable_line": "_copy_from_pages(char *p, struct page **pages, size_t pgbase, size_t len)",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The function can be exploited by providing excessively large 'len' value, leading to an integer overflow when calculating the number of bytes to copy, which may result in memory corruption or denial of service."
  },
  {
    "vulcode": "static void nfs4_xdr_enc_getacl(struct rpc_rqst *req, struct xdr_stream *xdr,\n\t\t\t\tstruct nfs_getaclargs *args)\n{\n\tstruct compound_hdr hdr = {\n\t\t.minorversion = nfs4_xdr_minorversion(&args->seq_args),\n\t};\n\tuint32_t replen;\n\n\tencode_compound_hdr(xdr, req, &hdr);\n\tencode_sequence(xdr, &args->seq_args, &hdr);\n\tencode_putfh(xdr, args->fh, &hdr);\n\treplen = hdr.replen + op_decode_hdr_maxsz + nfs4_fattr_bitmap_maxsz + 1;\n\tencode_getattr_two(xdr, FATTR4_WORD0_ACL, 0, &hdr);\n\n\txdr_inline_pages(&req->rq_rcv_buf, replen << 2,\n\t\targs->acl_pages, args->acl_pgbase, args->acl_len);\n\tencode_nops(&hdr);\n}",
    "diff": "@@ -2517,11 +2517,13 @@ static void nfs4_xdr_enc_getacl(struct rpc_rqst *req, struct xdr_stream *xdr,\n \tencode_compound_hdr(xdr, req, &hdr);\n \tencode_sequence(xdr, &args->seq_args, &hdr);\n \tencode_putfh(xdr, args->fh, &hdr);\n-\treplen = hdr.replen + op_decode_hdr_maxsz + nfs4_fattr_bitmap_maxsz + 1;\n+\treplen = hdr.replen + op_decode_hdr_maxsz + 1;\n \tencode_getattr_two(xdr, FATTR4_WORD0_ACL, 0, &hdr);\n \n \txdr_inline_pages(&req->rq_rcv_buf, replen << 2,\n \t\targs->acl_pages, args->acl_pgbase, args->acl_len);\n+\txdr_set_scratch_buffer(xdr, page_address(args->acl_scratch), PAGE_SIZE);\n+\n \tencode_nops(&hdr);\n }\n \n@@ -4957,17 +4959,18 @@ decode_restorefh(struct xdr_stream *xdr)\n }\n \n static int decode_getacl(struct xdr_stream *xdr, struct rpc_rqst *req,\n-\t\tsize_t *acl_len)\n+\t\t\t struct nfs_getaclres *res)\n {\n-\t__be32 *savep;\n+\t__be32 *savep, *bm_p;\n \tuint32_t attrlen,\n \t\t bitmap[3] = {0};\n \tstruct kvec *iov = req->rq_rcv_buf.head;\n \tint status;\n \n-\t*acl_len = 0;\n+\tres->acl_len = 0;\n \tif ((status = decode_op_hdr(xdr, OP_GETATTR)) != 0)\n \t\tgoto out;\n+\tbm_p = xdr->p;\n \tif ((status = decode_attr_bitmap(xdr, bitmap)) != 0)\n \t\tgoto out;\n \tif ((status = decode_attr_length(xdr, &attrlen, &savep)) != 0)\n@@ -4979,18 +4982,30 @@ static int decode_getacl(struct xdr_stream *xdr, struct rpc_rqst *req,\n \t\tsize_t hdrlen;\n \t\tu32 recvd;\n \n+\t\t/* The bitmap (xdr len + bitmaps) and the attr xdr len words\n+\t\t * are stored with the acl data to handle the problem of\n+\t\t * variable length bitmaps.*/\n+\t\txdr->p = bm_p;\n+\t\tres->acl_data_offset = be32_to_cpup(bm_p) + 2;\n+\t\tres->acl_data_offset <<= 2;\n+\n \t\t/* We ignore &savep and don't do consistency checks on\n \t\t * the attr length.  Let userspace figure it out.... */\n \t\thdrlen = (u8 *)xdr->p - (u8 *)iov->iov_base;\n+\t\tattrlen += res->acl_data_offset;\n \t\trecvd = req->rq_rcv_buf.len - hdrlen;\n \t\tif (attrlen > recvd) {\n-\t\t\tdprintk(\"NFS: server cheating in getattr\"\n-\t\t\t\t\t\" acl reply: attrlen %u > recvd %u\\n\",\n+\t\t\tif (res->acl_flags & NFS4_ACL_LEN_REQUEST) {\n+\t\t\t\t/* getxattr interface called with a NULL buf */\n+\t\t\t\tres->acl_len = attrlen;\n+\t\t\t\tgoto out;\n+\t\t\t}\n+\t\t\tdprintk(\"NFS: acl reply: attrlen %u > recvd %u\\n\",\n \t\t\t\t\tattrlen, recvd);\n \t\t\treturn -EINVAL;\n \t\t}\n \t\txdr_read_pages(xdr, attrlen);\n-\t\t*acl_len = attrlen;\n+\t\tres->acl_len = attrlen;\n \t} else\n \t\tstatus = -EOPNOTSUPP;\n \n@@ -6028,7 +6043,7 @@ nfs4_xdr_dec_getacl(struct rpc_rqst *rqstp, struct xdr_stream *xdr,\n \tstatus = decode_putfh(xdr);\n \tif (status)\n \t\tgoto out;\n-\tstatus = decode_getacl(xdr, rqstp, &res->acl_len);\n+\tstatus = decode_getacl(xdr, rqstp, res);\n \n out:\n \treturn status;\n",
    "critical_vars": [
      "replen"
    ],
    "variable_definitions": {
      "replen": "uint32_t replen;"
    },
    "variable_types": {
      "replen": "integer"
    },
    "type_mapping": {
      "replen": "Integer"
    },
    "cve": "CVE-2011-4131",
    "vulnerable_line": "replen = hdr.replen + op_decode_hdr_maxsz + nfs4_fattr_bitmap_maxsz + 1;",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The calculation of 'replen' can exceed the maximum value for its data type if the values of 'hdr.replen', 'op_decode_hdr_maxsz', or 'nfs4_fattr_bitmap_maxsz' are too large, leading to integer overflow issues."
  },
  {
    "vulcode": "static __inline__ void\njiffies_to_compat_timeval(unsigned long jiffies, struct compat_timeval *value)\n{\n\t/*\n\t * Convert jiffies to nanoseconds and separate with\n\t * one divide.\n\t */\n\tu64 nsec = (u64)jiffies * TICK_NSEC;\n\tlong rem;\n\tvalue->tv_sec = div_long_long_rem(nsec, NSEC_PER_SEC, &rem);\n\tvalue->tv_usec = rem / NSEC_PER_USEC;\n}",
    "diff": "@@ -54,6 +54,7 @@ typedef elf_fpreg_t elf_fpregset_t[ELF_NFPREG];\n #include <linux/module.h>\n #include <linux/elfcore.h>\n #include <linux/compat.h>\n+#include <linux/math64.h>\n \n #define elf_prstatus elf_prstatus32\n struct elf_prstatus32\n@@ -102,8 +103,8 @@ jiffies_to_compat_timeval(unsigned long jiffies, struct compat_timeval *value)\n \t * one divide.\n \t */\n \tu64 nsec = (u64)jiffies * TICK_NSEC;\n-\tlong rem;\n-\tvalue->tv_sec = div_long_long_rem(nsec, NSEC_PER_SEC, &rem);\n+\tu32 rem;\n+\tvalue->tv_sec = div_u64_rem(nsec, NSEC_PER_SEC, &rem);\n \tvalue->tv_usec = rem / NSEC_PER_USEC;\n }\n \n",
    "critical_vars": [
      "rem"
    ],
    "variable_definitions": {
      "rem": "u32 rem;"
    },
    "variable_types": {
      "rem": "integer"
    },
    "type_mapping": {
      "rem": "Integer"
    },
    "cve": "CVE-2011-3209",
    "vulnerable_line": "value->tv_sec = div_long_long_rem(nsec, NSEC_PER_SEC, &rem);",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Division by Zero",
    "reasoning": "The function div_long_long_rem may cause a divide error if NSEC_PER_SEC is improperly set to zero, leading to a denial of service."
  },
  {
    "vulcode": "EXPORT_SYMBOL(timeval_to_jiffies);\n\nvoid jiffies_to_timeval(const unsigned long jiffies, struct timeval *value)\n{\n\t/*\n\t * Convert jiffies to nanoseconds and separate with\n\t * one divide.\n\t */\n\tu64 nsec = (u64)jiffies * TICK_NSEC;\n\tlong tv_usec;\n\n\tvalue->tv_sec = div_long_long_rem(nsec, NSEC_PER_SEC, &tv_usec);\n\ttv_usec /= NSEC_PER_USEC;\n\tvalue->tv_usec = tv_usec;\n}",
    "diff": "@@ -392,13 +392,17 @@ EXPORT_SYMBOL(set_normalized_timespec);\n struct timespec ns_to_timespec(const s64 nsec)\n {\n \tstruct timespec ts;\n+\ts32 rem;\n \n \tif (!nsec)\n \t\treturn (struct timespec) {0, 0};\n \n-\tts.tv_sec = div_long_long_rem_signed(nsec, NSEC_PER_SEC, &ts.tv_nsec);\n-\tif (unlikely(nsec < 0))\n-\t\tset_normalized_timespec(&ts, ts.tv_sec, ts.tv_nsec);\n+\tts.tv_sec = div_s64_rem(nsec, NSEC_PER_SEC, &rem);\n+\tif (unlikely(rem < 0)) {\n+\t\tts.tv_sec--;\n+\t\trem += NSEC_PER_SEC;\n+\t}\n+\tts.tv_nsec = rem;\n \n \treturn ts;\n }\n@@ -528,8 +532,10 @@ jiffies_to_timespec(const unsigned long jiffies, struct timespec *value)\n \t * Convert jiffies to nanoseconds and separate with\n \t * one divide.\n \t */\n-\tu64 nsec = (u64)jiffies * TICK_NSEC;\n-\tvalue->tv_sec = div_long_long_rem(nsec, NSEC_PER_SEC, &value->tv_nsec);\n+\tu32 rem;\n+\tvalue->tv_sec = div_u64_rem((u64)jiffies * TICK_NSEC,\n+\t\t\t\t    NSEC_PER_SEC, &rem);\n+\tvalue->tv_nsec = rem;\n }\n EXPORT_SYMBOL(jiffies_to_timespec);\n \n@@ -567,12 +573,11 @@ void jiffies_to_timeval(const unsigned long jiffies, struct timeval *value)\n \t * Convert jiffies to nanoseconds and separate with\n \t * one divide.\n \t */\n-\tu64 nsec = (u64)jiffies * TICK_NSEC;\n-\tlong tv_usec;\n+\tu32 rem;\n \n-\tvalue->tv_sec = div_long_long_rem(nsec, NSEC_PER_SEC, &tv_usec);\n-\ttv_usec /= NSEC_PER_USEC;\n-\tvalue->tv_usec = tv_usec;\n+\tvalue->tv_sec = div_u64_rem((u64)jiffies * TICK_NSEC,\n+\t\t\t\t    NSEC_PER_SEC, &rem);\n+\tvalue->tv_usec = rem / NSEC_PER_USEC;\n }\n EXPORT_SYMBOL(jiffies_to_timeval);\n \n",
    "critical_vars": [
      "nsec"
    ],
    "variable_definitions": {
      "nsec": "u64 nsec = (u64)jiffies * TICK_NSEC;"
    },
    "variable_types": {
      "nsec": "integer"
    },
    "type_mapping": {
      "nsec": "Integer"
    },
    "cve": "CVE-2011-3209",
    "vulnerable_line": "value->tv_sec = div_long_long_rem(nsec, NSEC_PER_SEC, &tv_usec);",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Division by Zero",
    "reasoning": "The function div_long_long_rem can cause a divide error if NSEC_PER_SEC equals zero, potentially leading to a panic in the system."
  },
  {
    "vulcode": "static int sgi_clock_set(clockid_t clockid, struct timespec *tp)\n{\n\n\tu64 nsec;\n\tu64 rem;\n\n\tnsec = rtc_time() * sgi_clock_period;\n\n\tsgi_clock_offset.tv_sec = tp->tv_sec - div_long_long_rem(nsec, NSEC_PER_SEC, &rem);\n\n\tif (rem <= tp->tv_nsec)\n\t\tsgi_clock_offset.tv_nsec = tp->tv_sec - rem;\n\telse {\n\t\tsgi_clock_offset.tv_nsec = tp->tv_sec + NSEC_PER_SEC - rem;\n\t\tsgi_clock_offset.tv_sec--;\n\t}\n\treturn 0;\n}",
    "diff": "@@ -30,6 +30,8 @@\n #include <linux/miscdevice.h>\n #include <linux/posix-timers.h>\n #include <linux/interrupt.h>\n+#include <linux/time.h>\n+#include <linux/math64.h>\n \n #include <asm/uaccess.h>\n #include <asm/sn/addrs.h>\n@@ -472,8 +474,8 @@ static int sgi_clock_get(clockid_t clockid, struct timespec *tp)\n \n \tnsec = rtc_time() * sgi_clock_period\n \t\t\t+ sgi_clock_offset.tv_nsec;\n-\ttp->tv_sec = div_long_long_rem(nsec, NSEC_PER_SEC, &tp->tv_nsec)\n-\t\t\t+ sgi_clock_offset.tv_sec;\n+\t*tp = ns_to_timespec(nsec);\n+\ttp->tv_sec += sgi_clock_offset.tv_sec;\n \treturn 0;\n };\n \n@@ -481,11 +483,11 @@ static int sgi_clock_set(clockid_t clockid, struct timespec *tp)\n {\n \n \tu64 nsec;\n-\tu64 rem;\n+\tu32 rem;\n \n \tnsec = rtc_time() * sgi_clock_period;\n \n-\tsgi_clock_offset.tv_sec = tp->tv_sec - div_long_long_rem(nsec, NSEC_PER_SEC, &rem);\n+\tsgi_clock_offset.tv_sec = tp->tv_sec - div_u64_rem(nsec, NSEC_PER_SEC, &rem);\n \n \tif (rem <= tp->tv_nsec)\n \t\tsgi_clock_offset.tv_nsec = tp->tv_sec - rem;\n@@ -644,9 +646,6 @@ static int sgi_timer_del(struct k_itimer *timr)\n \treturn 0;\n }\n \n-#define timespec_to_ns(x) ((x).tv_nsec + (x).tv_sec * NSEC_PER_SEC)\n-#define ns_to_timespec(ts, nsec) (ts).tv_sec = div_long_long_rem(nsec, NSEC_PER_SEC, &(ts).tv_nsec)\n-\n /* Assumption: it_lock is already held with irq's disabled */\n static void sgi_timer_get(struct k_itimer *timr, struct itimerspec *cur_setting)\n {\n@@ -659,9 +658,8 @@ static void sgi_timer_get(struct k_itimer *timr, struct itimerspec *cur_setting)\n \t\treturn;\n \t}\n \n-\tns_to_timespec(cur_setting->it_interval, timr->it.mmtimer.incr * sgi_clock_period);\n-\tns_to_timespec(cur_setting->it_value, (timr->it.mmtimer.expires - rtc_time())* sgi_clock_period);\n-\treturn;\n+\tcur_setting->it_interval = ns_to_timespec(timr->it.mmtimer.incr * sgi_clock_period);\n+\tcur_setting->it_value = ns_to_timespec((timr->it.mmtimer.expires - rtc_time()) * sgi_clock_period);\n }\n \n \n@@ -679,8 +677,8 @@ static int sgi_timer_set(struct k_itimer *timr, int flags,\n \t\tsgi_timer_get(timr, old_setting);\n \n \tsgi_timer_del(timr);\n-\twhen = timespec_to_ns(new_setting->it_value);\n-\tperiod = timespec_to_ns(new_setting->it_interval);\n+\twhen = timespec_to_ns(&new_setting->it_value);\n+\tperiod = timespec_to_ns(&new_setting->it_interval);\n \n \tif (when == 0)\n \t\t/* Clear timer */\n@@ -695,7 +693,7 @@ static int sgi_timer_set(struct k_itimer *timr, int flags,\n \t\tunsigned long now;\n \n \t\tgetnstimeofday(&n);\n-\t\tnow = timespec_to_ns(n);\n+\t\tnow = timespec_to_ns(&n);\n \t\tif (when > now)\n \t\t\twhen -= now;\n \t\telse\n",
    "critical_vars": [
      "rem"
    ],
    "variable_definitions": {
      "rem": "u32 rem;"
    },
    "variable_types": {
      "rem": "integer"
    },
    "type_mapping": {
      "rem": "Integer"
    },
    "cve": "CVE-2011-3209",
    "vulnerable_line": "sgi_clock_offset.tv_sec = tp->tv_sec - div_long_long_rem(nsec, NSEC_PER_SEC, &rem);",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Division by Zero",
    "reasoning": "The potential for a Divide Error Fault arises when div_long_long_rem is called with a divisor of NSEC_PER_SEC if nsec is calculated incorrectly or if NSEC_PER_SEC is zero, leading to a divide by zero condition."
  },
  {
    "vulcode": "static ssize_t lbs_debugfs_write(struct file *f, const char __user *buf,\n\t\t\t    size_t cnt, loff_t *ppos)\n{\n\tint r, i;\n\tchar *pdata;\n\tchar *p;\n\tchar *p0;\n\tchar *p1;\n\tchar *p2;\n\tstruct debug_data *d = f->private_data;\n\n\tpdata = kmalloc(cnt, GFP_KERNEL);\n\tif (pdata == NULL)\n\t\treturn 0;\n\n\tif (copy_from_user(pdata, buf, cnt)) {\n\t\tlbs_deb_debugfs(\"Copy from user failed\\n\");\n\t\tkfree(pdata);\n\t\treturn 0;\n\t}\n\n\tp0 = pdata;\n\tfor (i = 0; i < num_of_items; i++) {\n\t\tdo {\n\t\t\tp = strstr(p0, d[i].name);\n\t\t\tif (p == NULL)\n\t\t\t\tbreak;\n\t\t\tp1 = strchr(p, '\\n');\n\t\t\tif (p1 == NULL)\n\t\t\t\tbreak;\n\t\t\tp0 = p1++;\n\t\t\tp2 = strchr(p, '=');\n\t\t\tif (!p2)\n\t\t\t\tbreak;\n\t\t\tp2++;\n\t\t\tr = simple_strtoul(p2, NULL, 0);\n\t\t\tif (d[i].size == 1)\n\t\t\t\t*((u8 *) d[i].addr) = (u8) r;\n\t\t\telse if (d[i].size == 2)\n\t\t\t\t*((u16 *) d[i].addr) = (u16) r;\n\t\t\telse if (d[i].size == 4)\n\t\t\t\t*((u32 *) d[i].addr) = (u32) r;\n\t\t\telse if (d[i].size == 8)\n\t\t\t\t*((u64 *) d[i].addr) = (u64) r;\n\t\t\tbreak;\n\t\t} while (1);\n\t}\n\tkfree(pdata);\n\n\treturn (ssize_t)cnt;\n}",
    "diff": "@@ -913,7 +913,10 @@ static ssize_t lbs_debugfs_write(struct file *f, const char __user *buf,\n \tchar *p2;\n \tstruct debug_data *d = f->private_data;\n \n-\tpdata = kmalloc(cnt, GFP_KERNEL);\n+\tif (cnt == 0)\n+\t\treturn 0;\n+\n+\tpdata = kmalloc(cnt + 1, GFP_KERNEL);\n \tif (pdata == NULL)\n \t\treturn 0;\n \n@@ -922,6 +925,7 @@ static ssize_t lbs_debugfs_write(struct file *f, const char __user *buf,\n \t\tkfree(pdata);\n \t\treturn 0;\n \t}\n+\tpdata[cnt] = '\\0';\n \n \tp0 = pdata;\n \tfor (i = 0; i < num_of_items; i++) {\n",
    "critical_vars": [
      "pdata"
    ],
    "variable_definitions": {
      "pdata": "char *pdata;"
    },
    "variable_types": {
      "pdata": "char pointer"
    },
    "type_mapping": {
      "pdata": "char pointer"
    },
    "cve": "CVE-2013-6378",
    "vulnerable_line": "pdata = kmalloc(cnt, GFP_KERNEL);",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The original allocation of 'pdata' using 'kmalloc' with 'cnt' could lead to an integer overflow if 'cnt' is too large, potentially resulting in allocation of insufficient memory, causing a denial of service."
  },
  {
    "vulcode": "static ssize_t lbs_debugfs_write(struct file *f, const char __user *buf,\n\t\t\t    size_t cnt, loff_t *ppos)\n{\n\tint r, i;\n\tchar *pdata;\n\tchar *p;\n\tchar *p0;\n\tchar *p1;\n\tchar *p2;\n\tstruct debug_data *d = f->private_data;\n\n\tpdata = kmalloc(cnt, GFP_KERNEL);\n\tif (pdata == NULL)\n\t\treturn 0;\n\n\tif (copy_from_user(pdata, buf, cnt)) {\n\t\tlbs_deb_debugfs(\"Copy from user failed\\n\");\n\t\tkfree(pdata);\n\t\treturn 0;\n\t}\n\n\tp0 = pdata;\n\tfor (i = 0; i < num_of_items; i++) {\n\t\tdo {\n\t\t\tp = strstr(p0, d[i].name);\n\t\t\tif (p == NULL)\n\t\t\t\tbreak;\n\t\t\tp1 = strchr(p, '\\n');\n\t\t\tif (p1 == NULL)\n\t\t\t\tbreak;\n\t\t\tp0 = p1++;\n\t\t\tp2 = strchr(p, '=');\n\t\t\tif (!p2)\n\t\t\t\tbreak;\n\t\t\tp2++;\n\t\t\tr = simple_strtoul(p2, NULL, 0);\n\t\t\tif (d[i].size == 1)\n\t\t\t\t*((u8 *) d[i].addr) = (u8) r;\n\t\t\telse if (d[i].size == 2)\n\t\t\t\t*((u16 *) d[i].addr) = (u16) r;\n\t\t\telse if (d[i].size == 4)\n\t\t\t\t*((u32 *) d[i].addr) = (u32) r;\n\t\t\telse if (d[i].size == 8)\n\t\t\t\t*((u64 *) d[i].addr) = (u64) r;\n\t\t\tbreak;\n\t\t} while (1);\n\t}\n\tkfree(pdata);\n\n\treturn (ssize_t)cnt;\n}",
    "diff": "@@ -913,7 +913,10 @@ static ssize_t lbs_debugfs_write(struct file *f, const char __user *buf,\n \tchar *p2;\n \tstruct debug_data *d = f->private_data;\n \n-\tpdata = kmalloc(cnt, GFP_KERNEL);\n+\tif (cnt == 0)\n+\t\treturn 0;\n+\n+\tpdata = kmalloc(cnt + 1, GFP_KERNEL);\n \tif (pdata == NULL)\n \t\treturn 0;\n \n@@ -922,6 +925,7 @@ static ssize_t lbs_debugfs_write(struct file *f, const char __user *buf,\n \t\tkfree(pdata);\n \t\treturn 0;\n \t}\n+\tpdata[cnt] = '\\0';\n \n \tp0 = pdata;\n \tfor (i = 0; i < num_of_items; i++) {\n",
    "critical_vars": [
      "cnt"
    ],
    "variable_definitions": {
      "cnt": "size_t cnt"
    },
    "variable_types": {
      "cnt": "integer"
    },
    "type_mapping": {
      "cnt": "Integer"
    },
    "cve": "CVE-2013-6378",
    "vulnerable_line": "pdata = kmalloc(cnt, GFP_KERNEL);",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The 'cnt' value is not checked for being negative or too large before being used for memory allocation, which could let it exceed the maximum allowed size, leading to an integer overflow during memory allocation."
  },
  {
    "vulcode": "static int ext4_fill_flex_info(struct super_block *sb)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_group_desc *gdp = NULL;\n\text4_group_t flex_group_count;\n\text4_group_t flex_group;\n\tint groups_per_flex = 0;\n\tsize_t size;\n\tint i;\n\n\tsbi->s_log_groups_per_flex = sbi->s_es->s_log_groups_per_flex;\n\tgroups_per_flex = 1 << sbi->s_log_groups_per_flex;\n\n\tif (groups_per_flex < 2) {\n\t\tsbi->s_log_groups_per_flex = 0;\n\t\treturn 1;\n\t}\n\n\t/* We allocate both existing and potentially added groups */\n\tflex_group_count = ((sbi->s_groups_count + groups_per_flex - 1) +\n\t\t\t((le16_to_cpu(sbi->s_es->s_reserved_gdt_blocks) + 1) <<\n\t\t\t      EXT4_DESC_PER_BLOCK_BITS(sb))) / groups_per_flex;\n\tsize = flex_group_count * sizeof(struct flex_groups);\n\tsbi->s_flex_groups = ext4_kvzalloc(size, GFP_KERNEL);\n\tif (sbi->s_flex_groups == NULL) {\n\t\text4_msg(sb, KERN_ERR, \"not enough memory for %u flex groups\",\n\t\t\t flex_group_count);\n\t\tgoto failed;\n\t}\n\n\tfor (i = 0; i < sbi->s_groups_count; i++) {\n\t\tgdp = ext4_get_group_desc(sb, i, NULL);\n\n\t\tflex_group = ext4_flex_group(sbi, i);\n\t\tatomic_add(ext4_free_inodes_count(sb, gdp),\n\t\t\t   &sbi->s_flex_groups[flex_group].free_inodes);\n\t\tatomic_add(ext4_free_group_clusters(sb, gdp),\n\t\t\t   &sbi->s_flex_groups[flex_group].free_clusters);\n\t\tatomic_add(ext4_used_dirs_count(sb, gdp),\n\t\t\t   &sbi->s_flex_groups[flex_group].used_dirs);\n\t}\n\n\treturn 1;\nfailed:\n\treturn 0;\n}",
    "diff": "@@ -2006,17 +2006,16 @@ static int ext4_fill_flex_info(struct super_block *sb)\n \tstruct ext4_group_desc *gdp = NULL;\n \text4_group_t flex_group_count;\n \text4_group_t flex_group;\n-\tint groups_per_flex = 0;\n+\tunsigned int groups_per_flex = 0;\n \tsize_t size;\n \tint i;\n \n \tsbi->s_log_groups_per_flex = sbi->s_es->s_log_groups_per_flex;\n-\tgroups_per_flex = 1 << sbi->s_log_groups_per_flex;\n-\n-\tif (groups_per_flex < 2) {\n+\tif (sbi->s_log_groups_per_flex < 1 || sbi->s_log_groups_per_flex > 31) {\n \t\tsbi->s_log_groups_per_flex = 0;\n \t\treturn 1;\n \t}\n+\tgroups_per_flex = 1 << sbi->s_log_groups_per_flex;\n \n \t/* We allocate both existing and potentially added groups */\n \tflex_group_count = ((sbi->s_groups_count + groups_per_flex - 1) +\n",
    "critical_vars": [
      "groups_per_flex"
    ],
    "variable_definitions": {
      "groups_per_flex": "unsigned int groups_per_flex = 0;"
    },
    "variable_types": {
      "groups_per_flex": "integer"
    },
    "type_mapping": {
      "groups_per_flex": "Integer"
    },
    "cve": "CVE-2012-2100",
    "vulnerable_line": "flex_group_count = ((sbi->s_groups_count + groups_per_flex - 1) +",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The computation of flex_group_count may exceed the maximum value of the integer type, causing an overflow which leads to inconsistent filesystem-groups data."
  },
  {
    "vulcode": "static int ext4_fill_flex_info(struct super_block *sb)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_group_desc *gdp = NULL;\n\text4_group_t flex_group_count;\n\text4_group_t flex_group;\n\tint groups_per_flex = 0;\n\tsize_t size;\n\tint i;\n\n\tsbi->s_log_groups_per_flex = sbi->s_es->s_log_groups_per_flex;\n\tgroups_per_flex = 1 << sbi->s_log_groups_per_flex;\n\n\tif (groups_per_flex < 2) {\n\t\tsbi->s_log_groups_per_flex = 0;\n\t\treturn 1;\n\t}\n\n\t/* We allocate both existing and potentially added groups */\n\tflex_group_count = ((sbi->s_groups_count + groups_per_flex - 1) +\n\t\t\t((le16_to_cpu(sbi->s_es->s_reserved_gdt_blocks) + 1) <<\n\t\t\t      EXT4_DESC_PER_BLOCK_BITS(sb))) / groups_per_flex;\n\tsize = flex_group_count * sizeof(struct flex_groups);\n\tsbi->s_flex_groups = ext4_kvzalloc(size, GFP_KERNEL);\n\tif (sbi->s_flex_groups == NULL) {\n\t\text4_msg(sb, KERN_ERR, \"not enough memory for %u flex groups\",\n\t\t\t flex_group_count);\n\t\tgoto failed;\n\t}\n\n\tfor (i = 0; i < sbi->s_groups_count; i++) {\n\t\tgdp = ext4_get_group_desc(sb, i, NULL);\n\n\t\tflex_group = ext4_flex_group(sbi, i);\n\t\tatomic_add(ext4_free_inodes_count(sb, gdp),\n\t\t\t   &sbi->s_flex_groups[flex_group].free_inodes);\n\t\tatomic_add(ext4_free_group_clusters(sb, gdp),\n\t\t\t   &sbi->s_flex_groups[flex_group].free_clusters);\n\t\tatomic_add(ext4_used_dirs_count(sb, gdp),\n\t\t\t   &sbi->s_flex_groups[flex_group].used_dirs);\n\t}\n\n\treturn 1;\nfailed:\n\treturn 0;\n}",
    "diff": "@@ -2006,17 +2006,16 @@ static int ext4_fill_flex_info(struct super_block *sb)\n \tstruct ext4_group_desc *gdp = NULL;\n \text4_group_t flex_group_count;\n \text4_group_t flex_group;\n-\tint groups_per_flex = 0;\n+\tunsigned int groups_per_flex = 0;\n \tsize_t size;\n \tint i;\n \n \tsbi->s_log_groups_per_flex = sbi->s_es->s_log_groups_per_flex;\n-\tgroups_per_flex = 1 << sbi->s_log_groups_per_flex;\n-\n-\tif (groups_per_flex < 2) {\n+\tif (sbi->s_log_groups_per_flex < 1 || sbi->s_log_groups_per_flex > 31) {\n \t\tsbi->s_log_groups_per_flex = 0;\n \t\treturn 1;\n \t}\n+\tgroups_per_flex = 1 << sbi->s_log_groups_per_flex;\n \n \t/* We allocate both existing and potentially added groups */\n \tflex_group_count = ((sbi->s_groups_count + groups_per_flex - 1) +\n",
    "critical_vars": [
      "groups_per_flex"
    ],
    "variable_definitions": {
      "groups_per_flex": "int groups_per_flex = 0;"
    },
    "variable_types": {
      "groups_per_flex": "integer"
    },
    "type_mapping": {
      "groups_per_flex": "Integer"
    },
    "cve": "CVE-2012-2100",
    "vulnerable_line": "groups_per_flex = 1 << sbi->s_log_groups_per_flex;",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The left shift operation can produce an integer overflow if s_log_groups_per_flex is greater than 31, leading to large, unexpected values for groups_per_flex."
  },
  {
    "vulcode": "struct xt_table_info *xt_alloc_table_info(unsigned int size)\n{\n\tstruct xt_table_info *info = NULL;\n\tsize_t sz = sizeof(*info) + size;\n\n\t/* Pedantry: prevent them from hitting BUG() in vmalloc.c --RR */\n\tif ((SMP_ALIGN(size) >> PAGE_SHIFT) + 2 > totalram_pages)\n\t\treturn NULL;\n\n\tif (sz <= (PAGE_SIZE << PAGE_ALLOC_COSTLY_ORDER))\n\t\tinfo = kmalloc(sz, GFP_KERNEL | __GFP_NOWARN | __GFP_NORETRY);\n\tif (!info) {\n\t\tinfo = vmalloc(sz);\n\t\tif (!info)\n\t\t\treturn NULL;\n\t}\n\tmemset(info, 0, sizeof(*info));\n\tinfo->size = size;\n\treturn info;\n}",
    "diff": "@@ -659,6 +659,9 @@ struct xt_table_info *xt_alloc_table_info(unsigned int size)\n \tstruct xt_table_info *info = NULL;\n \tsize_t sz = sizeof(*info) + size;\n \n+\tif (sz < sizeof(*info))\n+\t\treturn NULL;\n+\n \t/* Pedantry: prevent them from hitting BUG() in vmalloc.c --RR */\n \tif ((SMP_ALIGN(size) >> PAGE_SHIFT) + 2 > totalram_pages)\n \t\treturn NULL;\n",
    "critical_vars": [
      "sz"
    ],
    "variable_definitions": {
      "sz": "size_t sz = sizeof(*info) + size;"
    },
    "variable_types": {
      "sz": "integer"
    },
    "type_mapping": {
      "sz": "Integer"
    },
    "cve": "CVE-2016-3135",
    "vulnerable_line": "size_t sz = sizeof(*info) + size;",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The addition of sizeof(*info) and size could exceed the maximum size value for size_t, leading to an overflow, which results in incorrect memory allocation and potential heap corruption."
  },
  {
    "vulcode": "PHPAPI zend_string *php_escape_shell_cmd(char *str)\n{\n\tregister int x, y, l = (int)strlen(str);\n\tsize_t estimate = (2 * l) + 1;\n\tzend_string *cmd;\n#ifndef PHP_WIN32\n\tchar *p = NULL;\n#endif\n\n\n\tcmd = zend_string_alloc(2 * l, 0);\n\n\tfor (x = 0, y = 0; x < l; x++) {\n\t\tint mb_len = php_mblen(str + x, (l - x));\n\n\t\t/* skip non-valid multibyte characters */\n\t\tif (mb_len < 0) {\n\t\t\tcontinue;\n\t\t} else if (mb_len > 1) {\n\t\t\tmemcpy(ZSTR_VAL(cmd) + y, str + x, mb_len);\n\t\t\ty += mb_len;\n\t\t\tx += mb_len - 1;\n\t\t\tcontinue;\n\t\t}\n\n\t\tswitch (str[x]) {\n#ifndef PHP_WIN32\n\t\t\tcase '\"':\n\t\t\tcase '\\'':\n\t\t\t\tif (!p && (p = memchr(str + x + 1, str[x], l - x - 1))) {\n\t\t\t\t\t/* noop */\n\t\t\t\t} else if (p && *p == str[x]) {\n\t\t\t\t\tp = NULL;\n\t\t\t\t} else {\n\t\t\t\t\tZSTR_VAL(cmd)[y++] = '\\\\';\n\t\t\t\t}\n\t\t\t\tZSTR_VAL(cmd)[y++] = str[x];\n\t\t\t\tbreak;\n#else\n\t\t\t/* % is Windows specific for environmental variables, ^%PATH% will \n\t\t\t\toutput PATH while ^%PATH^% will not. escapeshellcmd->val will escape all % and !.\n\t\t\t*/\n\t\t\tcase '%':\n\t\t\tcase '!':\n\t\t\tcase '\"':\n\t\t\tcase '\\'':\n#endif\n\t\t\tcase '#': /* This is character-set independent */\n\t\t\tcase '&':\n\t\t\tcase ';':\n\t\t\tcase '`':\n\t\t\tcase '|':\n\t\t\tcase '*':\n\t\t\tcase '?':\n\t\t\tcase '~':\n\t\t\tcase '<':\n\t\t\tcase '>':\n\t\t\tcase '^':\n\t\t\tcase '(':\n\t\t\tcase ')':\n\t\t\tcase '[':\n\t\t\tcase ']':\n\t\t\tcase '{':\n\t\t\tcase '}':\n\t\t\tcase '$':\n\t\t\tcase '\\\\':\n\t\t\tcase '\\x0A': /* excluding these two */\n\t\t\tcase '\\xFF':\n#ifdef PHP_WIN32\n\t\t\t\tZSTR_VAL(cmd)[y++] = '^';\n#else\n\t\t\t\tZSTR_VAL(cmd)[y++] = '\\\\';\n#endif\n\t\t\t\t/* fall-through */\n\t\t\tdefault:\n\t\t\t\tZSTR_VAL(cmd)[y++] = str[x];\n\n\t\t}\n\t}\n\tZSTR_VAL(cmd)[y] = '\\0';\n\n\tif ((estimate - y) > 4096) {\n\t\t/* realloc if the estimate was way overill\n\t\t * Arbitrary cutoff point of 4096 */\n\t\tcmd = zend_string_truncate(cmd, y, 0);\n\t}\n\n\tZSTR_LEN(cmd) = y;\n\n\treturn cmd;\n}",
    "diff": "@@ -253,7 +253,7 @@ PHPAPI zend_string *php_escape_shell_cmd(char *str)\n #endif\n \n \n-\tcmd = zend_string_alloc(2 * l, 0);\n+\tcmd = zend_string_safe_alloc(2, l, 0, 0);\n \n \tfor (x = 0, y = 0; x < l; x++) {\n \t\tint mb_len = php_mblen(str + x, (l - x));\n@@ -345,7 +345,7 @@ PHPAPI zend_string *php_escape_shell_arg(char *str)\n \tsize_t estimate = (4 * l) + 3;\n \n \n-\tcmd = zend_string_alloc(4 * l + 2, 0); /* worst case */\n+\tcmd = zend_string_safe_alloc(4, l, 2, 0); /* worst case */\n \n #ifdef PHP_WIN32\n \tZSTR_VAL(cmd)[y++] = '\"';\n",
    "critical_vars": [
      "cmd"
    ],
    "variable_definitions": {
      "cmd": "zend_string *cmd;"
    },
    "variable_types": {
      "cmd": "struct pointer"
    },
    "type_mapping": {
      "cmd": "struct pointer"
    },
    "cve": "CVE-2016-1904",
    "vulnerable_line": "cmd = zend_string_alloc(2 * l, 0);",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The multiplication of 2 * l can exceed the maximum value for an integer, leading to a heap-based buffer overflow, which allows attackers to exploit this and cause a denial of service."
  },
  {
    "vulcode": "PHPAPI zend_string *php_escape_shell_arg(char *str)\n{\n\tint x, y = 0, l = (int)strlen(str);\n\tzend_string *cmd;\n\tsize_t estimate = (4 * l) + 3;\n\n\n\tcmd = zend_string_alloc(4 * l + 2, 0); /* worst case */\n\n#ifdef PHP_WIN32\n\tZSTR_VAL(cmd)[y++] = '\"';\n#else\n\tZSTR_VAL(cmd)[y++] = '\\'';\n#endif\n\n\tfor (x = 0; x < l; x++) {\n\t\tint mb_len = php_mblen(str + x, (l - x));\n\n\t\t/* skip non-valid multibyte characters */\n\t\tif (mb_len < 0) {\n\t\t\tcontinue;\n\t\t} else if (mb_len > 1) {\n\t\t\tmemcpy(ZSTR_VAL(cmd) + y, str + x, mb_len);\n\t\t\ty += mb_len;\n\t\t\tx += mb_len - 1;\n\t\t\tcontinue;\n\t\t}\n\n\t\tswitch (str[x]) {\n#ifdef PHP_WIN32\n\t\tcase '\"':\n\t\tcase '%':\n\t\tcase '!':\n\t\t\tZSTR_VAL(cmd)[y++] = ' ';\n\t\t\tbreak;\n#else\n\t\tcase '\\'':\n\t\t\tZSTR_VAL(cmd)[y++] = '\\'';\n\t\t\tZSTR_VAL(cmd)[y++] = '\\\\';\n\t\t\tZSTR_VAL(cmd)[y++] = '\\'';\n#endif\n\t\t\t/* fall-through */\n\t\tdefault:\n\t\t\tZSTR_VAL(cmd)[y++] = str[x];\n\t\t}\n\t}\n#ifdef PHP_WIN32\n\tif (y > 0 && '\\\\' == ZSTR_VAL(cmd)[y - 1]) {\n\t\tint k = 0, n = y - 1;\n\t\tfor (; n >= 0 && '\\\\' == ZSTR_VAL(cmd)[n]; n--, k++);\n\t\tif (k % 2) {\n\t\t\tZSTR_VAL(cmd)[y++] = '\\\\';\n\t\t}\n\t}\n\n\tZSTR_VAL(cmd)[y++] = '\"';\n#else\n\tZSTR_VAL(cmd)[y++] = '\\'';\n#endif\n\tZSTR_VAL(cmd)[y] = '\\0';\n\n\tif ((estimate - y) > 4096) {\n\t\t/* realloc if the estimate was way overill\n\t\t * Arbitrary cutoff point of 4096 */\n\t\tcmd = zend_string_truncate(cmd, y, 0);\n\t}\n\tZSTR_LEN(cmd) = y;\n\treturn cmd;\n}",
    "diff": "@@ -253,7 +253,7 @@ PHPAPI zend_string *php_escape_shell_cmd(char *str)\n #endif\n \n \n-\tcmd = zend_string_alloc(2 * l, 0);\n+\tcmd = zend_string_safe_alloc(2, l, 0, 0);\n \n \tfor (x = 0, y = 0; x < l; x++) {\n \t\tint mb_len = php_mblen(str + x, (l - x));\n@@ -345,7 +345,7 @@ PHPAPI zend_string *php_escape_shell_arg(char *str)\n \tsize_t estimate = (4 * l) + 3;\n \n \n-\tcmd = zend_string_alloc(4 * l + 2, 0); /* worst case */\n+\tcmd = zend_string_safe_alloc(4, l, 2, 0); /* worst case */\n \n #ifdef PHP_WIN32\n \tZSTR_VAL(cmd)[y++] = '\"';\n",
    "critical_vars": [
      "cmd"
    ],
    "variable_definitions": {
      "cmd": "zend_string *cmd;"
    },
    "variable_types": {
      "cmd": "struct pointer"
    },
    "type_mapping": {
      "cmd": "struct pointer"
    },
    "cve": "CVE-2016-1904",
    "vulnerable_line": "cmd = zend_string_alloc(4 * l + 2, 0); /* worst case */",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The calculation 4 * l + 2 can overflow if l is too large, leading to allocation of insufficient memory for cmd and causing a potential heap-based buffer overflow when processing user input."
  },
  {
    "vulcode": "cdf_read_property_info(const cdf_stream_t *sst, const cdf_header_t *h,\n    uint32_t offs, cdf_property_info_t **info, size_t *count, size_t *maxcount)\n{\n\tconst cdf_section_header_t *shp;\n\tcdf_section_header_t sh;\n\tconst uint8_t *p, *q, *e;\n\tint16_t s16;\n\tint32_t s32;\n\tuint32_t u32;\n\tint64_t s64;\n\tuint64_t u64;\n\tcdf_timestamp_t tp;\n\tsize_t i, o, o4, nelements, j;\n\tcdf_property_info_t *inp;\n\n\tif (offs > UINT32_MAX / 4) {\n\t\terrno = EFTYPE;\n\t\tgoto out;\n\t}\n\tshp = CAST(const cdf_section_header_t *, (const void *)\n\t    ((const char *)sst->sst_tab + offs));\n\tif (cdf_check_stream_offset(sst, h, shp, sizeof(*shp), __LINE__) == -1)\n\t\tgoto out;\n\tsh.sh_len = CDF_TOLE4(shp->sh_len);\n#define CDF_SHLEN_LIMIT (UINT32_MAX / 8)\n\tif (sh.sh_len > CDF_SHLEN_LIMIT) {\n\t\terrno = EFTYPE;\n\t\tgoto out;\n\t}\n\tsh.sh_properties = CDF_TOLE4(shp->sh_properties);\n#define CDF_PROP_LIMIT (UINT32_MAX / (4 * sizeof(*inp)))\n\tif (sh.sh_properties > CDF_PROP_LIMIT)\n\t\tgoto out;\n\tDPRINTF((\"section len: %u properties %u\\n\", sh.sh_len,\n\t    sh.sh_properties));\n\tif (*maxcount) {\n\t\tif (*maxcount > CDF_PROP_LIMIT)\n\t\t\tgoto out;\n\t\t*maxcount += sh.sh_properties;\n\t\tinp = CAST(cdf_property_info_t *,\n\t\t    realloc(*info, *maxcount * sizeof(*inp)));\n\t} else {\n\t\t*maxcount = sh.sh_properties;\n\t\tinp = CAST(cdf_property_info_t *,\n\t\t    malloc(*maxcount * sizeof(*inp)));\n\t}\n\tif (inp == NULL)\n\t\tgoto out;\n\t*info = inp;\n\tinp += *count;\n\t*count += sh.sh_properties;\n\tp = CAST(const uint8_t *, (const void *)\n\t    ((const char *)(const void *)sst->sst_tab +\n\t    offs + sizeof(sh)));\n\te = CAST(const uint8_t *, (const void *)\n\t    (((const char *)(const void *)shp) + sh.sh_len));\n\tif (cdf_check_stream_offset(sst, h, e, 0, __LINE__) == -1)\n\t\tgoto out;\n\tfor (i = 0; i < sh.sh_properties; i++) {\n\t\tsize_t ofs, tail = (i << 1) + 1;\n\t\tif (cdf_check_stream_offset(sst, h, p, tail * sizeof(uint32_t),\n\t\t    __LINE__) == -1)\n\t\t\tgoto out;\n\t\tofs = CDF_GETUINT32(p, tail);\n\t\tq = (const uint8_t *)(const void *)\n\t\t    ((const char *)(const void *)p + ofs\n\t\t    - 2 * sizeof(uint32_t));\n\t\tif (q > e) {\n\t\t\tDPRINTF((\"Ran of the end %p > %p\\n\", q, e));\n\t\t\tgoto out;\n\t\t}\n\t\tinp[i].pi_id = CDF_GETUINT32(p, i << 1);\n\t\tinp[i].pi_type = CDF_GETUINT32(q, 0);\n\t\tDPRINTF((\"%\" SIZE_T_FORMAT \"u) id=%x type=%x offs=0x%tx,0x%x\\n\",\n\t\t    i, inp[i].pi_id, inp[i].pi_type, q - p, offs));\n\t\tif (inp[i].pi_type & CDF_VECTOR) {\n\t\t\tnelements = CDF_GETUINT32(q, 1);\n\t\t\tif (nelements == 0) {\n\t\t\t\tDPRINTF((\"CDF_VECTOR with nelements == 0\\n\"));\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\to = 2;\n\t\t} else {\n\t\t\tnelements = 1;\n\t\t\to = 1;\n\t\t}\n\t\to4 = o * sizeof(uint32_t);\n\t\tif (inp[i].pi_type & (CDF_ARRAY|CDF_BYREF|CDF_RESERVED))\n\t\t\tgoto unknown;\n\t\tswitch (inp[i].pi_type & CDF_TYPEMASK) {\n\t\tcase CDF_NULL:\n\t\tcase CDF_EMPTY:\n\t\t\tbreak;\n\t\tcase CDF_SIGNED16:\n\t\t\tif (inp[i].pi_type & CDF_VECTOR)\n\t\t\t\tgoto unknown;\n\t\t\t(void)memcpy(&s16, &q[o4], sizeof(s16));\n\t\t\tinp[i].pi_s16 = CDF_TOLE2(s16);\n\t\t\tbreak;\n\t\tcase CDF_SIGNED32:\n\t\t\tif (inp[i].pi_type & CDF_VECTOR)\n\t\t\t\tgoto unknown;\n\t\t\t(void)memcpy(&s32, &q[o4], sizeof(s32));\n\t\t\tinp[i].pi_s32 = CDF_TOLE4((uint32_t)s32);\n\t\t\tbreak;\n\t\tcase CDF_BOOL:\n\t\tcase CDF_UNSIGNED32:\n\t\t\tif (inp[i].pi_type & CDF_VECTOR)\n\t\t\t\tgoto unknown;\n\t\t\t(void)memcpy(&u32, &q[o4], sizeof(u32));\n\t\t\tinp[i].pi_u32 = CDF_TOLE4(u32);\n\t\t\tbreak;\n\t\tcase CDF_SIGNED64:\n\t\t\tif (inp[i].pi_type & CDF_VECTOR)\n\t\t\t\tgoto unknown;\n\t\t\t(void)memcpy(&s64, &q[o4], sizeof(s64));\n\t\t\tinp[i].pi_s64 = CDF_TOLE8((uint64_t)s64);\n\t\t\tbreak;\n\t\tcase CDF_UNSIGNED64:\n\t\t\tif (inp[i].pi_type & CDF_VECTOR)\n\t\t\t\tgoto unknown;\n\t\t\t(void)memcpy(&u64, &q[o4], sizeof(u64));\n\t\t\tinp[i].pi_u64 = CDF_TOLE8((uint64_t)u64);\n\t\t\tbreak;\n\t\tcase CDF_FLOAT:\n\t\t\tif (inp[i].pi_type & CDF_VECTOR)\n\t\t\t\tgoto unknown;\n\t\t\t(void)memcpy(&u32, &q[o4], sizeof(u32));\n\t\t\tu32 = CDF_TOLE4(u32);\n\t\t\tmemcpy(&inp[i].pi_f, &u32, sizeof(inp[i].pi_f));\n\t\t\tbreak;\n\t\tcase CDF_DOUBLE:\n\t\t\tif (inp[i].pi_type & CDF_VECTOR)\n\t\t\t\tgoto unknown;\n\t\t\t(void)memcpy(&u64, &q[o4], sizeof(u64));\n\t\t\tu64 = CDF_TOLE8((uint64_t)u64);\n\t\t\tmemcpy(&inp[i].pi_d, &u64, sizeof(inp[i].pi_d));\n\t\t\tbreak;\n\t\tcase CDF_LENGTH32_STRING:\n\t\tcase CDF_LENGTH32_WSTRING:\n\t\t\tif (nelements > 1) {\n\t\t\t\tsize_t nelem = inp - *info;\n\t\t\t\tif (*maxcount > CDF_PROP_LIMIT\n\t\t\t\t    || nelements > CDF_PROP_LIMIT)\n\t\t\t\t\tgoto out;\n\t\t\t\t*maxcount += nelements;\n\t\t\t\tinp = CAST(cdf_property_info_t *,\n\t\t\t\t    realloc(*info, *maxcount * sizeof(*inp)));\n\t\t\t\tif (inp == NULL)\n\t\t\t\t\tgoto out;\n\t\t\t\t*info = inp;\n\t\t\t\tinp = *info + nelem;\n\t\t\t}\n\t\t\tDPRINTF((\"nelements = %\" SIZE_T_FORMAT \"u\\n\",\n\t\t\t    nelements));\n\t\t\tfor (j = 0; j < nelements && i < sh.sh_properties; \n\t\t\t    j++, i++) \n\t\t\t{\n\t\t\t\tuint32_t l = CDF_GETUINT32(q, o);\n\t\t\t\tinp[i].pi_str.s_len = l;\n\t\t\t\tinp[i].pi_str.s_buf = (const char *)\n\t\t\t\t    (const void *)(&q[o4 + sizeof(l)]);\n\t\t\t\tDPRINTF((\"l = %d, r = %\" SIZE_T_FORMAT\n\t\t\t\t    \"u, s = %s\\n\", l,\n\t\t\t\t    CDF_ROUND(l, sizeof(l)),\n\t\t\t\t    inp[i].pi_str.s_buf));\n\t\t\t\tif (l & 1)\n\t\t\t\t\tl++;\n\t\t\t\to += l >> 1;\n\t\t\t\tif (q + o >= e)\n\t\t\t\t\tgoto out;\n\t\t\t\to4 = o * sizeof(uint32_t);\n\t\t\t}\n\t\t\ti--;\n\t\t\tbreak;\n\t\tcase CDF_FILETIME:\n\t\t\tif (inp[i].pi_type & CDF_VECTOR)\n\t\t\t\tgoto unknown;\n\t\t\t(void)memcpy(&tp, &q[o4], sizeof(tp));\n\t\t\tinp[i].pi_tp = CDF_TOLE8((uint64_t)tp);\n\t\t\tbreak;\n\t\tcase CDF_CLIPBOARD:\n\t\t\tif (inp[i].pi_type & CDF_VECTOR)\n\t\t\t\tgoto unknown;\n\t\t\tbreak;\n\t\tdefault:\n\t\tunknown:\n\t\t\tDPRINTF((\"Don't know how to deal with %x\\n\",\n\t\t\t    inp[i].pi_type));\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn 0;\nout:\n\tfree(*info);\n\treturn -1;\n}",
    "diff": "@@ -820,7 +820,7 @@ cdf_read_property_info(const cdf_stream_t *sst, const cdf_header_t *h,\n \t\tq = (const uint8_t *)(const void *)\n \t\t    ((const char *)(const void *)p + ofs\n \t\t    - 2 * sizeof(uint32_t));\n-\t\tif (q > e) {\n+\t\tif (q < p || q > e) {\n \t\t\tDPRINTF((\"Ran of the end %p > %p\\n\", q, e));\n \t\t\tgoto out;\n \t\t}\n",
    "critical_vars": [
      "q"
    ],
    "variable_definitions": {
      "q": "const uint8_t *p, *q, *e;"
    },
    "variable_types": {
      "q": "integer pointer"
    },
    "type_mapping": {
      "q": "integer pointer"
    },
    "cve": "CVE-2014-3587",
    "vulnerable_line": "if (q > e) {",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The line checks if 'q', which is calculated with the potential for overflow, is greater than 'e'. If overflow occurs, 'q' could have an incorrect large value, allowing remote attackers to exploit this by crashing the application."
  },
  {
    "vulcode": "int lzxd_decompress(struct lzxd_stream *lzx, off_t out_bytes) {\n  /* bitstream and huffman reading variables */\n  register unsigned int bit_buffer;\n  register int bits_left, i=0;\n  unsigned char *i_ptr, *i_end;\n  register unsigned short sym;\n\n  int match_length, length_footer, extra, verbatim_bits, bytes_todo;\n  int this_run, main_element, aligned_bits, j;\n  unsigned char *window, *runsrc, *rundest, buf[12];\n  unsigned int frame_size=0, end_frame, match_offset, window_posn;\n  unsigned int R0, R1, R2;\n\n  /* easy answers */\n  if (!lzx || (out_bytes < 0)) return MSPACK_ERR_ARGS;\n  if (lzx->error) return lzx->error;\n\n  /* flush out any stored-up bytes before we begin */\n  i = lzx->o_end - lzx->o_ptr;\n  if ((off_t) i > out_bytes) i = (int) out_bytes;\n  if (i) {\n    if (lzx->sys->write(lzx->output, lzx->o_ptr, i) != i) {\n      return lzx->error = MSPACK_ERR_WRITE;\n    }\n    lzx->o_ptr  += i;\n    lzx->offset += i;\n    out_bytes   -= i;\n  }\n  if (out_bytes == 0) return MSPACK_ERR_OK;\n\n  /* restore local state */\n  RESTORE_BITS;\n  window = lzx->window;\n  window_posn = lzx->window_posn;\n  R0 = lzx->R0;\n  R1 = lzx->R1;\n  R2 = lzx->R2;\n\n  end_frame = (unsigned int)((lzx->offset + out_bytes) / LZX_FRAME_SIZE) + 1;\n\n  while (lzx->frame < end_frame) {\n    /* have we reached the reset interval? (if there is one?) */\n    if (lzx->reset_interval && ((lzx->frame % lzx->reset_interval) == 0)) {\n      if (lzx->block_remaining) {\n\tD((\"%d bytes remaining at reset interval\", lzx->block_remaining))\n\treturn lzx->error = MSPACK_ERR_DECRUNCH;\n      }\n\n      /* re-read the intel header and reset the huffman lengths */\n      lzxd_reset_state(lzx);\n      R0 = lzx->R0;\n      R1 = lzx->R1;\n      R2 = lzx->R2;\n    }\n\n    /* LZX DELTA format has chunk_size, not present in LZX format */\n    if (lzx->is_delta) {\n      ENSURE_BITS(16);\n      REMOVE_BITS(16);\n    }\n\n    /* read header if necessary */\n    if (!lzx->header_read) {\n      /* read 1 bit. if bit=0, intel filesize = 0.\n       * if bit=1, read intel filesize (32 bits) */\n      j = 0; READ_BITS(i, 1); if (i) { READ_BITS(i, 16); READ_BITS(j, 16); }\n      lzx->intel_filesize = (i << 16) | j;\n      lzx->header_read = 1;\n    } \n\n    /* calculate size of frame: all frames are 32k except the final frame\n     * which is 32kb or less. this can only be calculated when lzx->length\n     * has been filled in. */\n    frame_size = LZX_FRAME_SIZE;\n    if (lzx->length && (lzx->length - lzx->offset) < (off_t)frame_size) {\n      frame_size = lzx->length - lzx->offset;\n    }\n\n    /* decode until one more frame is available */\n    bytes_todo = lzx->frame_posn + frame_size - window_posn;\n    while (bytes_todo > 0) {\n      /* initialise new block, if one is needed */\n      if (lzx->block_remaining == 0) {\n\t/* realign if previous block was an odd-sized UNCOMPRESSED block */\n\tif ((lzx->block_type == LZX_BLOCKTYPE_UNCOMPRESSED) &&\n\t    (lzx->block_length & 1))\n\t{\n\t  READ_IF_NEEDED;\n\t  i_ptr++;\n\t}\n\n\t/* read block type (3 bits) and block length (24 bits) */\n\tREAD_BITS(lzx->block_type, 3);\n\tREAD_BITS(i, 16); READ_BITS(j, 8);\n\tlzx->block_remaining = lzx->block_length = (i << 8) | j;\n\t/*D((\"new block t%d len %u\", lzx->block_type, lzx->block_length))*/\n\n\t/* read individual block headers */\n\tswitch (lzx->block_type) {\n\tcase LZX_BLOCKTYPE_ALIGNED:\n\t  /* read lengths of and build aligned huffman decoding tree */\n\t  for (i = 0; i < 8; i++) { READ_BITS(j, 3); lzx->ALIGNED_len[i] = j; }\n\t  BUILD_TABLE(ALIGNED);\n\t  /* no break -- rest of aligned header is same as verbatim */\n\tcase LZX_BLOCKTYPE_VERBATIM:\n\t  /* read lengths of and build main huffman decoding tree */\n\t  READ_LENGTHS(MAINTREE, 0, 256);\n\t  READ_LENGTHS(MAINTREE, 256, LZX_NUM_CHARS + lzx->num_offsets);\n\t  BUILD_TABLE(MAINTREE);\n\t  /* if the literal 0xE8 is anywhere in the block... */\n\t  if (lzx->MAINTREE_len[0xE8] != 0) lzx->intel_started = 1;\n\t  /* read lengths of and build lengths huffman decoding tree */\n\t  READ_LENGTHS(LENGTH, 0, LZX_NUM_SECONDARY_LENGTHS);\n\t  BUILD_TABLE_MAYBE_EMPTY(LENGTH);\n\t  break;\n\n\tcase LZX_BLOCKTYPE_UNCOMPRESSED:\n\t  /* because we can't assume otherwise */\n\t  lzx->intel_started = 1;\n\n\t  /* read 1-16 (not 0-15) bits to align to bytes */\n\t  ENSURE_BITS(16);\n\t  if (bits_left > 16) i_ptr -= 2;\n\t  bits_left = 0; bit_buffer = 0;\n\n\t  /* read 12 bytes of stored R0 / R1 / R2 values */\n\t  for (rundest = &buf[0], i = 0; i < 12; i++) {\n\t    READ_IF_NEEDED;\n\t    *rundest++ = *i_ptr++;\n\t  }\n\t  R0 = buf[0] | (buf[1] << 8) | (buf[2]  << 16) | (buf[3]  << 24);\n\t  R1 = buf[4] | (buf[5] << 8) | (buf[6]  << 16) | (buf[7]  << 24);\n\t  R2 = buf[8] | (buf[9] << 8) | (buf[10] << 16) | (buf[11] << 24);\n\t  break;\n\n\tdefault:\n\t  D((\"bad block type\"))\n\t  return lzx->error = MSPACK_ERR_DECRUNCH;\n\t}\n      }\n\n      /* decode more of the block:\n       * run = min(what's available, what's needed) */\n      this_run = lzx->block_remaining;\n      if (this_run > bytes_todo) this_run = bytes_todo;\n\n      /* assume we decode exactly this_run bytes, for now */\n      bytes_todo           -= this_run;\n      lzx->block_remaining -= this_run;\n\n      /* decode at least this_run bytes */\n      switch (lzx->block_type) {\n      case LZX_BLOCKTYPE_VERBATIM:\n\twhile (this_run > 0) {\n\t  READ_HUFFSYM(MAINTREE, main_element);\n\t  if (main_element < LZX_NUM_CHARS) {\n\t    /* literal: 0 to LZX_NUM_CHARS-1 */\n\t    window[window_posn++] = main_element;\n\t    this_run--;\n\t  }\n\t  else {\n\t    /* match: LZX_NUM_CHARS + ((slot<<3) | length_header (3 bits)) */\n\t    main_element -= LZX_NUM_CHARS;\n\n\t    /* get match length */\n\t    match_length = main_element & LZX_NUM_PRIMARY_LENGTHS;\n\t    if (match_length == LZX_NUM_PRIMARY_LENGTHS) {\n\t      if (lzx->LENGTH_empty) {\n                D((\"LENGTH symbol needed but tree is empty\"))\n                return lzx->error = MSPACK_ERR_DECRUNCH;\n              }\n\t      READ_HUFFSYM(LENGTH, length_footer);\n\t      match_length += length_footer;\n\t    }\n\t    match_length += LZX_MIN_MATCH;\n\n\t    /* get match offset */\n\t    switch ((match_offset = (main_element >> 3))) {\n\t    case 0: match_offset = R0;                                  break;\n\t    case 1: match_offset = R1; R1=R0;        R0 = match_offset; break;\n\t    case 2: match_offset = R2; R2=R0;        R0 = match_offset; break;\n\t    case 3: match_offset = 1;  R2=R1; R1=R0; R0 = match_offset; break;\n\t    default:\n\t      extra = (match_offset >= 36) ? 17 : extra_bits[match_offset];\n\t      READ_BITS(verbatim_bits, extra);\n\t      match_offset = position_base[match_offset] - 2 + verbatim_bits;\n\t      R2 = R1; R1 = R0; R0 = match_offset;\n\t    }\n\n\t    /* LZX DELTA uses max match length to signal even longer match */\n\t    if (match_length == LZX_MAX_MATCH && lzx->is_delta) {\n\t\tint extra_len = 0;\n\t\tENSURE_BITS(3); /* 4 entry huffman tree */\n\t\tif (PEEK_BITS(1) == 0) {\n\t\t    REMOVE_BITS(1); /* '0' -> 8 extra length bits */\n\t\t    READ_BITS(extra_len, 8);\n\t\t}\n\t\telse if (PEEK_BITS(2) == 2) {\n\t\t    REMOVE_BITS(2); /* '10' -> 10 extra length bits + 0x100 */\n\t\t    READ_BITS(extra_len, 10);\n\t\t    extra_len += 0x100;\n\t\t}\n\t\telse if (PEEK_BITS(3) == 6) {\n\t\t    REMOVE_BITS(3); /* '110' -> 12 extra length bits + 0x500 */\n\t\t    READ_BITS(extra_len, 12);\n\t\t    extra_len += 0x500;\n\t\t}\n\t\telse {\n\t\t    REMOVE_BITS(3); /* '111' -> 15 extra length bits */\n\t\t    READ_BITS(extra_len, 15);\n\t\t}\n\t\tmatch_length += extra_len;\n\t    }\n\n\t    if ((window_posn + match_length) > lzx->window_size) {\n\t      D((\"match ran over window wrap\"))\n\t      return lzx->error = MSPACK_ERR_DECRUNCH;\n\t    }\n\t    \n\t    /* copy match */\n\t    rundest = &window[window_posn];\n\t    i = match_length;\n\t    /* does match offset wrap the window? */\n\t    if (match_offset > window_posn) {\n\t      if (match_offset > lzx->offset &&\n\t\t  (match_offset - window_posn) > lzx->ref_data_size)\n\t      {\n\t\tD((\"match offset beyond LZX stream\"))\n\t\treturn lzx->error = MSPACK_ERR_DECRUNCH;\n\t      }\n\t      /* j = length from match offset to end of window */\n\t      j = match_offset - window_posn;\n\t      if (j > (int) lzx->window_size) {\n\t\tD((\"match offset beyond window boundaries\"))\n\t\treturn lzx->error = MSPACK_ERR_DECRUNCH;\n\t      }\n\t      runsrc = &window[lzx->window_size - j];\n\t      if (j < i) {\n\t\t/* if match goes over the window edge, do two copy runs */\n\t\ti -= j; while (j-- > 0) *rundest++ = *runsrc++;\n\t\trunsrc = window;\n\t      }\n\t      while (i-- > 0) *rundest++ = *runsrc++;\n\t    }\n\t    else {\n\t      runsrc = rundest - match_offset;\n\t      while (i-- > 0) *rundest++ = *runsrc++;\n\t    }\n\n\t    this_run    -= match_length;\n\t    window_posn += match_length;\n\t  }\n\t} /* while (this_run > 0) */\n\tbreak;\n\n      case LZX_BLOCKTYPE_ALIGNED:\n\twhile (this_run > 0) {\n\t  READ_HUFFSYM(MAINTREE, main_element);\n\t  if (main_element < LZX_NUM_CHARS) {\n\t    /* literal: 0 to LZX_NUM_CHARS-1 */\n\t    window[window_posn++] = main_element;\n\t    this_run--;\n\t  }\n\t  else {\n\t    /* match: LZX_NUM_CHARS + ((slot<<3) | length_header (3 bits)) */\n\t    main_element -= LZX_NUM_CHARS;\n\n\t    /* get match length */\n\t    match_length = main_element & LZX_NUM_PRIMARY_LENGTHS;\n\t    if (match_length == LZX_NUM_PRIMARY_LENGTHS) {\n              if (lzx->LENGTH_empty) {\n                D((\"LENGTH symbol needed but tree is empty\"))\n                return lzx->error = MSPACK_ERR_DECRUNCH;\n              } \n\t      READ_HUFFSYM(LENGTH, length_footer);\n\t      match_length += length_footer;\n\t    }\n\t    match_length += LZX_MIN_MATCH;\n\n\t    /* get match offset */\n\t    switch ((match_offset = (main_element >> 3))) {\n\t    case 0: match_offset = R0;                             break;\n\t    case 1: match_offset = R1; R1 = R0; R0 = match_offset; break;\n\t    case 2: match_offset = R2; R2 = R0; R0 = match_offset; break;\n\t    default:\n\t      extra = (match_offset >= 36) ? 17 : extra_bits[match_offset];\n\t      match_offset = position_base[match_offset] - 2;\n\t      if (extra > 3) {\n\t\t/* verbatim and aligned bits */\n\t\textra -= 3;\n\t\tREAD_BITS(verbatim_bits, extra);\n\t\tmatch_offset += (verbatim_bits << 3);\n\t\tREAD_HUFFSYM(ALIGNED, aligned_bits);\n\t\tmatch_offset += aligned_bits;\n\t      }\n\t      else if (extra == 3) {\n\t\t/* aligned bits only */\n\t\tREAD_HUFFSYM(ALIGNED, aligned_bits);\n\t\tmatch_offset += aligned_bits;\n\t      }\n\t      else if (extra > 0) { /* extra==1, extra==2 */\n\t\t/* verbatim bits only */\n\t\tREAD_BITS(verbatim_bits, extra);\n\t\tmatch_offset += verbatim_bits;\n\t      }\n\t      else /* extra == 0 */ {\n\t\t/* ??? not defined in LZX specification! */\n\t\tmatch_offset = 1;\n\t      }\n\t      /* update repeated offset LRU queue */\n\t      R2 = R1; R1 = R0; R0 = match_offset;\n\t    }\n\n\t    /* LZX DELTA uses max match length to signal even longer match */\n\t    if (match_length == LZX_MAX_MATCH && lzx->is_delta) {\n\t\tint extra_len = 0;\n\t\tENSURE_BITS(3); /* 4 entry huffman tree */\n\t\tif (PEEK_BITS(1) == 0) {\n\t\t    REMOVE_BITS(1); /* '0' -> 8 extra length bits */\n\t\t    READ_BITS(extra_len, 8);\n\t\t}\n\t\telse if (PEEK_BITS(2) == 2) {\n\t\t    REMOVE_BITS(2); /* '10' -> 10 extra length bits + 0x100 */\n\t\t    READ_BITS(extra_len, 10);\n\t\t    extra_len += 0x100;\n\t\t}\n\t\telse if (PEEK_BITS(3) == 6) {\n\t\t    REMOVE_BITS(3); /* '110' -> 12 extra length bits + 0x500 */\n\t\t    READ_BITS(extra_len, 12);\n\t\t    extra_len += 0x500;\n\t\t}\n\t\telse {\n\t\t    REMOVE_BITS(3); /* '111' -> 15 extra length bits */\n\t\t    READ_BITS(extra_len, 15);\n\t\t}\n\t\tmatch_length += extra_len;\n\t    }\n\n\t    if ((window_posn + match_length) > lzx->window_size) {\n\t      D((\"match ran over window wrap\"))\n\t      return lzx->error = MSPACK_ERR_DECRUNCH;\n\t    }\n\n\t    /* copy match */\n\t    rundest = &window[window_posn];\n\t    i = match_length;\n\t    /* does match offset wrap the window? */\n\t    if (match_offset > window_posn) {\n\t      if (match_offset > lzx->offset &&\n\t\t  (match_offset - window_posn) > lzx->ref_data_size)\n\t      {\n\t\tD((\"match offset beyond LZX stream\"))\n\t\treturn lzx->error = MSPACK_ERR_DECRUNCH;\n\t      }\n\t      /* j = length from match offset to end of window */\n\t      j = match_offset - window_posn;\n\t      if (j > (int) lzx->window_size) {\n\t\tD((\"match offset beyond window boundaries\"))\n\t\treturn lzx->error = MSPACK_ERR_DECRUNCH;\n\t      }\n\t      runsrc = &window[lzx->window_size - j];\n\t      if (j < i) {\n\t\t/* if match goes over the window edge, do two copy runs */\n\t\ti -= j; while (j-- > 0) *rundest++ = *runsrc++;\n\t\trunsrc = window;\n\t      }\n\t      while (i-- > 0) *rundest++ = *runsrc++;\n\t    }\n\t    else {\n\t      runsrc = rundest - match_offset;\n\t      while (i-- > 0) *rundest++ = *runsrc++;\n\t    }\n\n\t    this_run    -= match_length;\n\t    window_posn += match_length;\n\t  }\n\t} /* while (this_run > 0) */\n\tbreak;\n\n      case LZX_BLOCKTYPE_UNCOMPRESSED:\n\t/* as this_run is limited not to wrap a frame, this also means it\n\t * won't wrap the window (as the window is a multiple of 32k) */\n\trundest = &window[window_posn];\n\twindow_posn += this_run;\n\twhile (this_run > 0) {\n\t  if ((i = i_end - i_ptr) == 0) {\n\t    READ_IF_NEEDED;\n\t  }\n\t  else {\n\t    if (i > this_run) i = this_run;\n\t    lzx->sys->copy(i_ptr, rundest, (size_t) i);\n\t    rundest  += i;\n\t    i_ptr    += i;\n\t    this_run -= i;\n\t  }\n\t}\n\tbreak;\n\n      default:\n\treturn lzx->error = MSPACK_ERR_DECRUNCH; /* might as well */\n      }\n\n      /* did the final match overrun our desired this_run length? */\n      if (this_run < 0) {\n\tif ((unsigned int)(-this_run) > lzx->block_remaining) {\n\t  D((\"overrun went past end of block by %d (%d remaining)\",\n\t     -this_run, lzx->block_remaining ))\n\t  return lzx->error = MSPACK_ERR_DECRUNCH;\n\t}\n\tlzx->block_remaining -= -this_run;\n      }\n    } /* while (bytes_todo > 0) */\n\n    /* streams don't extend over frame boundaries */\n    if ((window_posn - lzx->frame_posn) != frame_size) {\n      D((\"decode beyond output frame limits! %d != %d\",\n\t window_posn - lzx->frame_posn, frame_size))\n      return lzx->error = MSPACK_ERR_DECRUNCH;\n    }\n\n    /* re-align input bitstream */\n    if (bits_left > 0) ENSURE_BITS(16);\n    if (bits_left & 15) REMOVE_BITS(bits_left & 15);\n\n    /* check that we've used all of the previous frame first */\n    if (lzx->o_ptr != lzx->o_end) {\n      D((\"%ld avail bytes, new %d frame\",\n          (long)(lzx->o_end - lzx->o_ptr), frame_size))\n      return lzx->error = MSPACK_ERR_DECRUNCH;\n    }\n\n    /* does this intel block _really_ need decoding? */\n    if (lzx->intel_started && lzx->intel_filesize &&\n\t(lzx->frame <= 32768) && (frame_size > 10))\n    {\n      unsigned char *data    = &lzx->e8_buf[0];\n      unsigned char *dataend = &lzx->e8_buf[frame_size - 10];\n      signed int curpos      = lzx->intel_curpos;\n      signed int filesize    = lzx->intel_filesize;\n      signed int abs_off, rel_off;\n\n      /* copy e8 block to the e8 buffer and tweak if needed */\n      lzx->o_ptr = data;\n      lzx->sys->copy(&lzx->window[lzx->frame_posn], data, frame_size);\n\n      while (data < dataend) {\n\tif (*data++ != 0xE8) { curpos++; continue; }\n\tabs_off = data[0] | (data[1]<<8) | (data[2]<<16) | (data[3]<<24);\n\tif ((abs_off >= -curpos) && (abs_off < filesize)) {\n\t  rel_off = (abs_off >= 0) ? abs_off - curpos : abs_off + filesize;\n\t  data[0] = (unsigned char) rel_off;\n\t  data[1] = (unsigned char) (rel_off >> 8);\n\t  data[2] = (unsigned char) (rel_off >> 16);\n\t  data[3] = (unsigned char) (rel_off >> 24);\n\t}\n\tdata += 4;\n\tcurpos += 5;\n      }\n      lzx->intel_curpos += frame_size;\n    }\n    else {\n      lzx->o_ptr = &lzx->window[lzx->frame_posn];\n      if (lzx->intel_filesize) lzx->intel_curpos += frame_size;\n    }\n    lzx->o_end = &lzx->o_ptr[frame_size];\n\n    /* write a frame */\n    i = (out_bytes < (off_t)frame_size) ? (unsigned int)out_bytes : frame_size;\n    if (lzx->sys->write(lzx->output, lzx->o_ptr, i) != i) {\n      return lzx->error = MSPACK_ERR_WRITE;\n    }\n    lzx->o_ptr  += i;\n    lzx->offset += i;\n    out_bytes   -= i;\n\n    /* advance frame start position */\n    lzx->frame_posn += frame_size;\n    lzx->frame++;\n\n    /* wrap window / frame position pointers */\n    if (window_posn == lzx->window_size)     window_posn = 0;\n    if (lzx->frame_posn == lzx->window_size) lzx->frame_posn = 0;\n\n  } /* while (lzx->frame < end_frame) */\n\n  if (out_bytes) {\n    D((\"bytes left to output\"))\n    return lzx->error = MSPACK_ERR_DECRUNCH;\n  }\n\n  /* store local state */\n  STORE_BITS;\n  lzx->window_posn = window_posn;\n  lzx->R0 = R0;\n  lzx->R1 = R1;\n  lzx->R2 = R2;\n\n  return MSPACK_ERR_OK;\n}",
    "diff": "@@ -506,8 +506,7 @@ int lzxd_decompress(struct lzxd_stream *lzx, off_t out_bytes) {\n \t  lzx->intel_started = 1;\n \n \t  /* read 1-16 (not 0-15) bits to align to bytes */\n-\t  ENSURE_BITS(16);\n-\t  if (bits_left > 16) i_ptr -= 2;\n+\t  if (bits_left == 0) ENSURE_BITS(16);\n \t  bits_left = 0; bit_buffer = 0;\n \n \t  /* read 12 bytes of stored R0 / R1 / R2 values */\n",
    "critical_vars": [
      "bits_left"
    ],
    "variable_definitions": {
      "bits_left": "register int bits_left, i=0;"
    },
    "variable_types": {
      "bits_left": "integer"
    },
    "type_mapping": {
      "bits_left": "Integer"
    },
    "cve": "CVE-2015-4471",
    "vulnerable_line": "if (bits_left > 16) i_ptr -= 2;",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Underflow",
    "reasoning": "When bits_left is incorrectly set to 0 or a negative value, the check allows i_ptr to be decremented inappropriately, potentially leading to an under-read condition and crashing the application."
  },
  {
    "vulcode": "int lzxd_decompress(struct lzxd_stream *lzx, off_t out_bytes) {\n  /* bitstream and huffman reading variables */\n  register unsigned int bit_buffer;\n  register int bits_left, i=0;\n  unsigned char *i_ptr, *i_end;\n  register unsigned short sym;\n\n  int match_length, length_footer, extra, verbatim_bits, bytes_todo;\n  int this_run, main_element, aligned_bits, j;\n  unsigned char *window, *runsrc, *rundest, buf[12];\n  unsigned int frame_size=0, end_frame, match_offset, window_posn;\n  unsigned int R0, R1, R2;\n\n  /* easy answers */\n  if (!lzx || (out_bytes < 0)) return MSPACK_ERR_ARGS;\n  if (lzx->error) return lzx->error;\n\n  /* flush out any stored-up bytes before we begin */\n  i = lzx->o_end - lzx->o_ptr;\n  if ((off_t) i > out_bytes) i = (int) out_bytes;\n  if (i) {\n    if (lzx->sys->write(lzx->output, lzx->o_ptr, i) != i) {\n      return lzx->error = MSPACK_ERR_WRITE;\n    }\n    lzx->o_ptr  += i;\n    lzx->offset += i;\n    out_bytes   -= i;\n  }\n  if (out_bytes == 0) return MSPACK_ERR_OK;\n\n  /* restore local state */\n  RESTORE_BITS;\n  window = lzx->window;\n  window_posn = lzx->window_posn;\n  R0 = lzx->R0;\n  R1 = lzx->R1;\n  R2 = lzx->R2;\n\n  end_frame = (unsigned int)((lzx->offset + out_bytes) / LZX_FRAME_SIZE) + 1;\n\n  while (lzx->frame < end_frame) {\n    /* have we reached the reset interval? (if there is one?) */\n    if (lzx->reset_interval && ((lzx->frame % lzx->reset_interval) == 0)) {\n      if (lzx->block_remaining) {\n\tD((\"%d bytes remaining at reset interval\", lzx->block_remaining))\n\treturn lzx->error = MSPACK_ERR_DECRUNCH;\n      }\n\n      /* re-read the intel header and reset the huffman lengths */\n      lzxd_reset_state(lzx);\n      R0 = lzx->R0;\n      R1 = lzx->R1;\n      R2 = lzx->R2;\n    }\n\n    /* LZX DELTA format has chunk_size, not present in LZX format */\n    if (lzx->is_delta) {\n      ENSURE_BITS(16);\n      REMOVE_BITS(16);\n    }\n\n    /* read header if necessary */\n    if (!lzx->header_read) {\n      /* read 1 bit. if bit=0, intel filesize = 0.\n       * if bit=1, read intel filesize (32 bits) */\n      j = 0; READ_BITS(i, 1); if (i) { READ_BITS(i, 16); READ_BITS(j, 16); }\n      lzx->intel_filesize = (i << 16) | j;\n      lzx->header_read = 1;\n    } \n\n    /* calculate size of frame: all frames are 32k except the final frame\n     * which is 32kb or less. this can only be calculated when lzx->length\n     * has been filled in. */\n    frame_size = LZX_FRAME_SIZE;\n    if (lzx->length && (lzx->length - lzx->offset) < (off_t)frame_size) {\n      frame_size = lzx->length - lzx->offset;\n    }\n\n    /* decode until one more frame is available */\n    bytes_todo = lzx->frame_posn + frame_size - window_posn;\n    while (bytes_todo > 0) {\n      /* initialise new block, if one is needed */\n      if (lzx->block_remaining == 0) {\n\t/* realign if previous block was an odd-sized UNCOMPRESSED block */\n\tif ((lzx->block_type == LZX_BLOCKTYPE_UNCOMPRESSED) &&\n\t    (lzx->block_length & 1))\n\t{\n\t  READ_IF_NEEDED;\n\t  i_ptr++;\n\t}\n\n\t/* read block type (3 bits) and block length (24 bits) */\n\tREAD_BITS(lzx->block_type, 3);\n\tREAD_BITS(i, 16); READ_BITS(j, 8);\n\tlzx->block_remaining = lzx->block_length = (i << 8) | j;\n\t/*D((\"new block t%d len %u\", lzx->block_type, lzx->block_length))*/\n\n\t/* read individual block headers */\n\tswitch (lzx->block_type) {\n\tcase LZX_BLOCKTYPE_ALIGNED:\n\t  /* read lengths of and build aligned huffman decoding tree */\n\t  for (i = 0; i < 8; i++) { READ_BITS(j, 3); lzx->ALIGNED_len[i] = j; }\n\t  BUILD_TABLE(ALIGNED);\n\t  /* no break -- rest of aligned header is same as verbatim */\n\tcase LZX_BLOCKTYPE_VERBATIM:\n\t  /* read lengths of and build main huffman decoding tree */\n\t  READ_LENGTHS(MAINTREE, 0, 256);\n\t  READ_LENGTHS(MAINTREE, 256, LZX_NUM_CHARS + lzx->num_offsets);\n\t  BUILD_TABLE(MAINTREE);\n\t  /* if the literal 0xE8 is anywhere in the block... */\n\t  if (lzx->MAINTREE_len[0xE8] != 0) lzx->intel_started = 1;\n\t  /* read lengths of and build lengths huffman decoding tree */\n\t  READ_LENGTHS(LENGTH, 0, LZX_NUM_SECONDARY_LENGTHS);\n\t  BUILD_TABLE_MAYBE_EMPTY(LENGTH);\n\t  break;\n\n\tcase LZX_BLOCKTYPE_UNCOMPRESSED:\n\t  /* because we can't assume otherwise */\n\t  lzx->intel_started = 1;\n\n\t  /* read 1-16 (not 0-15) bits to align to bytes */\n\t  ENSURE_BITS(16);\n\t  if (bits_left > 16) i_ptr -= 2;\n\t  bits_left = 0; bit_buffer = 0;\n\n\t  /* read 12 bytes of stored R0 / R1 / R2 values */\n\t  for (rundest = &buf[0], i = 0; i < 12; i++) {\n\t    READ_IF_NEEDED;\n\t    *rundest++ = *i_ptr++;\n\t  }\n\t  R0 = buf[0] | (buf[1] << 8) | (buf[2]  << 16) | (buf[3]  << 24);\n\t  R1 = buf[4] | (buf[5] << 8) | (buf[6]  << 16) | (buf[7]  << 24);\n\t  R2 = buf[8] | (buf[9] << 8) | (buf[10] << 16) | (buf[11] << 24);\n\t  break;\n\n\tdefault:\n\t  D((\"bad block type\"))\n\t  return lzx->error = MSPACK_ERR_DECRUNCH;\n\t}\n      }\n\n      /* decode more of the block:\n       * run = min(what's available, what's needed) */\n      this_run = lzx->block_remaining;\n      if (this_run > bytes_todo) this_run = bytes_todo;\n\n      /* assume we decode exactly this_run bytes, for now */\n      bytes_todo           -= this_run;\n      lzx->block_remaining -= this_run;\n\n      /* decode at least this_run bytes */\n      switch (lzx->block_type) {\n      case LZX_BLOCKTYPE_VERBATIM:\n\twhile (this_run > 0) {\n\t  READ_HUFFSYM(MAINTREE, main_element);\n\t  if (main_element < LZX_NUM_CHARS) {\n\t    /* literal: 0 to LZX_NUM_CHARS-1 */\n\t    window[window_posn++] = main_element;\n\t    this_run--;\n\t  }\n\t  else {\n\t    /* match: LZX_NUM_CHARS + ((slot<<3) | length_header (3 bits)) */\n\t    main_element -= LZX_NUM_CHARS;\n\n\t    /* get match length */\n\t    match_length = main_element & LZX_NUM_PRIMARY_LENGTHS;\n\t    if (match_length == LZX_NUM_PRIMARY_LENGTHS) {\n\t      if (lzx->LENGTH_empty) {\n                D((\"LENGTH symbol needed but tree is empty\"))\n                return lzx->error = MSPACK_ERR_DECRUNCH;\n              }\n\t      READ_HUFFSYM(LENGTH, length_footer);\n\t      match_length += length_footer;\n\t    }\n\t    match_length += LZX_MIN_MATCH;\n\n\t    /* get match offset */\n\t    switch ((match_offset = (main_element >> 3))) {\n\t    case 0: match_offset = R0;                                  break;\n\t    case 1: match_offset = R1; R1=R0;        R0 = match_offset; break;\n\t    case 2: match_offset = R2; R2=R0;        R0 = match_offset; break;\n\t    case 3: match_offset = 1;  R2=R1; R1=R0; R0 = match_offset; break;\n\t    default:\n\t      extra = (match_offset >= 36) ? 17 : extra_bits[match_offset];\n\t      READ_BITS(verbatim_bits, extra);\n\t      match_offset = position_base[match_offset] - 2 + verbatim_bits;\n\t      R2 = R1; R1 = R0; R0 = match_offset;\n\t    }\n\n\t    /* LZX DELTA uses max match length to signal even longer match */\n\t    if (match_length == LZX_MAX_MATCH && lzx->is_delta) {\n\t\tint extra_len = 0;\n\t\tENSURE_BITS(3); /* 4 entry huffman tree */\n\t\tif (PEEK_BITS(1) == 0) {\n\t\t    REMOVE_BITS(1); /* '0' -> 8 extra length bits */\n\t\t    READ_BITS(extra_len, 8);\n\t\t}\n\t\telse if (PEEK_BITS(2) == 2) {\n\t\t    REMOVE_BITS(2); /* '10' -> 10 extra length bits + 0x100 */\n\t\t    READ_BITS(extra_len, 10);\n\t\t    extra_len += 0x100;\n\t\t}\n\t\telse if (PEEK_BITS(3) == 6) {\n\t\t    REMOVE_BITS(3); /* '110' -> 12 extra length bits + 0x500 */\n\t\t    READ_BITS(extra_len, 12);\n\t\t    extra_len += 0x500;\n\t\t}\n\t\telse {\n\t\t    REMOVE_BITS(3); /* '111' -> 15 extra length bits */\n\t\t    READ_BITS(extra_len, 15);\n\t\t}\n\t\tmatch_length += extra_len;\n\t    }\n\n\t    if ((window_posn + match_length) > lzx->window_size) {\n\t      D((\"match ran over window wrap\"))\n\t      return lzx->error = MSPACK_ERR_DECRUNCH;\n\t    }\n\t    \n\t    /* copy match */\n\t    rundest = &window[window_posn];\n\t    i = match_length;\n\t    /* does match offset wrap the window? */\n\t    if (match_offset > window_posn) {\n\t      if (match_offset > lzx->offset &&\n\t\t  (match_offset - window_posn) > lzx->ref_data_size)\n\t      {\n\t\tD((\"match offset beyond LZX stream\"))\n\t\treturn lzx->error = MSPACK_ERR_DECRUNCH;\n\t      }\n\t      /* j = length from match offset to end of window */\n\t      j = match_offset - window_posn;\n\t      if (j > (int) lzx->window_size) {\n\t\tD((\"match offset beyond window boundaries\"))\n\t\treturn lzx->error = MSPACK_ERR_DECRUNCH;\n\t      }\n\t      runsrc = &window[lzx->window_size - j];\n\t      if (j < i) {\n\t\t/* if match goes over the window edge, do two copy runs */\n\t\ti -= j; while (j-- > 0) *rundest++ = *runsrc++;\n\t\trunsrc = window;\n\t      }\n\t      while (i-- > 0) *rundest++ = *runsrc++;\n\t    }\n\t    else {\n\t      runsrc = rundest - match_offset;\n\t      while (i-- > 0) *rundest++ = *runsrc++;\n\t    }\n\n\t    this_run    -= match_length;\n\t    window_posn += match_length;\n\t  }\n\t} /* while (this_run > 0) */\n\tbreak;\n\n      case LZX_BLOCKTYPE_ALIGNED:\n\twhile (this_run > 0) {\n\t  READ_HUFFSYM(MAINTREE, main_element);\n\t  if (main_element < LZX_NUM_CHARS) {\n\t    /* literal: 0 to LZX_NUM_CHARS-1 */\n\t    window[window_posn++] = main_element;\n\t    this_run--;\n\t  }\n\t  else {\n\t    /* match: LZX_NUM_CHARS + ((slot<<3) | length_header (3 bits)) */\n\t    main_element -= LZX_NUM_CHARS;\n\n\t    /* get match length */\n\t    match_length = main_element & LZX_NUM_PRIMARY_LENGTHS;\n\t    if (match_length == LZX_NUM_PRIMARY_LENGTHS) {\n              if (lzx->LENGTH_empty) {\n                D((\"LENGTH symbol needed but tree is empty\"))\n                return lzx->error = MSPACK_ERR_DECRUNCH;\n              } \n\t      READ_HUFFSYM(LENGTH, length_footer);\n\t      match_length += length_footer;\n\t    }\n\t    match_length += LZX_MIN_MATCH;\n\n\t    /* get match offset */\n\t    switch ((match_offset = (main_element >> 3))) {\n\t    case 0: match_offset = R0;                             break;\n\t    case 1: match_offset = R1; R1 = R0; R0 = match_offset; break;\n\t    case 2: match_offset = R2; R2 = R0; R0 = match_offset; break;\n\t    default:\n\t      extra = (match_offset >= 36) ? 17 : extra_bits[match_offset];\n\t      match_offset = position_base[match_offset] - 2;\n\t      if (extra > 3) {\n\t\t/* verbatim and aligned bits */\n\t\textra -= 3;\n\t\tREAD_BITS(verbatim_bits, extra);\n\t\tmatch_offset += (verbatim_bits << 3);\n\t\tREAD_HUFFSYM(ALIGNED, aligned_bits);\n\t\tmatch_offset += aligned_bits;\n\t      }\n\t      else if (extra == 3) {\n\t\t/* aligned bits only */\n\t\tREAD_HUFFSYM(ALIGNED, aligned_bits);\n\t\tmatch_offset += aligned_bits;\n\t      }\n\t      else if (extra > 0) { /* extra==1, extra==2 */\n\t\t/* verbatim bits only */\n\t\tREAD_BITS(verbatim_bits, extra);\n\t\tmatch_offset += verbatim_bits;\n\t      }\n\t      else /* extra == 0 */ {\n\t\t/* ??? not defined in LZX specification! */\n\t\tmatch_offset = 1;\n\t      }\n\t      /* update repeated offset LRU queue */\n\t      R2 = R1; R1 = R0; R0 = match_offset;\n\t    }\n\n\t    /* LZX DELTA uses max match length to signal even longer match */\n\t    if (match_length == LZX_MAX_MATCH && lzx->is_delta) {\n\t\tint extra_len = 0;\n\t\tENSURE_BITS(3); /* 4 entry huffman tree */\n\t\tif (PEEK_BITS(1) == 0) {\n\t\t    REMOVE_BITS(1); /* '0' -> 8 extra length bits */\n\t\t    READ_BITS(extra_len, 8);\n\t\t}\n\t\telse if (PEEK_BITS(2) == 2) {\n\t\t    REMOVE_BITS(2); /* '10' -> 10 extra length bits + 0x100 */\n\t\t    READ_BITS(extra_len, 10);\n\t\t    extra_len += 0x100;\n\t\t}\n\t\telse if (PEEK_BITS(3) == 6) {\n\t\t    REMOVE_BITS(3); /* '110' -> 12 extra length bits + 0x500 */\n\t\t    READ_BITS(extra_len, 12);\n\t\t    extra_len += 0x500;\n\t\t}\n\t\telse {\n\t\t    REMOVE_BITS(3); /* '111' -> 15 extra length bits */\n\t\t    READ_BITS(extra_len, 15);\n\t\t}\n\t\tmatch_length += extra_len;\n\t    }\n\n\t    if ((window_posn + match_length) > lzx->window_size) {\n\t      D((\"match ran over window wrap\"))\n\t      return lzx->error = MSPACK_ERR_DECRUNCH;\n\t    }\n\n\t    /* copy match */\n\t    rundest = &window[window_posn];\n\t    i = match_length;\n\t    /* does match offset wrap the window? */\n\t    if (match_offset > window_posn) {\n\t      if (match_offset > lzx->offset &&\n\t\t  (match_offset - window_posn) > lzx->ref_data_size)\n\t      {\n\t\tD((\"match offset beyond LZX stream\"))\n\t\treturn lzx->error = MSPACK_ERR_DECRUNCH;\n\t      }\n\t      /* j = length from match offset to end of window */\n\t      j = match_offset - window_posn;\n\t      if (j > (int) lzx->window_size) {\n\t\tD((\"match offset beyond window boundaries\"))\n\t\treturn lzx->error = MSPACK_ERR_DECRUNCH;\n\t      }\n\t      runsrc = &window[lzx->window_size - j];\n\t      if (j < i) {\n\t\t/* if match goes over the window edge, do two copy runs */\n\t\ti -= j; while (j-- > 0) *rundest++ = *runsrc++;\n\t\trunsrc = window;\n\t      }\n\t      while (i-- > 0) *rundest++ = *runsrc++;\n\t    }\n\t    else {\n\t      runsrc = rundest - match_offset;\n\t      while (i-- > 0) *rundest++ = *runsrc++;\n\t    }\n\n\t    this_run    -= match_length;\n\t    window_posn += match_length;\n\t  }\n\t} /* while (this_run > 0) */\n\tbreak;\n\n      case LZX_BLOCKTYPE_UNCOMPRESSED:\n\t/* as this_run is limited not to wrap a frame, this also means it\n\t * won't wrap the window (as the window is a multiple of 32k) */\n\trundest = &window[window_posn];\n\twindow_posn += this_run;\n\twhile (this_run > 0) {\n\t  if ((i = i_end - i_ptr) == 0) {\n\t    READ_IF_NEEDED;\n\t  }\n\t  else {\n\t    if (i > this_run) i = this_run;\n\t    lzx->sys->copy(i_ptr, rundest, (size_t) i);\n\t    rundest  += i;\n\t    i_ptr    += i;\n\t    this_run -= i;\n\t  }\n\t}\n\tbreak;\n\n      default:\n\treturn lzx->error = MSPACK_ERR_DECRUNCH; /* might as well */\n      }\n\n      /* did the final match overrun our desired this_run length? */\n      if (this_run < 0) {\n\tif ((unsigned int)(-this_run) > lzx->block_remaining) {\n\t  D((\"overrun went past end of block by %d (%d remaining)\",\n\t     -this_run, lzx->block_remaining ))\n\t  return lzx->error = MSPACK_ERR_DECRUNCH;\n\t}\n\tlzx->block_remaining -= -this_run;\n      }\n    } /* while (bytes_todo > 0) */\n\n    /* streams don't extend over frame boundaries */\n    if ((window_posn - lzx->frame_posn) != frame_size) {\n      D((\"decode beyond output frame limits! %d != %d\",\n\t window_posn - lzx->frame_posn, frame_size))\n      return lzx->error = MSPACK_ERR_DECRUNCH;\n    }\n\n    /* re-align input bitstream */\n    if (bits_left > 0) ENSURE_BITS(16);\n    if (bits_left & 15) REMOVE_BITS(bits_left & 15);\n\n    /* check that we've used all of the previous frame first */\n    if (lzx->o_ptr != lzx->o_end) {\n      D((\"%ld avail bytes, new %d frame\",\n          (long)(lzx->o_end - lzx->o_ptr), frame_size))\n      return lzx->error = MSPACK_ERR_DECRUNCH;\n    }\n\n    /* does this intel block _really_ need decoding? */\n    if (lzx->intel_started && lzx->intel_filesize &&\n\t(lzx->frame <= 32768) && (frame_size > 10))\n    {\n      unsigned char *data    = &lzx->e8_buf[0];\n      unsigned char *dataend = &lzx->e8_buf[frame_size - 10];\n      signed int curpos      = lzx->intel_curpos;\n      signed int filesize    = lzx->intel_filesize;\n      signed int abs_off, rel_off;\n\n      /* copy e8 block to the e8 buffer and tweak if needed */\n      lzx->o_ptr = data;\n      lzx->sys->copy(&lzx->window[lzx->frame_posn], data, frame_size);\n\n      while (data < dataend) {\n\tif (*data++ != 0xE8) { curpos++; continue; }\n\tabs_off = data[0] | (data[1]<<8) | (data[2]<<16) | (data[3]<<24);\n\tif ((abs_off >= -curpos) && (abs_off < filesize)) {\n\t  rel_off = (abs_off >= 0) ? abs_off - curpos : abs_off + filesize;\n\t  data[0] = (unsigned char) rel_off;\n\t  data[1] = (unsigned char) (rel_off >> 8);\n\t  data[2] = (unsigned char) (rel_off >> 16);\n\t  data[3] = (unsigned char) (rel_off >> 24);\n\t}\n\tdata += 4;\n\tcurpos += 5;\n      }\n      lzx->intel_curpos += frame_size;\n    }\n    else {\n      lzx->o_ptr = &lzx->window[lzx->frame_posn];\n      if (lzx->intel_filesize) lzx->intel_curpos += frame_size;\n    }\n    lzx->o_end = &lzx->o_ptr[frame_size];\n\n    /* write a frame */\n    i = (out_bytes < (off_t)frame_size) ? (unsigned int)out_bytes : frame_size;\n    if (lzx->sys->write(lzx->output, lzx->o_ptr, i) != i) {\n      return lzx->error = MSPACK_ERR_WRITE;\n    }\n    lzx->o_ptr  += i;\n    lzx->offset += i;\n    out_bytes   -= i;\n\n    /* advance frame start position */\n    lzx->frame_posn += frame_size;\n    lzx->frame++;\n\n    /* wrap window / frame position pointers */\n    if (window_posn == lzx->window_size)     window_posn = 0;\n    if (lzx->frame_posn == lzx->window_size) lzx->frame_posn = 0;\n\n  } /* while (lzx->frame < end_frame) */\n\n  if (out_bytes) {\n    D((\"bytes left to output\"))\n    return lzx->error = MSPACK_ERR_DECRUNCH;\n  }\n\n  /* store local state */\n  STORE_BITS;\n  lzx->window_posn = window_posn;\n  lzx->R0 = R0;\n  lzx->R1 = R1;\n  lzx->R2 = R2;\n\n  return MSPACK_ERR_OK;\n}",
    "diff": "@@ -506,8 +506,7 @@ int lzxd_decompress(struct lzxd_stream *lzx, off_t out_bytes) {\n \t  lzx->intel_started = 1;\n \n \t  /* read 1-16 (not 0-15) bits to align to bytes */\n-\t  ENSURE_BITS(16);\n-\t  if (bits_left > 16) i_ptr -= 2;\n+\t  if (bits_left == 0) ENSURE_BITS(16);\n \t  bits_left = 0; bit_buffer = 0;\n \n \t  /* read 12 bytes of stored R0 / R1 / R2 values */\n",
    "critical_vars": [
      "bits_left"
    ],
    "variable_definitions": {
      "bits_left": "register int bits_left, i=0;"
    },
    "variable_types": {
      "bits_left": "integer"
    },
    "type_mapping": {
      "bits_left": "Integer"
    },
    "cve": "CVE-2015-4471",
    "vulnerable_line": "if (bits_left > 16) i_ptr -= 2;",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Underflow",
    "reasoning": "When bits_left is 0, subtracting 2 from i_ptr can cause an underflow, leading to incorrect memory access and potential denial of service."
  },
  {
    "vulcode": "parse_tsquery(char *buf,\n\t\t\t  PushFunction pushval,\n\t\t\t  Datum opaque,\n\t\t\t  bool isplain)\n{\n\tstruct TSQueryParserStateData state;\n\tint\t\t\ti;\n\tTSQuery\t\tquery;\n\tint\t\t\tcommonlen;\n\tQueryItem  *ptr;\n\tListCell   *cell;\n\n\t/* init state */\n\tstate.buffer = buf;\n\tstate.buf = buf;\n\tstate.state = (isplain) ? WAITSINGLEOPERAND : WAITFIRSTOPERAND;\n\tstate.count = 0;\n\tstate.polstr = NIL;\n\n\t/* init value parser's state */\n\tstate.valstate = init_tsvector_parser(state.buffer, true, true);\n\n\t/* init list of operand */\n\tstate.sumlen = 0;\n\tstate.lenop = 64;\n\tstate.curop = state.op = (char *) palloc(state.lenop);\n\t*(state.curop) = '\\0';\n\n\t/* parse query & make polish notation (postfix, but in reverse order) */\n\tmakepol(&state, pushval, opaque);\n\n\tclose_tsvector_parser(state.valstate);\n\n\tif (list_length(state.polstr) == 0)\n\t{\n\t\tereport(NOTICE,\n\t\t\t\t(errmsg(\"text-search query doesn't contain lexemes: \\\"%s\\\"\",\n\t\t\t\t\t\tstate.buffer)));\n\t\tquery = (TSQuery) palloc(HDRSIZETQ);\n\t\tSET_VARSIZE(query, HDRSIZETQ);\n\t\tquery->size = 0;\n\t\treturn query;\n\t}\n\n\t/* Pack the QueryItems in the final TSQuery struct to return to caller */\n\tcommonlen = COMPUTESIZE(list_length(state.polstr), state.sumlen);\n\tquery = (TSQuery) palloc0(commonlen);\n\tSET_VARSIZE(query, commonlen);\n\tquery->size = list_length(state.polstr);\n\tptr = GETQUERY(query);\n\n\t/* Copy QueryItems to TSQuery */\n\ti = 0;\n\tforeach(cell, state.polstr)\n\t{\n\t\tQueryItem  *item = (QueryItem *) lfirst(cell);\n\n\t\tswitch (item->type)\n\t\t{\n\t\t\tcase QI_VAL:\n\t\t\t\tmemcpy(&ptr[i], item, sizeof(QueryOperand));\n\t\t\t\tbreak;\n\t\t\tcase QI_VALSTOP:\n\t\t\t\tptr[i].type = QI_VALSTOP;\n\t\t\t\tbreak;\n\t\t\tcase QI_OPR:\n\t\t\t\tmemcpy(&ptr[i], item, sizeof(QueryOperator));\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\telog(ERROR, \"unrecognized QueryItem type: %d\", item->type);\n\t\t}\n\t\ti++;\n\t}\n\n\t/* Copy all the operand strings to TSQuery */\n\tmemcpy((void *) GETOPERAND(query), (void *) state.op, state.sumlen);\n\tpfree(state.op);\n\n\t/* Set left operand pointers for every operator. */\n\tfindoprnd(ptr, query->size);\n\n\treturn query;\n}",
    "diff": "@@ -514,8 +514,13 @@ parse_tsquery(char *buf,\n \t\treturn query;\n \t}\n \n-\t/* Pack the QueryItems in the final TSQuery struct to return to caller */\n+\tif (TSQUERY_TOO_BIG(list_length(state.polstr), state.sumlen))\n+\t\tereport(ERROR,\n+\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n+\t\t\t\t errmsg(\"tsquery is too large\")));\n \tcommonlen = COMPUTESIZE(list_length(state.polstr), state.sumlen);\n+\n+\t/* Pack the QueryItems in the final TSQuery struct to return to caller */\n \tquery = (TSQuery) palloc0(commonlen);\n \tSET_VARSIZE(query, commonlen);\n \tquery->size = list_length(state.polstr);\n",
    "critical_vars": [
      "state.sumlen",
      "state.polstr"
    ],
    "variable_definitions": {
      "state.sumlen": "Definition not found",
      "state.polstr": "Definition not found"
    },
    "variable_types": {
      "state.sumlen": "struct.integer",
      "state.polstr": "struct.struct pointer"
    },
    "type_mapping": {
      "state.sumlen": "st.i",
      "state.polstr": "st.sp"
    },
    "cve": "CVE-2014-0064",
    "vulnerable_line": "commonlen = COMPUTESIZE(list_length(state.polstr), state.sumlen);",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "COMPUTESIZE potentially leads to an integer overflow if the computed size exceeds the maximum value allowed for integers, compromising buffer allocation and leading to a buffer overflow when packing TSQuery."
  },
  {
    "vulcode": "QTN2QT(QTNode *in)\n{\n\tTSQuery\t\tout;\n\tint\t\t\tlen;\n\tint\t\t\tsumlen = 0,\n\t\t\t\tnnode = 0;\n\tQTN2QTState state;\n\n\tcntsize(in, &sumlen, &nnode);\n\tlen = COMPUTESIZE(nnode, sumlen);\n\n\tout = (TSQuery) palloc0(len);\n\tSET_VARSIZE(out, len);\n\tout->size = nnode;\n\n\tstate.curitem = GETQUERY(out);\n\tstate.operand = state.curoperand = GETOPERAND(out);\n\n\tfillQT(&state, in);\n\treturn out;\n}",
    "diff": "@@ -333,6 +333,11 @@ QTN2QT(QTNode *in)\n \tQTN2QTState state;\n \n \tcntsize(in, &sumlen, &nnode);\n+\n+\tif (TSQUERY_TOO_BIG(nnode, sumlen))\n+\t\tereport(ERROR,\n+\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n+\t\t\t\t errmsg(\"tsquery is too large\")));\n \tlen = COMPUTESIZE(nnode, sumlen);\n \n \tout = (TSQuery) palloc0(len);\n",
    "critical_vars": [
      "nnode",
      "sumlen"
    ],
    "variable_definitions": {
      "nnode": "int sumlen = 0, nnode = 0;",
      "sumlen": "int sumlen = 0, nnode = 0;"
    },
    "variable_types": {
      "nnode": "integer",
      "sumlen": "integer"
    },
    "type_mapping": {
      "nnode": "Integer",
      "sumlen": "Integer"
    },
    "cve": "CVE-2014-0064",
    "vulnerable_line": "len = COMPUTESIZE(nnode, sumlen);",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The result of COMPUTESIZE may exceed maximum integer size due to the values of nnode and sumlen, leading to insufficient buffer allocation when calling palloc0(len), which can cause a buffer overflow."
  },
  {
    "vulcode": "String string_chunk_split(const char *src, int srclen, const char *end,\n                          int endlen, int chunklen) {\n  int chunks = srclen / chunklen; // complete chunks!\n  int restlen = srclen - chunks * chunklen; /* srclen % chunklen */\n\n  int out_len = (chunks + 1) * endlen + srclen;\n  String ret(out_len, ReserveString);\n  char *dest = ret.bufferSlice().ptr;\n\n  const char *p; char *q;\n  const char *pMax = src + srclen - chunklen + 1;\n  for (p = src, q = dest; p < pMax; ) {\n    memcpy(q, p, chunklen);\n    q += chunklen;\n    memcpy(q, end, endlen);\n    q += endlen;\n    p += chunklen;\n  }\n\n  if (restlen) {\n    memcpy(q, p, restlen);\n    q += restlen;\n    memcpy(q, end, endlen);\n    q += endlen;\n  }\n\n  ret.setSize(q - dest);\n  return ret;\n}",
    "diff": "@@ -626,8 +626,14 @@ String string_chunk_split(const char *src, int srclen, const char *end,\n   int chunks = srclen / chunklen; // complete chunks!\n   int restlen = srclen - chunks * chunklen; /* srclen % chunklen */\n \n-  int out_len = (chunks + 1) * endlen + srclen;\n-  String ret(out_len, ReserveString);\n+  String ret(\n+    safe_address(\n+      chunks + 1,\n+      endlen,\n+      srclen\n+    ),\n+    ReserveString\n+  );\n   char *dest = ret.bufferSlice().ptr;\n \n   const char *p; char *q;\n",
    "critical_vars": [
      "out_len"
    ],
    "variable_definitions": {
      "out_len": "int out_len = (chunks + 1) * endlen + srclen;"
    },
    "variable_types": {
      "out_len": "integer"
    },
    "type_mapping": {
      "out_len": "Integer"
    },
    "cve": "CVE-2014-6228",
    "vulnerable_line": "int out_len = (chunks + 1) * endlen + srclen;",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The calculation for out_len can exceed the maximum value for an int due to the addition of chunks + 1, leading to an integer overflow that may cause application crashes or unspecified impacts."
  },
  {
    "vulcode": "String string_chunk_split(const char *src, int srclen, const char *end,\n                          int endlen, int chunklen) {\n  int chunks = srclen / chunklen; // complete chunks!\n  int restlen = srclen - chunks * chunklen; /* srclen % chunklen */\n\n  int out_len = (chunks + 1) * endlen + srclen;\n  String ret(out_len, ReserveString);\n  char *dest = ret.bufferSlice().ptr;\n\n  const char *p; char *q;\n  const char *pMax = src + srclen - chunklen + 1;\n  for (p = src, q = dest; p < pMax; ) {\n    memcpy(q, p, chunklen);\n    q += chunklen;\n    memcpy(q, end, endlen);\n    q += endlen;\n    p += chunklen;\n  }\n\n  if (restlen) {\n    memcpy(q, p, restlen);\n    q += restlen;\n    memcpy(q, end, endlen);\n    q += endlen;\n  }\n\n  ret.setSize(q - dest);\n  return ret;\n}",
    "diff": "@@ -626,8 +626,14 @@ String string_chunk_split(const char *src, int srclen, const char *end,\n   int chunks = srclen / chunklen; // complete chunks!\n   int restlen = srclen - chunks * chunklen; /* srclen % chunklen */\n \n-  int out_len = (chunks + 1) * endlen + srclen;\n-  String ret(out_len, ReserveString);\n+  String ret(\n+    safe_address(\n+      chunks + 1,\n+      endlen,\n+      srclen\n+    ),\n+    ReserveString\n+  );\n   char *dest = ret.bufferSlice().ptr;\n \n   const char *p; char *q;\n",
    "critical_vars": [
      "ReserveString",
      "endlen",
      "chunks",
      "srclen"
    ],
    "variable_definitions": {
      "ReserveString": "Definition not found",
      "endlen": "int endlen",
      "chunks": "int chunks = srclen / chunklen; // complete chunks!",
      "srclen": "int srclen"
    },
    "variable_types": {
      "ReserveString": "struct",
      "endlen": "integer",
      "chunks": "integer",
      "srclen": "integer"
    },
    "type_mapping": {
      "ReserveString": "struct",
      "endlen": "Integer",
      "chunks": "Integer",
      "srclen": "Integer"
    },
    "cve": "CVE-2014-6228",
    "vulnerable_line": "int out_len = (chunks + 1) * endlen + srclen;",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The calculation of out_len could exceed the maximum value for an integer type if chunks, endlen, or srclen are large enough, leading to overflow and potentially causing a denial of service."
  },
  {
    "vulcode": "BOOL license_read_scope_list(wStream* s, SCOPE_LIST* scopeList)\n{\n\tUINT32 i;\n\tUINT32 scopeCount;\n\n\tif (Stream_GetRemainingLength(s) < 4)\n\t\treturn FALSE;\n\n\tStream_Read_UINT32(s, scopeCount); /* ScopeCount (4 bytes) */\n\n\tscopeList->count = scopeCount;\n\tscopeList->array = (LICENSE_BLOB*) malloc(sizeof(LICENSE_BLOB) * scopeCount);\n\n\t/* ScopeArray */\n\tfor (i = 0; i < scopeCount; i++)\n\t{\n\t\tscopeList->array[i].type = BB_SCOPE_BLOB;\n\n\t\tif (!license_read_binary_blob(s, &scopeList->array[i]))\n\t\t\treturn FALSE;\n\t}\n\n\treturn TRUE;\n}",
    "diff": "@@ -670,6 +670,9 @@ BOOL license_read_scope_list(wStream* s, SCOPE_LIST* scopeList)\n \n \tStream_Read_UINT32(s, scopeCount); /* ScopeCount (4 bytes) */\n \n+        if (Stream_GetRemainingLength(s) / sizeof(LICENSE_BLOB) < scopeCount)\n+                return FALSE;  /* Avoid overflow in malloc */\n+\n \tscopeList->count = scopeCount;\n \tscopeList->array = (LICENSE_BLOB*) malloc(sizeof(LICENSE_BLOB) * scopeCount);\n \n",
    "critical_vars": [
      "s"
    ],
    "variable_definitions": {
      "s": "wStream* s"
    },
    "variable_types": {
      "s": "struct pointer"
    },
    "type_mapping": {
      "s": "struct pointer"
    },
    "cve": "CVE-2014-0791",
    "vulnerable_line": "scopeList->array = (LICENSE_BLOB*) malloc(sizeof(LICENSE_BLOB) * scopeCount);",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The multiplication of sizeof(LICENSE_BLOB) and scopeCount can overflow if scopeCount is too large, leading to a possible denial of service or crash by allocating insufficient memory."
  },
  {
    "vulcode": "static void diff_bytes_c(uint8_t *dst, const uint8_t *src1, const uint8_t *src2, int w){\n    long i;\n#if !HAVE_FAST_UNALIGNED\n    if((long)src2 & (sizeof(long)-1)){\n        for(i=0; i+7<w; i+=8){\n            dst[i+0] = src1[i+0]-src2[i+0];\n            dst[i+1] = src1[i+1]-src2[i+1];\n            dst[i+2] = src1[i+2]-src2[i+2];\n            dst[i+3] = src1[i+3]-src2[i+3];\n            dst[i+4] = src1[i+4]-src2[i+4];\n            dst[i+5] = src1[i+5]-src2[i+5];\n            dst[i+6] = src1[i+6]-src2[i+6];\n            dst[i+7] = src1[i+7]-src2[i+7];\n        }\n    }else\n#endif\n    for(i=0; i<=w-sizeof(long); i+=sizeof(long)){\n        long a = *(long*)(src1+i);\n        long b = *(long*)(src2+i);\n        *(long*)(dst+i) = ((a|pb_80) - (b&pb_7f)) ^ ((a^b^pb_80)&pb_80);\n    }\n    for(; i<w; i++)\n        dst[i+0] = src1[i+0]-src2[i+0];\n}",
    "diff": "@@ -1931,7 +1931,7 @@ void ff_set_cmp(DSPContext* c, me_cmp_func *cmp, int type){\n \n static void add_bytes_c(uint8_t *dst, uint8_t *src, int w){\n     long i;\n-    for(i=0; i<=w-sizeof(long); i+=sizeof(long)){\n+    for(i=0; i<=w-(int)sizeof(long); i+=sizeof(long)){\n         long a = *(long*)(src+i);\n         long b = *(long*)(dst+i);\n         *(long*)(dst+i) = ((a&pb_7f) + (b&pb_7f)) ^ ((a^b)&pb_80);\n@@ -1956,7 +1956,7 @@ static void diff_bytes_c(uint8_t *dst, const uint8_t *src1, const uint8_t *src2,\n         }\n     }else\n #endif\n-    for(i=0; i<=w-sizeof(long); i+=sizeof(long)){\n+    for(i=0; i<=w-(int)sizeof(long); i+=sizeof(long)){\n         long a = *(long*)(src1+i);\n         long b = *(long*)(src2+i);\n         *(long*)(dst+i) = ((a|pb_80) - (b&pb_7f)) ^ ((a^b^pb_80)&pb_80);\n",
    "critical_vars": [
      "i"
    ],
    "variable_definitions": {
      "i": "long i;"
    },
    "variable_types": {
      "i": "integer"
    },
    "type_mapping": {
      "i": "Integer"
    },
    "cve": "CVE-2013-7010",
    "vulnerable_line": "for(i=0; i<=w-sizeof(long); i+=sizeof(long)){",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Array out of bounds",
    "reasoning": "The original check allows `i` to exceed the bounds of the array if `w` is less than or equal to the size of `long`, leading to out-of-bounds access when accessing `src` or `dst`."
  },
  {
    "vulcode": "static int g2m_init_buffers(G2MContext *c)\n{\n    int aligned_height;\n\n    if (!c->framebuf || c->old_width < c->width || c->old_height < c->height) {\n        c->framebuf_stride = FFALIGN(c->width * 3, 16);\n        aligned_height     = FFALIGN(c->height,    16);\n        av_free(c->framebuf);\n        c->framebuf = av_mallocz(c->framebuf_stride * aligned_height);\n        if (!c->framebuf)\n            return AVERROR(ENOMEM);\n    }\n    if (!c->synth_tile || !c->jpeg_tile ||\n        c->old_tile_w < c->tile_width ||\n        c->old_tile_h < c->tile_height) {\n        c->tile_stride = FFALIGN(c->tile_width * 3, 16);\n        aligned_height = FFALIGN(c->tile_height,    16);\n        av_free(c->synth_tile);\n        av_free(c->jpeg_tile);\n        av_free(c->kempf_buf);\n        av_free(c->kempf_flags);\n        c->synth_tile  = av_mallocz(c->tile_stride      * aligned_height);\n        c->jpeg_tile   = av_mallocz(c->tile_stride      * aligned_height);\n        c->kempf_buf   = av_mallocz((c->tile_width + 1) * aligned_height\n                                    + FF_INPUT_BUFFER_PADDING_SIZE);\n        c->kempf_flags = av_mallocz( c->tile_width      * aligned_height);\n        if (!c->synth_tile || !c->jpeg_tile ||\n            !c->kempf_buf || !c->kempf_flags)\n            return AVERROR(ENOMEM);\n    }\n\n    return 0;\n}",
    "diff": "@@ -453,7 +453,7 @@ static int g2m_init_buffers(G2MContext *c)\n     if (!c->synth_tile || !c->jpeg_tile ||\n         c->old_tile_w < c->tile_width ||\n         c->old_tile_h < c->tile_height) {\n-        c->tile_stride = FFALIGN(c->tile_width * 3, 16);\n+        c->tile_stride = FFALIGN(c->tile_width, 16) * 3;\n         aligned_height = FFALIGN(c->tile_height,    16);\n         av_free(c->synth_tile);\n         av_free(c->jpeg_tile);\n",
    "critical_vars": [
      "c->tile_stride"
    ],
    "variable_definitions": {
      "c->tile_stride": "Definition not found"
    },
    "variable_types": {
      "c->tile_stride": "struct pointer_integer"
    },
    "type_mapping": {
      "c->tile_stride": "sp_integer"
    },
    "cve": "CVE-2013-7013",
    "vulnerable_line": "c->tile_stride = FFALIGN(c->tile_width * 3, 16);",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Array out of bounds",
    "reasoning": "Multiplying c->tile_width by 3 before aligning can cause an integer overflow, leading to a calculated tile_stride that exceeds the array bounds."
  }
]