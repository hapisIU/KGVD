[
  {
    "fix_code": "static inline struct sem_array *sem_obtain_lock(struct ipc_namespace *ns,\n\t\t\tint id, struct sembuf *sops, int nsops, int *locknum)\n{\n\tstruct kern_ipc_perm *ipcp;\n\tstruct sem_array *sma;\n\n\trcu_read_lock();\n\tipcp = ipc_obtain_object(&sem_ids(ns), id);\n\tif (IS_ERR(ipcp)) {\n\t\tsma = ERR_CAST(ipcp);\n\t\tgoto err;\n\t}\n\n\tsma = container_of(ipcp, struct sem_array, sem_perm);\n\t*locknum = sem_lock(sma, sops, nsops);\n\n\t/* ipc_rmid() may have already freed the ID while sem_lock\n\t * was spinning: verify that the structure is still valid\n\t */\n\tif (!ipcp->deleted)\n\t\treturn container_of(ipcp, struct sem_array, sem_perm);\n\n\tsem_unlock(sma, *locknum);\n\tsma = ERR_PTR(-EINVAL);\nerr:\n\trcu_read_unlock();\n\treturn sma;\n}",
    "diff": "-#define sem_unlock(sma)\t\tipc_unlock(&(sma)->sem_perm)\n #define sem_checkid(sma, semid)\tipc_checkid(&sma->sem_perm, semid)\n+ * Carefully guard against sma->complex_count changing between zero\n+ * sma->complex_count cannot change while we are holding the lock,\n+static inline int sem_lock(struct sem_array *sma, struct sembuf *sops,\n+\tif (nsops == 1 && !sma->complex_count) {\n+\t\tstruct sem *sem = sma->sem_base + sops->sem_num;\n+\t\t * If sma->complex_count was set while we were spinning,\n+\t\tif (unlikely(sma->complex_count)) {\n+\t\tif (unlikely(spin_is_locked(&sma->sem_perm.lock))) {\n+\t\t\tspin_unlock_wait(&sma->sem_perm.lock);\n+\t\tspin_lock(&sma->sem_perm.lock);\n+\t\tfor (i = 0; i < sma->sem_nsems; i++) {\n+\t\t\tstruct sem *sem = sma->sem_base + i;\n+static inline void sem_unlock(struct sem_array *sma, int locknum)\n+\t\tspin_unlock(&sma->sem_perm.lock);\n+\t\tstruct sem *sem = sma->sem_base + locknum;\n \tstruct sem_array *sma;\n+\tsma = container_of(ipcp, struct sem_array, sem_perm);\n+\t*locknum = sem_lock(sma, sops, nsops);\n+\tsem_unlock(sma, *locknum);\n \tsma = ERR_PTR(-EINVAL);\n static inline void sem_lock_and_putref(struct sem_array *sma)\n-\tipc_lock_by_ptr(&sma->sem_perm);\n+\tsem_lock(sma, NULL, -1);\n \tipc_rcu_putref(sma);\n static inline void sem_getref_and_unlock(struct sem_array *sma)\n-\tipc_rcu_getref(sma);\n-\tipc_unlock(&(sma)->sem_perm);\n+\tWARN_ON_ONCE(!ipc_rcu_getref(sma));\n+\tsem_unlock(sma, -1);\n static inline void sem_putref(struct sem_array *sma)\n-\tipc_lock_by_ptr(&sma->sem_perm);\n-\tipc_rcu_putref(sma);\n-\tipc_unlock(&(sma)->sem_perm);\n+\tsem_lock_and_putref(sma);\n+\tsem_unlock(sma, -1);\n@@ -276,9 +345,9 @@ static inline void sem_putref(struct sem_array *sma)\n static inline void sem_getref(struct sem_array *sma)\n-\tspin_lock(&(sma)->sem_perm.lock);\n-\tipc_rcu_getref(sma);\n-\tipc_unlock(&(sma)->sem_perm);\n+\tsem_lock(sma, NULL, -1);\n+\tWARN_ON_ONCE(!ipc_rcu_getref(sma));\n+\tsem_unlock(sma, -1);\n \tsma->sem_base = (struct sem *) &sma[1];\n \t\tINIT_LIST_HEAD(&sma->sem_base[i].sem_pending);\n+\t\tspin_lock_init(&sma->sem_base[i].lock);\n \tsma->complex_count = 0;\n \tINIT_LIST_HEAD(&sma->sem_pending);\n \tINIT_LIST_HEAD(&sma->list_id);\n \tsma->sem_nsems = nsems;\n \tsma->sem_ctime = get_seconds();\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, -1);\n \treturn sma->sem_perm.id;\n \tsem_rmid(ns, sma);\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, -1);\n \tns->used_sems -= sma->sem_nsems;\n \tstruct sem_array *sma;\n-\tsma = sem_lock_check(ns, semid);\n-\tif (IS_ERR(sma))\n-\t\treturn PTR_ERR(sma);\n-\tnsems = sma->sem_nsems;\n-\tif (ipcperms(ns, &sma->sem_perm, S_IWUGO))\n+\tsma = sem_obtain_object_check(ns, semid);\n+\tif (IS_ERR(sma)) {\n+\t\treturn PTR_ERR(sma);\n+\tif (semnum < 0 || semnum >= sma->sem_nsems) {\n+\tif (ipcperms(ns, &sma->sem_perm, S_IWUGO)) {\n \terr = security_sem_semctl(sma, SETVAL);\n+\tsem_lock(sma, NULL, -1);\n \tcurr = &sma->sem_base[semnum];\n \tassert_spin_locked(&sma->sem_perm.lock);\n \tlist_for_each_entry(un, &sma->list_id, list_id)\n \tsma->sem_ctime = get_seconds();\n \tdo_smart_update(sma, NULL, 0, 0, &tasks);\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, -1);\n \t\t\tsem_lock_and_putref(sma);\n \t\t\tif (sma->sem_perm.deleted) {\n-\t\t\t\tsem_unlock(sma);\n+\t\t\t\tsem_unlock(sma, -1);\n+\t\t\tsem_lock(sma, NULL, -1);\n-\t\tspin_lock(&sma->sem_perm.lock);\n \t\tfor (i = 0; i < sma->sem_nsems; i++)\n \t\t\tsem_io[i] = sma->sem_base[i].semval;\n-\t\tsem_unlock(sma);\n+\t\tsem_unlock(sma, -1);\n-\t\tipc_rcu_getref(sma);\n+\t\tif (!ipc_rcu_getref(sma)) {\n \t\tsem_lock_and_putref(sma);\n \t\tif (sma->sem_perm.deleted) {\n-\t\t\tsem_unlock(sma);\n+\t\t\tsem_unlock(sma, -1);\n-\tspin_lock(&sma->sem_perm.lock);\n+\tsem_lock(sma, NULL, -1);\n \tcurr = &sma->sem_base[semnum];\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, -1);\n-\t\tipc_lock_object(&sma->sem_perm);\n+\t\tsem_lock(sma, NULL, -1);\n-\t\tipc_lock_object(&sma->sem_perm);\n+\t\tsem_lock(sma, NULL, -1);\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, -1);\n \tstruct sem_array *sma;\n \tnsems = sma->sem_nsems;\n-\tipc_rcu_getref(sma);\n+\tif (!ipc_rcu_getref(sma)) {\n \tsem_lock_and_putref(sma);\n \tif (sma->sem_perm.deleted) {\n-\t\tsem_unlock(sma);\n+\t\tsem_unlock(sma, -1);\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, -1);\n \tsma = sem_obtain_object_check(ns, semid);\n \tif (IS_ERR(sma)) {\n \t\terror = PTR_ERR(sma);\n-\tipc_lock_object(&sma->sem_perm);\n+\tlocknum = sem_lock(sma, sops, nsops);\n \terror = try_atomic_semop (sma, sops, nsops, un, task_tgid_vnr(current));\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, locknum);\n-\tsma = sem_obtain_lock(ns, semid);\n+\tsma = sem_obtain_lock(ns, semid, sops, nsops, &locknum);\n \tunlink_queue(sma, &queue);\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, locknum);\n \t\tstruct sem_array *sma;\n-\t\tsma = sem_lock_check(tsk->nsproxy->ipc_ns, un->semid);\n+\t\tsma = sem_obtain_object_check(tsk->nsproxy->ipc_ns, un->semid);\n-\t\tif (IS_ERR(sma))\n+\t\tif (IS_ERR(sma)) {\n+\t\tsem_lock(sma, NULL, -1);\n-\t\t\tsem_unlock(sma);\n+\t\t\tsem_unlock(sma, -1);\n \t\tdo_smart_update(sma, NULL, 0, 1, &tasks);\n-\t\tsem_unlock(sma);\n+\t\tsem_unlock(sma, -1);",
    "critical_vars": [
      "sma"
    ],
    "variable_definitions": {
      "sma": "struct sem_array *sma;"
    },
    "variable_types": {
      "sma": "struct pointer"
    },
    "type_mapping": {
      "sma": "struct pointer"
    },
    "vulnerable_line": "sma = container_of(ipcp, struct sem_array, sem_perm);",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The manipulation of semaphore operations and the reference counting can lead to an overflow scenario if certain operations exceed the allowed maximum values, potentially causing access to invalid memory locations."
  },
  {
    "fix_code": "static inline void sem_putref(struct sem_array *sma)\n{\n\tsem_lock_and_putref(sma);\n\tsem_unlock(sma, -1);\n}",
    "diff": "-#define sem_unlock(sma)\t\tipc_unlock(&(sma)->sem_perm)\n #define sem_checkid(sma, semid)\tipc_checkid(&sma->sem_perm, semid)\n+ * Carefully guard against sma->complex_count changing between zero\n+ * sma->complex_count cannot change while we are holding the lock,\n+static inline int sem_lock(struct sem_array *sma, struct sembuf *sops,\n+\tif (nsops == 1 && !sma->complex_count) {\n+\t\tstruct sem *sem = sma->sem_base + sops->sem_num;\n+\t\t * If sma->complex_count was set while we were spinning,\n+\t\tif (unlikely(sma->complex_count)) {\n+\t\tif (unlikely(spin_is_locked(&sma->sem_perm.lock))) {\n+\t\t\tspin_unlock_wait(&sma->sem_perm.lock);\n+\t\tspin_lock(&sma->sem_perm.lock);\n+\t\tfor (i = 0; i < sma->sem_nsems; i++) {\n+\t\t\tstruct sem *sem = sma->sem_base + i;\n+static inline void sem_unlock(struct sem_array *sma, int locknum)\n+\t\tspin_unlock(&sma->sem_perm.lock);\n+\t\tstruct sem *sem = sma->sem_base + locknum;\n \tstruct sem_array *sma;\n+\tsma = container_of(ipcp, struct sem_array, sem_perm);\n+\t*locknum = sem_lock(sma, sops, nsops);\n+\tsem_unlock(sma, *locknum);\n \tsma = ERR_PTR(-EINVAL);\n static inline void sem_lock_and_putref(struct sem_array *sma)\n-\tipc_lock_by_ptr(&sma->sem_perm);\n+\tsem_lock(sma, NULL, -1);\n \tipc_rcu_putref(sma);\n static inline void sem_getref_and_unlock(struct sem_array *sma)\n-\tipc_rcu_getref(sma);\n-\tipc_unlock(&(sma)->sem_perm);\n+\tWARN_ON_ONCE(!ipc_rcu_getref(sma));\n+\tsem_unlock(sma, -1);\n static inline void sem_putref(struct sem_array *sma)\n-\tipc_lock_by_ptr(&sma->sem_perm);\n-\tipc_rcu_putref(sma);\n-\tipc_unlock(&(sma)->sem_perm);\n+\tsem_lock_and_putref(sma);\n+\tsem_unlock(sma, -1);\n@@ -276,9 +345,9 @@ static inline void sem_putref(struct sem_array *sma)\n static inline void sem_getref(struct sem_array *sma)\n-\tspin_lock(&(sma)->sem_perm.lock);\n-\tipc_rcu_getref(sma);\n-\tipc_unlock(&(sma)->sem_perm);\n+\tsem_lock(sma, NULL, -1);\n+\tWARN_ON_ONCE(!ipc_rcu_getref(sma));\n+\tsem_unlock(sma, -1);\n \tsma->sem_base = (struct sem *) &sma[1];\n \t\tINIT_LIST_HEAD(&sma->sem_base[i].sem_pending);\n+\t\tspin_lock_init(&sma->sem_base[i].lock);\n \tsma->complex_count = 0;\n \tINIT_LIST_HEAD(&sma->sem_pending);\n \tINIT_LIST_HEAD(&sma->list_id);\n \tsma->sem_nsems = nsems;\n \tsma->sem_ctime = get_seconds();\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, -1);\n \treturn sma->sem_perm.id;\n \tsem_rmid(ns, sma);\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, -1);\n \tns->used_sems -= sma->sem_nsems;\n \tstruct sem_array *sma;\n-\tsma = sem_lock_check(ns, semid);\n-\tif (IS_ERR(sma))\n-\t\treturn PTR_ERR(sma);\n-\tnsems = sma->sem_nsems;\n-\tif (ipcperms(ns, &sma->sem_perm, S_IWUGO))\n+\tsma = sem_obtain_object_check(ns, semid);\n+\tif (IS_ERR(sma)) {\n+\t\treturn PTR_ERR(sma);\n+\tif (semnum < 0 || semnum >= sma->sem_nsems) {\n+\tif (ipcperms(ns, &sma->sem_perm, S_IWUGO)) {\n \terr = security_sem_semctl(sma, SETVAL);\n+\tsem_lock(sma, NULL, -1);\n \tcurr = &sma->sem_base[semnum];\n \tassert_spin_locked(&sma->sem_perm.lock);\n \tlist_for_each_entry(un, &sma->list_id, list_id)\n \tsma->sem_ctime = get_seconds();\n \tdo_smart_update(sma, NULL, 0, 0, &tasks);\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, -1);\n \t\t\tsem_lock_and_putref(sma);\n \t\t\tif (sma->sem_perm.deleted) {\n-\t\t\t\tsem_unlock(sma);\n+\t\t\t\tsem_unlock(sma, -1);\n+\t\t\tsem_lock(sma, NULL, -1);\n-\t\tspin_lock(&sma->sem_perm.lock);\n \t\tfor (i = 0; i < sma->sem_nsems; i++)\n \t\t\tsem_io[i] = sma->sem_base[i].semval;\n-\t\tsem_unlock(sma);\n+\t\tsem_unlock(sma, -1);\n-\t\tipc_rcu_getref(sma);\n+\t\tif (!ipc_rcu_getref(sma)) {\n \t\tsem_lock_and_putref(sma);\n \t\tif (sma->sem_perm.deleted) {\n-\t\t\tsem_unlock(sma);\n+\t\t\tsem_unlock(sma, -1);\n-\tspin_lock(&sma->sem_perm.lock);\n+\tsem_lock(sma, NULL, -1);\n \tcurr = &sma->sem_base[semnum];\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, -1);\n-\t\tipc_lock_object(&sma->sem_perm);\n+\t\tsem_lock(sma, NULL, -1);\n-\t\tipc_lock_object(&sma->sem_perm);\n+\t\tsem_lock(sma, NULL, -1);\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, -1);\n \tstruct sem_array *sma;\n \tnsems = sma->sem_nsems;\n-\tipc_rcu_getref(sma);\n+\tif (!ipc_rcu_getref(sma)) {\n \tsem_lock_and_putref(sma);\n \tif (sma->sem_perm.deleted) {\n-\t\tsem_unlock(sma);\n+\t\tsem_unlock(sma, -1);\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, -1);\n \tsma = sem_obtain_object_check(ns, semid);\n \tif (IS_ERR(sma)) {\n \t\terror = PTR_ERR(sma);\n-\tipc_lock_object(&sma->sem_perm);\n+\tlocknum = sem_lock(sma, sops, nsops);\n \terror = try_atomic_semop (sma, sops, nsops, un, task_tgid_vnr(current));\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, locknum);\n-\tsma = sem_obtain_lock(ns, semid);\n+\tsma = sem_obtain_lock(ns, semid, sops, nsops, &locknum);\n \tunlink_queue(sma, &queue);\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, locknum);\n \t\tstruct sem_array *sma;\n-\t\tsma = sem_lock_check(tsk->nsproxy->ipc_ns, un->semid);\n+\t\tsma = sem_obtain_object_check(tsk->nsproxy->ipc_ns, un->semid);\n-\t\tif (IS_ERR(sma))\n+\t\tif (IS_ERR(sma)) {\n+\t\tsem_lock(sma, NULL, -1);\n-\t\t\tsem_unlock(sma);\n+\t\t\tsem_unlock(sma, -1);\n \t\tdo_smart_update(sma, NULL, 0, 1, &tasks);\n-\t\tsem_unlock(sma);\n+\t\tsem_unlock(sma, -1);",
    "critical_vars": [
      "sma"
    ],
    "variable_definitions": {
      "sma": "struct sem_array *sma"
    },
    "variable_types": {
      "sma": "struct pointer"
    },
    "type_mapping": {
      "sma": "struct pointer"
    },
    "vulnerable_line": "sem_unlock(sma, -1);",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Function Operation Out of Bounds",
    "reasoning": "The use of -1 as the 'locknum' assumes a valid lock value; if locknum is incorrectly set due to an integer overflow or improper management of locks, it could lead to erroneous unlock operations, potentially allowing for race conditions and misuse of the semaphore array."
  },
  {
    "fix_code": "static void freeary(struct ipc_namespace *ns, struct kern_ipc_perm *ipcp)\n{\n\tstruct sem_undo *un, *tu;\n\tstruct sem_queue *q, *tq;\n\tstruct sem_array *sma = container_of(ipcp, struct sem_array, sem_perm);\n\tstruct list_head tasks;\n\tint i;\n\n\t/* Free the existing undo structures for this semaphore set.  */\n\tassert_spin_locked(&sma->sem_perm.lock);\n\tlist_for_each_entry_safe(un, tu, &sma->list_id, list_id) {\n\t\tlist_del(&un->list_id);\n\t\tspin_lock(&un->ulp->lock);\n\t\tun->semid = -1;\n\t\tlist_del_rcu(&un->list_proc);\n\t\tspin_unlock(&un->ulp->lock);\n\t\tkfree_rcu(un, rcu);\n\t}\n\n\t/* Wake up all pending processes and let them fail with EIDRM. */\n\tINIT_LIST_HEAD(&tasks);\n\tlist_for_each_entry_safe(q, tq, &sma->sem_pending, list) {\n\t\tunlink_queue(sma, q);\n\t\twake_up_sem_queue_prepare(&tasks, q, -EIDRM);\n\t}\n\tfor (i = 0; i < sma->sem_nsems; i++) {\n\t\tstruct sem *sem = sma->sem_base + i;\n\t\tlist_for_each_entry_safe(q, tq, &sem->sem_pending, list) {\n\t\t\tunlink_queue(sma, q);\n\t\t\twake_up_sem_queue_prepare(&tasks, q, -EIDRM);\n\t\t}\n\t}\n\n\t/* Remove the semaphore set from the IDR */\n\tsem_rmid(ns, sma);\n\tsem_unlock(sma, -1);\n\n\twake_up_sem_queue_do(&tasks);\n\tns->used_sems -= sma->sem_nsems;\n\tsecurity_sem_free(sma);\n\tipc_rcu_putref(sma);\n}",
    "diff": "-#define sem_unlock(sma)\t\tipc_unlock(&(sma)->sem_perm)\n #define sem_checkid(sma, semid)\tipc_checkid(&sma->sem_perm, semid)\n+ * Carefully guard against sma->complex_count changing between zero\n+ * sma->complex_count cannot change while we are holding the lock,\n+static inline int sem_lock(struct sem_array *sma, struct sembuf *sops,\n+\tif (nsops == 1 && !sma->complex_count) {\n+\t\tstruct sem *sem = sma->sem_base + sops->sem_num;\n+\t\t * If sma->complex_count was set while we were spinning,\n+\t\tif (unlikely(sma->complex_count)) {\n+\t\tif (unlikely(spin_is_locked(&sma->sem_perm.lock))) {\n+\t\t\tspin_unlock_wait(&sma->sem_perm.lock);\n+\t\tspin_lock(&sma->sem_perm.lock);\n+\t\tfor (i = 0; i < sma->sem_nsems; i++) {\n+\t\t\tstruct sem *sem = sma->sem_base + i;\n+static inline void sem_unlock(struct sem_array *sma, int locknum)\n+\t\tspin_unlock(&sma->sem_perm.lock);\n+\t\tstruct sem *sem = sma->sem_base + locknum;\n \tstruct sem_array *sma;\n+\tsma = container_of(ipcp, struct sem_array, sem_perm);\n+\t*locknum = sem_lock(sma, sops, nsops);\n+\tsem_unlock(sma, *locknum);\n \tsma = ERR_PTR(-EINVAL);\n static inline void sem_lock_and_putref(struct sem_array *sma)\n-\tipc_lock_by_ptr(&sma->sem_perm);\n+\tsem_lock(sma, NULL, -1);\n \tipc_rcu_putref(sma);\n static inline void sem_getref_and_unlock(struct sem_array *sma)\n-\tipc_rcu_getref(sma);\n-\tipc_unlock(&(sma)->sem_perm);\n+\tWARN_ON_ONCE(!ipc_rcu_getref(sma));\n+\tsem_unlock(sma, -1);\n static inline void sem_putref(struct sem_array *sma)\n-\tipc_lock_by_ptr(&sma->sem_perm);\n-\tipc_rcu_putref(sma);\n-\tipc_unlock(&(sma)->sem_perm);\n+\tsem_lock_and_putref(sma);\n+\tsem_unlock(sma, -1);\n@@ -276,9 +345,9 @@ static inline void sem_putref(struct sem_array *sma)\n static inline void sem_getref(struct sem_array *sma)\n-\tspin_lock(&(sma)->sem_perm.lock);\n-\tipc_rcu_getref(sma);\n-\tipc_unlock(&(sma)->sem_perm);\n+\tsem_lock(sma, NULL, -1);\n+\tWARN_ON_ONCE(!ipc_rcu_getref(sma));\n+\tsem_unlock(sma, -1);\n \tsma->sem_base = (struct sem *) &sma[1];\n \t\tINIT_LIST_HEAD(&sma->sem_base[i].sem_pending);\n+\t\tspin_lock_init(&sma->sem_base[i].lock);\n \tsma->complex_count = 0;\n \tINIT_LIST_HEAD(&sma->sem_pending);\n \tINIT_LIST_HEAD(&sma->list_id);\n \tsma->sem_nsems = nsems;\n \tsma->sem_ctime = get_seconds();\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, -1);\n \treturn sma->sem_perm.id;\n \tsem_rmid(ns, sma);\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, -1);\n \tns->used_sems -= sma->sem_nsems;\n \tstruct sem_array *sma;\n-\tsma = sem_lock_check(ns, semid);\n-\tif (IS_ERR(sma))\n-\t\treturn PTR_ERR(sma);\n-\tnsems = sma->sem_nsems;\n-\tif (ipcperms(ns, &sma->sem_perm, S_IWUGO))\n+\tsma = sem_obtain_object_check(ns, semid);\n+\tif (IS_ERR(sma)) {\n+\t\treturn PTR_ERR(sma);\n+\tif (semnum < 0 || semnum >= sma->sem_nsems) {\n+\tif (ipcperms(ns, &sma->sem_perm, S_IWUGO)) {\n \terr = security_sem_semctl(sma, SETVAL);\n+\tsem_lock(sma, NULL, -1);\n \tcurr = &sma->sem_base[semnum];\n \tassert_spin_locked(&sma->sem_perm.lock);\n \tlist_for_each_entry(un, &sma->list_id, list_id)\n \tsma->sem_ctime = get_seconds();\n \tdo_smart_update(sma, NULL, 0, 0, &tasks);\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, -1);\n \t\t\tsem_lock_and_putref(sma);\n \t\t\tif (sma->sem_perm.deleted) {\n-\t\t\t\tsem_unlock(sma);\n+\t\t\t\tsem_unlock(sma, -1);\n+\t\t\tsem_lock(sma, NULL, -1);\n-\t\tspin_lock(&sma->sem_perm.lock);\n \t\tfor (i = 0; i < sma->sem_nsems; i++)\n \t\t\tsem_io[i] = sma->sem_base[i].semval;\n-\t\tsem_unlock(sma);\n+\t\tsem_unlock(sma, -1);\n-\t\tipc_rcu_getref(sma);\n+\t\tif (!ipc_rcu_getref(sma)) {\n \t\tsem_lock_and_putref(sma);\n \t\tif (sma->sem_perm.deleted) {\n-\t\t\tsem_unlock(sma);\n+\t\t\tsem_unlock(sma, -1);\n-\tspin_lock(&sma->sem_perm.lock);\n+\tsem_lock(sma, NULL, -1);\n \tcurr = &sma->sem_base[semnum];\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, -1);\n-\t\tipc_lock_object(&sma->sem_perm);\n+\t\tsem_lock(sma, NULL, -1);\n-\t\tipc_lock_object(&sma->sem_perm);\n+\t\tsem_lock(sma, NULL, -1);\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, -1);\n \tstruct sem_array *sma;\n \tnsems = sma->sem_nsems;\n-\tipc_rcu_getref(sma);\n+\tif (!ipc_rcu_getref(sma)) {\n \tsem_lock_and_putref(sma);\n \tif (sma->sem_perm.deleted) {\n-\t\tsem_unlock(sma);\n+\t\tsem_unlock(sma, -1);\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, -1);\n \tsma = sem_obtain_object_check(ns, semid);\n \tif (IS_ERR(sma)) {\n \t\terror = PTR_ERR(sma);\n-\tipc_lock_object(&sma->sem_perm);\n+\tlocknum = sem_lock(sma, sops, nsops);\n \terror = try_atomic_semop (sma, sops, nsops, un, task_tgid_vnr(current));\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, locknum);\n-\tsma = sem_obtain_lock(ns, semid);\n+\tsma = sem_obtain_lock(ns, semid, sops, nsops, &locknum);\n \tunlink_queue(sma, &queue);\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, locknum);\n \t\tstruct sem_array *sma;\n-\t\tsma = sem_lock_check(tsk->nsproxy->ipc_ns, un->semid);\n+\t\tsma = sem_obtain_object_check(tsk->nsproxy->ipc_ns, un->semid);\n-\t\tif (IS_ERR(sma))\n+\t\tif (IS_ERR(sma)) {\n+\t\tsem_lock(sma, NULL, -1);\n-\t\t\tsem_unlock(sma);\n+\t\t\tsem_unlock(sma, -1);\n \t\tdo_smart_update(sma, NULL, 0, 1, &tasks);\n-\t\tsem_unlock(sma);\n+\t\tsem_unlock(sma, -1);",
    "critical_vars": [
      "sma"
    ],
    "variable_definitions": {
      "sma": "struct sem_array *sma = container_of(ipcp, struct sem_array, sem_perm);"
    },
    "variable_types": {
      "sma": "struct pointer"
    },
    "type_mapping": {
      "sma": "struct pointer"
    },
    "vulnerable_line": "for (i = 0; i < nsems; i++)",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Array out of bounds",
    "reasoning": "The loop condition uses 'nsems' which could have been altered improperly leading to potential out-of-bounds access of the sem_base array if 'nsems' is set to an insufficient value."
  },
  {
    "fix_code": "static int semctl_setval(struct ipc_namespace *ns, int semid, int semnum,\n\t\tunsigned long arg)\n{\n\tstruct sem_undo *un;\n\tstruct sem_array *sma;\n\tstruct sem* curr;\n\tint err;\n\tstruct list_head tasks;\n\tint val;\n#if defined(CONFIG_64BIT) && defined(__BIG_ENDIAN)\n\t/* big-endian 64bit */\n\tval = arg >> 32;\n#else\n\t/* 32bit or little-endian 64bit */\n\tval = arg;\n#endif\n\n\tif (val > SEMVMX || val < 0)\n\t\treturn -ERANGE;\n\n\tINIT_LIST_HEAD(&tasks);\n\n\trcu_read_lock();\n\tsma = sem_obtain_object_check(ns, semid);\n\tif (IS_ERR(sma)) {\n\t\trcu_read_unlock();\n\t\treturn PTR_ERR(sma);\n\t}\n\n\tif (semnum < 0 || semnum >= sma->sem_nsems) {\n\t\trcu_read_unlock();\n\t\treturn -EINVAL;\n\t}\n\n\n\tif (ipcperms(ns, &sma->sem_perm, S_IWUGO)) {\n\t\trcu_read_unlock();\n\t\treturn -EACCES;\n\t}\n\n\terr = security_sem_semctl(sma, SETVAL);\n\tif (err) {\n\t\trcu_read_unlock();\n\t\treturn -EACCES;\n\t}\n\n\tsem_lock(sma, NULL, -1);\n\n\tcurr = &sma->sem_base[semnum];\n\n\tassert_spin_locked(&sma->sem_perm.lock);\n\tlist_for_each_entry(un, &sma->list_id, list_id)\n\t\tun->semadj[semnum] = 0;\n\n\tcurr->semval = val;\n\tcurr->sempid = task_tgid_vnr(current);\n\tsma->sem_ctime = get_seconds();\n\t/* maybe some queued-up processes were waiting for this */\n\tdo_smart_update(sma, NULL, 0, 0, &tasks);\n\tsem_unlock(sma, -1);\n\twake_up_sem_queue_do(&tasks);\n\treturn 0;\n}",
    "diff": "+\t\tfor (i = 0; i < sma->sem_nsems; i++) {\n-\tfor (i = 0; i < nsems; i++)\n+\tfor (i = 0; i < nsems; i++) {\n \tsma->sem_nsems = nsems;\n \tns->used_sems -= sma->sem_nsems;\n-\tint nsems;\n-\tnsems = sma->sem_nsems;\n+\tif (semnum < 0 || semnum >= sma->sem_nsems) {\n-\tif(semnum < 0 || semnum >= nsems)\n \t\tfor (i = 0; i < sma->sem_nsems; i++)\n \t\tif(copy_to_user(array, sem_io, nsems*sizeof(ushort)))\n \t\tif(nsems > SEMMSL_FAST) {\n-\tint nsems;\n+\tint nsems, error;\n \tnsems = sma->sem_nsems;",
    "critical_vars": [
      "nsems"
    ],
    "variable_definitions": {
      "nsems": "int nsems;"
    },
    "variable_types": {
      "nsems": "integer"
    },
    "type_mapping": {
      "nsems": "Integer"
    },
    "vulnerable_line": "for (i = 0; i < nsems; i++)",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Array out of bounds",
    "reasoning": "The loop iterates based on nsems value, which may be affected by an integer overflow, allowing access beyond the allocated array boundary."
  },
  {
    "fix_code": "static int semctl_setval(struct ipc_namespace *ns, int semid, int semnum,\n\t\tunsigned long arg)\n{\n\tstruct sem_undo *un;\n\tstruct sem_array *sma;\n\tstruct sem* curr;\n\tint err;\n\tstruct list_head tasks;\n\tint val;\n#if defined(CONFIG_64BIT) && defined(__BIG_ENDIAN)\n\t/* big-endian 64bit */\n\tval = arg >> 32;\n#else\n\t/* 32bit or little-endian 64bit */\n\tval = arg;\n#endif\n\n\tif (val > SEMVMX || val < 0)\n\t\treturn -ERANGE;\n\n\tINIT_LIST_HEAD(&tasks);\n\n\trcu_read_lock();\n\tsma = sem_obtain_object_check(ns, semid);\n\tif (IS_ERR(sma)) {\n\t\trcu_read_unlock();\n\t\treturn PTR_ERR(sma);\n\t}\n\n\tif (semnum < 0 || semnum >= sma->sem_nsems) {\n\t\trcu_read_unlock();\n\t\treturn -EINVAL;\n\t}\n\n\n\tif (ipcperms(ns, &sma->sem_perm, S_IWUGO)) {\n\t\trcu_read_unlock();\n\t\treturn -EACCES;\n\t}\n\n\terr = security_sem_semctl(sma, SETVAL);\n\tif (err) {\n\t\trcu_read_unlock();\n\t\treturn -EACCES;\n\t}\n\n\tsem_lock(sma, NULL, -1);\n\n\tcurr = &sma->sem_base[semnum];\n\n\tassert_spin_locked(&sma->sem_perm.lock);\n\tlist_for_each_entry(un, &sma->list_id, list_id)\n\t\tun->semadj[semnum] = 0;\n\n\tcurr->semval = val;\n\tcurr->sempid = task_tgid_vnr(current);\n\tsma->sem_ctime = get_seconds();\n\t/* maybe some queued-up processes were waiting for this */\n\tdo_smart_update(sma, NULL, 0, 0, &tasks);\n\tsem_unlock(sma, -1);\n\twake_up_sem_queue_do(&tasks);\n\treturn 0;\n}",
    "diff": " \tint\tsemval;\t\t/* current value */\n+ * and non-zero while we are spinning for the lock. The value of\n \t * was spinning: verify that the structure is still valid\n@@ -947,7 +1018,6 @@ static int semctl_setval(struct ipc_namespace *ns, int semid, int semnum,\n \tint val;\n@@ -958,31 +1028,39 @@ static int semctl_setval(struct ipc_namespace *ns, int semid, int semnum,\n \tval = arg;\n+\tif (val > SEMVMX || val < 0)\n-\tif (val > SEMVMX || val < 0)\n@@ -992,11 +1070,9 @@ static int semctl_setval(struct ipc_namespace *ns, int semid, int semnum,\n \t\t\tsem_io[i] = sma->sem_base[i].semval;",
    "critical_vars": [
      "val"
    ],
    "variable_definitions": {
      "val": "int val;"
    },
    "variable_types": {
      "val": "integer"
    },
    "type_mapping": {
      "val": "Integer"
    },
    "vulnerable_line": "if (val > SEMVMX || val < 0)",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The variable 'val', derived from a user-controlled input, is compared against SEMVMX. If the input exceeds the maximum limit of SEMVMX, this can lead to integer overflow, causing erroneous behavior such as granting access to invalid semaphore operations."
  },
  {
    "fix_code": "static int semctl_main(struct ipc_namespace *ns, int semid, int semnum,\n\t\tint cmd, void __user *p)\n{\n\tstruct sem_array *sma;\n\tstruct sem* curr;\n\tint err, nsems;\n\tushort fast_sem_io[SEMMSL_FAST];\n\tushort* sem_io = fast_sem_io;\n\tstruct list_head tasks;\n\n\tINIT_LIST_HEAD(&tasks);\n\n\trcu_read_lock();\n\tsma = sem_obtain_object_check(ns, semid);\n\tif (IS_ERR(sma)) {\n\t\trcu_read_unlock();\n\t\treturn PTR_ERR(sma);\n\t}\n\n\tnsems = sma->sem_nsems;\n\n\terr = -EACCES;\n\tif (ipcperms(ns, &sma->sem_perm,\n\t\t\tcmd == SETALL ? S_IWUGO : S_IRUGO)) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terr = security_sem_semctl(sma, cmd);\n\tif (err) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terr = -EACCES;\n\tswitch (cmd) {\n\tcase GETALL:\n\t{\n\t\tushort __user *array = p;\n\t\tint i;\n\n\t\tif(nsems > SEMMSL_FAST) {\n\t\t\tsem_getref(sma);\n\n\t\t\tsem_io = ipc_alloc(sizeof(ushort)*nsems);\n\t\t\tif(sem_io == NULL) {\n\t\t\t\tsem_putref(sma);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\n\t\t\tsem_lock_and_putref(sma);\n\t\t\tif (sma->sem_perm.deleted) {\n\t\t\t\tsem_unlock(sma, -1);\n\t\t\t\terr = -EIDRM;\n\t\t\t\tgoto out_free;\n\t\t\t}\n\t\t} else\n\t\t\tsem_lock(sma, NULL, -1);\n\n\t\tfor (i = 0; i < sma->sem_nsems; i++)\n\t\t\tsem_io[i] = sma->sem_base[i].semval;\n\t\tsem_unlock(sma, -1);\n\t\terr = 0;\n\t\tif(copy_to_user(array, sem_io, nsems*sizeof(ushort)))\n\t\t\terr = -EFAULT;\n\t\tgoto out_free;\n\t}\n\tcase SETALL:\n\t{\n\t\tint i;\n\t\tstruct sem_undo *un;\n\n\t\tif (!ipc_rcu_getref(sma)) {\n\t\t\trcu_read_unlock();\n\t\t\treturn -EIDRM;\n\t\t}\n\t\trcu_read_unlock();\n\n\t\tif(nsems > SEMMSL_FAST) {\n\t\t\tsem_io = ipc_alloc(sizeof(ushort)*nsems);\n\t\t\tif(sem_io == NULL) {\n\t\t\t\tsem_putref(sma);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t}\n\n\t\tif (copy_from_user (sem_io, p, nsems*sizeof(ushort))) {\n\t\t\tsem_putref(sma);\n\t\t\terr = -EFAULT;\n\t\t\tgoto out_free;\n\t\t}\n\n\t\tfor (i = 0; i < nsems; i++) {\n\t\t\tif (sem_io[i] > SEMVMX) {\n\t\t\t\tsem_putref(sma);\n\t\t\t\terr = -ERANGE;\n\t\t\t\tgoto out_free;\n\t\t\t}\n\t\t}\n\t\tsem_lock_and_putref(sma);\n\t\tif (sma->sem_perm.deleted) {\n\t\t\tsem_unlock(sma, -1);\n\t\t\terr = -EIDRM;\n\t\t\tgoto out_free;\n\t\t}\n\n\t\tfor (i = 0; i < nsems; i++)\n\t\t\tsma->sem_base[i].semval = sem_io[i];\n\n\t\tassert_spin_locked(&sma->sem_perm.lock);\n\t\tlist_for_each_entry(un, &sma->list_id, list_id) {\n\t\t\tfor (i = 0; i < nsems; i++)\n\t\t\t\tun->semadj[i] = 0;\n\t\t}\n\t\tsma->sem_ctime = get_seconds();\n\t\t/* maybe some queued-up processes were waiting for this */\n\t\tdo_smart_update(sma, NULL, 0, 0, &tasks);\n\t\terr = 0;\n\t\tgoto out_unlock;\n\t}\n\t/* GETVAL, GETPID, GETNCTN, GETZCNT: fall-through */\n\t}\n\terr = -EINVAL;\n\tif (semnum < 0 || semnum >= nsems) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\tsem_lock(sma, NULL, -1);\n\tcurr = &sma->sem_base[semnum];\n\n\tswitch (cmd) {\n\tcase GETVAL:\n\t\terr = curr->semval;\n\t\tgoto out_unlock;\n\tcase GETPID:\n\t\terr = curr->sempid;\n\t\tgoto out_unlock;\n\tcase GETNCNT:\n\t\terr = count_semncnt(sma,semnum);\n\t\tgoto out_unlock;\n\tcase GETZCNT:\n\t\terr = count_semzcnt(sma,semnum);\n\t\tgoto out_unlock;\n\t}\n\nout_unlock:\n\tsem_unlock(sma, -1);\nout_wakeup:\n\twake_up_sem_queue_do(&tasks);\nout_free:\n\tif(sem_io != fast_sem_io)\n\t\tipc_free(sem_io, sizeof(ushort)*nsems);\n\treturn err;\n}",
    "diff": "-#define sem_unlock(sma)\t\tipc_unlock(&(sma)->sem_perm)\n #define sem_checkid(sma, semid)\tipc_checkid(&sma->sem_perm, semid)\n+ * Carefully guard against sma->complex_count changing between zero\n+ * sma->complex_count cannot change while we are holding the lock,\n+static inline int sem_lock(struct sem_array *sma, struct sembuf *sops,\n+\tif (nsops == 1 && !sma->complex_count) {\n+\t\tstruct sem *sem = sma->sem_base + sops->sem_num;\n+\t\t * If sma->complex_count was set while we were spinning,\n+\t\tif (unlikely(sma->complex_count)) {\n+\t\tif (unlikely(spin_is_locked(&sma->sem_perm.lock))) {\n+\t\t\tspin_unlock_wait(&sma->sem_perm.lock);\n+\t\tspin_lock(&sma->sem_perm.lock);\n+\t\tfor (i = 0; i < sma->sem_nsems; i++) {\n+\t\t\tstruct sem *sem = sma->sem_base + i;\n+static inline void sem_unlock(struct sem_array *sma, int locknum)\n+\t\tspin_unlock(&sma->sem_perm.lock);\n+\t\tstruct sem *sem = sma->sem_base + locknum;\n \tstruct sem_array *sma;\n+\tsma = container_of(ipcp, struct sem_array, sem_perm);\n+\t*locknum = sem_lock(sma, sops, nsops);\n+\tsem_unlock(sma, *locknum);\n \tsma = ERR_PTR(-EINVAL);\n static inline void sem_lock_and_putref(struct sem_array *sma)\n-\tipc_lock_by_ptr(&sma->sem_perm);\n+\tsem_lock(sma, NULL, -1);\n \tipc_rcu_putref(sma);\n static inline void sem_getref_and_unlock(struct sem_array *sma)\n-\tipc_rcu_getref(sma);\n-\tipc_unlock(&(sma)->sem_perm);\n+\tWARN_ON_ONCE(!ipc_rcu_getref(sma));\n+\tsem_unlock(sma, -1);\n static inline void sem_putref(struct sem_array *sma)\n-\tipc_lock_by_ptr(&sma->sem_perm);\n-\tipc_rcu_putref(sma);\n-\tipc_unlock(&(sma)->sem_perm);\n+\tsem_lock_and_putref(sma);\n+\tsem_unlock(sma, -1);\n@@ -276,9 +345,9 @@ static inline void sem_putref(struct sem_array *sma)\n static inline void sem_getref(struct sem_array *sma)\n-\tspin_lock(&(sma)->sem_perm.lock);\n-\tipc_rcu_getref(sma);\n-\tipc_unlock(&(sma)->sem_perm);\n+\tsem_lock(sma, NULL, -1);\n+\tWARN_ON_ONCE(!ipc_rcu_getref(sma));\n+\tsem_unlock(sma, -1);\n \tsma->sem_base = (struct sem *) &sma[1];\n \t\tINIT_LIST_HEAD(&sma->sem_base[i].sem_pending);\n+\t\tspin_lock_init(&sma->sem_base[i].lock);\n \tsma->complex_count = 0;\n \tINIT_LIST_HEAD(&sma->sem_pending);\n \tINIT_LIST_HEAD(&sma->list_id);\n \tsma->sem_nsems = nsems;\n \tsma->sem_ctime = get_seconds();\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, -1);\n \treturn sma->sem_perm.id;\n \tsem_rmid(ns, sma);\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, -1);\n \tns->used_sems -= sma->sem_nsems;\n \tstruct sem_array *sma;\n-\tsma = sem_lock_check(ns, semid);\n-\tif (IS_ERR(sma))\n-\t\treturn PTR_ERR(sma);\n-\tnsems = sma->sem_nsems;\n-\tif (ipcperms(ns, &sma->sem_perm, S_IWUGO))\n+\tsma = sem_obtain_object_check(ns, semid);\n+\tif (IS_ERR(sma)) {\n+\t\treturn PTR_ERR(sma);\n+\tif (semnum < 0 || semnum >= sma->sem_nsems) {\n+\tif (ipcperms(ns, &sma->sem_perm, S_IWUGO)) {\n \terr = security_sem_semctl(sma, SETVAL);\n+\tsem_lock(sma, NULL, -1);\n \tcurr = &sma->sem_base[semnum];\n \tassert_spin_locked(&sma->sem_perm.lock);\n \tlist_for_each_entry(un, &sma->list_id, list_id)\n \tsma->sem_ctime = get_seconds();\n \tdo_smart_update(sma, NULL, 0, 0, &tasks);\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, -1);\n \t\t\tsem_lock_and_putref(sma);\n \t\t\tif (sma->sem_perm.deleted) {\n-\t\t\t\tsem_unlock(sma);\n+\t\t\t\tsem_unlock(sma, -1);\n+\t\t\tsem_lock(sma, NULL, -1);\n-\t\tspin_lock(&sma->sem_perm.lock);\n \t\tfor (i = 0; i < sma->sem_nsems; i++)\n \t\t\tsem_io[i] = sma->sem_base[i].semval;\n-\t\tsem_unlock(sma);\n+\t\tsem_unlock(sma, -1);\n-\t\tipc_rcu_getref(sma);\n+\t\tif (!ipc_rcu_getref(sma)) {\n \t\tsem_lock_and_putref(sma);\n \t\tif (sma->sem_perm.deleted) {\n-\t\t\tsem_unlock(sma);\n+\t\t\tsem_unlock(sma, -1);\n-\tspin_lock(&sma->sem_perm.lock);\n+\tsem_lock(sma, NULL, -1);\n \tcurr = &sma->sem_base[semnum];\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, -1);\n-\t\tipc_lock_object(&sma->sem_perm);\n+\t\tsem_lock(sma, NULL, -1);\n-\t\tipc_lock_object(&sma->sem_perm);\n+\t\tsem_lock(sma, NULL, -1);\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, -1);\n \tstruct sem_array *sma;\n \tnsems = sma->sem_nsems;\n-\tipc_rcu_getref(sma);\n+\tif (!ipc_rcu_getref(sma)) {\n \tsem_lock_and_putref(sma);\n \tif (sma->sem_perm.deleted) {\n-\t\tsem_unlock(sma);\n+\t\tsem_unlock(sma, -1);\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, -1);\n \tsma = sem_obtain_object_check(ns, semid);\n \tif (IS_ERR(sma)) {\n \t\terror = PTR_ERR(sma);\n-\tipc_lock_object(&sma->sem_perm);\n+\tlocknum = sem_lock(sma, sops, nsops);\n \terror = try_atomic_semop (sma, sops, nsops, un, task_tgid_vnr(current));\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, locknum);\n-\tsma = sem_obtain_lock(ns, semid);\n+\tsma = sem_obtain_lock(ns, semid, sops, nsops, &locknum);\n \tunlink_queue(sma, &queue);\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, locknum);\n \t\tstruct sem_array *sma;\n-\t\tsma = sem_lock_check(tsk->nsproxy->ipc_ns, un->semid);\n+\t\tsma = sem_obtain_object_check(tsk->nsproxy->ipc_ns, un->semid);\n-\t\tif (IS_ERR(sma))\n+\t\tif (IS_ERR(sma)) {\n+\t\tsem_lock(sma, NULL, -1);\n-\t\t\tsem_unlock(sma);\n+\t\t\tsem_unlock(sma, -1);\n \t\tdo_smart_update(sma, NULL, 0, 1, &tasks);\n-\t\tsem_unlock(sma);\n+\t\tsem_unlock(sma, -1);",
    "critical_vars": [
      "sma"
    ],
    "variable_definitions": {
      "sma": "struct sem_array *sma;"
    },
    "variable_types": {
      "sma": "struct pointer"
    },
    "type_mapping": {
      "sma": "struct pointer"
    },
    "vulnerable_line": "for (i = 0; i < nsems; i++) { if (sem_io[i] > SEMVMX) { sem_putref(sma); err = -ERANGE; goto out_free; }}",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The loop checks if sem_io[i] exceeds SEMVMX without validating that nsems is a legitimate size, potentially allowing an oversized allocation or invalid accesses resulting from nsems being manipulated, leading to integer overflow or out-of-bounds errors."
  },
  {
    "fix_code": "static int semctl_down(struct ipc_namespace *ns, int semid,\n\t\t       int cmd, int version, void __user *p)\n{\n\tstruct sem_array *sma;\n\tint err;\n\tstruct semid64_ds semid64;\n\tstruct kern_ipc_perm *ipcp;\n\n\tif(cmd == IPC_SET) {\n\t\tif (copy_semid_from_user(&semid64, p, version))\n\t\t\treturn -EFAULT;\n\t}\n\n\tipcp = ipcctl_pre_down_nolock(ns, &sem_ids(ns), semid, cmd,\n\t\t\t\t      &semid64.sem_perm, 0);\n\tif (IS_ERR(ipcp))\n\t\treturn PTR_ERR(ipcp);\n\n\tsma = container_of(ipcp, struct sem_array, sem_perm);\n\n\terr = security_sem_semctl(sma, cmd);\n\tif (err) {\n\t\trcu_read_unlock();\n\t\tgoto out_unlock;\n\t}\n\n\tswitch(cmd){\n\tcase IPC_RMID:\n\t\tsem_lock(sma, NULL, -1);\n\t\tfreeary(ns, ipcp);\n\t\tgoto out_up;\n\tcase IPC_SET:\n\t\tsem_lock(sma, NULL, -1);\n\t\terr = ipc_update_perm(&semid64.sem_perm, ipcp);\n\t\tif (err)\n\t\t\tgoto out_unlock;\n\t\tsma->sem_ctime = get_seconds();\n\t\tbreak;\n\tdefault:\n\t\trcu_read_unlock();\n\t\terr = -EINVAL;\n\t\tgoto out_up;\n\t}\n\nout_unlock:\n\tsem_unlock(sma, -1);\nout_up:\n\tup_write(&sem_ids(ns).rw_mutex);\n\treturn err;\n}",
    "diff": "-#define sem_unlock(sma)\t\tipc_unlock(&(sma)->sem_perm)\n #define sem_checkid(sma, semid)\tipc_checkid(&sma->sem_perm, semid)\n+ * Carefully guard against sma->complex_count changing between zero\n+ * sma->complex_count cannot change while we are holding the lock,\n+static inline int sem_lock(struct sem_array *sma, struct sembuf *sops,\n+\tif (nsops == 1 && !sma->complex_count) {\n+\t\tstruct sem *sem = sma->sem_base + sops->sem_num;\n+\t\t * If sma->complex_count was set while we were spinning,\n+\t\tif (unlikely(sma->complex_count)) {\n+\t\tif (unlikely(spin_is_locked(&sma->sem_perm.lock))) {\n+\t\t\tspin_unlock_wait(&sma->sem_perm.lock);\n+\t\tspin_lock(&sma->sem_perm.lock);\n+\t\tfor (i = 0; i < sma->sem_nsems; i++) {\n+\t\t\tstruct sem *sem = sma->sem_base + i;\n+static inline void sem_unlock(struct sem_array *sma, int locknum)\n+\t\tspin_unlock(&sma->sem_perm.lock);\n+\t\tstruct sem *sem = sma->sem_base + locknum;\n \tstruct sem_array *sma;\n+\tsma = container_of(ipcp, struct sem_array, sem_perm);\n+\t*locknum = sem_lock(sma, sops, nsops);\n+\tsem_unlock(sma, *locknum);\n \tsma = ERR_PTR(-EINVAL);\n static inline void sem_lock_and_putref(struct sem_array *sma)\n-\tipc_lock_by_ptr(&sma->sem_perm);\n+\tsem_lock(sma, NULL, -1);\n \tipc_rcu_putref(sma);\n static inline void sem_getref_and_unlock(struct sem_array *sma)\n-\tipc_rcu_getref(sma);\n-\tipc_unlock(&(sma)->sem_perm);\n+\tWARN_ON_ONCE(!ipc_rcu_getref(sma));\n+\tsem_unlock(sma, -1);\n static inline void sem_putref(struct sem_array *sma)\n-\tipc_lock_by_ptr(&sma->sem_perm);\n-\tipc_rcu_putref(sma);\n-\tipc_unlock(&(sma)->sem_perm);\n+\tsem_lock_and_putref(sma);\n+\tsem_unlock(sma, -1);\n@@ -276,9 +345,9 @@ static inline void sem_putref(struct sem_array *sma)\n static inline void sem_getref(struct sem_array *sma)\n-\tspin_lock(&(sma)->sem_perm.lock);\n-\tipc_rcu_getref(sma);\n-\tipc_unlock(&(sma)->sem_perm);\n+\tsem_lock(sma, NULL, -1);\n+\tWARN_ON_ONCE(!ipc_rcu_getref(sma));\n+\tsem_unlock(sma, -1);\n \tsma->sem_base = (struct sem *) &sma[1];\n \t\tINIT_LIST_HEAD(&sma->sem_base[i].sem_pending);\n+\t\tspin_lock_init(&sma->sem_base[i].lock);\n \tsma->complex_count = 0;\n \tINIT_LIST_HEAD(&sma->sem_pending);\n \tINIT_LIST_HEAD(&sma->list_id);\n \tsma->sem_nsems = nsems;\n \tsma->sem_ctime = get_seconds();\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, -1);\n \treturn sma->sem_perm.id;\n \tsem_rmid(ns, sma);\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, -1);\n \tns->used_sems -= sma->sem_nsems;\n \tstruct sem_array *sma;\n-\tsma = sem_lock_check(ns, semid);\n-\tif (IS_ERR(sma))\n-\t\treturn PTR_ERR(sma);\n-\tnsems = sma->sem_nsems;\n-\tif (ipcperms(ns, &sma->sem_perm, S_IWUGO))\n+\tsma = sem_obtain_object_check(ns, semid);\n+\tif (IS_ERR(sma)) {\n+\t\treturn PTR_ERR(sma);\n+\tif (semnum < 0 || semnum >= sma->sem_nsems) {\n+\tif (ipcperms(ns, &sma->sem_perm, S_IWUGO)) {\n \terr = security_sem_semctl(sma, SETVAL);\n+\tsem_lock(sma, NULL, -1);\n \tcurr = &sma->sem_base[semnum];\n \tassert_spin_locked(&sma->sem_perm.lock);\n \tlist_for_each_entry(un, &sma->list_id, list_id)\n \tsma->sem_ctime = get_seconds();\n \tdo_smart_update(sma, NULL, 0, 0, &tasks);\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, -1);\n \t\t\tsem_lock_and_putref(sma);\n \t\t\tif (sma->sem_perm.deleted) {\n-\t\t\t\tsem_unlock(sma);\n+\t\t\t\tsem_unlock(sma, -1);\n+\t\t\tsem_lock(sma, NULL, -1);\n-\t\tspin_lock(&sma->sem_perm.lock);\n \t\tfor (i = 0; i < sma->sem_nsems; i++)\n \t\t\tsem_io[i] = sma->sem_base[i].semval;\n-\t\tsem_unlock(sma);\n+\t\tsem_unlock(sma, -1);\n-\t\tipc_rcu_getref(sma);\n+\t\tif (!ipc_rcu_getref(sma)) {\n \t\tsem_lock_and_putref(sma);\n \t\tif (sma->sem_perm.deleted) {\n-\t\t\tsem_unlock(sma);\n+\t\t\tsem_unlock(sma, -1);\n-\tspin_lock(&sma->sem_perm.lock);\n+\tsem_lock(sma, NULL, -1);\n \tcurr = &sma->sem_base[semnum];\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, -1);\n-\t\tipc_lock_object(&sma->sem_perm);\n+\t\tsem_lock(sma, NULL, -1);\n-\t\tipc_lock_object(&sma->sem_perm);\n+\t\tsem_lock(sma, NULL, -1);\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, -1);\n \tstruct sem_array *sma;\n \tnsems = sma->sem_nsems;\n-\tipc_rcu_getref(sma);\n+\tif (!ipc_rcu_getref(sma)) {\n \tsem_lock_and_putref(sma);\n \tif (sma->sem_perm.deleted) {\n-\t\tsem_unlock(sma);\n+\t\tsem_unlock(sma, -1);\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, -1);\n \tsma = sem_obtain_object_check(ns, semid);\n \tif (IS_ERR(sma)) {\n \t\terror = PTR_ERR(sma);\n-\tipc_lock_object(&sma->sem_perm);\n+\tlocknum = sem_lock(sma, sops, nsops);\n \terror = try_atomic_semop (sma, sops, nsops, un, task_tgid_vnr(current));\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, locknum);\n-\tsma = sem_obtain_lock(ns, semid);\n+\tsma = sem_obtain_lock(ns, semid, sops, nsops, &locknum);\n \tunlink_queue(sma, &queue);\n-\tsem_unlock(sma);\n+\tsem_unlock(sma, locknum);\n \t\tstruct sem_array *sma;\n-\t\tsma = sem_lock_check(tsk->nsproxy->ipc_ns, un->semid);\n+\t\tsma = sem_obtain_object_check(tsk->nsproxy->ipc_ns, un->semid);\n-\t\tif (IS_ERR(sma))\n+\t\tif (IS_ERR(sma)) {\n+\t\tsem_lock(sma, NULL, -1);\n-\t\t\tsem_unlock(sma);\n+\t\t\tsem_unlock(sma, -1);\n \t\tdo_smart_update(sma, NULL, 0, 1, &tasks);\n-\t\tsem_unlock(sma);\n+\t\tsem_unlock(sma, -1);",
    "critical_vars": [
      "sma"
    ],
    "variable_definitions": {
      "sma": "struct sem_array *sma;"
    },
    "variable_types": {
      "sma": "struct pointer"
    },
    "type_mapping": {
      "sma": "struct pointer"
    },
    "vulnerable_line": "sma = container_of(ipcp, struct sem_array, sem_perm);",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "Improper management of reference count in ipc_rcu_putref can lead to overflows that may exceed the semaphore's intended limit, allowing an attacker to exhaust system resources and cause denial of service."
  },
  {
    "fix_code": "static struct sem_undo *find_alloc_undo(struct ipc_namespace *ns, int semid)\n{\n\tstruct sem_array *sma;\n\tstruct sem_undo_list *ulp;\n\tstruct sem_undo *un, *new;\n\tint nsems, error;\n\n\terror = get_undo_list(&ulp);\n\tif (error)\n\t\treturn ERR_PTR(error);\n\n\trcu_read_lock();\n\tspin_lock(&ulp->lock);\n\tun = lookup_undo(ulp, semid);\n\tspin_unlock(&ulp->lock);\n\tif (likely(un!=NULL))\n\t\tgoto out;\n\n\t/* no undo structure around - allocate one. */\n\t/* step 1: figure out the size of the semaphore array */\n\tsma = sem_obtain_object_check(ns, semid);\n\tif (IS_ERR(sma)) {\n\t\trcu_read_unlock();\n\t\treturn ERR_CAST(sma);\n\t}\n\n\tnsems = sma->sem_nsems;\n\tif (!ipc_rcu_getref(sma)) {\n\t\trcu_read_unlock();\n\t\tun = ERR_PTR(-EIDRM);\n\t\tgoto out;\n\t}\n\trcu_read_unlock();\n\n\t/* step 2: allocate new undo structure */\n\tnew = kzalloc(sizeof(struct sem_undo) + sizeof(short)*nsems, GFP_KERNEL);\n\tif (!new) {\n\t\tsem_putref(sma);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\t/* step 3: Acquire the lock on semaphore array */\n\tsem_lock_and_putref(sma);\n\tif (sma->sem_perm.deleted) {\n\t\tsem_unlock(sma, -1);\n\t\tkfree(new);\n\t\tun = ERR_PTR(-EIDRM);\n\t\tgoto out;\n\t}\n\tspin_lock(&ulp->lock);\n\n\t/*\n\t * step 4: check for races: did someone else allocate the undo struct?\n\t */\n\tun = lookup_undo(ulp, semid);\n\tif (un) {\n\t\tkfree(new);\n\t\tgoto success;\n\t}\n\t/* step 5: initialize & link new undo structure */\n\tnew->semadj = (short *) &new[1];\n\tnew->ulp = ulp;\n\tnew->semid = semid;\n\tassert_spin_locked(&ulp->lock);\n\tlist_add_rcu(&new->list_proc, &ulp->list_proc);\n\tassert_spin_locked(&sma->sem_perm.lock);\n\tlist_add(&new->list_id, &sma->list_id);\n\tun = new;\n\nsuccess:\n\tspin_unlock(&ulp->lock);\n\trcu_read_lock();\n\tsem_unlock(sma, -1);\nout:\n\treturn un;\n}",
    "diff": "-\tint error;\n+\tint nsems, error;\n \terror = get_undo_list(&ulp);\n \tif (error)\n \t\t\terror = PTR_ERR(un);\n \t\terror = PTR_ERR(sma);\n \terror = -EIDRM;\n \terror = try_atomic_semop (sma, sops, nsops, un, task_tgid_vnr(current));\n \tif (error <= 0) {",
    "critical_vars": [
      "error"
    ],
    "variable_definitions": {
      "error": "int nsems, error;"
    },
    "variable_types": {
      "error": "integer"
    },
    "type_mapping": {
      "error": "Integer"
    },
    "vulnerable_line": "nsems = sma->sem_nsems;",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The value of sma->sem_nsems is used to allocate an array which may overflow if nsems exceeds the maximum limit, leading to potential denial of service due to memory allocation issues."
  },
  {
    "fix_code": "static struct sem_undo *find_alloc_undo(struct ipc_namespace *ns, int semid)\n{\n\tstruct sem_array *sma;\n\tstruct sem_undo_list *ulp;\n\tstruct sem_undo *un, *new;\n\tint nsems, error;\n\n\terror = get_undo_list(&ulp);\n\tif (error)\n\t\treturn ERR_PTR(error);\n\n\trcu_read_lock();\n\tspin_lock(&ulp->lock);\n\tun = lookup_undo(ulp, semid);\n\tspin_unlock(&ulp->lock);\n\tif (likely(un!=NULL))\n\t\tgoto out;\n\n\t/* no undo structure around - allocate one. */\n\t/* step 1: figure out the size of the semaphore array */\n\tsma = sem_obtain_object_check(ns, semid);\n\tif (IS_ERR(sma)) {\n\t\trcu_read_unlock();\n\t\treturn ERR_CAST(sma);\n\t}\n\n\tnsems = sma->sem_nsems;\n\tif (!ipc_rcu_getref(sma)) {\n\t\trcu_read_unlock();\n\t\tun = ERR_PTR(-EIDRM);\n\t\tgoto out;\n\t}\n\trcu_read_unlock();\n\n\t/* step 2: allocate new undo structure */\n\tnew = kzalloc(sizeof(struct sem_undo) + sizeof(short)*nsems, GFP_KERNEL);\n\tif (!new) {\n\t\tsem_putref(sma);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\t/* step 3: Acquire the lock on semaphore array */\n\tsem_lock_and_putref(sma);\n\tif (sma->sem_perm.deleted) {\n\t\tsem_unlock(sma, -1);\n\t\tkfree(new);\n\t\tun = ERR_PTR(-EIDRM);\n\t\tgoto out;\n\t}\n\tspin_lock(&ulp->lock);\n\n\t/*\n\t * step 4: check for races: did someone else allocate the undo struct?\n\t */\n\tun = lookup_undo(ulp, semid);\n\tif (un) {\n\t\tkfree(new);\n\t\tgoto success;\n\t}\n\t/* step 5: initialize & link new undo structure */\n\tnew->semadj = (short *) &new[1];\n\tnew->ulp = ulp;\n\tnew->semid = semid;\n\tassert_spin_locked(&ulp->lock);\n\tlist_add_rcu(&new->list_proc, &ulp->list_proc);\n\tassert_spin_locked(&sma->sem_perm.lock);\n\tlist_add(&new->list_id, &sma->list_id);\n\tun = new;\n\nsuccess:\n\tspin_unlock(&ulp->lock);\n\trcu_read_lock();\n\tsem_unlock(sma, -1);\nout:\n\treturn un;\n}",
    "diff": "-\tint error;\n+\tint nsems, error;\n \terror = get_undo_list(&ulp);\n \tif (error)\n \t\t\terror = PTR_ERR(un);\n \t\terror = PTR_ERR(sma);\n \terror = -EIDRM;\n \terror = try_atomic_semop (sma, sops, nsops, un, task_tgid_vnr(current));\n \tif (error <= 0) {",
    "critical_vars": [
      "error"
    ],
    "variable_definitions": {
      "error": "int error;"
    },
    "variable_types": {
      "error": "integer"
    },
    "type_mapping": {
      "error": "Integer"
    },
    "vulnerable_line": "nsems = sma->sem_nsems;",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The variable nsems is assigned from sma->sem_nsems which could lead to an integer overflow during subsequent memory allocations if sem_nsems holds a value too large for an integer, causing vulnerabilities in memory management and potentially leading to denial of service."
  },
  {
    "fix_code": "SYSCALL_DEFINE4(semtimedop, int, semid, struct sembuf __user *, tsops,\n\t\tunsigned, nsops, const struct timespec __user *, timeout)\n{\n\tint error = -EINVAL;\n\tstruct sem_array *sma;\n\tstruct sembuf fast_sops[SEMOPM_FAST];\n\tstruct sembuf* sops = fast_sops, *sop;\n\tstruct sem_undo *un;\n\tint undos = 0, alter = 0, max, locknum;\n\tstruct sem_queue queue;\n\tunsigned long jiffies_left = 0;\n\tstruct ipc_namespace *ns;\n\tstruct list_head tasks;\n\n\tns = current->nsproxy->ipc_ns;\n\n\tif (nsops < 1 || semid < 0)\n\t\treturn -EINVAL;\n\tif (nsops > ns->sc_semopm)\n\t\treturn -E2BIG;\n\tif(nsops > SEMOPM_FAST) {\n\t\tsops = kmalloc(sizeof(*sops)*nsops,GFP_KERNEL);\n\t\tif(sops==NULL)\n\t\t\treturn -ENOMEM;\n\t}\n\tif (copy_from_user (sops, tsops, nsops * sizeof(*tsops))) {\n\t\terror=-EFAULT;\n\t\tgoto out_free;\n\t}\n\tif (timeout) {\n\t\tstruct timespec _timeout;\n\t\tif (copy_from_user(&_timeout, timeout, sizeof(*timeout))) {\n\t\t\terror = -EFAULT;\n\t\t\tgoto out_free;\n\t\t}\n\t\tif (_timeout.tv_sec < 0 || _timeout.tv_nsec < 0 ||\n\t\t\t_timeout.tv_nsec >= 1000000000L) {\n\t\t\terror = -EINVAL;\n\t\t\tgoto out_free;\n\t\t}\n\t\tjiffies_left = timespec_to_jiffies(&_timeout);\n\t}\n\tmax = 0;\n\tfor (sop = sops; sop < sops + nsops; sop++) {\n\t\tif (sop->sem_num >= max)\n\t\t\tmax = sop->sem_num;\n\t\tif (sop->sem_flg & SEM_UNDO)\n\t\t\tundos = 1;\n\t\tif (sop->sem_op != 0)\n\t\t\talter = 1;\n\t}\n\n\tINIT_LIST_HEAD(&tasks);\n\n\tif (undos) {\n\t\t/* On success, find_alloc_undo takes the rcu_read_lock */\n\t\tun = find_alloc_undo(ns, semid);\n\t\tif (IS_ERR(un)) {\n\t\t\terror = PTR_ERR(un);\n\t\t\tgoto out_free;\n\t\t}\n\t} else {\n\t\tun = NULL;\n\t\trcu_read_lock();\n\t}\n\n\tsma = sem_obtain_object_check(ns, semid);\n\tif (IS_ERR(sma)) {\n\t\trcu_read_unlock();\n\t\terror = PTR_ERR(sma);\n\t\tgoto out_free;\n\t}\n\n\terror = -EFBIG;\n\tif (max >= sma->sem_nsems) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terror = -EACCES;\n\tif (ipcperms(ns, &sma->sem_perm, alter ? S_IWUGO : S_IRUGO)) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terror = security_sem_semop(sma, sops, nsops, alter);\n\tif (error) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\t/*\n\t * semid identifiers are not unique - find_alloc_undo may have\n\t * allocated an undo structure, it was invalidated by an RMID\n\t * and now a new array with received the same id. Check and fail.\n\t * This case can be detected checking un->semid. The existence of\n\t * \"un\" itself is guaranteed by rcu.\n\t */\n\terror = -EIDRM;\n\tlocknum = sem_lock(sma, sops, nsops);\n\tif (un && un->semid == -1)\n\t\tgoto out_unlock_free;\n\n\terror = try_atomic_semop (sma, sops, nsops, un, task_tgid_vnr(current));\n\tif (error <= 0) {\n\t\tif (alter && error == 0)\n\t\t\tdo_smart_update(sma, sops, nsops, 1, &tasks);\n\n\t\tgoto out_unlock_free;\n\t}\n\n\t/* We need to sleep on this operation, so we put the current\n\t * task into the pending queue and go to sleep.\n\t */\n\t\t\n\tqueue.sops = sops;\n\tqueue.nsops = nsops;\n\tqueue.undo = un;\n\tqueue.pid = task_tgid_vnr(current);\n\tqueue.alter = alter;\n\n\tif (nsops == 1) {\n\t\tstruct sem *curr;\n\t\tcurr = &sma->sem_base[sops->sem_num];\n\n\t\tif (alter)\n\t\t\tlist_add_tail(&queue.list, &curr->sem_pending);\n\t\telse\n\t\t\tlist_add(&queue.list, &curr->sem_pending);\n\t} else {\n\t\tif (alter)\n\t\t\tlist_add_tail(&queue.list, &sma->sem_pending);\n\t\telse\n\t\t\tlist_add(&queue.list, &sma->sem_pending);\n\t\tsma->complex_count++;\n\t}\n\n\tqueue.status = -EINTR;\n\tqueue.sleeper = current;\n\nsleep_again:\n\tcurrent->state = TASK_INTERRUPTIBLE;\n\tsem_unlock(sma, locknum);\n\n\tif (timeout)\n\t\tjiffies_left = schedule_timeout(jiffies_left);\n\telse\n\t\tschedule();\n\n\terror = get_queue_result(&queue);\n\n\tif (error != -EINTR) {\n\t\t/* fast path: update_queue already obtained all requested\n\t\t * resources.\n\t\t * Perform a smp_mb(): User space could assume that semop()\n\t\t * is a memory barrier: Without the mb(), the cpu could\n\t\t * speculatively read in user space stale data that was\n\t\t * overwritten by the previous owner of the semaphore.\n\t\t */\n\t\tsmp_mb();\n\n\t\tgoto out_free;\n\t}\n\n\tsma = sem_obtain_lock(ns, semid, sops, nsops, &locknum);\n\n\t/*\n\t * Wait until it's guaranteed that no wakeup_sem_queue_do() is ongoing.\n\t */\n\terror = get_queue_result(&queue);\n\n\t/*\n\t * Array removed? If yes, leave without sem_unlock().\n\t */\n\tif (IS_ERR(sma)) {\n\t\tgoto out_free;\n\t}\n\n\n\t/*\n\t * If queue.status != -EINTR we are woken up by another process.\n\t * Leave without unlink_queue(), but with sem_unlock().\n\t */\n\n\tif (error != -EINTR) {\n\t\tgoto out_unlock_free;\n\t}\n\n\t/*\n\t * If an interrupt occurred we have to clean up the queue\n\t */\n\tif (timeout && jiffies_left == 0)\n\t\terror = -EAGAIN;\n\n\t/*\n\t * If the wakeup was spurious, just retry\n\t */\n\tif (error == -EINTR && !signal_pending(current))\n\t\tgoto sleep_again;\n\n\tunlink_queue(sma, &queue);\n\nout_unlock_free:\n\tsem_unlock(sma, locknum);\nout_wakeup:\n\twake_up_sem_queue_do(&tasks);\nout_free:\n\tif(sops != fast_sops)\n\t\tkfree(sops);\n\treturn error;\n}\n\nSYSCALL_DEFINE3(semop, int, semid, struct sembuf __user *, tsops,\n\t\tunsigned, nsops)\n{\n\treturn sys_semtimedop(semid, tsops, nsops, NULL);\n}\n\n/* If CLONE_SYSVSEM is set, establish sharing of SEM_UNDO state between\n * parent and child tasks.\n */\n\nint copy_semundo(unsigned long clone_flags, struct task_struct *tsk)\n{\n\tstruct sem_undo_list *undo_list;\n\tint error;\n\n\tif (clone_flags & CLONE_SYSVSEM) {\n\t\terror = get_undo_list(&undo_list);\n\t\tif (error)\n\t\t\treturn error;\n\t\tatomic_inc(&undo_list->refcnt);\n\t\ttsk->sysvsem.undo_list = undo_list;\n\t} else \n\t\ttsk->sysvsem.undo_list = NULL;\n\n\treturn 0;\n}\n\n/*\n * add semadj values to semaphores, free undo structures.\n * undo structures are not freed when semaphore arrays are destroyed\n * so some of them may be out of date.\n * IMPLEMENTATION NOTE: There is some confusion over whether the\n * set of adjustments that needs to be done should be done in an atomic\n * manner or not. That is, if we are attempting to decrement the semval\n * should we queue up and wait until we can do so legally?\n * The original implementation attempted to do this (queue and wait).\n * The current implementation does not do so. The POSIX standard\n * and SVID should be consulted to determine what behavior is mandated.\n */\nvoid exit_sem(struct task_struct *tsk)\n{\n\tstruct sem_undo_list *ulp;\n\n\tulp = tsk->sysvsem.undo_list;\n\tif (!ulp)\n\t\treturn;\n\ttsk->sysvsem.undo_list = NULL;\n\n\tif (!atomic_dec_and_test(&ulp->refcnt))\n\t\treturn;\n\n\tfor (;;) {\n\t\tstruct sem_array *sma;\n\t\tstruct sem_undo *un;\n\t\tstruct list_head tasks;\n\t\tint semid, i;\n\n\t\trcu_read_lock();\n\t\tun = list_entry_rcu(ulp->list_proc.next,\n\t\t\t\t    struct sem_undo, list_proc);\n\t\tif (&un->list_proc == &ulp->list_proc)\n\t\t\tsemid = -1;\n\t\t else\n\t\t\tsemid = un->semid;\n\n\t\tif (semid == -1) {\n\t\t\trcu_read_unlock();\n\t\t\tbreak;\n\t\t}\n\n\t\tsma = sem_obtain_object_check(tsk->nsproxy->ipc_ns, un->semid);\n\t\t/* exit_sem raced with IPC_RMID, nothing to do */\n\t\tif (IS_ERR(sma)) {\n\t\t\trcu_read_unlock();\n\t\t\tcontinue;\n\t\t}\n\n\t\tsem_lock(sma, NULL, -1);\n\t\tun = __lookup_undo(ulp, semid);\n\t\tif (un == NULL) {\n\t\t\t/* exit_sem raced with IPC_RMID+semget() that created\n\t\t\t * exactly the same semid. Nothing to do.\n\t\t\t */\n\t\t\tsem_unlock(sma, -1);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* remove un from the linked lists */\n\t\tassert_spin_locked(&sma->sem_perm.lock);\n\t\tlist_del(&un->list_id);\n\n\t\tspin_lock(&ulp->lock);\n\t\tlist_del_rcu(&un->list_proc);\n\t\tspin_unlock(&ulp->lock);\n\n\t\t/* perform adjustments registered in un */\n\t\tfor (i = 0; i < sma->sem_nsems; i++) {\n\t\t\tstruct sem * semaphore = &sma->sem_base[i];\n\t\t\tif (un->semadj[i]) {\n\t\t\t\tsemaphore->semval += un->semadj[i];\n\t\t\t\t/*\n\t\t\t\t * Range checks of the new semaphore value,\n\t\t\t\t * not defined by sus:\n\t\t\t\t * - Some unices ignore the undo entirely\n\t\t\t\t *   (e.g. HP UX 11i 11.22, Tru64 V5.1)\n\t\t\t\t * - some cap the value (e.g. FreeBSD caps\n\t\t\t\t *   at 0, but doesn't enforce SEMVMX)\n\t\t\t\t *\n\t\t\t\t * Linux caps the semaphore value, both at 0\n\t\t\t\t * and at SEMVMX.\n\t\t\t\t *\n\t\t\t\t * \tManfred <manfred@colorfullife.com>\n\t\t\t\t */\n\t\t\t\tif (semaphore->semval < 0)\n\t\t\t\t\tsemaphore->semval = 0;\n\t\t\t\tif (semaphore->semval > SEMVMX)\n\t\t\t\t\tsemaphore->semval = SEMVMX;\n\t\t\t\tsemaphore->sempid = task_tgid_vnr(current);\n\t\t\t}",
    "diff": "-\tint undos = 0, alter = 0, max;\n+\tint undos = 0, alter = 0, max, locknum;\n \tif (undos) {",
    "critical_vars": [
      "undos"
    ],
    "variable_definitions": {
      "undos": "int undos = 0, alter = 0, max, locknum;"
    },
    "variable_types": {
      "undos": "integer"
    },
    "type_mapping": {
      "undos": "Integer"
    },
    "vulnerable_line": "if (max >= sma->sem_nsems)",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Array out of bounds",
    "reasoning": "The max calculation might exceed the array bounds of sma->sem_base, leading to an invalid memory access if sem_num is improperly set in the input."
  },
  {
    "fix_code": "Function not found",
    "diff": " \tint\tsemval;\t\t/* current value */\n \tint\tsempid;\t\t/* pid of last operation */\n+\tspinlock_t\tlock;\t/* spinlock for fine-grained semtimedop */\n \tstruct list_head sem_pending; /* pending single-sop operations */\n@@ -137,7 +138,6 @@ struct sem_undo_list {\n #define sem_ids(ns)\t((ns)->ids[IPC_SEM_IDS])\n-#define sem_unlock(sma)\t\tipc_unlock(&(sma)->sem_perm)\n #define sem_checkid(sma, semid)\tipc_checkid(&sma->sem_perm, semid)\n static int newary(struct ipc_namespace *, struct ipc_params *);\n@@ -189,11 +189,90 @@ void __init sem_init (void)\n \t\t\t\tIPC_SEM_IDS, sysvipc_sem_proc_show);\n+ * If the request contains only one semaphore operation, and there are\n+ * no complex transactions pending, lock only the semaphore involved.\n+ * Otherwise, lock the entire semaphore array, since we either have\n+ * multiple semaphores in our own semops, or we need to look at\n+ * semaphores from other pending complex operations.\n+ * Carefully guard against sma->complex_count changing between zero\n+ * and non-zero while we are spinning for the lock. The value of\n+ * sma->complex_count cannot change while we are holding the lock,\n+ * so sem_unlock should be fine.\n+ * checking each local lock once. This means that the local lock paths\n+ * cannot start their critical sections while the global lock is held.\n+static inline int sem_lock(struct sem_array *sma, struct sembuf *sops,\n+\t\t\t      int nsops)\n+\tint locknum;\n+ again:\n+\tif (nsops == 1 && !sma->complex_count) {\n+\t\t/* Lock just the semaphore we are interested in. */\n+\t\tspin_lock(&sem->lock);\n+\t\t * If sma->complex_count was set while we were spinning,\n+\t\t * we may need to look at things we did not lock here.\n+\t\tif (unlikely(sma->complex_count)) {\n+\t\t\tspin_unlock(&sem->lock);\n+\t\t * Another process is holding the global lock on the\n+\t\t * sem_array; we cannot enter our critical section,\n+\t\t * but have to wait for the global lock to be released.\n+\t\tif (unlikely(spin_is_locked(&sma->sem_perm.lock))) {\n+\t\t\tspin_unlock(&sem->lock);\n+\t\t\tspin_unlock_wait(&sma->sem_perm.lock);\n+\t\t\tgoto again;\n+\t\tint i;\n+\t\t * Lock the semaphore array, and wait for all of the\n+\t\t * individual semaphore locks to go away.  The code\n+\t\t * above ensures no new single-lock holders will enter\n+\t\t * their critical section while the array lock is held.\n+\t\tspin_lock(&sma->sem_perm.lock);\n+\t\tfor (i = 0; i < sma->sem_nsems; i++) {\n+\t\t\tstruct sem *sem = sma->sem_base + i;\n+\t\t\tspin_unlock_wait(&sem->lock);\n+static inline void sem_unlock(struct sem_array *sma, int locknum)\n+\tif (locknum == -1) {\n+\t\tspin_unlock(&sma->sem_perm.lock);\n+\t\tspin_unlock(&sem->lock);\n  * sem_lock_(check_) routines are called in the paths where the rw_mutex\n  * is not held.\n-static inline struct sem_array *sem_obtain_lock(struct ipc_namespace *ns, int id)\n+static inline struct sem_array *sem_obtain_lock(struct ipc_namespace *ns,\n+\t\t\tint id, struct sembuf *sops, int nsops, int *locknum)\n \tstruct kern_ipc_perm *ipcp;\n@@ -205,7 +284,8 @@ static inline struct sem_array *sem_obtain_lock(struct ipc_namespace *ns, int id\n-\tspin_lock(&ipcp->lock);\n+\tsma = container_of(ipcp, struct sem_array, sem_perm);\n \t/* ipc_rmid() may have already freed the ID while sem_lock\n \t * was spinning: verify that the structure is still valid\n@@ -213,7 +293,7 @@ static inline struct sem_array *sem_obtain_lock(struct ipc_namespace *ns, int id\n \tif (!ipcp->deleted)\n \t\treturn container_of(ipcp, struct sem_array, sem_perm);\n-\tspin_unlock(&ipcp->lock);\n@@ -230,17 +310,6 @@ static inline struct sem_array *sem_obtain_object(struct ipc_namespace *ns, int\n \treturn container_of(ipcp, struct sem_array, sem_perm);\n-static inline struct sem_array *sem_lock_check(struct ipc_namespace *ns,\n-\t\t\t\t\t\tint id)\n-\tstruct kern_ipc_perm *ipcp = ipc_lock_check(&sem_ids(ns), id);\n-\tif (IS_ERR(ipcp))\n-\t\treturn ERR_CAST(ipcp);\n-\treturn container_of(ipcp, struct sem_array, sem_perm);\n static inline struct sem_array *sem_obtain_object_check(struct ipc_namespace *ns,\n \t\t\t\t\t\t\tint id)\n@@ -254,21 +323,21 @@ static inline struct sem_array *sem_obtain_object_check(struct ipc_namespace *ns\n static inline void sem_lock_and_putref(struct sem_array *sma)\n-\tipc_lock_by_ptr(&sma->sem_perm);\n \tipc_rcu_putref(sma);\n static inline void sem_getref_and_unlock(struct sem_array *sma)\n-\tipc_rcu_getref(sma);\n-\tipc_unlock(&(sma)->sem_perm);\n+\tWARN_ON_ONCE(!ipc_rcu_getref(sma));\n static inline void sem_putref(struct sem_array *sma)\n-\tipc_lock_by_ptr(&sma->sem_perm);\n-\tipc_rcu_putref(sma);\n-\tipc_unlock(&(sma)->sem_perm);\n@@ -276,9 +345,9 @@ static inline void sem_putref(struct sem_array *sma)\n static inline void sem_getref(struct sem_array *sma)\n-\tspin_lock(&(sma)->sem_perm.lock);\n-\tipc_rcu_getref(sma);\n-\tipc_unlock(&(sma)->sem_perm);\n+\tWARN_ON_ONCE(!ipc_rcu_getref(sma));\n static inline void sem_rmid(struct ipc_namespace *ns, struct sem_array *s)\n@@ -371,15 +440,17 @@ static int newary(struct ipc_namespace *ns, struct ipc_params *params)\n-\tfor (i = 0; i < nsems; i++)\n+\tfor (i = 0; i < nsems; i++) {\n \t\tINIT_LIST_HEAD(&sma->sem_base[i].sem_pending);\n+\t\tspin_lock_init(&sma->sem_base[i].lock);\n \tINIT_LIST_HEAD(&sma->sem_pending);\n \tINIT_LIST_HEAD(&sma->list_id);\n \tsma->sem_ctime = get_seconds();\n \treturn sma->sem_perm.id;\n@@ -818,7 +889,7 @@ static void freeary(struct ipc_namespace *ns, struct kern_ipc_perm *ipcp)\n \tsem_rmid(ns, sma);\n@@ -947,7 +1018,6 @@ static int semctl_setval(struct ipc_namespace *ns, int semid, int semnum,\n \tint err;\n-\tint nsems;\n \tstruct list_head tasks;\n \tint val;\n #if defined(CONFIG_64BIT) && defined(__BIG_ENDIAN)\n@@ -958,31 +1028,39 @@ static int semctl_setval(struct ipc_namespace *ns, int semid, int semnum,\n #endif\n-\tsma = sem_lock_check(ns, semid);\n-\tif (IS_ERR(sma))\n+\tif (val > SEMVMX || val < 0)\n-\tif (ipcperms(ns, &sma->sem_perm, S_IWUGO))\n+\tsma = sem_obtain_object_check(ns, semid);\n+\tif (IS_ERR(sma)) {\n+\tif (semnum < 0 || semnum >= sma->sem_nsems) {\n+\tif (ipcperms(ns, &sma->sem_perm, S_IWUGO)) {\n \terr = security_sem_semctl(sma, SETVAL);\n-\tif (err)\n+\tif (err) {\n-\tif(semnum < 0 || semnum >= nsems)\n-\tif (val > SEMVMX || val < 0)\n \tassert_spin_locked(&sma->sem_perm.lock);\n \tlist_for_each_entry(un, &sma->list_id, list_id)\n@@ -992,11 +1070,9 @@ static int semctl_setval(struct ipc_namespace *ns, int semid, int semnum,\n \tsma->sem_ctime = get_seconds();\n \t/* maybe some queued-up processes were waiting for this */\n static int semctl_main(struct ipc_namespace *ns, int semid, int semnum,\n@@ -1051,16 +1127,16 @@ static int semctl_main(struct ipc_namespace *ns, int semid, int semnum,\n \t\t\tif (sma->sem_perm.deleted) {\n-\t\tspin_lock(&sma->sem_perm.lock);\n \t\tfor (i = 0; i < sma->sem_nsems; i++)\n \t\t\tsem_io[i] = sma->sem_base[i].semval;\n \t\tif(copy_to_user(array, sem_io, nsems*sizeof(ushort)))\n@@ -1071,7 +1147,10 @@ static int semctl_main(struct ipc_namespace *ns, int semid, int semnum,\n \t\tint i;\n-\t\tipc_rcu_getref(sma);\n+\t\tif (!ipc_rcu_getref(sma)) {\n \t\tif(nsems > SEMMSL_FAST) {\n@@ -1097,7 +1176,7 @@ static int semctl_main(struct ipc_namespace *ns, int semid, int semnum,\n \t\tif (sma->sem_perm.deleted) {\n@@ -1124,7 +1203,7 @@ static int semctl_main(struct ipc_namespace *ns, int semid, int semnum,\n-\tspin_lock(&sma->sem_perm.lock);\n \tswitch (cmd) {\n@@ -1143,7 +1222,7 @@ static int semctl_main(struct ipc_namespace *ns, int semid, int semnum,\n@@ -1211,11 +1290,11 @@ static int semctl_down(struct ipc_namespace *ns, int semid,\n \tswitch(cmd){\n-\t\tipc_lock_object(&sma->sem_perm);\n \t\tfreeary(ns, ipcp);\n-\t\tipc_lock_object(&sma->sem_perm);\n \t\terr = ipc_update_perm(&semid64.sem_perm, ipcp);\n \t\tif (err)\n@@ -1228,7 +1307,7 @@ static int semctl_down(struct ipc_namespace *ns, int semid,\n \tup_write(&sem_ids(ns).rw_mutex);\n@@ -1340,8 +1419,7 @@ static struct sem_undo *find_alloc_undo(struct ipc_namespace *ns, int semid)\n \tstruct sem_undo_list *ulp;\n-\tint nsems;\n-\tint error;\n+\tint nsems, error;\n \terror = get_undo_list(&ulp);\n \tif (error)\n@@ -1363,7 +1441,11 @@ static struct sem_undo *find_alloc_undo(struct ipc_namespace *ns, int semid)\n-\tipc_rcu_getref(sma);\n+\tif (!ipc_rcu_getref(sma)) {\n@@ -1376,7 +1458,7 @@ static struct sem_undo *find_alloc_undo(struct ipc_namespace *ns, int semid)\n \t/* step 3: Acquire the lock on semaphore array */\n \tif (sma->sem_perm.deleted) {\n@@ -1404,7 +1486,7 @@ static struct sem_undo *find_alloc_undo(struct ipc_namespace *ns, int semid)\n \tspin_unlock(&ulp->lock);\n@@ -1444,7 +1526,7 @@ SYSCALL_DEFINE4(semtimedop, int, semid, struct sembuf __user *, tsops,\n-\tint undos = 0, alter = 0, max;\n+\tint undos = 0, alter = 0, max, locknum;\n \tunsigned long jiffies_left = 0;\n \tstruct ipc_namespace *ns;\n@@ -1488,22 +1570,23 @@ SYSCALL_DEFINE4(semtimedop, int, semid, struct sembuf __user *, tsops,\n \tif (undos) {\n+\t\t/* On success, find_alloc_undo takes the rcu_read_lock */\n \t\tun = find_alloc_undo(ns, semid);\n \t\tif (IS_ERR(un)) {\n \tsma = sem_obtain_object_check(ns, semid);\n \tif (IS_ERR(sma)) {\n-\t\tif (un)\n@@ -1534,23 +1617,9 @@ SYSCALL_DEFINE4(semtimedop, int, semid, struct sembuf __user *, tsops,\n \t * \"un\" itself is guaranteed by rcu.\n-\tipc_lock_object(&sma->sem_perm);\n-\tif (un) {\n-\t\tif (un->semid == -1) {\n-\t\t\t * rcu lock can be released, \"un\" cannot disappear:\n-\t\t\t * - sem_lock is acquired, thus IPC_RMID is\n-\t\t\t *   impossible.\n-\t\t\t * - exit_sem is impossible, it always operates on\n+\tif (un && un->semid == -1)\n \terror = try_atomic_semop (sma, sops, nsops, un, task_tgid_vnr(current));\n \tif (error <= 0) {\n@@ -1591,7 +1660,7 @@ SYSCALL_DEFINE4(semtimedop, int, semid, struct sembuf __user *, tsops,\n sleep_again:\n \tif (timeout)\n \t\tjiffies_left = schedule_timeout(jiffies_left);\n@@ -1613,7 +1682,7 @@ SYSCALL_DEFINE4(semtimedop, int, semid, struct sembuf __user *, tsops,\n-\tsma = sem_obtain_lock(ns, semid);\n+\tsma = sem_obtain_lock(ns, semid, sops, nsops, &locknum);\n \t * Wait until it's guaranteed that no wakeup_sem_queue_do() is ongoing.\n@@ -1652,7 +1721,7 @@ SYSCALL_DEFINE4(semtimedop, int, semid, struct sembuf __user *, tsops,\n \tunlink_queue(sma, &queue);\n@@ -1716,8 +1785,7 @@ void exit_sem(struct task_struct *tsk)\n \t\tstruct list_head tasks;\n-\t\tint semid;\n-\t\tint i;\n+\t\tint semid, i;\n \t\tun = list_entry_rcu(ulp->list_proc.next,\n@@ -1726,23 +1794,26 @@ void exit_sem(struct task_struct *tsk)\n \t\t\tsemid = -1;\n \t\t\tsemid = un->semid;\n-\t\tif (semid == -1)\n+\t\tif (semid == -1) {\n-\t\tsma = sem_lock_check(tsk->nsproxy->ipc_ns, un->semid);\n+\t\tsma = sem_obtain_object_check(tsk->nsproxy->ipc_ns, un->semid);\n \t\t/* exit_sem raced with IPC_RMID, nothing to do */\n-\t\tif (IS_ERR(sma))\n+\t\tif (IS_ERR(sma)) {\n \t\t\tcontinue;\n \t\tun = __lookup_undo(ulp, semid);\n \t\tif (un == NULL) {\n \t\t\t/* exit_sem raced with IPC_RMID+semget() that created\n \t\t\t * exactly the same semid. Nothing to do.\n \t\t\tcontinue;\n@@ -1782,7 +1853,7 @@ void exit_sem(struct task_struct *tsk)\n \t\t/* maybe some queued-up processes were waiting for this */",
    "critical_vars": [
      "i"
    ],
    "variable_definitions": {
      "i": "int semid, i;"
    },
    "variable_types": {
      "i": "integer"
    },
    "type_mapping": {
      "i": "Integer"
    },
    "vulnerable_line": "for (i = 0; i < nsems; i++)",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Array out of bounds",
    "reasoning": "nsems can potentially be set to a value that exceeds the allocated array, leading to out-of-bounds access."
  },
  {
    "fix_code": "Function not found",
    "diff": " \tint\tsemval;\t\t/* current value */\n \tint\tsempid;\t\t/* pid of last operation */\n+\tspinlock_t\tlock;\t/* spinlock for fine-grained semtimedop */\n \tstruct list_head sem_pending; /* pending single-sop operations */\n@@ -137,7 +138,6 @@ struct sem_undo_list {\n #define sem_ids(ns)\t((ns)->ids[IPC_SEM_IDS])\n-#define sem_unlock(sma)\t\tipc_unlock(&(sma)->sem_perm)\n #define sem_checkid(sma, semid)\tipc_checkid(&sma->sem_perm, semid)\n static int newary(struct ipc_namespace *, struct ipc_params *);\n@@ -189,11 +189,90 @@ void __init sem_init (void)\n \t\t\t\tIPC_SEM_IDS, sysvipc_sem_proc_show);\n+ * If the request contains only one semaphore operation, and there are\n+ * no complex transactions pending, lock only the semaphore involved.\n+ * Otherwise, lock the entire semaphore array, since we either have\n+ * multiple semaphores in our own semops, or we need to look at\n+ * semaphores from other pending complex operations.\n+ * Carefully guard against sma->complex_count changing between zero\n+ * and non-zero while we are spinning for the lock. The value of\n+ * sma->complex_count cannot change while we are holding the lock,\n+ * so sem_unlock should be fine.\n+ * checking each local lock once. This means that the local lock paths\n+ * cannot start their critical sections while the global lock is held.\n+static inline int sem_lock(struct sem_array *sma, struct sembuf *sops,\n+\t\t\t      int nsops)\n+\tint locknum;\n+ again:\n+\tif (nsops == 1 && !sma->complex_count) {\n+\t\t/* Lock just the semaphore we are interested in. */\n+\t\tspin_lock(&sem->lock);\n+\t\t * If sma->complex_count was set while we were spinning,\n+\t\t * we may need to look at things we did not lock here.\n+\t\tif (unlikely(sma->complex_count)) {\n+\t\t\tspin_unlock(&sem->lock);\n+\t\t * Another process is holding the global lock on the\n+\t\t * sem_array; we cannot enter our critical section,\n+\t\t * but have to wait for the global lock to be released.\n+\t\tif (unlikely(spin_is_locked(&sma->sem_perm.lock))) {\n+\t\t\tspin_unlock(&sem->lock);\n+\t\t\tspin_unlock_wait(&sma->sem_perm.lock);\n+\t\t\tgoto again;\n+\t\tint i;\n+\t\t * Lock the semaphore array, and wait for all of the\n+\t\t * individual semaphore locks to go away.  The code\n+\t\t * above ensures no new single-lock holders will enter\n+\t\t * their critical section while the array lock is held.\n+\t\tspin_lock(&sma->sem_perm.lock);\n+\t\tfor (i = 0; i < sma->sem_nsems; i++) {\n+\t\t\tstruct sem *sem = sma->sem_base + i;\n+\t\t\tspin_unlock_wait(&sem->lock);\n+static inline void sem_unlock(struct sem_array *sma, int locknum)\n+\tif (locknum == -1) {\n+\t\tspin_unlock(&sma->sem_perm.lock);\n+\t\tspin_unlock(&sem->lock);\n  * sem_lock_(check_) routines are called in the paths where the rw_mutex\n  * is not held.\n-static inline struct sem_array *sem_obtain_lock(struct ipc_namespace *ns, int id)\n+static inline struct sem_array *sem_obtain_lock(struct ipc_namespace *ns,\n+\t\t\tint id, struct sembuf *sops, int nsops, int *locknum)\n \tstruct kern_ipc_perm *ipcp;\n@@ -205,7 +284,8 @@ static inline struct sem_array *sem_obtain_lock(struct ipc_namespace *ns, int id\n-\tspin_lock(&ipcp->lock);\n+\tsma = container_of(ipcp, struct sem_array, sem_perm);\n \t/* ipc_rmid() may have already freed the ID while sem_lock\n \t * was spinning: verify that the structure is still valid\n@@ -213,7 +293,7 @@ static inline struct sem_array *sem_obtain_lock(struct ipc_namespace *ns, int id\n \tif (!ipcp->deleted)\n \t\treturn container_of(ipcp, struct sem_array, sem_perm);\n-\tspin_unlock(&ipcp->lock);\n@@ -230,17 +310,6 @@ static inline struct sem_array *sem_obtain_object(struct ipc_namespace *ns, int\n \treturn container_of(ipcp, struct sem_array, sem_perm);\n-static inline struct sem_array *sem_lock_check(struct ipc_namespace *ns,\n-\t\t\t\t\t\tint id)\n-\tstruct kern_ipc_perm *ipcp = ipc_lock_check(&sem_ids(ns), id);\n-\tif (IS_ERR(ipcp))\n-\t\treturn ERR_CAST(ipcp);\n-\treturn container_of(ipcp, struct sem_array, sem_perm);\n static inline struct sem_array *sem_obtain_object_check(struct ipc_namespace *ns,\n \t\t\t\t\t\t\tint id)\n@@ -254,21 +323,21 @@ static inline struct sem_array *sem_obtain_object_check(struct ipc_namespace *ns\n static inline void sem_lock_and_putref(struct sem_array *sma)\n-\tipc_lock_by_ptr(&sma->sem_perm);\n \tipc_rcu_putref(sma);\n static inline void sem_getref_and_unlock(struct sem_array *sma)\n-\tipc_rcu_getref(sma);\n-\tipc_unlock(&(sma)->sem_perm);\n+\tWARN_ON_ONCE(!ipc_rcu_getref(sma));\n static inline void sem_putref(struct sem_array *sma)\n-\tipc_lock_by_ptr(&sma->sem_perm);\n-\tipc_rcu_putref(sma);\n-\tipc_unlock(&(sma)->sem_perm);\n@@ -276,9 +345,9 @@ static inline void sem_putref(struct sem_array *sma)\n static inline void sem_getref(struct sem_array *sma)\n-\tspin_lock(&(sma)->sem_perm.lock);\n-\tipc_rcu_getref(sma);\n-\tipc_unlock(&(sma)->sem_perm);\n+\tWARN_ON_ONCE(!ipc_rcu_getref(sma));\n static inline void sem_rmid(struct ipc_namespace *ns, struct sem_array *s)\n@@ -371,15 +440,17 @@ static int newary(struct ipc_namespace *ns, struct ipc_params *params)\n-\tfor (i = 0; i < nsems; i++)\n+\tfor (i = 0; i < nsems; i++) {\n \t\tINIT_LIST_HEAD(&sma->sem_base[i].sem_pending);\n+\t\tspin_lock_init(&sma->sem_base[i].lock);\n \tINIT_LIST_HEAD(&sma->sem_pending);\n \tINIT_LIST_HEAD(&sma->list_id);\n \tsma->sem_ctime = get_seconds();\n \treturn sma->sem_perm.id;\n@@ -818,7 +889,7 @@ static void freeary(struct ipc_namespace *ns, struct kern_ipc_perm *ipcp)\n \tsem_rmid(ns, sma);\n@@ -947,7 +1018,6 @@ static int semctl_setval(struct ipc_namespace *ns, int semid, int semnum,\n \tint err;\n-\tint nsems;\n \tstruct list_head tasks;\n \tint val;\n #if defined(CONFIG_64BIT) && defined(__BIG_ENDIAN)\n@@ -958,31 +1028,39 @@ static int semctl_setval(struct ipc_namespace *ns, int semid, int semnum,\n #endif\n-\tsma = sem_lock_check(ns, semid);\n-\tif (IS_ERR(sma))\n+\tif (val > SEMVMX || val < 0)\n-\tif (ipcperms(ns, &sma->sem_perm, S_IWUGO))\n+\tsma = sem_obtain_object_check(ns, semid);\n+\tif (IS_ERR(sma)) {\n+\tif (semnum < 0 || semnum >= sma->sem_nsems) {\n+\tif (ipcperms(ns, &sma->sem_perm, S_IWUGO)) {\n \terr = security_sem_semctl(sma, SETVAL);\n-\tif (err)\n+\tif (err) {\n-\tif(semnum < 0 || semnum >= nsems)\n-\tif (val > SEMVMX || val < 0)\n \tassert_spin_locked(&sma->sem_perm.lock);\n \tlist_for_each_entry(un, &sma->list_id, list_id)\n@@ -992,11 +1070,9 @@ static int semctl_setval(struct ipc_namespace *ns, int semid, int semnum,\n \tsma->sem_ctime = get_seconds();\n \t/* maybe some queued-up processes were waiting for this */\n static int semctl_main(struct ipc_namespace *ns, int semid, int semnum,\n@@ -1051,16 +1127,16 @@ static int semctl_main(struct ipc_namespace *ns, int semid, int semnum,\n \t\t\tif (sma->sem_perm.deleted) {\n-\t\tspin_lock(&sma->sem_perm.lock);\n \t\tfor (i = 0; i < sma->sem_nsems; i++)\n \t\t\tsem_io[i] = sma->sem_base[i].semval;\n \t\tif(copy_to_user(array, sem_io, nsems*sizeof(ushort)))\n@@ -1071,7 +1147,10 @@ static int semctl_main(struct ipc_namespace *ns, int semid, int semnum,\n \t\tint i;\n-\t\tipc_rcu_getref(sma);\n+\t\tif (!ipc_rcu_getref(sma)) {\n \t\tif(nsems > SEMMSL_FAST) {\n@@ -1097,7 +1176,7 @@ static int semctl_main(struct ipc_namespace *ns, int semid, int semnum,\n \t\tif (sma->sem_perm.deleted) {\n@@ -1124,7 +1203,7 @@ static int semctl_main(struct ipc_namespace *ns, int semid, int semnum,\n-\tspin_lock(&sma->sem_perm.lock);\n \tswitch (cmd) {\n@@ -1143,7 +1222,7 @@ static int semctl_main(struct ipc_namespace *ns, int semid, int semnum,\n@@ -1211,11 +1290,11 @@ static int semctl_down(struct ipc_namespace *ns, int semid,\n \tswitch(cmd){\n-\t\tipc_lock_object(&sma->sem_perm);\n \t\tfreeary(ns, ipcp);\n-\t\tipc_lock_object(&sma->sem_perm);\n \t\terr = ipc_update_perm(&semid64.sem_perm, ipcp);\n \t\tif (err)\n@@ -1228,7 +1307,7 @@ static int semctl_down(struct ipc_namespace *ns, int semid,\n \tup_write(&sem_ids(ns).rw_mutex);\n@@ -1340,8 +1419,7 @@ static struct sem_undo *find_alloc_undo(struct ipc_namespace *ns, int semid)\n \tstruct sem_undo_list *ulp;\n-\tint nsems;\n-\tint error;\n+\tint nsems, error;\n \terror = get_undo_list(&ulp);\n \tif (error)\n@@ -1363,7 +1441,11 @@ static struct sem_undo *find_alloc_undo(struct ipc_namespace *ns, int semid)\n-\tipc_rcu_getref(sma);\n+\tif (!ipc_rcu_getref(sma)) {\n@@ -1376,7 +1458,7 @@ static struct sem_undo *find_alloc_undo(struct ipc_namespace *ns, int semid)\n \t/* step 3: Acquire the lock on semaphore array */\n \tif (sma->sem_perm.deleted) {\n@@ -1404,7 +1486,7 @@ static struct sem_undo *find_alloc_undo(struct ipc_namespace *ns, int semid)\n \tspin_unlock(&ulp->lock);\n@@ -1444,7 +1526,7 @@ SYSCALL_DEFINE4(semtimedop, int, semid, struct sembuf __user *, tsops,\n-\tint undos = 0, alter = 0, max;\n+\tint undos = 0, alter = 0, max, locknum;\n \tunsigned long jiffies_left = 0;\n \tstruct ipc_namespace *ns;\n@@ -1488,22 +1570,23 @@ SYSCALL_DEFINE4(semtimedop, int, semid, struct sembuf __user *, tsops,\n \tif (undos) {\n+\t\t/* On success, find_alloc_undo takes the rcu_read_lock */\n \t\tun = find_alloc_undo(ns, semid);\n \t\tif (IS_ERR(un)) {\n \tsma = sem_obtain_object_check(ns, semid);\n \tif (IS_ERR(sma)) {\n-\t\tif (un)\n@@ -1534,23 +1617,9 @@ SYSCALL_DEFINE4(semtimedop, int, semid, struct sembuf __user *, tsops,\n \t * \"un\" itself is guaranteed by rcu.\n-\tipc_lock_object(&sma->sem_perm);\n-\tif (un) {\n-\t\tif (un->semid == -1) {\n-\t\t\t * rcu lock can be released, \"un\" cannot disappear:\n-\t\t\t * - sem_lock is acquired, thus IPC_RMID is\n-\t\t\t *   impossible.\n-\t\t\t * - exit_sem is impossible, it always operates on\n+\tif (un && un->semid == -1)\n \terror = try_atomic_semop (sma, sops, nsops, un, task_tgid_vnr(current));\n \tif (error <= 0) {\n@@ -1591,7 +1660,7 @@ SYSCALL_DEFINE4(semtimedop, int, semid, struct sembuf __user *, tsops,\n sleep_again:\n \tif (timeout)\n \t\tjiffies_left = schedule_timeout(jiffies_left);\n@@ -1613,7 +1682,7 @@ SYSCALL_DEFINE4(semtimedop, int, semid, struct sembuf __user *, tsops,\n-\tsma = sem_obtain_lock(ns, semid);\n+\tsma = sem_obtain_lock(ns, semid, sops, nsops, &locknum);\n \t * Wait until it's guaranteed that no wakeup_sem_queue_do() is ongoing.\n@@ -1652,7 +1721,7 @@ SYSCALL_DEFINE4(semtimedop, int, semid, struct sembuf __user *, tsops,\n \tunlink_queue(sma, &queue);\n@@ -1716,8 +1785,7 @@ void exit_sem(struct task_struct *tsk)\n \t\tstruct list_head tasks;\n-\t\tint semid;\n-\t\tint i;\n+\t\tint semid, i;\n \t\tun = list_entry_rcu(ulp->list_proc.next,\n@@ -1726,23 +1794,26 @@ void exit_sem(struct task_struct *tsk)\n \t\t\tsemid = -1;\n \t\t\tsemid = un->semid;\n-\t\tif (semid == -1)\n+\t\tif (semid == -1) {\n-\t\tsma = sem_lock_check(tsk->nsproxy->ipc_ns, un->semid);\n+\t\tsma = sem_obtain_object_check(tsk->nsproxy->ipc_ns, un->semid);\n \t\t/* exit_sem raced with IPC_RMID, nothing to do */\n-\t\tif (IS_ERR(sma))\n+\t\tif (IS_ERR(sma)) {\n \t\t\tcontinue;\n \t\tun = __lookup_undo(ulp, semid);\n \t\tif (un == NULL) {\n \t\t\t/* exit_sem raced with IPC_RMID+semget() that created\n \t\t\t * exactly the same semid. Nothing to do.\n \t\t\tcontinue;\n@@ -1782,7 +1853,7 @@ void exit_sem(struct task_struct *tsk)\n \t\t/* maybe some queued-up processes were waiting for this */",
    "critical_vars": [
      "i"
    ],
    "variable_definitions": {
      "i": "int i;"
    },
    "variable_types": {
      "i": "integer"
    },
    "type_mapping": {
      "i": "Integer"
    },
    "vulnerable_line": "for (i = 0; i < nsems; i++)",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Array out of bounds",
    "reasoning": "The variable nsems could be set to an excessively high value causing the loop to exceed the allocated bounds of the array, potentially leading to a denial of service via memory corruption or system crash."
  },
  {
    "fix_code": "long do_msgsnd(int msqid, long mtype, void __user *mtext,\n\t\tsize_t msgsz, int msgflg)\n{\n\tstruct msg_queue *msq;\n\tstruct msg_msg *msg;\n\tint err;\n\tstruct ipc_namespace *ns;\n\n\tns = current->nsproxy->ipc_ns;\n\n\tif (msgsz > ns->msg_ctlmax || (long) msgsz < 0 || msqid < 0)\n\t\treturn -EINVAL;\n\tif (mtype < 1)\n\t\treturn -EINVAL;\n\n\tmsg = load_msg(mtext, msgsz);\n\tif (IS_ERR(msg))\n\t\treturn PTR_ERR(msg);\n\n\tmsg->m_type = mtype;\n\tmsg->m_ts = msgsz;\n\n\tmsq = msg_lock_check(ns, msqid);\n\tif (IS_ERR(msq)) {\n\t\terr = PTR_ERR(msq);\n\t\tgoto out_free;\n\t}\n\n\tfor (;;) {\n\t\tstruct msg_sender s;\n\n\t\terr = -EACCES;\n\t\tif (ipcperms(ns, &msq->q_perm, S_IWUGO))\n\t\t\tgoto out_unlock_free;\n\n\t\terr = security_msg_queue_msgsnd(msq, msg, msgflg);\n\t\tif (err)\n\t\t\tgoto out_unlock_free;\n\n\t\tif (msgsz + msq->q_cbytes <= msq->q_qbytes &&\n\t\t\t\t1 + msq->q_qnum <= msq->q_qbytes) {\n\t\t\tbreak;\n\t\t}\n\n\t\t/* queue full, wait: */\n\t\tif (msgflg & IPC_NOWAIT) {\n\t\t\terr = -EAGAIN;\n\t\t\tgoto out_unlock_free;\n\t\t}\n\t\tss_add(msq, &s);\n\n\t\tif (!ipc_rcu_getref(msq)) {\n\t\t\terr = -EIDRM;\n\t\t\tgoto out_unlock_free;\n\t\t}\n\n\t\tmsg_unlock(msq);\n\t\tschedule();\n\n\t\tipc_lock_by_ptr(&msq->q_perm);\n\t\tipc_rcu_putref(msq);\n\t\tif (msq->q_perm.deleted) {\n\t\t\terr = -EIDRM;\n\t\t\tgoto out_unlock_free;\n\t\t}\n\t\tss_del(&s);\n\n\t\tif (signal_pending(current)) {\n\t\t\terr = -ERESTARTNOHAND;\n\t\t\tgoto out_unlock_free;\n\t\t}\n\t}\n\n\tmsq->q_lspid = task_tgid_vnr(current);\n\tmsq->q_stime = get_seconds();\n\n\tif (!pipelined_send(msq, msg)) {\n\t\t/* no one is waiting for this message, enqueue it */\n\t\tlist_add_tail(&msg->m_list, &msq->q_messages);\n\t\tmsq->q_cbytes += msgsz;\n\t\tmsq->q_qnum++;\n\t\tatomic_add(msgsz, &ns->msg_bytes);\n\t\tatomic_inc(&ns->msg_hdrs);\n\t}\n\n\terr = 0;\n\tmsg = NULL;\n\nout_unlock_free:\n\tmsg_unlock(msq);\nout_free:\n\tif (msg != NULL)\n\t\tfree_msg(msg);\n\treturn err;\n}",
    "diff": "@@ -687,7 +687,12 @@ long do_msgsnd(int msqid, long mtype, void __user *mtext,\n \t\tss_add(msq, &s);\n-\t\tipc_rcu_getref(msq);\n+\t\tif (!ipc_rcu_getref(msq)) {\n \t\tmsg_unlock(msq);",
    "critical_vars": [
      "msq"
    ],
    "variable_definitions": {
      "msq": "struct msg_queue *msq;"
    },
    "variable_types": {
      "msq": "struct pointer"
    },
    "type_mapping": {
      "msq": "struct pointer"
    },
    "vulnerable_line": "if (!ipc_rcu_getref(msq)) {",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The ipc_rcu_getref function does not correctly manage the reference count, potentially leading to a situation where the reference count exceeds the maximum integer limit, causing integer overflow which can lead to denial of service."
  },
  {
    "fix_code": "long do_msgsnd(int msqid, long mtype, void __user *mtext,\n\t\tsize_t msgsz, int msgflg)\n{\n\tstruct msg_queue *msq;\n\tstruct msg_msg *msg;\n\tint err;\n\tstruct ipc_namespace *ns;\n\n\tns = current->nsproxy->ipc_ns;\n\n\tif (msgsz > ns->msg_ctlmax || (long) msgsz < 0 || msqid < 0)\n\t\treturn -EINVAL;\n\tif (mtype < 1)\n\t\treturn -EINVAL;\n\n\tmsg = load_msg(mtext, msgsz);\n\tif (IS_ERR(msg))\n\t\treturn PTR_ERR(msg);\n\n\tmsg->m_type = mtype;\n\tmsg->m_ts = msgsz;\n\n\tmsq = msg_lock_check(ns, msqid);\n\tif (IS_ERR(msq)) {\n\t\terr = PTR_ERR(msq);\n\t\tgoto out_free;\n\t}\n\n\tfor (;;) {\n\t\tstruct msg_sender s;\n\n\t\terr = -EACCES;\n\t\tif (ipcperms(ns, &msq->q_perm, S_IWUGO))\n\t\t\tgoto out_unlock_free;\n\n\t\terr = security_msg_queue_msgsnd(msq, msg, msgflg);\n\t\tif (err)\n\t\t\tgoto out_unlock_free;\n\n\t\tif (msgsz + msq->q_cbytes <= msq->q_qbytes &&\n\t\t\t\t1 + msq->q_qnum <= msq->q_qbytes) {\n\t\t\tbreak;\n\t\t}\n\n\t\t/* queue full, wait: */\n\t\tif (msgflg & IPC_NOWAIT) {\n\t\t\terr = -EAGAIN;\n\t\t\tgoto out_unlock_free;\n\t\t}\n\t\tss_add(msq, &s);\n\n\t\tif (!ipc_rcu_getref(msq)) {\n\t\t\terr = -EIDRM;\n\t\t\tgoto out_unlock_free;\n\t\t}\n\n\t\tmsg_unlock(msq);\n\t\tschedule();\n\n\t\tipc_lock_by_ptr(&msq->q_perm);\n\t\tipc_rcu_putref(msq);\n\t\tif (msq->q_perm.deleted) {\n\t\t\terr = -EIDRM;\n\t\t\tgoto out_unlock_free;\n\t\t}\n\t\tss_del(&s);\n\n\t\tif (signal_pending(current)) {\n\t\t\terr = -ERESTARTNOHAND;\n\t\t\tgoto out_unlock_free;\n\t\t}\n\t}\n\n\tmsq->q_lspid = task_tgid_vnr(current);\n\tmsq->q_stime = get_seconds();\n\n\tif (!pipelined_send(msq, msg)) {\n\t\t/* no one is waiting for this message, enqueue it */\n\t\tlist_add_tail(&msg->m_list, &msq->q_messages);\n\t\tmsq->q_cbytes += msgsz;\n\t\tmsq->q_qnum++;\n\t\tatomic_add(msgsz, &ns->msg_bytes);\n\t\tatomic_inc(&ns->msg_hdrs);\n\t}\n\n\terr = 0;\n\tmsg = NULL;\n\nout_unlock_free:\n\tmsg_unlock(msq);\nout_free:\n\tif (msg != NULL)\n\t\tfree_msg(msg);\n\treturn err;\n}",
    "diff": "@@ -687,7 +687,12 @@ long do_msgsnd(int msqid, long mtype, void __user *mtext,\n \t\tss_add(msq, &s);\n-\t\tipc_rcu_getref(msq);\n+\t\tif (!ipc_rcu_getref(msq)) {\n \t\tmsg_unlock(msq);",
    "critical_vars": [
      "msq"
    ],
    "variable_definitions": {
      "msq": "struct msg_queue *msq;"
    },
    "variable_types": {
      "msq": "struct pointer"
    },
    "type_mapping": {
      "msq": "struct pointer"
    },
    "vulnerable_line": "if (!ipc_rcu_getref(msq)) {",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "If msq reference count overflows, ipc_rcu_getref can incorrectly return zero, leading to denial of service by premature resource release."
  },
  {
    "fix_code": "void\n_copy_from_pages(char *p, struct page **pages, size_t pgbase, size_t len)\n{\n\tstruct page **pgfrom;\n\tchar *vfrom;\n\tsize_t copy;\n\n\tpgfrom = pages + (pgbase >> PAGE_CACHE_SHIFT);\n\tpgbase &= ~PAGE_CACHE_MASK;\n\n\tdo {\n\t\tcopy = PAGE_CACHE_SIZE - pgbase;\n\t\tif (copy > len)\n\t\t\tcopy = len;\n\n\t\tvfrom = kmap_atomic(*pgfrom, KM_USER0);\n\t\tmemcpy(p, vfrom + pgbase, copy);\n\t\tkunmap_atomic(vfrom, KM_USER0);\n\n\t\tpgbase += copy;\n\t\tif (pgbase == PAGE_CACHE_SIZE) {\n\t\t\tpgbase = 0;\n\t\t\tpgfrom++;\n\t\t}\n\t\tp += copy;\n\n\t} while ((len -= copy) != 0);\n}",
    "diff": " _copy_from_pages(char *p, struct page **pages, size_t pgbase, size_t len)\n@@ -324,6 +324,7 @@ _copy_from_pages(char *p, struct page **pages, size_t pgbase, size_t len)\n+EXPORT_SYMBOL_GPL(_copy_from_pages);",
    "critical_vars": [
      "_copy_from_pages"
    ],
    "variable_definitions": {
      "_copy_from_pages": "Definition not found"
    },
    "variable_types": {
      "_copy_from_pages": "unknown"
    },
    "type_mapping": {
      "_copy_from_pages": "unknown"
    },
    "vulnerable_line": "copy = PAGE_CACHE_SIZE - pgbase;",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "If pgbase is manipulated excessively, the subtraction can result in a value that wraps around, causing copy to erroneously be a large positive value and potentially leading to buffer overflows."
  },
  {
    "fix_code": "static ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)\n{\n\tstruct page *pages[NFS4ACL_MAXPAGES] = {NULL, };\n\tstruct nfs_getaclargs args = {\n\t\t.fh = NFS_FH(inode),\n\t\t.acl_pages = pages,\n\t\t.acl_len = buflen,\n\t};\n\tstruct nfs_getaclres res = {\n\t\t.acl_len = buflen,\n\t};\n\tvoid *resp_buf;\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &nfs4_procedures[NFSPROC4_CLNT_GETACL],\n\t\t.rpc_argp = &args,\n\t\t.rpc_resp = &res,\n\t};\n\tint ret = -ENOMEM, npages, i, acl_len = 0;\n\n\tnpages = (buflen + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t/* As long as we're doing a round trip to the server anyway,\n\t * let's be prepared for a page of acl data. */\n\tif (npages == 0)\n\t\tnpages = 1;\n\n\tfor (i = 0; i < npages; i++) {\n\t\tpages[i] = alloc_page(GFP_KERNEL);\n\t\tif (!pages[i])\n\t\t\tgoto out_free;\n\t}\n\tif (npages > 1) {\n\t\t/* for decoding across pages */\n\t\targs.acl_scratch = alloc_page(GFP_KERNEL);\n\t\tif (!args.acl_scratch)\n\t\t\tgoto out_free;\n\t}\n\targs.acl_len = npages * PAGE_SIZE;\n\targs.acl_pgbase = 0;\n\t/* Let decode_getfacl know not to fail if the ACL data is larger than\n\t * the page we send as a guess */\n\tif (buf == NULL)\n\t\tres.acl_flags |= NFS4_ACL_LEN_REQUEST;\n\tresp_buf = page_address(pages[0]);\n\n\tdprintk(\"%s  buf %p buflen %ld npages %d args.acl_len %ld\\n\",\n\t\t__func__, buf, buflen, npages, args.acl_len);\n\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode),\n\t\t\t     &msg, &args.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\tgoto out_free;\n\n\tacl_len = res.acl_len - res.acl_data_offset;\n\tif (acl_len > args.acl_len)\n\t\tnfs4_write_cached_acl(inode, NULL, acl_len);\n\telse\n\t\tnfs4_write_cached_acl(inode, resp_buf + res.acl_data_offset,\n\t\t\t\t      acl_len);\n\tif (buf) {\n\t\tret = -ERANGE;\n\t\tif (acl_len > buflen)\n\t\t\tgoto out_free;\n\t\t_copy_from_pages(buf, pages, res.acl_data_offset,\n\t\t\t\tres.acl_len);\n\t}\n\tret = acl_len;\nout_free:\n\tfor (i = 0; i < npages; i++)\n\t\tif (pages[i])\n\t\t\t__free_page(pages[i]);\n\tif (args.acl_scratch)\n\t\t__free_page(args.acl_scratch);\n\treturn ret;\n}",
    "diff": "+ * The getxattr API returns the required buffer length when called with a\n-\tint ret;\n+\tint ret = -ENOMEM, npages, i, acl_len = 0;\n-\t\t\treturn -ENOMEM;\n-\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode), &msg, &args.seq_args, &res.seq_res, 0);\n+\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode),\n \tif (ret)\n \t\tret = -ERANGE;\n-\tret = res.acl_len;\n+\tret = acl_len;\n \treturn ret;\n \tret = nfs4_read_cached_acl(inode, buf, buflen);\n \tif (ret != -ENOENT)\n+\t\t/* -ENOENT is returned if there is no ACL or if there is an ACL\n \t\treturn ret;\n \treturn nfs4_get_acl_uncached(inode, buf, buflen);",
    "critical_vars": [
      "ret"
    ],
    "variable_definitions": {
      "ret": "int ret = -ENOMEM, npages, i, acl_len = 0;"
    },
    "variable_types": {
      "ret": "integer"
    },
    "type_mapping": {
      "ret": "Integer"
    },
    "vulnerable_line": "npages = (buflen + PAGE_SIZE - 1) >> PAGE_SHIFT;",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The calculation of npages can overflow if 'buflen' is large enough, leading to incorrect memory allocation and potential denial of service."
  },
  {
    "fix_code": "static void nfs4_xdr_enc_getacl(struct rpc_rqst *req, struct xdr_stream *xdr,\n\t\t\t\tstruct nfs_getaclargs *args)\n{\n\tstruct compound_hdr hdr = {\n\t\t.minorversion = nfs4_xdr_minorversion(&args->seq_args),\n\t};\n\tuint32_t replen;\n\n\tencode_compound_hdr(xdr, req, &hdr);\n\tencode_sequence(xdr, &args->seq_args, &hdr);\n\tencode_putfh(xdr, args->fh, &hdr);\n\treplen = hdr.replen + op_decode_hdr_maxsz + 1;\n\tencode_getattr_two(xdr, FATTR4_WORD0_ACL, 0, &hdr);\n\n\txdr_inline_pages(&req->rq_rcv_buf, replen << 2,\n\t\targs->acl_pages, args->acl_pgbase, args->acl_len);\n\txdr_set_scratch_buffer(xdr, page_address(args->acl_scratch), PAGE_SIZE);\n\n\tencode_nops(&hdr);\n}",
    "diff": "-\treplen = hdr.replen + op_decode_hdr_maxsz + nfs4_fattr_bitmap_maxsz + 1;\n+\treplen = hdr.replen + op_decode_hdr_maxsz + 1;\n \txdr_inline_pages(&req->rq_rcv_buf, replen << 2,",
    "critical_vars": [
      "replen"
    ],
    "variable_definitions": {
      "replen": "uint32_t replen;"
    },
    "variable_types": {
      "replen": "integer"
    },
    "type_mapping": {
      "replen": "Integer"
    },
    "vulnerable_line": "replen = hdr.replen + op_decode_hdr_maxsz + nfs4_fattr_bitmap_maxsz + 1;",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The calculation of 'replen' involves adding multiple potentially large values together. If the total exceeds the maximum value for an integer, it could lead to an overflow, causing a denial of service when excessive bitmap words are processed."
  },
  {
    "fix_code": "Function not found",
    "diff": "+\tbm_p = xdr->p;\n+\t\txdr->p = bm_p;\n \t\thdrlen = (u8 *)xdr->p - (u8 *)iov->iov_base;",
    "critical_vars": [
      "xdr->p"
    ],
    "variable_definitions": {
      "xdr->p": "Definition not found"
    },
    "variable_types": {
      "xdr->p": "struct pointer_integer pointer"
    },
    "type_mapping": {
      "xdr->p": "struct pointer_integer pointer"
    },
    "vulnerable_line": "replen = hdr.replen + op_decode_hdr_maxsz + nfs4_fattr_bitmap_maxsz + 1;",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The addition of multiple variables can exceed the maximum value for the integer type, leading to incorrect buffer sizes or application crashes in the event of large bitmap sizes being handled."
  },
  {
    "fix_code": "static int\nnfs4_xdr_dec_getacl(struct rpc_rqst *rqstp, struct xdr_stream *xdr,\n\t\t    struct nfs_getaclres *res)\n{\n\tstruct compound_hdr hdr;\n\tint status;\n\n\tstatus = decode_compound_hdr(xdr, &hdr);\n\tif (status)\n\t\tgoto out;\n\tstatus = decode_sequence(xdr, &res->seq_res, rqstp);\n\tif (status)\n\t\tgoto out;\n\tstatus = decode_putfh(xdr);\n\tif (status)\n\t\tgoto out;\n\tstatus = decode_getacl(xdr, rqstp, res);\n\nout:\n\treturn status;\n}",
    "diff": " \tint status;\n \tif ((status = decode_op_hdr(xdr, OP_GETATTR)) != 0)\n \tif ((status = decode_attr_bitmap(xdr, bitmap)) != 0)\n \tif ((status = decode_attr_length(xdr, &attrlen, &savep)) != 0)\n \t\tstatus = -EOPNOTSUPP;\n \tstatus = decode_putfh(xdr);\n \tif (status)\n-\tstatus = decode_getacl(xdr, rqstp, &res->acl_len);\n+\tstatus = decode_getacl(xdr, rqstp, res);\n \treturn status;",
    "critical_vars": [
      "status"
    ],
    "variable_definitions": {
      "status": "int status;"
    },
    "variable_types": {
      "status": "integer"
    },
    "type_mapping": {
      "status": "Integer"
    },
    "vulnerable_line": "replen = hdr.replen + op_decode_hdr_maxsz + nfs4_fattr_bitmap_maxsz + 1;",
    "critical_variable_in_vulnerable_line": false,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The calculation for replen may exceed the maximum size for a data type, leading to integer overflow when excessive bitmap words are processed, which can cause a Denial of Service."
  },
  {
    "fix_code": "int sequencer_write(int dev, struct file *file, const char __user *buf, int count)\n{\n\tunsigned char event_rec[EV_SZ], ev_code;\n\tint p = 0, c, ev_size;\n\tint mode = translate_mode(file);\n\n\tdev = dev >> 4;\n\n\tDEB(printk(\"sequencer_write(dev=%d, count=%d)\\n\", dev, count));\n\n\tif (mode == OPEN_READ)\n\t\treturn -EIO;\n\n\tc = count;\n\n\twhile (c >= 4)\n\t{\n\t\tif (copy_from_user((char *) event_rec, &(buf)[p], 4))\n\t\t\tgoto out;\n\t\tev_code = event_rec[0];\n\n\t\tif (ev_code == SEQ_FULLSIZE)\n\t\t{\n\t\t\tint err, fmt;\n\n\t\t\tdev = *(unsigned short *) &event_rec[2];\n\t\t\tif (dev < 0 || dev >= max_synthdev || synth_devs[dev] == NULL)\n\t\t\t\treturn -ENXIO;\n\n\t\t\tif (!(synth_open_mask & (1 << dev)))\n\t\t\t\treturn -ENXIO;\n\n\t\t\tfmt = (*(short *) &event_rec[0]) & 0xffff;\n\t\t\terr = synth_devs[dev]->load_patch(dev, fmt, buf + p, c, 0);\n\t\t\tif (err < 0)\n\t\t\t\treturn err;\n\n\t\t\treturn err;\n\t\t}\n\t\tif (ev_code >= 128)\n\t\t{\n\t\t\tif (seq_mode == SEQ_2 && ev_code == SEQ_EXTENDED)\n\t\t\t{\n\t\t\t\tprintk(KERN_WARNING \"Sequencer: Invalid level 2 event %x\\n\", ev_code);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tev_size = 8;\n\n\t\t\tif (c < ev_size)\n\t\t\t{\n\t\t\t\tif (!seq_playing)\n\t\t\t\t\tseq_startplay();\n\t\t\t\treturn count - c;\n\t\t\t}\n\t\t\tif (copy_from_user((char *)&event_rec[4],\n\t\t\t\t\t   &(buf)[p + 4], 4))\n\t\t\t\tgoto out;\n\n\t\t}\n\t\telse\n\t\t{\n\t\t\tif (seq_mode == SEQ_2)\n\t\t\t{\n\t\t\t\tprintk(KERN_WARNING \"Sequencer: 4 byte event in level 2 mode\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tev_size = 4;\n\n\t\t\tif (event_rec[0] != SEQ_MIDIPUTC)\n\t\t\t\tobsolete_api_used = 1;\n\t\t}\n\n\t\tif (event_rec[0] == SEQ_MIDIPUTC)\n\t\t{\n\t\t\tif (!midi_opened[event_rec[2]])\n\t\t\t{\n\t\t\t\tint err, mode;\n\t\t\t\tint dev = event_rec[2];\n\n\t\t\t\tif (dev >= max_mididev || midi_devs[dev]==NULL)\n\t\t\t\t{\n\t\t\t\t\t/*printk(\"Sequencer Error: Nonexistent MIDI device %d\\n\", dev);*/\n\t\t\t\t\treturn -ENXIO;\n\t\t\t\t}\n\t\t\t\tmode = translate_mode(file);\n\n\t\t\t\tif ((err = midi_devs[dev]->open(dev, mode,\n\t\t\t\t\t\t\t\tsequencer_midi_input, sequencer_midi_output)) < 0)\n\t\t\t\t{\n\t\t\t\t\tseq_reset();\n\t\t\t\t\tprintk(KERN_WARNING \"Sequencer Error: Unable to open Midi #%d\\n\", dev);\n\t\t\t\t\treturn err;\n\t\t\t\t}\n\t\t\t\tmidi_opened[dev] = 1;\n\t\t\t}\n\t\t}\n\t\tif (!seq_queue(event_rec, (file->f_flags & (O_NONBLOCK) ? 1 : 0)))\n\t\t{\n\t\t\tint processed = count - c;\n\n\t\t\tif (!seq_playing)\n\t\t\t\tseq_startplay();\n\n\t\t\tif (!processed && (file->f_flags & O_NONBLOCK))\n\t\t\t\treturn -EAGAIN;\n\t\t\telse\n\t\t\t\treturn processed;\n\t\t}\n\t\tp += ev_size;\n\t\tc -= ev_size;\n\t}\n\n\tif (!seq_playing)\n\t\tseq_startplay();\nout:\n\treturn count;\n}",
    "diff": "-\t\t\terr = synth_devs[dev]->load_patch(dev, fmt, buf, p + 4, c, 0);\n+\t\t\terr = synth_devs[dev]->load_patch(dev, fmt, buf + p, c, 0);\n \t\t\tif (err < 0)\n \t\t\t\treturn err;",
    "critical_vars": [
      "err"
    ],
    "variable_definitions": {
      "err": "int err, fmt;"
    },
    "variable_types": {
      "err": "integer"
    },
    "type_mapping": {
      "err": "Integer"
    },
    "vulnerable_line": "err = synth_devs[dev]->load_patch(dev, fmt, buf + p, c, 0);",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Underflow",
    "reasoning": "The variable 'c' is decremented in the while loop without proper bounds checks, which may lead to an underflow if 'count' is less than 4, causing memory corruption by overwriting data when accessing buf + p."
  },
  {
    "fix_code": "int\nmidi_synth_load_patch(int dev, int format, const char __user *addr,\n\t\t      int count, int pmgr_flag)\n{\n\tint             orig_dev = synth_devs[dev]->midi_dev;\n\n\tstruct sysex_info sysex;\n\tint             i;\n\tunsigned long   left, src_offs, eox_seen = 0;\n\tint             first_byte = 1;\n\tint             hdr_size = (unsigned long) &sysex.data[0] - (unsigned long) &sysex;\n\n\tleave_sysex(dev);\n\n\tif (!prefix_cmd(orig_dev, 0xf0))\n\t\treturn 0;\n\n\t/* Invalid patch format */\n\tif (format != SYSEX_PATCH)\n\t\t  return -EINVAL;\n\n\t/* Patch header too short */\n\tif (count < hdr_size)\n\t\treturn -EINVAL;\n\n\tcount -= hdr_size;\n\n\t/*\n\t * Copy the header from user space\n\t */\n\n\tif (copy_from_user(&sysex, addr, hdr_size))\n\t\treturn -EFAULT;\n\n\t/* Sysex record too short */\n\tif ((unsigned)count < (unsigned)sysex.len)\n\t\tsysex.len = count;\n\n\tleft = sysex.len;\n\tsrc_offs = 0;\n\n\tfor (i = 0; i < left && !signal_pending(current); i++)\n\t{\n\t\tunsigned char   data;\n\n\t\tif (get_user(data,\n\t\t    (unsigned char __user *)(addr + hdr_size + i)))\n\t\t\treturn -EFAULT;\n\n\t\teox_seen = (i > 0 && data & 0x80);\t/* End of sysex */\n\n\t\tif (eox_seen && data != 0xf7)\n\t\t\tdata = 0xf7;\n\n\t\tif (i == 0)\n\t\t{\n\t\t\tif (data != 0xf0)\n\t\t\t{\n\t\t\t\tprintk(KERN_WARNING \"midi_synth: Sysex start missing\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\t\twhile (!midi_devs[orig_dev]->outputc(orig_dev, (unsigned char) (data & 0xff)) &&\n\t\t\t!signal_pending(current))\n\t\t\tschedule();\n\n\t\tif (!first_byte && data & 0x80)\n\t\t\treturn 0;\n\t\tfirst_byte = 0;\n\t}\n\n\tif (!eox_seen)\n\t\tmidi_outc(orig_dev, 0xf7);\n\treturn 0;\n}",
    "diff": "-\t\t      int offs, int count, int pmgr_flag)\n+\t\t      int count, int pmgr_flag)\n \tif (count < hdr_size)\n \tcount -= hdr_size;\n- \tif (count < sysex.len)\n-/*\t\tprintk(KERN_WARNING \"MIDI Warning: Sysex record too short (%d<%d)\\n\", count, (int) sysex.len);*/\n+\tif ((unsigned)count < (unsigned)sysex.len)\n \t\tsysex.len = count;",
    "critical_vars": [
      "count"
    ],
    "variable_definitions": {
      "count": "int count"
    },
    "variable_types": {
      "count": "integer"
    },
    "type_mapping": {
      "count": "Integer"
    },
    "vulnerable_line": "if ((unsigned)count < (unsigned)sysex.len)",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Underflow",
    "reasoning": "The comparison cast of 'count' and 'sysex.len' may allow an integer underflow if 'count' is smaller than 'hdr_size', leading to a situation where 'sysex.len' could be improperly set, which could ultimately lead to memory corruption."
  },
  {
    "fix_code": "int kvm_iommu_map_pages(struct kvm *kvm, struct kvm_memory_slot *slot)\n{\n\tgfn_t gfn, end_gfn;\n\tpfn_t pfn;\n\tint r = 0;\n\tstruct iommu_domain *domain = kvm->arch.iommu_domain;\n\tint flags;\n\n\t/* check if iommu exists and in use */\n\tif (!domain)\n\t\treturn 0;\n\n\tgfn     = slot->base_gfn;\n\tend_gfn = gfn + slot->npages;\n\n\tflags = IOMMU_READ;\n\tif (!(slot->flags & KVM_MEM_READONLY))\n\t\tflags |= IOMMU_WRITE;\n\tif (!kvm->arch.iommu_noncoherent)\n\t\tflags |= IOMMU_CACHE;\n\n\n\twhile (gfn < end_gfn) {\n\t\tunsigned long page_size;\n\n\t\t/* Check if already mapped */\n\t\tif (iommu_iova_to_phys(domain, gfn_to_gpa(gfn))) {\n\t\t\tgfn += 1;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Get the page size we could use to map */\n\t\tpage_size = kvm_host_page_size(kvm, gfn);\n\n\t\t/* Make sure the page_size does not exceed the memslot */\n\t\twhile ((gfn + (page_size >> PAGE_SHIFT)) > end_gfn)\n\t\t\tpage_size >>= 1;\n\n\t\t/* Make sure gfn is aligned to the page size we want to map */\n\t\twhile ((gfn << PAGE_SHIFT) & (page_size - 1))\n\t\t\tpage_size >>= 1;\n\n\t\t/* Make sure hva is aligned to the page size we want to map */\n\t\twhile (__gfn_to_hva_memslot(slot, gfn) & (page_size - 1))\n\t\t\tpage_size >>= 1;\n\n\t\t/*\n\t\t * Pin all pages we are about to map in memory. This is\n\t\t * important because we unmap and unpin in 4kb steps later.\n\t\t */\n\t\tpfn = kvm_pin_pages(slot, gfn, page_size);\n\t\tif (is_error_noslot_pfn(pfn)) {\n\t\t\tgfn += 1;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Map into IO address space */\n\t\tr = iommu_map(domain, gfn_to_gpa(gfn), pfn_to_hpa(pfn),\n\t\t\t      page_size, flags);\n\t\tif (r) {\n\t\t\tprintk(KERN_ERR \"kvm_iommu_map_address:\"\n\t\t\t       \"iommu failed to map pfn=%llx\\n\", pfn);\n\t\t\tkvm_unpin_pages(kvm, pfn, page_size);\n\t\t\tgoto unmap_pages;\n\t\t}\n\n\t\tgfn += page_size >> PAGE_SHIFT;\n\n\n\t}\n\n\treturn 0;\n\nunmap_pages:\n\tkvm_iommu_put_pages(kvm, slot->base_gfn, gfn - slot->base_gfn);\n\treturn r;\n}",
    "diff": "@@ -61,6 +61,14 @@ static pfn_t kvm_pin_pages(struct kvm_memory_slot *slot, gfn_t gfn,\n \treturn pfn;\n+static void kvm_unpin_pages(struct kvm *kvm, pfn_t pfn, unsigned long npages)\n+\t\tkvm_release_pfn_clean(pfn + i);\n int kvm_iommu_map_pages(struct kvm *kvm, struct kvm_memory_slot *slot)\n@@ -123,6 +131,7 @@ int kvm_iommu_map_pages(struct kvm *kvm, struct kvm_memory_slot *slot)\n \t\t\tprintk(KERN_ERR \"kvm_iommu_map_address:\"\n \t\t\t       \"iommu failed to map pfn=%llx\\n\", pfn);\n+\t\t\tkvm_unpin_pages(kvm, pfn, page_size);\n@@ -134,7 +143,7 @@ int kvm_iommu_map_pages(struct kvm *kvm, struct kvm_memory_slot *slot)\n-\tkvm_iommu_put_pages(kvm, slot->base_gfn, gfn);\n+\tkvm_iommu_put_pages(kvm, slot->base_gfn, gfn - slot->base_gfn);\n@@ -266,14 +275,6 @@ int kvm_iommu_map_guest(struct kvm *kvm)\n-static void kvm_unpin_pages(struct kvm *kvm, pfn_t pfn, unsigned long npages)\n-\t\tkvm_release_pfn_clean(pfn + i);\n static void kvm_iommu_put_pages(struct kvm *kvm,",
    "critical_vars": [
      "page_size",
      "pfn",
      "kvm"
    ],
    "variable_definitions": {
      "page_size": "unsigned long page_size;",
      "pfn": "pfn_t pfn;",
      "kvm": "struct kvm *kvm"
    },
    "variable_types": {
      "page_size": "integer",
      "pfn": "integer",
      "kvm": "struct pointer"
    },
    "type_mapping": {
      "page_size": "Integer",
      "pfn": "Integer",
      "kvm": "struct pointer"
    },
    "vulnerable_line": "kvm_unpin_pages(kvm, pfn, page_size);",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The page_size variable can potentially be very large, leading to an integer overflow when used to determine the number of pages to unpin, which could allow for memory corruption or denial of service."
  },
  {
    "fix_code": "static int uio_mmap_physical(struct vm_area_struct *vma)\n{\n\tstruct uio_device *idev = vma->vm_private_data;\n\tint mi = uio_find_mem_index(vma);\n\tstruct uio_mem *mem;\n\tif (mi < 0)\n\t\treturn -EINVAL;\n\tmem = idev->info->mem + mi;\n\n\tif (vma->vm_end - vma->vm_start > mem->size)\n\t\treturn -EINVAL;\n\n\tvma->vm_ops = &uio_physical_vm_ops;\n\tvma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);\n\n\t/*\n\t * We cannot use the vm_iomap_memory() helper here,\n\t * because vma->vm_pgoff is the map index we looked\n\t * up above in uio_find_mem_index(), rather than an\n\t * actual page offset into the mmap.\n\t *\n\t * So we just do the physical mmap without a page\n\t * offset.\n\t */\n\treturn remap_pfn_range(vma,\n\t\t\t       vma->vm_start,\n\t\t\t       mem->addr >> PAGE_SHIFT,\n\t\t\t       vma->vm_end - vma->vm_start,\n\t\t\t       vma->vm_page_prot);\n}",
    "diff": "-\tvma->vm_ops = &uio_physical_vm_ops;\n+\tvma->vm_ops = &uio_physical_vm_ops;",
    "critical_vars": [
      "vma->vm_ops"
    ],
    "variable_definitions": {
      "vma->vm_ops": "Definition not found"
    },
    "variable_types": {
      "vma->vm_ops": "struct pointer_struct pointer"
    },
    "type_mapping": {
      "vma->vm_ops": "sp_sp"
    },
    "vulnerable_line": "if (vma->vm_end - vma->vm_start > mem->size)",
    "critical_variable_in_vulnerable_line": false,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The expression 'vma->vm_end - vma->vm_start' can result in an integer overflow if 'vma->vm_start' is greater than 'vma->vm_end', leading to a calculation that exceeds the maximum integer value, allowing for improper memory mapping operations."
  },
  {
    "fix_code": "static void tcp_illinois_info(struct sock *sk, u32 ext,\n\t\t\t      struct sk_buff *skb)\n{\n\tconst struct illinois *ca = inet_csk_ca(sk);\n\n\tif (ext & (1 << (INET_DIAG_VEGASINFO - 1))) {\n\t\tstruct tcpvegas_info info = {\n\t\t\t.tcpv_enabled = 1,\n\t\t\t.tcpv_rttcnt = ca->cnt_rtt,\n\t\t\t.tcpv_minrtt = ca->base_rtt,\n\t\t};\n\n\t\tif (info.tcpv_rttcnt > 0) {\n\t\t\tu64 t = ca->sum_rtt;\n\n\t\t\tdo_div(t, info.tcpv_rttcnt);\n\t\t\tinfo.tcpv_rtt = t;\n\t\t}\n\t\tnla_put(skb, INET_DIAG_VEGASINFO, sizeof(info), &info);\n\t}\n}",
    "diff": "@@ -313,11 +313,13 @@ static void tcp_illinois_info(struct sock *sk, u32 ext,\n \t\t\t.tcpv_rttcnt = ca->cnt_rtt,\n \t\t\t.tcpv_minrtt = ca->base_rtt,\n-\t\tu64 t = ca->sum_rtt;\n-\t\tdo_div(t, ca->cnt_rtt);\n-\t\tinfo.tcpv_rtt = t;\n+\t\tif (info.tcpv_rttcnt > 0) {\n+\t\t\tu64 t = ca->sum_rtt;\n+\t\t\tdo_div(t, info.tcpv_rttcnt);\n+\t\t\tinfo.tcpv_rtt = t;\n \t\tnla_put(skb, INET_DIAG_VEGASINFO, sizeof(info), &info);",
    "critical_vars": [
      "t"
    ],
    "variable_definitions": {
      "t": "u64 t = ca->sum_rtt;"
    },
    "variable_types": {
      "t": "integer"
    },
    "type_mapping": {
      "t": "Integer"
    },
    "vulnerable_line": "do_div(t, ca->cnt_rtt);",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Division by Zero",
    "reasoning": "The original code did not check if ca->cnt_rtt was zero before performing the division, leading to a divide-by-zero error when this count was zero, which would cause a denial of service."
  },
  {
    "fix_code": "static void tcp_illinois_info(struct sock *sk, u32 ext,\n\t\t\t      struct sk_buff *skb)\n{\n\tconst struct illinois *ca = inet_csk_ca(sk);\n\n\tif (ext & (1 << (INET_DIAG_VEGASINFO - 1))) {\n\t\tstruct tcpvegas_info info = {\n\t\t\t.tcpv_enabled = 1,\n\t\t\t.tcpv_rttcnt = ca->cnt_rtt,\n\t\t\t.tcpv_minrtt = ca->base_rtt,\n\t\t};\n\n\t\tif (info.tcpv_rttcnt > 0) {\n\t\t\tu64 t = ca->sum_rtt;\n\n\t\t\tdo_div(t, info.tcpv_rttcnt);\n\t\t\tinfo.tcpv_rtt = t;\n\t\t}\n\t\tnla_put(skb, INET_DIAG_VEGASINFO, sizeof(info), &info);\n\t}\n}",
    "diff": "+\t\tif (info.tcpv_rttcnt > 0) {\n+\t\t\tdo_div(t, info.tcpv_rttcnt);",
    "critical_vars": [
      "info.tcpv_rttcnt"
    ],
    "variable_definitions": {
      "info.tcpv_rttcnt": "Definition not found"
    },
    "variable_types": {
      "info.tcpv_rttcnt": "struct.integer"
    },
    "type_mapping": {
      "info.tcpv_rttcnt": "st.i"
    },
    "vulnerable_line": "do_div(t, info.tcpv_rttcnt);",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Division by Zero",
    "reasoning": "The code attempts to divide 't' by 'info.tcpv_rttcnt', which could be zero, leading to a divide-by-zero error and potential denial of service."
  },
  {
    "fix_code": "i915_gem_check_execbuffer(struct drm_i915_gem_execbuffer2 *exec)\n{\n\treturn ((exec->batch_start_offset | exec->batch_len) & 0x7) == 0;\n}",
    "diff": "-\tif (args->buffer_count < 1) {\n+\tif (args->buffer_count < 1 ||\n+\t    args->buffer_count > UINT_MAX / sizeof(*exec2_list)) {\n \t\tDRM_DEBUG(\"execbuf2 with %d buffers\\n\", args->buffer_count);",
    "critical_vars": [
      "args->buffer_count"
    ],
    "variable_definitions": {
      "args->buffer_count": "Definition not found"
    },
    "variable_types": {
      "args->buffer_count": "struct pointer_integer"
    },
    "type_mapping": {
      "args->buffer_count": "sp_integer"
    },
    "vulnerable_line": "if (args->buffer_count < 1 || args->buffer_count > UINT_MAX / sizeof(*exec2_list)) {",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The condition checks both for lower bounds and a potential overflow when calculating maximum buffer_count based on the size of exec2_list, allowing for an integer overflow to occur if unguarded input is provided, leading to memory corruption and denial of service."
  },
  {
    "fix_code": "static void sample_to_timespec(const clockid_t which_clock,\n\t\t\t       union cpu_time_count cpu,\n\t\t\t       struct timespec *tp)\n{\n\tif (CPUCLOCK_WHICH(which_clock) == CPUCLOCK_SCHED)\n\t\t*tp = ns_to_timespec(cpu.sched);\n\telse\n\t\tcputime_to_timespec(cpu.cpu, tp);\n}",
    "diff": "-\t\ttp->tv_sec = div_long_long_rem(cpu.sched,",
    "critical_vars": [
      "tp->tv_sec"
    ],
    "variable_definitions": {
      "tp->tv_sec": "Definition not found"
    },
    "variable_types": {
      "tp->tv_sec": "struct pointer_integer"
    },
    "type_mapping": {
      "tp->tv_sec": "sp_integer"
    },
    "vulnerable_line": "tp->tv_sec = div_long_long_rem(cpu.sched, NSEC_PER_SEC, &tp->tv_nsec);",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Division by Zero",
    "reasoning": "The div_long_long_rem function may receive zero as the divisor (if NSEC_PER_SEC is manipulated), causing a divide error and triggering a denial of service."
  },
  {
    "fix_code": "static __inline__ void\njiffies_to_compat_timeval(unsigned long jiffies, struct compat_timeval *value)\n{\n\t/*\n\t * Convert jiffies to nanoseconds and separate with\n\t * one divide.\n\t */\n\tu64 nsec = (u64)jiffies * TICK_NSEC;\n\tu32 rem;\n\tvalue->tv_sec = div_u64_rem(nsec, NSEC_PER_SEC, &rem);\n\tvalue->tv_usec = rem / NSEC_PER_USEC;\n}",
    "diff": "-\tlong rem;\n-\tvalue->tv_sec = div_long_long_rem(nsec, NSEC_PER_SEC, &rem);\n+\tu32 rem;\n+\tvalue->tv_sec = div_u64_rem(nsec, NSEC_PER_SEC, &rem);\n \tvalue->tv_usec = rem / NSEC_PER_USEC;",
    "critical_vars": [
      "rem"
    ],
    "variable_definitions": {
      "rem": "u32 rem;"
    },
    "variable_types": {
      "rem": "integer"
    },
    "type_mapping": {
      "rem": "Integer"
    },
    "vulnerable_line": "value->tv_sec = div_long_long_rem(nsec, NSEC_PER_SEC, &rem);",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Division by Zero",
    "reasoning": "The div_long_long_rem function can lead to a divide error if NSEC_PER_SEC is zero, causing a denial of service."
  },
  {
    "fix_code": "void\njiffies_to_timespec(const unsigned long jiffies, struct timespec *value)\n{\n\t/*\n\t * Convert jiffies to nanoseconds and separate with\n\t * one divide.\n\t */\n\tu32 rem;\n\tvalue->tv_sec = div_u64_rem((u64)jiffies * TICK_NSEC,\n\t\t\t\t    NSEC_PER_SEC, &rem);\n\tvalue->tv_nsec = rem;\n}",
    "diff": " struct timespec ns_to_timespec(const s64 nsec)\n \tif (!nsec)\n-\tts.tv_sec = div_long_long_rem_signed(nsec, NSEC_PER_SEC, &ts.tv_nsec);\n-\tif (unlikely(nsec < 0))\n-\t\tset_normalized_timespec(&ts, ts.tv_sec, ts.tv_nsec);\n+\tts.tv_sec = div_s64_rem(nsec, NSEC_PER_SEC, &rem);\n+\tts.tv_nsec = rem;\n-\tu64 nsec = (u64)jiffies * TICK_NSEC;\n-\tvalue->tv_sec = div_long_long_rem(nsec, NSEC_PER_SEC, &value->tv_nsec);\n+\tvalue->tv_nsec = rem;\n-\tu64 nsec = (u64)jiffies * TICK_NSEC;\n-\tvalue->tv_sec = div_long_long_rem(nsec, NSEC_PER_SEC, &tv_usec);",
    "critical_vars": [
      "nsec"
    ],
    "variable_definitions": {
      "nsec": "u64 nsec = (u64)jiffies * TICK_NSEC;"
    },
    "variable_types": {
      "nsec": "integer"
    },
    "type_mapping": {
      "nsec": "Integer"
    },
    "vulnerable_line": "value->tv_sec = div_long_long_rem(nsec, NSEC_PER_SEC, &tv_usec);",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Division by Zero",
    "reasoning": "If nsec is zero, division by NSEC_PER_SEC may lead to a Divide Error Fault, causing the system to panic."
  },
  {
    "fix_code": "void\njiffies_to_timespec(const unsigned long jiffies, struct timespec *value)\n{\n\t/*\n\t * Convert jiffies to nanoseconds and separate with\n\t * one divide.\n\t */\n\tu32 rem;\n\tvalue->tv_sec = div_u64_rem((u64)jiffies * TICK_NSEC,\n\t\t\t\t    NSEC_PER_SEC, &rem);\n\tvalue->tv_nsec = rem;\n}",
    "diff": "+\ts32 rem;\n-\tts.tv_sec = div_long_long_rem_signed(nsec, NSEC_PER_SEC, &ts.tv_nsec);\n+\tts.tv_sec = div_s64_rem(nsec, NSEC_PER_SEC, &rem);\n+\tif (unlikely(rem < 0)) {\n+\t\trem += NSEC_PER_SEC;\n+\tts.tv_nsec = rem;\n-\tvalue->tv_sec = div_long_long_rem(nsec, NSEC_PER_SEC, &value->tv_nsec);\n+\tu32 rem;\n+\tvalue->tv_sec = div_u64_rem((u64)jiffies * TICK_NSEC,\n+\t\t\t\t    NSEC_PER_SEC, &rem);\n+\tvalue->tv_nsec = rem;\n+\tu32 rem;\n-\tvalue->tv_sec = div_long_long_rem(nsec, NSEC_PER_SEC, &tv_usec);\n+\tvalue->tv_sec = div_u64_rem((u64)jiffies * TICK_NSEC,\n+\t\t\t\t    NSEC_PER_SEC, &rem);\n+\tvalue->tv_usec = rem / NSEC_PER_USEC;",
    "critical_vars": [
      "rem"
    ],
    "variable_definitions": {
      "rem": "u32 rem;"
    },
    "variable_types": {
      "rem": "integer"
    },
    "type_mapping": {
      "rem": "Integer"
    },
    "vulnerable_line": "value->tv_sec = div_u64_rem((u64)jiffies * TICK_NSEC, NSEC_PER_SEC, &rem);",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Division by Zero",
    "reasoning": "The division operation in div_u64_rem could potentially encounter a zero divisor NSEC_PER_SEC if it is improperly defined or calculated, leading to a Divide Error Fault."
  },
  {
    "fix_code": "void jiffies_to_timeval(const unsigned long jiffies, struct timeval *value)\n{\n\t/*\n\t * Convert jiffies to nanoseconds and separate with\n\t * one divide.\n\t */\n\tu32 rem;\n\n\tvalue->tv_sec = div_u64_rem((u64)jiffies * TICK_NSEC,\n\t\t\t\t    NSEC_PER_SEC, &rem);\n\tvalue->tv_usec = rem / NSEC_PER_USEC;\n}",
    "diff": " struct timespec ns_to_timespec(const s64 nsec)\n \tif (!nsec)\n-\tts.tv_sec = div_long_long_rem_signed(nsec, NSEC_PER_SEC, &ts.tv_nsec);\n-\tif (unlikely(nsec < 0))\n-\t\tset_normalized_timespec(&ts, ts.tv_sec, ts.tv_nsec);\n+\tts.tv_sec = div_s64_rem(nsec, NSEC_PER_SEC, &rem);\n+\tts.tv_nsec = rem;\n-\tu64 nsec = (u64)jiffies * TICK_NSEC;\n-\tvalue->tv_sec = div_long_long_rem(nsec, NSEC_PER_SEC, &value->tv_nsec);\n+\tvalue->tv_nsec = rem;\n-\tu64 nsec = (u64)jiffies * TICK_NSEC;\n-\tvalue->tv_sec = div_long_long_rem(nsec, NSEC_PER_SEC, &tv_usec);",
    "critical_vars": [
      "nsec"
    ],
    "variable_definitions": {
      "nsec": "u64 nsec = (u64)jiffies * TICK_NSEC;"
    },
    "variable_types": {
      "nsec": "integer"
    },
    "type_mapping": {
      "nsec": "Integer"
    },
    "vulnerable_line": "value->tv_sec = div_long_long_rem(nsec, NSEC_PER_SEC, &tv_usec);",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Division by Zero",
    "reasoning": "The original code does not handle the case where NSEC_PER_SEC may be zero, leading to a risk of division by zero which could cause a system panic or denial of service."
  },
  {
    "fix_code": "void jiffies_to_timeval(const unsigned long jiffies, struct timeval *value)\n{\n\t/*\n\t * Convert jiffies to nanoseconds and separate with\n\t * one divide.\n\t */\n\tu32 rem;\n\n\tvalue->tv_sec = div_u64_rem((u64)jiffies * TICK_NSEC,\n\t\t\t\t    NSEC_PER_SEC, &rem);\n\tvalue->tv_usec = rem / NSEC_PER_USEC;\n}",
    "diff": "+\ts32 rem;\n-\tts.tv_sec = div_long_long_rem_signed(nsec, NSEC_PER_SEC, &ts.tv_nsec);\n+\tts.tv_sec = div_s64_rem(nsec, NSEC_PER_SEC, &rem);\n+\tif (unlikely(rem < 0)) {\n+\t\trem += NSEC_PER_SEC;\n+\tts.tv_nsec = rem;\n-\tvalue->tv_sec = div_long_long_rem(nsec, NSEC_PER_SEC, &value->tv_nsec);\n+\tu32 rem;\n+\tvalue->tv_sec = div_u64_rem((u64)jiffies * TICK_NSEC,\n+\t\t\t\t    NSEC_PER_SEC, &rem);\n+\tvalue->tv_nsec = rem;\n+\tu32 rem;\n-\tvalue->tv_sec = div_long_long_rem(nsec, NSEC_PER_SEC, &tv_usec);\n+\tvalue->tv_sec = div_u64_rem((u64)jiffies * TICK_NSEC,\n+\t\t\t\t    NSEC_PER_SEC, &rem);\n+\tvalue->tv_usec = rem / NSEC_PER_USEC;",
    "critical_vars": [
      "rem"
    ],
    "variable_definitions": {
      "rem": "u32 rem;"
    },
    "variable_types": {
      "rem": "integer"
    },
    "type_mapping": {
      "rem": "Integer"
    },
    "vulnerable_line": "value->tv_sec = div_long_long_rem(nsec, NSEC_PER_SEC, &tv_usec);",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Division by Zero",
    "reasoning": "The division operation could lead to a Divide Error Fault if NSEC_PER_SEC is zero or improperly calculated, causing system panic."
  },
  {
    "fix_code": "static int list_locations(struct kmem_cache *s, char *buf,\n\t\t\t\t\tenum track_item alloc)\n{\n\tint len = 0;\n\tunsigned long i;\n\tstruct loc_track t = { 0, 0, NULL };\n\tint node;\n\n\tif (!alloc_loc_track(&t, PAGE_SIZE / sizeof(struct location),\n\t\t\tGFP_TEMPORARY))\n\t\treturn sprintf(buf, \"Out of memory\\n\");\n\n\t/* Push back cpu slabs */\n\tflush_all(s);\n\n\tfor_each_node_state(node, N_NORMAL_MEMORY) {\n\t\tstruct kmem_cache_node *n = get_node(s, node);\n\t\tunsigned long flags;\n\t\tstruct page *page;\n\n\t\tif (!atomic_long_read(&n->nr_slabs))\n\t\t\tcontinue;\n\n\t\tspin_lock_irqsave(&n->list_lock, flags);\n\t\tlist_for_each_entry(page, &n->partial, lru)\n\t\t\tprocess_slab(&t, s, page, alloc);\n\t\tlist_for_each_entry(page, &n->full, lru)\n\t\t\tprocess_slab(&t, s, page, alloc);\n\t\tspin_unlock_irqrestore(&n->list_lock, flags);\n\t}\n\n\tfor (i = 0; i < t.count; i++) {\n\t\tstruct location *l = &t.loc[i];\n\n\t\tif (len > PAGE_SIZE - 100)\n\t\t\tbreak;\n\t\tlen += sprintf(buf + len, \"%7ld \", l->count);\n\n\t\tif (l->addr)\n\t\t\tlen += sprint_symbol(buf + len, (unsigned long)l->addr);\n\t\telse\n\t\t\tlen += sprintf(buf + len, \"<not-available>\");\n\n\t\tif (l->sum_time != l->min_time) {\n\t\t\tlen += sprintf(buf + len, \" age=%ld/%ld/%ld\",\n\t\t\t\tl->min_time,\n\t\t\t\t(long)div_u64(l->sum_time, l->count),\n\t\t\t\tl->max_time);\n\t\t} else\n\t\t\tlen += sprintf(buf + len, \" age=%ld\",\n\t\t\t\tl->min_time);\n\n\t\tif (l->min_pid != l->max_pid)\n\t\t\tlen += sprintf(buf + len, \" pid=%ld-%ld\",\n\t\t\t\tl->min_pid, l->max_pid);\n\t\telse\n\t\t\tlen += sprintf(buf + len, \" pid=%ld\",\n\t\t\t\tl->min_pid);\n\n\t\tif (num_online_cpus() > 1 && !cpus_empty(l->cpus) &&\n\t\t\t\tlen < PAGE_SIZE - 60) {\n\t\t\tlen += sprintf(buf + len, \" cpus=\");\n\t\t\tlen += cpulist_scnprintf(buf + len, PAGE_SIZE - len - 50,\n\t\t\t\t\tl->cpus);\n\t\t}\n\n\t\tif (num_online_nodes() > 1 && !nodes_empty(l->nodes) &&\n\t\t\t\tlen < PAGE_SIZE - 60) {\n\t\t\tlen += sprintf(buf + len, \" nodes=\");\n\t\t\tlen += nodelist_scnprintf(buf + len, PAGE_SIZE - len - 50,\n\t\t\t\t\tl->nodes);\n\t\t}\n\n\t\tlen += sprintf(buf + len, \"\\n\");\n\t}\n\n\tfree_loc_track(&t);\n\tif (!t.count)\n\t\tlen += sprintf(buf, \"No data\\n\");\n\treturn len;\n}",
    "diff": "-\t\t\tunsigned long remainder;\n-\t\t\tdiv_long_long_rem(l->sum_time, l->count, &remainder),",
    "critical_vars": [
      "remainder"
    ],
    "variable_definitions": {
      "remainder": "unsigned long remainder;"
    },
    "variable_types": {
      "remainder": "integer"
    },
    "type_mapping": {
      "remainder": "Integer"
    },
    "vulnerable_line": "len += sprintf(buf + len, \" age=%ld/%ld/%ld\",\n                l->min_time,\n                div_long_long_rem(l->sum_time, l->count, &remainder),\n                l->max_time);",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Division by Zero",
    "reasoning": "The division operation div_long_long_rem with l->count as divisor can lead to a divide error if l->count is zero, causing a denial of service."
  },
  {
    "fix_code": "static int sgi_clock_get(clockid_t clockid, struct timespec *tp)\n{\n\tu64 nsec;\n\n\tnsec = rtc_time() * sgi_clock_period\n\t\t\t+ sgi_clock_offset.tv_nsec;\n\t*tp = ns_to_timespec(nsec);\n\ttp->tv_sec += sgi_clock_offset.tv_sec;\n\treturn 0;\n}",
    "diff": "-\ttp->tv_sec = div_long_long_rem(nsec, NSEC_PER_SEC, &tp->tv_nsec)\n+\ttp->tv_sec += sgi_clock_offset.tv_sec;\n-\tsgi_clock_offset.tv_sec = tp->tv_sec - div_long_long_rem(nsec, NSEC_PER_SEC, &rem);\n+\tsgi_clock_offset.tv_sec = tp->tv_sec - div_u64_rem(nsec, NSEC_PER_SEC, &rem);\n \t\tsgi_clock_offset.tv_nsec = tp->tv_sec - rem;",
    "critical_vars": [
      "tp->tv_sec"
    ],
    "variable_definitions": {
      "tp->tv_sec": "Definition not found"
    },
    "variable_types": {
      "tp->tv_sec": "struct pointer_integer"
    },
    "type_mapping": {
      "tp->tv_sec": "sp_integer"
    },
    "vulnerable_line": "tp->tv_sec = div_long_long_rem(nsec, NSEC_PER_SEC, &tp->tv_nsec) + sgi_clock_offset.tv_sec;",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Division by Zero",
    "reasoning": "In the div_long_long_rem function call, if NSEC_PER_SEC becomes zero, a divide by zero error occurs causing system panic. The fix changes the function to div_u64_rem, which handles cases more securely."
  },
  {
    "fix_code": "static int sgi_clock_set(clockid_t clockid, struct timespec *tp)\n{\n\n\tu64 nsec;\n\tu32 rem;\n\n\tnsec = rtc_time() * sgi_clock_period;\n\n\tsgi_clock_offset.tv_sec = tp->tv_sec - div_u64_rem(nsec, NSEC_PER_SEC, &rem);\n\n\tif (rem <= tp->tv_nsec)\n\t\tsgi_clock_offset.tv_nsec = tp->tv_sec - rem;\n\telse {\n\t\tsgi_clock_offset.tv_nsec = tp->tv_sec + NSEC_PER_SEC - rem;\n\t\tsgi_clock_offset.tv_sec--;\n\t}\n\treturn 0;\n}",
    "diff": "-\ttp->tv_sec = div_long_long_rem(nsec, NSEC_PER_SEC, &tp->tv_nsec)\n-\tu64 rem;\n+\tu32 rem;\n-\tsgi_clock_offset.tv_sec = tp->tv_sec - div_long_long_rem(nsec, NSEC_PER_SEC, &rem);\n+\tsgi_clock_offset.tv_sec = tp->tv_sec - div_u64_rem(nsec, NSEC_PER_SEC, &rem);\n \tif (rem <= tp->tv_nsec)\n \t\tsgi_clock_offset.tv_nsec = tp->tv_sec - rem;\n-#define ns_to_timespec(ts, nsec) (ts).tv_sec = div_long_long_rem(nsec, NSEC_PER_SEC, &(ts).tv_nsec)",
    "critical_vars": [
      "rem"
    ],
    "variable_definitions": {
      "rem": "u32 rem;"
    },
    "variable_types": {
      "rem": "integer"
    },
    "type_mapping": {
      "rem": "Integer"
    },
    "vulnerable_line": "sgi_clock_offset.tv_sec = tp->tv_sec - div_long_long_rem(nsec, NSEC_PER_SEC, &rem);",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Division by Zero",
    "reasoning": "If NSEC_PER_SEC is zero, the div_long_long_rem function can cause a Divide Error Fault, leading to a denial of service."
  },
  {
    "fix_code": "static void sgi_timer_get(struct k_itimer *timr, struct itimerspec *cur_setting)\n{\n\n\tif (timr->it.mmtimer.clock == TIMER_OFF) {\n\t\tcur_setting->it_interval.tv_nsec = 0;\n\t\tcur_setting->it_interval.tv_sec = 0;\n\t\tcur_setting->it_value.tv_nsec = 0;\n\t\tcur_setting->it_value.tv_sec =0;\n\t\treturn;\n\t}\n\n\tcur_setting->it_interval = ns_to_timespec(timr->it.mmtimer.incr * sgi_clock_period);\n\tcur_setting->it_value = ns_to_timespec((timr->it.mmtimer.expires - rtc_time()) * sgi_clock_period);\n}",
    "diff": " \tnsec = rtc_time() * sgi_clock_period\n \tnsec = rtc_time() * sgi_clock_period;\n-\tns_to_timespec(cur_setting->it_interval, timr->it.mmtimer.incr * sgi_clock_period);\n-\tns_to_timespec(cur_setting->it_value, (timr->it.mmtimer.expires - rtc_time())* sgi_clock_period);\n+\tcur_setting->it_interval = ns_to_timespec(timr->it.mmtimer.incr * sgi_clock_period);\n+\tcur_setting->it_value = ns_to_timespec((timr->it.mmtimer.expires - rtc_time()) * sgi_clock_period);",
    "critical_vars": [
      "cur_setting->it_interval",
      "timr->it.mmtimer.incr",
      "sgi_clock_period"
    ],
    "variable_definitions": {
      "cur_setting->it_interval": "Definition not found",
      "timr->it.mmtimer.incr": "Definition not found",
      "sgi_clock_period": "Definition not found"
    },
    "variable_types": {
      "cur_setting->it_interval": "struct pointer_struct",
      "timr->it.mmtimer.incr": "struct pointer_struct",
      "sgi_clock_period": "integer"
    },
    "type_mapping": {
      "cur_setting->it_interval": "sp_struct",
      "timr->it.mmtimer.incr": "sp_struct",
      "sgi_clock_period": "Integer"
    },
    "vulnerable_line": "nsec = rtc_time() * sgi_clock_period;",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The multiplication of rtc_time() and sgi_clock_period can exceed the maximum value for a 64-bit integer, resulting in an overflow, potentially causing a denial of service."
  },
  {
    "fix_code": "static void sgi_timer_get(struct k_itimer *timr, struct itimerspec *cur_setting)\n{\n\n\tif (timr->it.mmtimer.clock == TIMER_OFF) {\n\t\tcur_setting->it_interval.tv_nsec = 0;\n\t\tcur_setting->it_interval.tv_sec = 0;\n\t\tcur_setting->it_value.tv_nsec = 0;\n\t\tcur_setting->it_value.tv_sec =0;\n\t\treturn;\n\t}\n\n\tcur_setting->it_interval = ns_to_timespec(timr->it.mmtimer.incr * sgi_clock_period);\n\tcur_setting->it_value = ns_to_timespec((timr->it.mmtimer.expires - rtc_time()) * sgi_clock_period);\n}",
    "diff": "-\tns_to_timespec(cur_setting->it_interval, timr->it.mmtimer.incr * sgi_clock_period);\n+\tcur_setting->it_interval = ns_to_timespec(timr->it.mmtimer.incr * sgi_clock_period);",
    "critical_vars": [
      "cur_setting->it_interval"
    ],
    "variable_definitions": {
      "cur_setting->it_interval": "Definition not found"
    },
    "variable_types": {
      "cur_setting->it_interval": "struct pointer_struct"
    },
    "type_mapping": {
      "cur_setting->it_interval": "sp_struct"
    },
    "vulnerable_line": "nsec = rtc_time() * sgi_clock_period + sgi_clock_offset.tv_nsec;",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The multiplication of rtc_time() and sgi_clock_period can exceed the maximum value for u64, leading to overflow. This can cause unexpected behavior when calculation results are used further in the program."
  },
  {
    "fix_code": "Function not found",
    "diff": "-\twhen = timespec_to_ns(new_setting->it_value);\n+\twhen = timespec_to_ns(&new_setting->it_value);\n \tif (when == 0)\n \t\tif (when > now)\n \t\t\twhen -= now;",
    "critical_vars": [
      "when"
    ],
    "variable_definitions": {
      "when": "unsigned long when, period, irqflags;"
    },
    "variable_types": {
      "when": "integer"
    },
    "type_mapping": {
      "when": "Integer"
    },
    "vulnerable_line": "sgi_clock_offset.tv_sec = tp->tv_sec - div_long_long_rem(nsec, NSEC_PER_SEC, &rem);",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Division by Zero",
    "reasoning": "The function div_long_long_rem can cause a Division Error Fault if NSEC_PER_SEC is zero, leading to a denial of service. The fix changes it to div_u64_rem which is likely to manage the zero divisor more safely."
  },
  {
    "fix_code": "asmlinkage long sys_oabi_semtimedop(int semid,\n\t\t\t\t    struct oabi_sembuf __user *tsops,\n\t\t\t\t    unsigned nsops,\n\t\t\t\t    const struct timespec __user *timeout)\n{\n\tstruct sembuf *sops;\n\tstruct timespec local_timeout;\n\tlong err;\n\tint i;\n\n\tif (nsops < 1 || nsops > SEMOPM)\n\t\treturn -EINVAL;\n\tsops = kmalloc(sizeof(*sops) * nsops, GFP_KERNEL);\n\tif (!sops)\n\t\treturn -ENOMEM;\n\terr = 0;\n\tfor (i = 0; i < nsops; i++) {\n\t\t__get_user_error(sops[i].sem_num, &tsops->sem_num, err);\n\t\t__get_user_error(sops[i].sem_op,  &tsops->sem_op,  err);\n\t\t__get_user_error(sops[i].sem_flg, &tsops->sem_flg, err);\n\t\ttsops++;\n\t}\n\tif (timeout) {\n\t\t/* copy this as well before changing domain protection */\n\t\terr |= copy_from_user(&local_timeout, timeout, sizeof(*timeout));\n\t\ttimeout = &local_timeout;\n\t}\n\tif (err) {\n\t\terr = -EFAULT;\n\t} else {\n\t\tmm_segment_t fs = get_fs();\n\t\tset_fs(KERNEL_DS);\n\t\terr = sys_semtimedop(semid, sops, nsops, timeout);\n\t\tset_fs(fs);\n\t}\n\tkfree(sops);\n\treturn err;\n}",
    "diff": "-\tif (nsops < 1)\n+\tif (nsops < 1 || nsops > SEMOPM)\n \tsops = kmalloc(sizeof(*sops) * nsops, GFP_KERNEL);",
    "critical_vars": [
      "nsops"
    ],
    "variable_definitions": {
      "nsops": "unsigned nsops,"
    },
    "variable_types": {
      "nsops": "integer"
    },
    "type_mapping": {
      "nsops": "Integer"
    },
    "vulnerable_line": "if (nsops < 1)",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The check for nsops < 1 does not account for potential integer overflow from the user input, leading to insufficient validation of the size calculated with nsops, which could subsequently cause heap memory corruption when passed to kmalloc."
  },
  {
    "fix_code": "static ssize_t lbs_debugfs_write(struct file *f, const char __user *buf,\n\t\t\t    size_t cnt, loff_t *ppos)\n{\n\tint r, i;\n\tchar *pdata;\n\tchar *p;\n\tchar *p0;\n\tchar *p1;\n\tchar *p2;\n\tstruct debug_data *d = f->private_data;\n\n\tif (cnt == 0)\n\t\treturn 0;\n\n\tpdata = kmalloc(cnt + 1, GFP_KERNEL);\n\tif (pdata == NULL)\n\t\treturn 0;\n\n\tif (copy_from_user(pdata, buf, cnt)) {\n\t\tlbs_deb_debugfs(\"Copy from user failed\\n\");\n\t\tkfree(pdata);\n\t\treturn 0;\n\t}\n\tpdata[cnt] = '\\0';\n\n\tp0 = pdata;\n\tfor (i = 0; i < num_of_items; i++) {\n\t\tdo {\n\t\t\tp = strstr(p0, d[i].name);\n\t\t\tif (p == NULL)\n\t\t\t\tbreak;\n\t\t\tp1 = strchr(p, '\\n');\n\t\t\tif (p1 == NULL)\n\t\t\t\tbreak;\n\t\t\tp0 = p1++;\n\t\t\tp2 = strchr(p, '=');\n\t\t\tif (!p2)\n\t\t\t\tbreak;\n\t\t\tp2++;\n\t\t\tr = simple_strtoul(p2, NULL, 0);\n\t\t\tif (d[i].size == 1)\n\t\t\t\t*((u8 *) d[i].addr) = (u8) r;\n\t\t\telse if (d[i].size == 2)\n\t\t\t\t*((u16 *) d[i].addr) = (u16) r;\n\t\t\telse if (d[i].size == 4)\n\t\t\t\t*((u32 *) d[i].addr) = (u32) r;\n\t\t\telse if (d[i].size == 8)\n\t\t\t\t*((u64 *) d[i].addr) = (u64) r;\n\t\t\tbreak;\n\t\t} while (1);\n\t}\n\tkfree(pdata);\n\n\treturn (ssize_t)cnt;\n}",
    "diff": "-\tpdata = kmalloc(cnt, GFP_KERNEL);\n+\tpdata = kmalloc(cnt + 1, GFP_KERNEL);\n \tif (pdata == NULL)\n \t\tkfree(pdata);\n+\tpdata[cnt] = '\\0';\n \tp0 = pdata;",
    "critical_vars": [
      "pdata"
    ],
    "variable_definitions": {
      "pdata": "char *pdata;"
    },
    "variable_types": {
      "pdata": "char pointer"
    },
    "type_mapping": {
      "pdata": "char pointer"
    },
    "vulnerable_line": "pdata = kmalloc(cnt, GFP_KERNEL);",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Buffer Size Calculation Error",
    "reasoning": "Initially allocating memory for pdata without checking cnt could lead to a zero-length allocation, which the fix addresses by ensuring cnt is greater than zero before allocation."
  },
  {
    "fix_code": "static ssize_t lbs_debugfs_write(struct file *f, const char __user *buf,\n\t\t\t    size_t cnt, loff_t *ppos)\n{\n\tint r, i;\n\tchar *pdata;\n\tchar *p;\n\tchar *p0;\n\tchar *p1;\n\tchar *p2;\n\tstruct debug_data *d = f->private_data;\n\n\tif (cnt == 0)\n\t\treturn 0;\n\n\tpdata = kmalloc(cnt + 1, GFP_KERNEL);\n\tif (pdata == NULL)\n\t\treturn 0;\n\n\tif (copy_from_user(pdata, buf, cnt)) {\n\t\tlbs_deb_debugfs(\"Copy from user failed\\n\");\n\t\tkfree(pdata);\n\t\treturn 0;\n\t}\n\tpdata[cnt] = '\\0';\n\n\tp0 = pdata;\n\tfor (i = 0; i < num_of_items; i++) {\n\t\tdo {\n\t\t\tp = strstr(p0, d[i].name);\n\t\t\tif (p == NULL)\n\t\t\t\tbreak;\n\t\t\tp1 = strchr(p, '\\n');\n\t\t\tif (p1 == NULL)\n\t\t\t\tbreak;\n\t\t\tp0 = p1++;\n\t\t\tp2 = strchr(p, '=');\n\t\t\tif (!p2)\n\t\t\t\tbreak;\n\t\t\tp2++;\n\t\t\tr = simple_strtoul(p2, NULL, 0);\n\t\t\tif (d[i].size == 1)\n\t\t\t\t*((u8 *) d[i].addr) = (u8) r;\n\t\t\telse if (d[i].size == 2)\n\t\t\t\t*((u16 *) d[i].addr) = (u16) r;\n\t\t\telse if (d[i].size == 4)\n\t\t\t\t*((u32 *) d[i].addr) = (u32) r;\n\t\t\telse if (d[i].size == 8)\n\t\t\t\t*((u64 *) d[i].addr) = (u64) r;\n\t\t\tbreak;\n\t\t} while (1);\n\t}\n\tkfree(pdata);\n\n\treturn (ssize_t)cnt;\n}",
    "diff": "-\tpdata = kmalloc(cnt, GFP_KERNEL);\n+\tif (cnt == 0)\n+\tpdata = kmalloc(cnt + 1, GFP_KERNEL);\n+\tpdata[cnt] = '\\0';",
    "critical_vars": [
      "cnt"
    ],
    "variable_definitions": {
      "cnt": "size_t cnt"
    },
    "variable_types": {
      "cnt": "integer"
    },
    "type_mapping": {
      "cnt": "Integer"
    },
    "vulnerable_line": "pdata = kmalloc(cnt, GFP_KERNEL);",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Buffer Size Calculation Error",
    "reasoning": "cnt could potentially be zero, which affects memory allocation, leading to a denial of service when it is used without a prior check, thus the fix checks for cnt to be non-zero before allocation."
  },
  {
    "fix_code": "static int copy_verifier_state(struct bpf_verifier_state *dst_state,\n\t\t\t       const struct bpf_verifier_state *src)\n{\n\tstruct bpf_func_state *dst;\n\tint i, err;\n\n\t/* if dst has more stack frames then src frame, free them */\n\tfor (i = src->curframe + 1; i <= dst_state->curframe; i++) {\n\t\tfree_func_state(dst_state->frame[i]);\n\t\tdst_state->frame[i] = NULL;\n\t}\n\tdst_state->speculative = src->speculative;\n\tdst_state->curframe = src->curframe;\n\tfor (i = 0; i <= src->curframe; i++) {\n\t\tdst = dst_state->frame[i];\n\t\tif (!dst) {\n\t\t\tdst = kzalloc(sizeof(*dst), GFP_KERNEL);\n\t\t\tif (!dst)\n\t\t\t\treturn -ENOMEM;\n\t\t\tdst_state->frame[i] = dst;\n\t\t}\n\t\terr = copy_func_state(dst, src->frame[i]);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\treturn 0;\n}",
    "diff": "+\tdst_state->speculative = src->speculative;",
    "critical_vars": [
      "dst_state->speculative"
    ],
    "variable_definitions": {
      "dst_state->speculative": "Definition not found"
    },
    "variable_types": {
      "dst_state->speculative": "struct pointer_integer"
    },
    "type_mapping": {
      "dst_state->speculative": "sp_integer"
    },
    "vulnerable_line": "dst_state->speculative = src->speculative;",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Pointer Arithmetic Error",
    "reasoning": "The assignment of speculative state can lead to incorrect pointer arithmetic if mixed states from speculative execution paths are not properly handled, thereby allowing side-channel attacks."
  },
  {
    "fix_code": "static struct bpf_verifier_state *push_stack(struct bpf_verifier_env *env,\n\t\t\t\t\t     int insn_idx, int prev_insn_idx,\n\t\t\t\t\t     bool speculative)\n{\n\tstruct bpf_verifier_state *cur = env->cur_state;\n\tstruct bpf_verifier_stack_elem *elem;\n\tint err;\n\n\telem = kzalloc(sizeof(struct bpf_verifier_stack_elem), GFP_KERNEL);\n\tif (!elem)\n\t\tgoto err;\n\n\telem->insn_idx = insn_idx;\n\telem->prev_insn_idx = prev_insn_idx;\n\telem->next = env->head;\n\tenv->head = elem;\n\tenv->stack_size++;\n\terr = copy_verifier_state(&elem->st, cur);\n\tif (err)\n\t\tgoto err;\n\telem->st.speculative |= speculative;\n\tif (env->stack_size > BPF_COMPLEXITY_LIMIT_STACK) {\n\t\tverbose(env, \"BPF program is too complex\\n\");\n\t\tgoto err;\n\t}\n\treturn &elem->st;\nerr:\n\tfree_verifier_state(env->cur_state, true);\n\tenv->cur_state = NULL;\n\t/* pop all elements and return */\n\twhile (!pop_stack(env, NULL, NULL));\n\treturn NULL;\n}",
    "diff": "+\telem->st.speculative |= speculative;",
    "critical_vars": [
      "elem->st.speculative"
    ],
    "variable_definitions": {
      "elem->st.speculative": "Definition not found"
    },
    "variable_types": {
      "elem->st.speculative": "struct pointer_struct"
    },
    "type_mapping": {
      "elem->st.speculative": "sp_struct"
    },
    "vulnerable_line": "elem->st.speculative |= speculative;",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Pointer Arithmetic Error",
    "reasoning": "The line introduces unwanted pointer state modification based on speculative execution paths. This can lead to out-of-bounds memory access, potentially exploited in side-channel attacks."
  },
  {
    "fix_code": "static int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn,\n\t\t\t\t   const struct bpf_reg_state *ptr_reg,\n\t\t\t\t   const struct bpf_reg_state *off_reg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg;\n\tbool known = tnum_is_const(off_reg->var_off);\n\ts64 smin_val = off_reg->smin_value, smax_val = off_reg->smax_value,\n\t    smin_ptr = ptr_reg->smin_value, smax_ptr = ptr_reg->smax_value;\n\tu64 umin_val = off_reg->umin_value, umax_val = off_reg->umax_value,\n\t    umin_ptr = ptr_reg->umin_value, umax_ptr = ptr_reg->umax_value;\n\tu32 dst = insn->dst_reg, src = insn->src_reg;\n\tu8 opcode = BPF_OP(insn->code);\n\tint ret;\n\n\tdst_reg = &regs[dst];\n\n\tif ((known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops on pointers produce (meaningless) scalars */\n\t\tverbose(env,\n\t\t\t\"R%d 32-bit pointer arithmetic prohibited\\n\",\n\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\n\tswitch (ptr_reg->type) {\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited, null-check it first\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tcase CONST_PTR_TO_MAP:\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (!env->allow_ptr_leaks && !known && (smin_val < 0) != (smax_val < 0)) {\n\t\t\tverbose(env, \"R%d has unknown scalar with mixed signed bounds, pointer arithmetic with it prohibited for !root\\n\",\n\t\t\t\toff_reg == dst_reg ? dst : src);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* fall-through */\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* In case of 'scalar += pointer', dst_reg inherits pointer type and id.\n\t * The id may be overwritten later if we create a new variable offset.\n\t */\n\tdst_reg->type = ptr_reg->type;\n\tdst_reg->id = ptr_reg->id;\n\n\tif (!check_reg_sane_offset(env, off_reg, ptr_reg->type) ||\n\t    !check_reg_sane_offset(env, ptr_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, dst_reg, smin_val < 0);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to add from different maps or paths\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\t/* We can take a fixed offset as long as it doesn't overflow\n\t\t * the s32 'off' field\n\t\t */\n\t\tif (known && (ptr_reg->off + smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off + smin_val))) {\n\t\t\t/* pointer += K.  Accumulate it into fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->off = ptr_reg->off + smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  Note that off_reg->off\n\t\t * == 0, since it's a scalar.\n\t\t * dst_reg gets the pointer type and since some positive\n\t\t * integer value was added to the pointer, give it a new 'id'\n\t\t * if it's a PTR_TO_PACKET.\n\t\t * this creates a new 'base' pointer, off_reg (variable) gets\n\t\t * added into the variable offset, and we copy the fixed offset\n\t\t * from ptr_reg.\n\t\t */\n\t\tif (signed_add_overflows(smin_ptr, smin_val) ||\n\t\t    signed_add_overflows(smax_ptr, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr + smin_val;\n\t\t\tdst_reg->smax_value = smax_ptr + smax_val;\n\t\t}\n\t\tif (umin_ptr + umin_val < umin_ptr ||\n\t\t    umax_ptr + umax_val < umax_ptr) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value = umin_ptr + umin_val;\n\t\t\tdst_reg->umax_value = umax_ptr + umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tdst_reg->raw = 0;\n\t\t}\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, dst_reg, smin_val < 0);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to sub from different maps or paths\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\tif (dst_reg == off_reg) {\n\t\t\t/* scalar -= pointer.  Creates an unknown scalar */\n\t\t\tverbose(env, \"R%d tried to subtract pointer from scalar\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* We don't allow subtraction from FP, because (according to\n\t\t * test_verifier.c test \"invalid fp arithmetic\", JITs might not\n\t\t * be able to deal with it.\n\t\t */\n\t\tif (ptr_reg->type == PTR_TO_STACK) {\n\t\t\tverbose(env, \"R%d subtraction from stack pointer prohibited\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (known && (ptr_reg->off - smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off - smin_val))) {\n\t\t\t/* pointer -= K.  Subtract it from fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->id = ptr_reg->id;\n\t\t\tdst_reg->off = ptr_reg->off - smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  If the subtrahend is known\n\t\t * nonnegative, then any reg->range we had before is still good.\n\t\t */\n\t\tif (signed_sub_overflows(smin_ptr, smax_val) ||\n\t\t    signed_sub_overflows(smax_ptr, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr - smax_val;\n\t\t\tdst_reg->smax_value = smax_ptr - smin_val;\n\t\t}\n\t\tif (umin_ptr < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value = umin_ptr - umax_val;\n\t\t\tdst_reg->umax_value = umax_ptr - umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tif (smin_val < 0)\n\t\t\t\tdst_reg->raw = 0;\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\tcase BPF_OR:\n\tcase BPF_XOR:\n\t\t/* bitwise ops on pointers are troublesome, prohibit. */\n\t\tverbose(env, \"R%d bitwise operator %s on pointer prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\tdefault:\n\t\t/* other operators (e.g. MUL,LSH) produce non-pointer results */\n\t\tverbose(env, \"R%d pointer arithmetic with %s operator prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!check_reg_sane_offset(env, dst_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t__update_reg_bounds(dst_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\n\t/* For unprivileged we require that resulting offset must be in bounds\n\t * in order to be able to sanitize access later on.\n\t */\n\tif (!env->allow_ptr_leaks) {\n\t\tif (dst_reg->type == PTR_TO_MAP_VALUE &&\n\t\t    check_map_access(env, dst, dst_reg->off, 1, false)) {\n\t\t\tverbose(env, \"R%d pointer arithmetic of map value goes out of range, \"\n\t\t\t\t\"prohibited for !root\\n\", dst);\n\t\t\treturn -EACCES;\n\t\t} else if (dst_reg->type == PTR_TO_STACK &&\n\t\t\t   check_stack_access(env, dst_reg, dst_reg->off +\n\t\t\t\t\t      dst_reg->var_off.value, 1)) {\n\t\t\tverbose(env, \"R%d stack pointer arithmetic goes out of range, \"\n\t\t\t\t\"prohibited for !root\\n\", dst);\n\t\t\treturn -EACCES;\n\t\t}\n\t}\n\n\treturn 0;\n}",
    "diff": " \treturn true;\n+\treturn &env->insn_aux_data[env->insn_idx];\n+static int retrieve_ptr_limit(const struct bpf_reg_state *ptr_reg,\n+\t\treturn 0;\n+\t\treturn 0;\n+\t\treturn -EINVAL;\n+\tbool ret;\n+\t\treturn 0;\n+\tif (retrieve_ptr_limit(ptr_reg, &alu_limit, opcode, off_is_neg))\n+\t\treturn 0;\n+\t\treturn -EACCES;\n+\tret = push_stack(env, env->insn_idx + 1, env->insn_idx, true);\n+\treturn !ret ? -EFAULT : 0;\n  * If we return -EACCES, caller may want to try again treating pointer as a\n+\tint ret;\n+\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, dst_reg, smin_val < 0);\n+\t\tif (ret < 0) {\n+\t\t\treturn ret;\n+\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, dst_reg, smin_val < 0);\n+\t\tif (ret < 0) {\n+\t\t\treturn ret;\n \t\treturn -EFAULT;\n \t\treturn false;\n+\t\treturn false;\n \t\treturn -ENOMEM;\n+\t\t\t\treturn -ENOMEM;",
    "critical_vars": [
      "ret"
    ],
    "variable_definitions": {
      "ret": "int ret;"
    },
    "variable_types": {
      "ret": "integer"
    },
    "type_mapping": {
      "ret": "Integer"
    },
    "vulnerable_line": "if (signed_add_overflows(smin_ptr, smin_val) || signed_add_overflows(smax_ptr, smax_val))",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The use of signed_add_overflows function checks for potential overflow when adding signed integers, which can lead to unexpected behavior if the sum exceeds the maximum representable value of the signed integer type."
  },
  {
    "fix_code": "static int check_cond_jmp_op(struct bpf_verifier_env *env,\n\t\t\t     struct bpf_insn *insn, int *insn_idx)\n{\n\tstruct bpf_verifier_state *this_branch = env->cur_state;\n\tstruct bpf_verifier_state *other_branch;\n\tstruct bpf_reg_state *regs = this_branch->frame[this_branch->curframe]->regs;\n\tstruct bpf_reg_state *dst_reg, *other_branch_regs;\n\tu8 opcode = BPF_OP(insn->code);\n\tint err;\n\n\tif (opcode > BPF_JSLE) {\n\t\tverbose(env, \"invalid BPF_JMP opcode %x\\n\", opcode);\n\t\treturn -EINVAL;\n\t}\n\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tif (insn->imm != 0) {\n\t\t\tverbose(env, \"BPF_JMP uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/* check src1 operand */\n\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (is_pointer_value(env, insn->src_reg)) {\n\t\t\tverbose(env, \"R%d pointer comparison prohibited\\n\",\n\t\t\t\tinsn->src_reg);\n\t\t\treturn -EACCES;\n\t\t}\n\t} else {\n\t\tif (insn->src_reg != BPF_REG_0) {\n\t\t\tverbose(env, \"BPF_JMP uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t/* check src2 operand */\n\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tdst_reg = &regs[insn->dst_reg];\n\n\tif (BPF_SRC(insn->code) == BPF_K) {\n\t\tint pred = is_branch_taken(dst_reg, insn->imm, opcode);\n\n\t\tif (pred == 1) {\n\t\t\t /* only follow the goto, ignore fall-through */\n\t\t\t*insn_idx += insn->off;\n\t\t\treturn 0;\n\t\t} else if (pred == 0) {\n\t\t\t/* only follow fall-through branch, since\n\t\t\t * that's where the program will go\n\t\t\t */\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tother_branch = push_stack(env, *insn_idx + insn->off + 1, *insn_idx,\n\t\t\t\t  false);\n\tif (!other_branch)\n\t\treturn -EFAULT;\n\tother_branch_regs = other_branch->frame[other_branch->curframe]->regs;\n\n\t/* detect if we are comparing against a constant value so we can adjust\n\t * our min/max values for our dst register.\n\t * this is only legit if both are scalars (or pointers to the same\n\t * object, I suppose, but we don't support that right now), because\n\t * otherwise the different base pointers mean the offsets aren't\n\t * comparable.\n\t */\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tif (dst_reg->type == SCALAR_VALUE &&\n\t\t    regs[insn->src_reg].type == SCALAR_VALUE) {\n\t\t\tif (tnum_is_const(regs[insn->src_reg].var_off))\n\t\t\t\treg_set_min_max(&other_branch_regs[insn->dst_reg],\n\t\t\t\t\t\tdst_reg, regs[insn->src_reg].var_off.value,\n\t\t\t\t\t\topcode);\n\t\t\telse if (tnum_is_const(dst_reg->var_off))\n\t\t\t\treg_set_min_max_inv(&other_branch_regs[insn->src_reg],\n\t\t\t\t\t\t    &regs[insn->src_reg],\n\t\t\t\t\t\t    dst_reg->var_off.value, opcode);\n\t\t\telse if (opcode == BPF_JEQ || opcode == BPF_JNE)\n\t\t\t\t/* Comparing for equality, we can combine knowledge */\n\t\t\t\treg_combine_min_max(&other_branch_regs[insn->src_reg],\n\t\t\t\t\t\t    &other_branch_regs[insn->dst_reg],\n\t\t\t\t\t\t    &regs[insn->src_reg],\n\t\t\t\t\t\t    &regs[insn->dst_reg], opcode);\n\t\t}\n\t} else if (dst_reg->type == SCALAR_VALUE) {\n\t\treg_set_min_max(&other_branch_regs[insn->dst_reg],\n\t\t\t\t\tdst_reg, insn->imm, opcode);\n\t}\n\n\t/* detect if R == 0 where R is returned from bpf_map_lookup_elem() */\n\tif (BPF_SRC(insn->code) == BPF_K &&\n\t    insn->imm == 0 && (opcode == BPF_JEQ || opcode == BPF_JNE) &&\n\t    reg_type_may_be_null(dst_reg->type)) {\n\t\t/* Mark all identical registers in each branch as either\n\t\t * safe or unknown depending R == 0 or R != 0 conditional.\n\t\t */\n\t\tmark_ptr_or_null_regs(this_branch, insn->dst_reg,\n\t\t\t\t      opcode == BPF_JNE);\n\t\tmark_ptr_or_null_regs(other_branch, insn->dst_reg,\n\t\t\t\t      opcode == BPF_JEQ);\n\t} else if (!try_match_pkt_pointers(insn, dst_reg, &regs[insn->src_reg],\n\t\t\t\t\t   this_branch, other_branch) &&\n\t\t   is_pointer_value(env, insn->dst_reg)) {\n\t\tverbose(env, \"R%d pointer comparison prohibited\\n\",\n\t\t\tinsn->dst_reg);\n\t\treturn -EACCES;\n\t}\n\tif (env->log.level)\n\t\tprint_verifier_state(env, this_branch->frame[this_branch->curframe]);\n\treturn 0;\n}\n\n/* return the map pointer stored inside BPF_LD_IMM64 instruction */\nstatic struct bpf_map *ld_imm64_to_map_ptr(struct bpf_insn *insn)\n{\n\tu64 imm64 = ((u64) (u32) insn[0].imm) | ((u64) (u32) insn[1].imm) << 32;\n\n\treturn (struct bpf_map *) (unsigned long) imm64;\n}\n\n/* verify BPF_LD_IMM64 instruction */\nstatic int check_ld_imm(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tint err;\n\n\tif (BPF_SIZE(insn->code) != BPF_DW) {\n\t\tverbose(env, \"invalid BPF_LD_IMM insn\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (insn->off != 0) {\n\t\tverbose(env, \"BPF_LD_IMM64 uses reserved fields\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\terr = check_reg_arg(env, insn->dst_reg, DST_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (insn->src_reg == 0) {\n\t\tu64 imm = ((u64)(insn + 1)->imm << 32) | (u32)insn->imm;\n\n\t\tregs[insn->dst_reg].type = SCALAR_VALUE;\n\t\t__mark_reg_known(&regs[insn->dst_reg], imm);\n\t\treturn 0;\n\t}\n\n\t/* replace_map_fd_with_map_ptr() should have caught bad ld_imm64 */\n\tBUG_ON(insn->src_reg != BPF_PSEUDO_MAP_FD);\n\n\tregs[insn->dst_reg].type = CONST_PTR_TO_MAP;\n\tregs[insn->dst_reg].map_ptr = ld_imm64_to_map_ptr(insn);\n\treturn 0;\n}\n\nstatic bool may_access_skb(enum bpf_prog_type type)\n{\n\tswitch (type) {\n\tcase BPF_PROG_TYPE_SOCKET_FILTER:\n\tcase BPF_PROG_TYPE_SCHED_CLS:\n\tcase BPF_PROG_TYPE_SCHED_ACT:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n/* verify safety of LD_ABS|LD_IND instructions:\n * - they can only appear in the programs where ctx == skb\n * - since they are wrappers of function calls, they scratch R1-R5 registers,\n *   preserve R6-R9, and store return value into R0\n *\n * Implicit input:\n *   ctx == skb == R6 == CTX\n *\n * Explicit input:\n *   SRC == any register\n *   IMM == 32-bit immediate\n *\n * Output:\n *   R0 - 8/16/32-bit skb data converted to cpu endianness\n */\nstatic int check_ld_abs(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 mode = BPF_MODE(insn->code);\n\tint i, err;\n\n\tif (!may_access_skb(env->prog->type)) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] instructions not allowed for this program type\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!env->ops->gen_ld_abs) {\n\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (env->subprog_cnt > 1) {\n\t\t/* when program has LD_ABS insn JITs and interpreter assume\n\t\t * that r1 == ctx == skb which is not the case for callees\n\t\t * that can have arbitrary arguments. It's problematic\n\t\t * for main prog as well since JITs would need to analyze\n\t\t * all functions in order to make proper register save/restore\n\t\t * decisions in the main prog. Hence disallow LD_ABS with calls\n\t\t */\n\t\tverbose(env, \"BPF_LD_[ABS|IND] instructions cannot be mixed with bpf-to-bpf calls\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (insn->dst_reg != BPF_REG_0 || insn->off != 0 ||\n\t    BPF_SIZE(insn->code) == BPF_DW ||\n\t    (mode == BPF_ABS && insn->src_reg != BPF_REG_0)) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] uses reserved fields\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* check whether implicit source operand (register R6) is readable */\n\terr = check_reg_arg(env, BPF_REG_6, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\t/* Disallow usage of BPF_LD_[ABS|IND] with reference tracking, as\n\t * gen_ld_abs() may terminate the program at runtime, leading to\n\t * reference leak.\n\t */\n\terr = check_reference_leak(env);\n\tif (err) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] cannot be mixed with socket references\\n\");\n\t\treturn err;\n\t}\n\n\tif (regs[BPF_REG_6].type != PTR_TO_CTX) {\n\t\tverbose(env,\n\t\t\t\"at the time of BPF_LD_ABS|IND R6 != pointer to skb\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (mode == BPF_IND) {\n\t\t/* check explicit source operand */\n\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\t/* reset caller saved regs to unreadable */\n\tfor (i = 0; i < CALLER_SAVED_REGS; i++) {\n\t\tmark_reg_not_init(env, regs, caller_saved[i]);\n\t\tcheck_reg_arg(env, caller_saved[i], DST_OP_NO_MARK);\n\t}\n\n\t/* mark destination R0 register as readable, since it contains\n\t * the value fetched from the packet.\n\t * Already marked as written above.\n\t */\n\tmark_reg_unknown(env, regs, BPF_REG_0);\n\treturn 0;\n}\n\nstatic int check_return_code(struct bpf_verifier_env *env)\n{\n\tstruct bpf_reg_state *reg;\n\tstruct tnum range = tnum_range(0, 1);\n\n\tswitch (env->prog->type) {\n\tcase BPF_PROG_TYPE_CGROUP_SKB:\n\tcase BPF_PROG_TYPE_CGROUP_SOCK:\n\tcase BPF_PROG_TYPE_CGROUP_SOCK_ADDR:\n\tcase BPF_PROG_TYPE_SOCK_OPS:\n\tcase BPF_PROG_TYPE_CGROUP_DEVICE:\n\t\tbreak;\n\tdefault:\n\t\treturn 0;\n\t}\n\n\treg = cur_regs(env) + BPF_REG_0;\n\tif (reg->type != SCALAR_VALUE) {\n\t\tverbose(env, \"At program exit the register R0 is not a known value (%s)\\n\",\n\t\t\treg_type_str[reg->type]);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!tnum_in(range, reg->var_off)) {\n\t\tverbose(env, \"At program exit the register R0 \");\n\t\tif (!tnum_is_unknown(reg->var_off)) {\n\t\t\tchar tn_buf[48];\n\n\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\tverbose(env, \"has value %s\", tn_buf);\n\t\t} else {\n\t\t\tverbose(env, \"has unknown scalar value\");\n\t\t}\n\t\tverbose(env, \" should have been 0 or 1\\n\");\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\n/* non-recursive DFS pseudo code\n * 1  procedure DFS-iterative(G,v):\n * 2      label v as discovered\n * 3      let S be a stack\n * 4      S.push(v)\n * 5      while S is not empty\n * 6            t <- S.pop()\n * 7            if t is what we're looking for:\n * 8                return t\n * 9            for all edges e in G.adjacentEdges(t) do\n * 10               if edge e is already labelled\n * 11                   continue with the next edge\n * 12               w <- G.adjacentVertex(t,e)\n * 13               if vertex w is not discovered and not explored\n * 14                   label e as tree-edge\n * 15                   label w as discovered\n * 16                   S.push(w)\n * 17                   continue at 5\n * 18               else if vertex w is discovered\n * 19                   label e as back-edge\n * 20               else\n * 21                   // vertex w is explored\n * 22                   label e as forward- or cross-edge\n * 23           label t as explored\n * 24           S.pop()\n *\n * convention:\n * 0x10 - discovered\n * 0x11 - discovered and fall-through edge labelled\n * 0x12 - discovered and fall-through and branch edges labelled\n * 0x20 - explored\n */\n\nenum {\n\tDISCOVERED = 0x10,\n\tEXPLORED = 0x20,\n\tFALLTHROUGH = 1,\n\tBRANCH = 2,\n};\n\n#define STATE_LIST_MARK ((struct bpf_verifier_state_list *) -1L)\n\nstatic int *insn_stack;\t/* stack of insns to process */\nstatic int cur_stack;\t/* current stack index */\nstatic int *insn_state;\n\n/* t, w, e - match pseudo-code above:\n * t - index of current instruction\n * w - next instruction\n * e - edge\n */\nstatic int push_insn(int t, int w, int e, struct bpf_verifier_env *env)\n{\n\tif (e == FALLTHROUGH && insn_state[t] >= (DISCOVERED | FALLTHROUGH))\n\t\treturn 0;\n\n\tif (e == BRANCH && insn_state[t] >= (DISCOVERED | BRANCH))\n\t\treturn 0;\n\n\tif (w < 0 || w >= env->prog->len) {\n\t\tverbose_linfo(env, t, \"%d: \", t);\n\t\tverbose(env, \"jump out of range from insn %d to %d\\n\", t, w);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e == BRANCH)\n\t\t/* mark branch target for state pruning */\n\t\tenv->explored_states[w] = STATE_LIST_MARK;\n\n\tif (insn_state[w] == 0) {\n\t\t/* tree-edge */\n\t\tinsn_state[t] = DISCOVERED | e;\n\t\tinsn_state[w] = DISCOVERED;\n\t\tif (cur_stack >= env->prog->len)\n\t\t\treturn -E2BIG;\n\t\tinsn_stack[cur_stack++] = w;\n\t\treturn 1;\n\t} else if ((insn_state[w] & 0xF0) == DISCOVERED) {\n\t\tverbose_linfo(env, t, \"%d: \", t);\n\t\tverbose_linfo(env, w, \"%d: \", w);\n\t\tverbose(env, \"back-edge from insn %d to %d\\n\", t, w);\n\t\treturn -EINVAL;\n\t} else if (insn_state[w] == EXPLORED) {\n\t\t/* forward- or cross-edge */\n\t\tinsn_state[t] = DISCOVERED | e;\n\t} else {\n\t\tverbose(env, \"insn state internal bug\\n\");\n\t\treturn -EFAULT;\n\t}\n\treturn 0;\n}\n\n/* non-recursive depth-first-search to detect loops in BPF program\n * loop == back-edge in directed graph\n */\nstatic int check_cfg(struct bpf_verifier_env *env)\n{\n\tstruct bpf_insn *insns = env->prog->insnsi;\n\tint insn_cnt = env->prog->len;\n\tint ret = 0;\n\tint i, t;\n\n\tinsn_state = kcalloc(insn_cnt, sizeof(int), GFP_KERNEL);\n\tif (!insn_state)\n\t\treturn -ENOMEM;\n\n\tinsn_stack = kcalloc(insn_cnt, sizeof(int), GFP_KERNEL);\n\tif (!insn_stack) {\n\t\tkfree(insn_state);\n\t\treturn -ENOMEM;\n\t}\n\n\tinsn_state[0] = DISCOVERED; /* mark 1st insn as discovered */\n\tinsn_stack[0] = 0; /* 0 is the first instruction */\n\tcur_stack = 1;\n\npeek_stack:\n\tif (cur_stack == 0)\n\t\tgoto check_state;\n\tt = insn_stack[cur_stack - 1];\n\n\tif (BPF_CLASS(insns[t].code) == BPF_JMP) {\n\t\tu8 opcode = BPF_OP(insns[t].code);\n\n\t\tif (opcode == BPF_EXIT) {\n\t\t\tgoto mark_explored;\n\t\t} else if (opcode == BPF_CALL) {\n\t\t\tret = push_insn(t, t + 1, FALLTHROUGH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t\tif (t + 1 < insn_cnt)\n\t\t\t\tenv->explored_states[t + 1] = STATE_LIST_MARK;\n\t\t\tif (insns[t].src_reg == BPF_PSEUDO_CALL) {\n\t\t\t\tenv->explored_states[t] = STATE_LIST_MARK;\n\t\t\t\tret = push_insn(t, t + insns[t].imm + 1, BRANCH, env);\n\t\t\t\tif (ret == 1)\n\t\t\t\t\tgoto peek_stack;\n\t\t\t\telse if (ret < 0)\n\t\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t} else if (opcode == BPF_JA) {\n\t\t\tif (BPF_SRC(insns[t].code) != BPF_K) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t\t/* unconditional jump with single edge */\n\t\t\tret = push_insn(t, t + insns[t].off + 1,\n\t\t\t\t\tFALLTHROUGH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t\t/* tell verifier to check for equivalent states\n\t\t\t * after every call and jump\n\t\t\t */\n\t\t\tif (t + 1 < insn_cnt)\n\t\t\t\tenv->explored_states[t + 1] = STATE_LIST_MARK;\n\t\t} else {\n\t\t\t/* conditional jump with two edges */\n\t\t\tenv->explored_states[t] = STATE_LIST_MARK;\n\t\t\tret = push_insn(t, t + 1, FALLTHROUGH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\n\t\t\tret = push_insn(t, t + insns[t].off + 1, BRANCH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t}\n\t} else {\n\t\t/* all other non-branch instructions with single\n\t\t * fall-through edge\n\t\t */\n\t\tret = push_insn(t, t + 1, FALLTHROUGH, env);\n\t\tif (ret == 1)\n\t\t\tgoto peek_stack;\n\t\telse if (ret < 0)\n\t\t\tgoto err_free;\n\t}\n\nmark_explored:\n\tinsn_state[t] = EXPLORED;\n\tif (cur_stack-- <= 0) {\n\t\tverbose(env, \"pop stack internal bug\\n\");\n\t\tret = -EFAULT;\n\t\tgoto err_free;\n\t}\n\tgoto peek_stack;\n\ncheck_state:\n\tfor (i = 0; i < insn_cnt; i++) {\n\t\tif (insn_state[i] != EXPLORED) {\n\t\t\tverbose(env, \"unreachable insn %d\\n\", i);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\t}\n\tret = 0; /* cfg looks good */\n\nerr_free:\n\tkfree(insn_state);\n\tkfree(insn_stack);\n\treturn ret;\n}\n\n/* The minimum supported BTF func info size */\n#define MIN_BPF_FUNCINFO_SIZE\t8\n#define MAX_FUNCINFO_REC_SIZE\t252\n\nstatic int check_btf_func(struct bpf_verifier_env *env,\n\t\t\t  const union bpf_attr *attr,\n\t\t\t  union bpf_attr __user *uattr)\n{\n\tu32 i, nfuncs, urec_size, min_size, prev_offset;\n\tu32 krec_size = sizeof(struct bpf_func_info);\n\tstruct bpf_func_info *krecord;\n\tconst struct btf_type *type;\n\tstruct bpf_prog *prog;\n\tconst struct btf *btf;\n\tvoid __user *urecord;\n\tint ret = 0;\n\n\tnfuncs = attr->func_info_cnt;\n\tif (!nfuncs)\n\t\treturn 0;\n\n\tif (nfuncs != env->subprog_cnt) {\n\t\tverbose(env, \"number of funcs in func_info doesn't match number of subprogs\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\turec_size = attr->func_info_rec_size;\n\tif (urec_size < MIN_BPF_FUNCINFO_SIZE ||\n\t    urec_size > MAX_FUNCINFO_REC_SIZE ||\n\t    urec_size % sizeof(u32)) {\n\t\tverbose(env, \"invalid func info rec size %u\\n\", urec_size);\n\t\treturn -EINVAL;\n\t}\n\n\tprog = env->prog;\n\tbtf = prog->aux->btf;\n\n\turecord = u64_to_user_ptr(attr->func_info);\n\tmin_size = min_t(u32, krec_size, urec_size);\n\n\tkrecord = kvcalloc(nfuncs, krec_size, GFP_KERNEL | __GFP_NOWARN);\n\tif (!krecord)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < nfuncs; i++) {\n\t\tret = bpf_check_uarg_tail_zero(urecord, krec_size, urec_size);\n\t\tif (ret) {\n\t\t\tif (ret == -E2BIG) {\n\t\t\t\tverbose(env, \"nonzero tailing record in func info\");\n\t\t\t\t/* set the size kernel expects so loader can zero\n\t\t\t\t * out the rest of the record.\n\t\t\t\t */\n\t\t\t\tif (put_user(min_size, &uattr->func_info_rec_size))\n\t\t\t\t\tret = -EFAULT;\n\t\t\t}\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (copy_from_user(&krecord[i], urecord, min_size)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/* check insn_off */\n\t\tif (i == 0) {\n\t\t\tif (krecord[i].insn_off) {\n\t\t\t\tverbose(env,\n\t\t\t\t\t\"nonzero insn_off %u for the first func info record\",\n\t\t\t\t\tkrecord[i].insn_off);\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t} else if (krecord[i].insn_off <= prev_offset) {\n\t\t\tverbose(env,\n\t\t\t\t\"same or smaller insn offset (%u) than previous func info record (%u)\",\n\t\t\t\tkrecord[i].insn_off, prev_offset);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (env->subprog_info[i].start != krecord[i].insn_off) {\n\t\t\tverbose(env, \"func_info BTF section doesn't match subprog layout in BPF program\\n\");\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/* check type_id */\n\t\ttype = btf_type_by_id(btf, krecord[i].type_id);\n\t\tif (!type || BTF_INFO_KIND(type->info) != BTF_KIND_FUNC) {\n\t\t\tverbose(env, \"invalid type id %d in func info\",\n\t\t\t\tkrecord[i].type_id);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tprev_offset = krecord[i].insn_off;\n\t\turecord += urec_size;\n\t}\n\n\tprog->aux->func_info = krecord;\n\tprog->aux->func_info_cnt = nfuncs;\n\treturn 0;\n\nerr_free:\n\tkvfree(krecord);\n\treturn ret;\n}\n\nstatic void adjust_btf_func(struct bpf_verifier_env *env)\n{\n\tint i;\n\n\tif (!env->prog->aux->func_info)\n\t\treturn;\n\n\tfor (i = 0; i < env->subprog_cnt; i++)\n\t\tenv->prog->aux->func_info[i].insn_off = env->subprog_info[i].start;\n}\n\n#define MIN_BPF_LINEINFO_SIZE\t(offsetof(struct bpf_line_info, line_col) + \\\n\t\tsizeof(((struct bpf_line_info *)(0))->line_col))\n#define MAX_LINEINFO_REC_SIZE\tMAX_FUNCINFO_REC_SIZE\n\nstatic int check_btf_line(struct bpf_verifier_env *env,\n\t\t\t  const union bpf_attr *attr,\n\t\t\t  union bpf_attr __user *uattr)\n{\n\tu32 i, s, nr_linfo, ncopy, expected_size, rec_size, prev_offset = 0;\n\tstruct bpf_subprog_info *sub;\n\tstruct bpf_line_info *linfo;\n\tstruct bpf_prog *prog;\n\tconst struct btf *btf;\n\tvoid __user *ulinfo;\n\tint err;\n\n\tnr_linfo = attr->line_info_cnt;\n\tif (!nr_linfo)\n\t\treturn 0;\n\n\trec_size = attr->line_info_rec_size;\n\tif (rec_size < MIN_BPF_LINEINFO_SIZE ||\n\t    rec_size > MAX_LINEINFO_REC_SIZE ||\n\t    rec_size & (sizeof(u32) - 1))\n\t\treturn -EINVAL;\n\n\t/* Need to zero it in case the userspace may\n\t * pass in a smaller bpf_line_info object.\n\t */\n\tlinfo = kvcalloc(nr_linfo, sizeof(struct bpf_line_info),\n\t\t\t GFP_KERNEL | __GFP_NOWARN);\n\tif (!linfo)\n\t\treturn -ENOMEM;\n\n\tprog = env->prog;\n\tbtf = prog->aux->btf;\n\n\ts = 0;\n\tsub = env->subprog_info;\n\tulinfo = u64_to_user_ptr(attr->line_info);\n\texpected_size = sizeof(struct bpf_line_info);\n\tncopy = min_t(u32, expected_size, rec_size);\n\tfor (i = 0; i < nr_linfo; i++) {\n\t\terr = bpf_check_uarg_tail_zero(ulinfo, expected_size, rec_size);\n\t\tif (err) {\n\t\t\tif (err == -E2BIG) {\n\t\t\t\tverbose(env, \"nonzero tailing record in line_info\");\n\t\t\t\tif (put_user(expected_size,\n\t\t\t\t\t     &uattr->line_info_rec_size))\n\t\t\t\t\terr = -EFAULT;\n\t\t\t}\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (copy_from_user(&linfo[i], ulinfo, ncopy)) {\n\t\t\terr = -EFAULT;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/*\n\t\t * Check insn_off to ensure\n\t\t * 1) strictly increasing AND\n\t\t * 2) bounded by prog->len\n\t\t *\n\t\t * The linfo[0].insn_off == 0 check logically falls into\n\t\t * the later \"missing bpf_line_info for func...\" case\n\t\t * because the first linfo[0].insn_off must be the\n\t\t * first sub also and the first sub must have\n\t\t * subprog_info[0].start == 0.\n\t\t */\n\t\tif ((i && linfo[i].insn_off <= prev_offset) ||\n\t\t    linfo[i].insn_off >= prog->len) {\n\t\t\tverbose(env, \"Invalid line_info[%u].insn_off:%u (prev_offset:%u prog->len:%u)\\n\",\n\t\t\t\ti, linfo[i].insn_off, prev_offset,\n\t\t\t\tprog->len);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (!prog->insnsi[linfo[i].insn_off].code) {\n\t\t\tverbose(env,\n\t\t\t\t\"Invalid insn code at line_info[%u].insn_off\\n\",\n\t\t\t\ti);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (!btf_name_by_offset(btf, linfo[i].line_off) ||\n\t\t    !btf_name_by_offset(btf, linfo[i].file_name_off)) {\n\t\t\tverbose(env, \"Invalid line_info[%u].line_off or .file_name_off\\n\", i);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (s != env->subprog_cnt) {\n\t\t\tif (linfo[i].insn_off == sub[s].start) {\n\t\t\t\tsub[s].linfo_idx = i;\n\t\t\t\ts++;\n\t\t\t} else if (sub[s].start < linfo[i].insn_off) {\n\t\t\t\tverbose(env, \"missing bpf_line_info for func#%u\\n\", s);\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t}\n\n\t\tprev_offset = linfo[i].insn_off;\n\t\tulinfo += rec_size;\n\t}\n\n\tif (s != env->subprog_cnt) {\n\t\tverbose(env, \"missing bpf_line_info for %u funcs starting from func#%u\\n\",\n\t\t\tenv->subprog_cnt - s, s);\n\t\terr = -EINVAL;\n\t\tgoto err_free;\n\t}\n\n\tprog->aux->linfo = linfo;\n\tprog->aux->nr_linfo = nr_linfo;\n\n\treturn 0;\n\nerr_free:\n\tkvfree(linfo);\n\treturn err;\n}\n\nstatic int check_btf_info(struct bpf_verifier_env *env,\n\t\t\t  const union bpf_attr *attr,\n\t\t\t  union bpf_attr __user *uattr)\n{\n\tstruct btf *btf;\n\tint err;\n\n\tif (!attr->func_info_cnt && !attr->line_info_cnt)\n\t\treturn 0;\n\n\tbtf = btf_get_by_fd(attr->prog_btf_fd);\n\tif (IS_ERR(btf))\n\t\treturn PTR_ERR(btf);\n\tenv->prog->aux->btf = btf;\n\n\terr = check_btf_func(env, attr, uattr);\n\tif (err)\n\t\treturn err;\n\n\terr = check_btf_line(env, attr, uattr);\n\tif (err)\n\t\treturn err;\n\n\treturn 0;\n}\n\n/* check %cur's range satisfies %old's */\nstatic bool range_within(struct bpf_reg_state *old,\n\t\t\t struct bpf_reg_state *cur)\n{\n\treturn old->umin_value <= cur->umin_value &&\n\t       old->umax_value >= cur->umax_value &&\n\t       old->smin_value <= cur->smin_value &&\n\t       old->smax_value >= cur->smax_value;\n}\n\n/* Maximum number of register states that can exist at once */\n#define ID_MAP_SIZE\t(MAX_BPF_REG + MAX_BPF_STACK / BPF_REG_SIZE)\nstruct idpair {\n\tu32 old;\n\tu32 cur;\n};\n\n/* If in the old state two registers had the same id, then they need to have\n * the same id in the new state as well.  But that id could be different from\n * the old state, so we need to track the mapping from old to new ids.\n * Once we have seen that, say, a reg with old id 5 had new id 9, any subsequent\n * regs with old id 5 must also have new id 9 for the new state to be safe.  But\n * regs with a different old id could still have new id 9, we don't care about\n * that.\n * So we look through our idmap to see if this old id has been seen before.  If\n * so, we require the new id to match; otherwise, we add the id pair to the map.\n */\nstatic bool check_ids(u32 old_id, u32 cur_id, struct idpair *idmap)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < ID_MAP_SIZE; i++) {\n\t\tif (!idmap[i].old) {\n\t\t\t/* Reached an empty slot; haven't seen this id before */\n\t\t\tidmap[i].old = old_id;\n\t\t\tidmap[i].cur = cur_id;\n\t\t\treturn true;\n\t\t}\n\t\tif (idmap[i].old == old_id)\n\t\t\treturn idmap[i].cur == cur_id;\n\t}\n\t/* We ran out of idmap slots, which should be impossible */\n\tWARN_ON_ONCE(1);\n\treturn false;\n}\n\nstatic void clean_func_state(struct bpf_verifier_env *env,\n\t\t\t     struct bpf_func_state *st)\n{\n\tenum bpf_reg_liveness live;\n\tint i, j;\n\n\tfor (i = 0; i < BPF_REG_FP; i++) {\n\t\tlive = st->regs[i].live;\n\t\t/* liveness must not touch this register anymore */\n\t\tst->regs[i].live |= REG_LIVE_DONE;\n\t\tif (!(live & REG_LIVE_READ))\n\t\t\t/* since the register is unused, clear its state\n\t\t\t * to make further comparison simpler\n\t\t\t */\n\t\t\t__mark_reg_not_init(&st->regs[i]);\n\t}\n\n\tfor (i = 0; i < st->allocated_stack / BPF_REG_SIZE; i++) {\n\t\tlive = st->stack[i].spilled_ptr.live;\n\t\t/* liveness must not touch this stack slot anymore */\n\t\tst->stack[i].spilled_ptr.live |= REG_LIVE_DONE;\n\t\tif (!(live & REG_LIVE_READ)) {\n\t\t\t__mark_reg_not_init(&st->stack[i].spilled_ptr);\n\t\t\tfor (j = 0; j < BPF_REG_SIZE; j++)\n\t\t\t\tst->stack[i].slot_type[j] = STACK_INVALID;\n\t\t}\n\t}\n}\n\nstatic void clean_verifier_state(struct bpf_verifier_env *env,\n\t\t\t\t struct bpf_verifier_state *st)\n{\n\tint i;\n\n\tif (st->frame[0]->regs[0].live & REG_LIVE_DONE)\n\t\t/* all regs in this state in all frames were already marked */\n\t\treturn;\n\n\tfor (i = 0; i <= st->curframe; i++)\n\t\tclean_func_state(env, st->frame[i]);\n}\n\n/* the parentage chains form a tree.\n * the verifier states are added to state lists at given insn and\n * pushed into state stack for future exploration.\n * when the verifier reaches bpf_exit insn some of the verifer states\n * stored in the state lists have their final liveness state already,\n * but a lot of states will get revised from liveness point of view when\n * the verifier explores other branches.\n * Example:\n * 1: r0 = 1\n * 2: if r1 == 100 goto pc+1\n * 3: r0 = 2\n * 4: exit\n * when the verifier reaches exit insn the register r0 in the state list of\n * insn 2 will be seen as !REG_LIVE_READ. Then the verifier pops the other_branch\n * of insn 2 and goes exploring further. At the insn 4 it will walk the\n * parentage chain from insn 4 into insn 2 and will mark r0 as REG_LIVE_READ.\n *\n * Since the verifier pushes the branch states as it sees them while exploring\n * the program the condition of walking the branch instruction for the second\n * time means that all states below this branch were already explored and\n * their final liveness markes are already propagated.\n * Hence when the verifier completes the search of state list in is_state_visited()\n * we can call this clean_live_states() function to mark all liveness states\n * as REG_LIVE_DONE to indicate that 'parent' pointers of 'struct bpf_reg_state'\n * will not be used.\n * This function also clears the registers and stack for states that !READ\n * to simplify state merging.\n *\n * Important note here that walking the same branch instruction in the callee\n * doesn't meant that the states are DONE. The verifier has to compare\n * the callsites\n */\nstatic void clean_live_states(struct bpf_verifier_env *env, int insn,\n\t\t\t      struct bpf_verifier_state *cur)\n{\n\tstruct bpf_verifier_state_list *sl;\n\tint i;\n\n\tsl = env->explored_states[insn];\n\tif (!sl)\n\t\treturn;\n\n\twhile (sl != STATE_LIST_MARK) {\n\t\tif (sl->state.curframe != cur->curframe)\n\t\t\tgoto next;\n\t\tfor (i = 0; i <= cur->curframe; i++)\n\t\t\tif (sl->state.frame[i]->callsite != cur->frame[i]->callsite)\n\t\t\t\tgoto next;\n\t\tclean_verifier_state(env, &sl->state);\nnext:\n\t\tsl = sl->next;\n\t}\n}\n\n/* Returns true if (rold safe implies rcur safe) */\nstatic bool regsafe(struct bpf_reg_state *rold, struct bpf_reg_state *rcur,\n\t\t    struct idpair *idmap)\n{\n\tbool equal;\n\n\tif (!(rold->live & REG_LIVE_READ))\n\t\t/* explored state didn't use this */\n\t\treturn true;\n\n\tequal = memcmp(rold, rcur, offsetof(struct bpf_reg_state, parent)) == 0;\n\n\tif (rold->type == PTR_TO_STACK)\n\t\t/* two stack pointers are equal only if they're pointing to\n\t\t * the same stack frame, since fp-8 in foo != fp-8 in bar\n\t\t */\n\t\treturn equal && rold->frameno == rcur->frameno;\n\n\tif (equal)\n\t\treturn true;\n\n\tif (rold->type == NOT_INIT)\n\t\t/* explored state can't have used this */\n\t\treturn true;\n\tif (rcur->type == NOT_INIT)\n\t\treturn false;\n\tswitch (rold->type) {\n\tcase SCALAR_VALUE:\n\t\tif (rcur->type == SCALAR_VALUE) {\n\t\t\t/* new val must satisfy old val knowledge */\n\t\t\treturn range_within(rold, rcur) &&\n\t\t\t       tnum_in(rold->var_off, rcur->var_off);\n\t\t} else {\n\t\t\t/* We're trying to use a pointer in place of a scalar.\n\t\t\t * Even if the scalar was unbounded, this could lead to\n\t\t\t * pointer leaks because scalars are allowed to leak\n\t\t\t * while pointers are not. We could make this safe in\n\t\t\t * special cases if root is calling us, but it's\n\t\t\t * probably not worth the hassle.\n\t\t\t */\n\t\t\treturn false;\n\t\t}\n\tcase PTR_TO_MAP_VALUE:\n\t\t/* If the new min/max/var_off satisfy the old ones and\n\t\t * everything else matches, we are OK.\n\t\t * We don't care about the 'id' value, because nothing\n\t\t * uses it for PTR_TO_MAP_VALUE (only for ..._OR_NULL)\n\t\t */\n\t\treturn memcmp(rold, rcur, offsetof(struct bpf_reg_state, id)) == 0 &&\n\t\t       range_within(rold, rcur) &&\n\t\t       tnum_in(rold->var_off, rcur->var_off);\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\t\t/* a PTR_TO_MAP_VALUE could be safe to use as a\n\t\t * PTR_TO_MAP_VALUE_OR_NULL into the same map.\n\t\t * However, if the old PTR_TO_MAP_VALUE_OR_NULL then got NULL-\n\t\t * checked, doing so could have affected others with the same\n\t\t * id, and we can't check for that because we lost the id when\n\t\t * we converted to a PTR_TO_MAP_VALUE.\n\t\t */\n\t\tif (rcur->type != PTR_TO_MAP_VALUE_OR_NULL)\n\t\t\treturn false;\n\t\tif (memcmp(rold, rcur, offsetof(struct bpf_reg_state, id)))\n\t\t\treturn false;\n\t\t/* Check our ids match any regs they're supposed to */\n\t\treturn check_ids(rold->id, rcur->id, idmap);\n\tcase PTR_TO_PACKET_META:\n\tcase PTR_TO_PACKET:\n\t\tif (rcur->type != rold->type)\n\t\t\treturn false;\n\t\t/* We must have at least as much range as the old ptr\n\t\t * did, so that any accesses which were safe before are\n\t\t * still safe.  This is true even if old range < old off,\n\t\t * since someone could have accessed through (ptr - k), or\n\t\t * even done ptr -= k in a register, to get a safe access.\n\t\t */\n\t\tif (rold->range > rcur->range)\n\t\t\treturn false;\n\t\t/* If the offsets don't match, we can't trust our alignment;\n\t\t * nor can we be sure that we won't fall out of range.\n\t\t */\n\t\tif (rold->off != rcur->off)\n\t\t\treturn false;\n\t\t/* id relations must be preserved */\n\t\tif (rold->id && !check_ids(rold->id, rcur->id, idmap))\n\t\t\treturn false;\n\t\t/* new val must satisfy old val knowledge */\n\t\treturn range_within(rold, rcur) &&\n\t\t       tnum_in(rold->var_off, rcur->var_off);\n\tcase PTR_TO_CTX:\n\tcase CONST_PTR_TO_MAP:\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_FLOW_KEYS:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\t\t/* Only valid matches are exact, which memcmp() above\n\t\t * would have accepted\n\t\t */\n\tdefault:\n\t\t/* Don't know what's going on, just say it's not safe */\n\t\treturn false;\n\t}\n\n\t/* Shouldn't get here; if we do, say it's not safe */\n\tWARN_ON_ONCE(1);\n\treturn false;\n}\n\nstatic bool stacksafe(struct bpf_func_state *old,\n\t\t      struct bpf_func_state *cur,\n\t\t      struct idpair *idmap)\n{\n\tint i, spi;\n\n\t/* walk slots of the explored stack and ignore any additional\n\t * slots in the current stack, since explored(safe) state\n\t * didn't use them\n\t */\n\tfor (i = 0; i < old->allocated_stack; i++) {\n\t\tspi = i / BPF_REG_SIZE;\n\n\t\tif (!(old->stack[spi].spilled_ptr.live & REG_LIVE_READ)) {\n\t\t\ti += BPF_REG_SIZE - 1;\n\t\t\t/* explored state didn't use this */\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (old->stack[spi].slot_type[i % BPF_REG_SIZE] == STACK_INVALID)\n\t\t\tcontinue;\n\n\t\t/* explored stack has more populated slots than current stack\n\t\t * and these slots were used\n\t\t */\n\t\tif (i >= cur->allocated_stack)\n\t\t\treturn false;\n\n\t\t/* if old state was safe with misc data in the stack\n\t\t * it will be safe with zero-initialized stack.\n\t\t * The opposite is not true\n\t\t */\n\t\tif (old->stack[spi].slot_type[i % BPF_REG_SIZE] == STACK_MISC &&\n\t\t    cur->stack[spi].slot_type[i % BPF_REG_SIZE] == STACK_ZERO)\n\t\t\tcontinue;\n\t\tif (old->stack[spi].slot_type[i % BPF_REG_SIZE] !=\n\t\t    cur->stack[spi].slot_type[i % BPF_REG_SIZE])\n\t\t\t/* Ex: old explored (safe) state has STACK_SPILL in\n\t\t\t * this stack slot, but current has has STACK_MISC ->\n\t\t\t * this verifier states are not equivalent,\n\t\t\t * return false to continue verification of this path\n\t\t\t */\n\t\t\treturn false;\n\t\tif (i % BPF_REG_SIZE)\n\t\t\tcontinue;\n\t\tif (old->stack[spi].slot_type[0] != STACK_SPILL)\n\t\t\tcontinue;\n\t\tif (!regsafe(&old->stack[spi].spilled_ptr,\n\t\t\t     &cur->stack[spi].spilled_ptr,\n\t\t\t     idmap))\n\t\t\t/* when explored and current stack slot are both storing\n\t\t\t * spilled registers, check that stored pointers types\n\t\t\t * are the same as well.\n\t\t\t * Ex: explored safe path could have stored\n\t\t\t * (bpf_reg_state) {.type = PTR_TO_STACK, .off = -8}\n\t\t\t * but current path has stored:\n\t\t\t * (bpf_reg_state) {.type = PTR_TO_STACK, .off = -16}\n\t\t\t * such verifier states are not equivalent.\n\t\t\t * return false to continue verification of this path\n\t\t\t */\n\t\t\treturn false;\n\t}\n\treturn true;\n}\n\nstatic bool refsafe(struct bpf_func_state *old, struct bpf_func_state *cur)\n{\n\tif (old->acquired_refs != cur->acquired_refs)\n\t\treturn false;\n\treturn !memcmp(old->refs, cur->refs,\n\t\t       sizeof(*old->refs) * old->acquired_refs);\n}\n\n/* compare two verifier states\n *\n * all states stored in state_list are known to be valid, since\n * verifier reached 'bpf_exit' instruction through them\n *\n * this function is called when verifier exploring different branches of\n * execution popped from the state stack. If it sees an old state that has\n * more strict register state and more strict stack state then this execution\n * branch doesn't need to be explored further, since verifier already\n * concluded that more strict state leads to valid finish.\n *\n * Therefore two states are equivalent if register state is more conservative\n * and explored stack state is more conservative than the current one.\n * Example:\n *       explored                   current\n * (slot1=INV slot2=MISC) == (slot1=MISC slot2=MISC)\n * (slot1=MISC slot2=MISC) != (slot1=INV slot2=MISC)\n *\n * In other words if current stack state (one being explored) has more\n * valid slots than old one that already passed validation, it means\n * the verifier can stop exploring and conclude that current state is valid too\n *\n * Similarly with registers. If explored state has register type as invalid\n * whereas register type in current state is meaningful, it means that\n * the current state will reach 'bpf_exit' instruction safely\n */\nstatic bool func_states_equal(struct bpf_func_state *old,\n\t\t\t      struct bpf_func_state *cur)\n{\n\tstruct idpair *idmap;\n\tbool ret = false;\n\tint i;\n\n\tidmap = kcalloc(ID_MAP_SIZE, sizeof(struct idpair), GFP_KERNEL);\n\t/* If we failed to allocate the idmap, just say it's not safe */\n\tif (!idmap)\n\t\treturn false;\n\n\tfor (i = 0; i < MAX_BPF_REG; i++) {\n\t\tif (!regsafe(&old->regs[i], &cur->regs[i], idmap))\n\t\t\tgoto out_free;\n\t}\n\n\tif (!stacksafe(old, cur, idmap))\n\t\tgoto out_free;\n\n\tif (!refsafe(old, cur))\n\t\tgoto out_free;\n\tret = true;\nout_free:\n\tkfree(idmap);\n\treturn ret;\n}",
    "diff": "-\tother_branch = push_stack(env, *insn_idx + insn->off + 1, *insn_idx);\n+\tother_branch = push_stack(env, *insn_idx + insn->off + 1, *insn_idx,\n \tif (!other_branch)\n \tother_branch_regs = other_branch->frame[other_branch->curframe]->regs;",
    "critical_vars": [
      "other_branch"
    ],
    "variable_definitions": {
      "other_branch": "struct bpf_verifier_state *other_branch;"
    },
    "variable_types": {
      "other_branch": "struct pointer"
    },
    "type_mapping": {
      "other_branch": "struct pointer"
    },
    "vulnerable_line": "other_branch = push_stack(env, *insn_idx + insn->off + 1, *insn_idx, false);",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Pointer Arithmetic Error",
    "reasoning": "The index calculation for the instruction pointer (insn_idx + insn->off + 1) can overflow, leading to a potentially erroneous memory access when pushing the stack, which may enable side-channel attacks."
  },
  {
    "fix_code": "static bool states_equal(struct bpf_verifier_env *env,\n\t\t\t struct bpf_verifier_state *old,\n\t\t\t struct bpf_verifier_state *cur)\n{\n\tint i;\n\n\tif (old->curframe != cur->curframe)\n\t\treturn false;\n\n\t/* Verification state from speculative execution simulation\n\t * must never prune a non-speculative execution one.\n\t */\n\tif (old->speculative && !cur->speculative)\n\t\treturn false;\n\n\t/* for states to be equal callsites have to be the same\n\t * and all frame states need to be equivalent\n\t */\n\tfor (i = 0; i <= old->curframe; i++) {\n\t\tif (old->frame[i]->callsite != cur->frame[i]->callsite)\n\t\t\treturn false;\n\t\tif (!func_states_equal(old->frame[i], cur->frame[i]))\n\t\t\treturn false;\n\t}\n\treturn true;\n}",
    "diff": "+\tif (old->speculative && !cur->speculative)",
    "critical_vars": [
      "cur->speculative",
      "old->speculative"
    ],
    "variable_definitions": {
      "cur->speculative": "Definition not found",
      "old->speculative": "Definition not found"
    },
    "variable_types": {
      "cur->speculative": "struct pointer_integer",
      "old->speculative": "struct pointer_integer"
    },
    "type_mapping": {
      "cur->speculative": "sp_integer",
      "old->speculative": "sp_integer"
    },
    "vulnerable_line": "if (old->speculative && !cur->speculative)",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Pointer Arithmetic Error",
    "reasoning": "The speculative state verification might impact the ability to identify safe memory access, leading to potential out-of-bounds access due to differences in speculative and non-speculative execution paths."
  },
  {
    "fix_code": "Function not found",
    "diff": "+\tdst_state->speculative = src->speculative;\n+\tif (vstate->speculative)\n+\tstate->speculative = false;\n+\t\t\t\t\t\tenv->cur_state->speculative ?\n+\t\t\t\t\tenv->cur_state->speculative ?",
    "critical_vars": [
      "state->speculative"
    ],
    "variable_definitions": {
      "state->speculative": "Definition not found"
    },
    "variable_types": {
      "state->speculative": "struct pointer_integer"
    },
    "type_mapping": {
      "state->speculative": "sp_integer"
    },
    "vulnerable_line": "if (aux->alu_state && (aux->alu_state != alu_state || aux->alu_limit != alu_limit))",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Pointer Arithmetic Error",
    "reasoning": "The vulnerable line checks the state of 'alu_limit' and 'alu_state' which can be impacted by overflow or underflow due to unsafe pointer arithmetic, leading to incorrect memory access and potential side-channel attacks."
  },
  {
    "fix_code": "Function not found",
    "diff": "+\tu8 opcode = BPF_OP(insn->code);\n+\tif (env->allow_ptr_leaks || BPF_SRC(insn->code) == BPF_K)\n \tu8 opcode = BPF_OP(insn->code);\n+\t\tif (insn->code == (BPF_ALU64 | BPF_ADD | BPF_X) ||\n+\t\t    insn->code == (BPF_ALU64 | BPF_SUB | BPF_X)) {\n+\t\t\t\tinsn->code = insn->code == code_add ?\n \t\tif (insn->code != (BPF_JMP | BPF_CALL))",
    "critical_vars": [
      "insn->code"
    ],
    "variable_definitions": {
      "insn->code": "Definition not found"
    },
    "variable_types": {
      "insn->code": "struct pointer_integer"
    },
    "type_mapping": {
      "insn->code": "sp_integer"
    },
    "vulnerable_line": "if (aux->alu_state && (aux->alu_state != alu_state || aux->alu_limit != alu_limit))",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Pointer Arithmetic Error",
    "reasoning": "The pointers' limits are derived from potentially invalid arithmetic operations, leading to out-of-bounds access during speculative execution if not properly sanitized."
  },
  {
    "fix_code": "static int sanitize_ptr_alu(struct bpf_verifier_env *env,\n\t\t\t    struct bpf_insn *insn,\n\t\t\t    const struct bpf_reg_state *ptr_reg,\n\t\t\t    struct bpf_reg_state *dst_reg,\n\t\t\t    bool off_is_neg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_insn_aux_data *aux = cur_aux(env);\n\tbool ptr_is_dst_reg = ptr_reg == dst_reg;\n\tu8 opcode = BPF_OP(insn->code);\n\tu32 alu_state, alu_limit;\n\tstruct bpf_reg_state tmp;\n\tbool ret;\n\n\tif (can_skip_alu_sanitation(env, insn))\n\t\treturn 0;\n\n\t/* We already marked aux for masking from non-speculative\n\t * paths, thus we got here in the first place. We only care\n\t * to explore bad access from here.\n\t */\n\tif (vstate->speculative)\n\t\tgoto do_sim;\n\n\talu_state  = off_is_neg ? BPF_ALU_NEG_VALUE : 0;\n\talu_state |= ptr_is_dst_reg ?\n\t\t     BPF_ALU_SANITIZE_SRC : BPF_ALU_SANITIZE_DST;\n\n\tif (retrieve_ptr_limit(ptr_reg, &alu_limit, opcode, off_is_neg))\n\t\treturn 0;\n\tif (update_alu_sanitation_state(aux, alu_state, alu_limit))\n\t\treturn -EACCES;\ndo_sim:\n\t/* Simulate and find potential out-of-bounds access under\n\t * speculative execution from truncation as a result of\n\t * masking when off was not within expected range. If off\n\t * sits in dst, then we temporarily need to move ptr there\n\t * to simulate dst (== 0) +/-= ptr. Needed, for example,\n\t * for cases where we use K-based arithmetic in one direction\n\t * and truncated reg-based in the other in order to explore\n\t * bad access.\n\t */\n\tif (!ptr_is_dst_reg) {\n\t\ttmp = *dst_reg;\n\t\t*dst_reg = *ptr_reg;\n\t}\n\tret = push_stack(env, env->insn_idx + 1, env->insn_idx, true);\n\tif (!ptr_is_dst_reg)\n\t\t*dst_reg = tmp;\n\treturn !ret ? -EFAULT : 0;\n}",
    "diff": "+\treturn env->allow_ptr_leaks || BPF_SRC(insn->code) == BPF_K;\n-\tif (env->allow_ptr_leaks || BPF_SRC(insn->code) == BPF_K)\n \tu64 insn_bitness = (BPF_CLASS(insn->code) == BPF_ALU64) ? 64 : 32;",
    "critical_vars": [
      "env->allow_ptr_leaks",
      "insn->code"
    ],
    "variable_definitions": {
      "env->allow_ptr_leaks": "Definition not found",
      "insn->code": "Definition not found"
    },
    "variable_types": {
      "env->allow_ptr_leaks": "struct pointer_integer",
      "insn->code": "struct pointer_char"
    },
    "type_mapping": {
      "env->allow_ptr_leaks": "sp_integer",
      "insn->code": "struct pointer_char"
    },
    "vulnerable_line": "if (retrieve_ptr_limit(ptr_reg, &alu_limit, opcode, off_is_neg))",
    "critical_variable_in_vulnerable_line": false,
    "vulnerability_type": "Pointer Arithmetic Error",
    "reasoning": "The function can lead to out-of-bounds access based on pointer arithmetic when state or limits diverge across different branches, potentially leading to side-channel attacks."
  },
  {
    "fix_code": "static int sanitize_ptr_alu(struct bpf_verifier_env *env,\n\t\t\t    struct bpf_insn *insn,\n\t\t\t    const struct bpf_reg_state *ptr_reg,\n\t\t\t    struct bpf_reg_state *dst_reg,\n\t\t\t    bool off_is_neg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_insn_aux_data *aux = cur_aux(env);\n\tbool ptr_is_dst_reg = ptr_reg == dst_reg;\n\tu8 opcode = BPF_OP(insn->code);\n\tu32 alu_state, alu_limit;\n\tstruct bpf_reg_state tmp;\n\tbool ret;\n\n\tif (can_skip_alu_sanitation(env, insn))\n\t\treturn 0;\n\n\t/* We already marked aux for masking from non-speculative\n\t * paths, thus we got here in the first place. We only care\n\t * to explore bad access from here.\n\t */\n\tif (vstate->speculative)\n\t\tgoto do_sim;\n\n\talu_state  = off_is_neg ? BPF_ALU_NEG_VALUE : 0;\n\talu_state |= ptr_is_dst_reg ?\n\t\t     BPF_ALU_SANITIZE_SRC : BPF_ALU_SANITIZE_DST;\n\n\tif (retrieve_ptr_limit(ptr_reg, &alu_limit, opcode, off_is_neg))\n\t\treturn 0;\n\tif (update_alu_sanitation_state(aux, alu_state, alu_limit))\n\t\treturn -EACCES;\ndo_sim:\n\t/* Simulate and find potential out-of-bounds access under\n\t * speculative execution from truncation as a result of\n\t * masking when off was not within expected range. If off\n\t * sits in dst, then we temporarily need to move ptr there\n\t * to simulate dst (== 0) +/-= ptr. Needed, for example,\n\t * for cases where we use K-based arithmetic in one direction\n\t * and truncated reg-based in the other in order to explore\n\t * bad access.\n\t */\n\tif (!ptr_is_dst_reg) {\n\t\ttmp = *dst_reg;\n\t\t*dst_reg = *ptr_reg;\n\t}\n\tret = push_stack(env, env->insn_idx + 1, env->insn_idx, true);\n\tif (!ptr_is_dst_reg)\n\t\t*dst_reg = tmp;\n\treturn !ret ? -EFAULT : 0;\n}",
    "diff": "+static bool can_skip_alu_sanitation(const struct bpf_verifier_env *env,\n+\t\t\t\t    const struct bpf_insn *insn)\n+\treturn env->allow_ptr_leaks || BPF_SRC(insn->code) == BPF_K;\n+static int update_alu_sanitation_state(struct bpf_insn_aux_data *aux,\n+static int sanitize_val_alu(struct bpf_verifier_env *env,\n+\t\t\t    struct bpf_insn *insn)\n+\tstruct bpf_insn_aux_data *aux = cur_aux(env);\n+\tif (can_skip_alu_sanitation(env, insn))\n static int sanitize_ptr_alu(struct bpf_verifier_env *env,\n \t\t\t    struct bpf_insn *insn,\n@@ -3117,7 +3151,7 @@ static int sanitize_ptr_alu(struct bpf_verifier_env *env,\n-\tif (env->allow_ptr_leaks || BPF_SRC(insn->code) == BPF_K)\n+\tif (can_skip_alu_sanitation(env, insn))\n@@ -3133,19 +3167,8 @@ static int sanitize_ptr_alu(struct bpf_verifier_env *env,\n@@ -3418,6 +3441,8 @@ static int adjust_scalar_min_max_vals(struct bpf_verifier_env *env,\n \tu64 insn_bitness = (BPF_CLASS(insn->code) == BPF_ALU64) ? 64 : 32;\n+\tu32 dst = insn->dst_reg;\n \tif (insn_bitness == 32) {\n@@ -3452,6 +3477,11 @@ static int adjust_scalar_min_max_vals(struct bpf_verifier_env *env,\n+\t\tret = sanitize_val_alu(env, insn);\n+\t\t\tverbose(env, \"R%d tried to add from different pointers or scalars\\n\", dst);\n@@ -3471,6 +3501,11 @@ static int adjust_scalar_min_max_vals(struct bpf_verifier_env *env,\n+\t\tret = sanitize_val_alu(env, insn);\n+\t\t\tverbose(env, \"R%d tried to sub from different pointers or scalars\\n\", dst);",
    "critical_vars": [
      "env",
      "insn"
    ],
    "variable_definitions": {
      "env": "struct bpf_verifier_env *env,",
      "insn": "struct bpf_insn *insn,"
    },
    "variable_types": {
      "env": "struct pointer",
      "insn": "struct pointer"
    },
    "type_mapping": {
      "env": "struct pointer",
      "insn": "struct pointer"
    },
    "vulnerable_line": "if (update_alu_sanitation_state(aux, alu_state, alu_limit))",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Function Operation Out of Bounds",
    "reasoning": "Calling update_alu_sanitation_state without sufficient checks allows for state or limit mismatches, which can lead to out-of-bounds access under speculative execution."
  },
  {
    "fix_code": "static int adjust_scalar_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t      struct bpf_insn *insn,\n\t\t\t\t      struct bpf_reg_state *dst_reg,\n\t\t\t\t      struct bpf_reg_state src_reg)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 opcode = BPF_OP(insn->code);\n\tbool src_known, dst_known;\n\ts64 smin_val, smax_val;\n\tu64 umin_val, umax_val;\n\tu64 insn_bitness = (BPF_CLASS(insn->code) == BPF_ALU64) ? 64 : 32;\n\tu32 dst = insn->dst_reg;\n\tint ret;\n\n\tif (insn_bitness == 32) {\n\t\t/* Relevant for 32-bit RSH: Information can propagate towards\n\t\t * LSB, so it isn't sufficient to only truncate the output to\n\t\t * 32 bits.\n\t\t */\n\t\tcoerce_reg_to_size(dst_reg, 4);\n\t\tcoerce_reg_to_size(&src_reg, 4);\n\t}\n\n\tsmin_val = src_reg.smin_value;\n\tsmax_val = src_reg.smax_value;\n\tumin_val = src_reg.umin_value;\n\tumax_val = src_reg.umax_value;\n\tsrc_known = tnum_is_const(src_reg.var_off);\n\tdst_known = tnum_is_const(dst_reg->var_off);\n\n\tif ((src_known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (!src_known &&\n\t    opcode != BPF_ADD && opcode != BPF_SUB && opcode != BPF_AND) {\n\t\t__mark_reg_unknown(dst_reg);\n\t\treturn 0;\n\t}\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\tret = sanitize_val_alu(env, insn);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to add from different pointers or scalars\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\tif (signed_add_overflows(dst_reg->smin_value, smin_val) ||\n\t\t    signed_add_overflows(dst_reg->smax_value, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value += smin_val;\n\t\t\tdst_reg->smax_value += smax_val;\n\t\t}\n\t\tif (dst_reg->umin_value + umin_val < umin_val ||\n\t\t    dst_reg->umax_value + umax_val < umax_val) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value += umin_val;\n\t\t\tdst_reg->umax_value += umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(dst_reg->var_off, src_reg.var_off);\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tret = sanitize_val_alu(env, insn);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to sub from different pointers or scalars\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\tif (signed_sub_overflows(dst_reg->smin_value, smax_val) ||\n\t\t    signed_sub_overflows(dst_reg->smax_value, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value -= smax_val;\n\t\t\tdst_reg->smax_value -= smin_val;\n\t\t}\n\t\tif (dst_reg->umin_value < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value -= umax_val;\n\t\t\tdst_reg->umax_value -= umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(dst_reg->var_off, src_reg.var_off);\n\t\tbreak;\n\tcase BPF_MUL:\n\t\tdst_reg->var_off = tnum_mul(dst_reg->var_off, src_reg.var_off);\n\t\tif (smin_val < 0 || dst_reg->smin_value < 0) {\n\t\t\t/* Ain't nobody got time to multiply that sign */\n\t\t\t__mark_reg_unbounded(dst_reg);\n\t\t\t__update_reg_bounds(dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\t/* Both values are positive, so we can work with unsigned and\n\t\t * copy the result to signed (unless it exceeds S64_MAX).\n\t\t */\n\t\tif (umax_val > U32_MAX || dst_reg->umax_value > U32_MAX) {\n\t\t\t/* Potential overflow, we know nothing */\n\t\t\t__mark_reg_unbounded(dst_reg);\n\t\t\t/* (except what we can learn from the var_off) */\n\t\t\t__update_reg_bounds(dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\tdst_reg->umin_value *= umin_val;\n\t\tdst_reg->umax_value *= umax_val;\n\t\tif (dst_reg->umax_value > S64_MAX) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = dst_reg->umin_value;\n\t\t\tdst_reg->smax_value = dst_reg->umax_value;\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\t\tif (src_known && dst_known) {\n\t\t\t__mark_reg_known(dst_reg, dst_reg->var_off.value &\n\t\t\t\t\t\t  src_reg.var_off.value);\n\t\t\tbreak;\n\t\t}\n\t\t/* We get our minimum from the var_off, since that's inherently\n\t\t * bitwise.  Our maximum is the minimum of the operands' maxima.\n\t\t */\n\t\tdst_reg->var_off = tnum_and(dst_reg->var_off, src_reg.var_off);\n\t\tdst_reg->umin_value = dst_reg->var_off.value;\n\t\tdst_reg->umax_value = min(dst_reg->umax_value, umax_val);\n\t\tif (dst_reg->smin_value < 0 || smin_val < 0) {\n\t\t\t/* Lose signed bounds when ANDing negative numbers,\n\t\t\t * ain't nobody got time for that.\n\t\t\t */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\t/* ANDing two positives gives a positive, so safe to\n\t\t\t * cast result into s64.\n\t\t\t */\n\t\t\tdst_reg->smin_value = dst_reg->umin_value;\n\t\t\tdst_reg->smax_value = dst_reg->umax_value;\n\t\t}\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tcase BPF_OR:\n\t\tif (src_known && dst_known) {\n\t\t\t__mark_reg_known(dst_reg, dst_reg->var_off.value |\n\t\t\t\t\t\t  src_reg.var_off.value);\n\t\t\tbreak;\n\t\t}\n\t\t/* We get our maximum from the var_off, and our minimum is the\n\t\t * maximum of the operands' minima\n\t\t */\n\t\tdst_reg->var_off = tnum_or(dst_reg->var_off, src_reg.var_off);\n\t\tdst_reg->umin_value = max(dst_reg->umin_value, umin_val);\n\t\tdst_reg->umax_value = dst_reg->var_off.value |\n\t\t\t\t      dst_reg->var_off.mask;\n\t\tif (dst_reg->smin_value < 0 || smin_val < 0) {\n\t\t\t/* Lose signed bounds when ORing negative numbers,\n\t\t\t * ain't nobody got time for that.\n\t\t\t */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\t/* ORing two positives gives a positive, so safe to\n\t\t\t * cast result into s64.\n\t\t\t */\n\t\t\tdst_reg->smin_value = dst_reg->umin_value;\n\t\t\tdst_reg->smax_value = dst_reg->umax_value;\n\t\t}\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tcase BPF_LSH:\n\t\tif (umax_val >= insn_bitness) {\n\t\t\t/* Shifts greater than 31 or 63 are undefined.\n\t\t\t * This includes shifts by a negative number.\n\t\t\t */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\t/* We lose all sign bit information (except what we can pick\n\t\t * up from var_off)\n\t\t */\n\t\tdst_reg->smin_value = S64_MIN;\n\t\tdst_reg->smax_value = S64_MAX;\n\t\t/* If we might shift our top bit out, then we know nothing */\n\t\tif (dst_reg->umax_value > 1ULL << (63 - umax_val)) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value <<= umin_val;\n\t\t\tdst_reg->umax_value <<= umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_lshift(dst_reg->var_off, umin_val);\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tcase BPF_RSH:\n\t\tif (umax_val >= insn_bitness) {\n\t\t\t/* Shifts greater than 31 or 63 are undefined.\n\t\t\t * This includes shifts by a negative number.\n\t\t\t */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\t/* BPF_RSH is an unsigned shift.  If the value in dst_reg might\n\t\t * be negative, then either:\n\t\t * 1) src_reg might be zero, so the sign bit of the result is\n\t\t *    unknown, so we lose our signed bounds\n\t\t * 2) it's known negative, thus the unsigned bounds capture the\n\t\t *    signed bounds\n\t\t * 3) the signed bounds cross zero, so they tell us nothing\n\t\t *    about the result\n\t\t * If the value in dst_reg is known nonnegative, then again the\n\t\t * unsigned bounts capture the signed bounds.\n\t\t * Thus, in all cases it suffices to blow away our signed bounds\n\t\t * and rely on inferring new ones from the unsigned bounds and\n\t\t * var_off of the result.\n\t\t */\n\t\tdst_reg->smin_value = S64_MIN;\n\t\tdst_reg->smax_value = S64_MAX;\n\t\tdst_reg->var_off = tnum_rshift(dst_reg->var_off, umin_val);\n\t\tdst_reg->umin_value >>= umax_val;\n\t\tdst_reg->umax_value >>= umin_val;\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tcase BPF_ARSH:\n\t\tif (umax_val >= insn_bitness) {\n\t\t\t/* Shifts greater than 31 or 63 are undefined.\n\t\t\t * This includes shifts by a negative number.\n\t\t\t */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Upon reaching here, src_known is true and\n\t\t * umax_val is equal to umin_val.\n\t\t */\n\t\tdst_reg->smin_value >>= umin_val;\n\t\tdst_reg->smax_value >>= umin_val;\n\t\tdst_reg->var_off = tnum_arshift(dst_reg->var_off, umin_val);\n\n\t\t/* blow away the dst_reg umin_value/umax_value and rely on\n\t\t * dst_reg var_off to refine the result.\n\t\t */\n\t\tdst_reg->umin_value = 0;\n\t\tdst_reg->umax_value = U64_MAX;\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tdefault:\n\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\tbreak;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops are (32,32)->32 */\n\t\tcoerce_reg_to_size(dst_reg, 4);\n\t}\n\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\treturn 0;\n}\n\n/* Handles ALU ops other than BPF_END, BPF_NEG and BPF_MOV: computes new min/max\n * and var_off.\n */\nstatic int adjust_reg_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg, *src_reg;\n\tstruct bpf_reg_state *ptr_reg = NULL, off_reg = {0};\n\tu8 opcode = BPF_OP(insn->code);\n\n\tdst_reg = &regs[insn->dst_reg];\n\tsrc_reg = NULL;\n\tif (dst_reg->type != SCALAR_VALUE)\n\t\tptr_reg = dst_reg;\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tsrc_reg = &regs[insn->src_reg];\n\t\tif (src_reg->type != SCALAR_VALUE) {\n\t\t\tif (dst_reg->type != SCALAR_VALUE) {\n\t\t\t\t/* Combining two pointers by any ALU op yields\n\t\t\t\t * an arbitrary scalar. Disallow all math except\n\t\t\t\t * pointer subtraction\n\t\t\t\t */\n\t\t\t\tif (opcode == BPF_SUB && env->allow_ptr_leaks) {\n\t\t\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t\tverbose(env, \"R%d pointer %s pointer prohibited\\n\",\n\t\t\t\t\tinsn->dst_reg,\n\t\t\t\t\tbpf_alu_string[opcode >> 4]);\n\t\t\t\treturn -EACCES;\n\t\t\t} else {\n\t\t\t\t/* scalar += pointer\n\t\t\t\t * This is legal, but we have to reverse our\n\t\t\t\t * src/dest handling in computing the range\n\t\t\t\t */\n\t\t\t\treturn adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t\t       src_reg, dst_reg);\n\t\t\t}\n\t\t} else if (ptr_reg) {\n\t\t\t/* pointer += scalar */\n\t\t\treturn adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t       dst_reg, src_reg);\n\t\t}\n\t} else {\n\t\t/* Pretend the src is a reg with a known value, since we only\n\t\t * need to be able to read from this state.\n\t\t */\n\t\toff_reg.type = SCALAR_VALUE;\n\t\t__mark_reg_known(&off_reg, insn->imm);\n\t\tsrc_reg = &off_reg;\n\t\tif (ptr_reg) /* pointer += K */\n\t\t\treturn adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t       ptr_reg, src_reg);\n\t}\n\n\t/* Got here implies adding two SCALAR_VALUEs */\n\tif (WARN_ON_ONCE(ptr_reg)) {\n\t\tprint_verifier_state(env, state);\n\t\tverbose(env, \"verifier internal error: unexpected ptr_reg\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (WARN_ON(!src_reg)) {\n\t\tprint_verifier_state(env, state);\n\t\tverbose(env, \"verifier internal error: no src_reg\\n\");\n\t\treturn -EINVAL;\n\t}\n\treturn adjust_scalar_min_max_vals(env, insn, dst_reg, *src_reg);\n}\n\n/* check validity of 32-bit and 64-bit arithmetic operations */\nstatic int check_alu_op(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 opcode = BPF_OP(insn->code);\n\tint err;\n\n\tif (opcode == BPF_END || opcode == BPF_NEG) {\n\t\tif (opcode == BPF_NEG) {\n\t\t\tif (BPF_SRC(insn->code) != 0 ||\n\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t    insn->off != 0 || insn->imm != 0) {\n\t\t\t\tverbose(env, \"BPF_NEG uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else {\n\t\t\tif (insn->src_reg != BPF_REG_0 || insn->off != 0 ||\n\t\t\t    (insn->imm != 16 && insn->imm != 32 && insn->imm != 64) ||\n\t\t\t    BPF_CLASS(insn->code) == BPF_ALU64) {\n\t\t\t\tverbose(env, \"BPF_END uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check src operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (is_pointer_value(env, insn->dst_reg)) {\n\t\t\tverbose(env, \"R%d pointer arithmetic prohibited\\n\",\n\t\t\t\tinsn->dst_reg);\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\t/* check dest operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t} else if (opcode == BPF_MOV) {\n\n\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\tif (insn->imm != 0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_MOV uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\t/* check src operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t} else {\n\t\t\tif (insn->src_reg != BPF_REG_0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_MOV uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check dest operand, mark as required later */\n\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP_NO_MARK);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\tstruct bpf_reg_state *src_reg = regs + insn->src_reg;\n\t\t\tstruct bpf_reg_state *dst_reg = regs + insn->dst_reg;\n\n\t\t\tif (BPF_CLASS(insn->code) == BPF_ALU64) {\n\t\t\t\t/* case: R1 = R2\n\t\t\t\t * copy register state to dest reg\n\t\t\t\t */\n\t\t\t\t*dst_reg = *src_reg;\n\t\t\t\tdst_reg->live |= REG_LIVE_WRITTEN;\n\t\t\t} else {\n\t\t\t\t/* R1 = (u32) R2 */\n\t\t\t\tif (is_pointer_value(env, insn->src_reg)) {\n\t\t\t\t\tverbose(env,\n\t\t\t\t\t\t\"R%d partial copy of pointer\\n\",\n\t\t\t\t\t\tinsn->src_reg);\n\t\t\t\t\treturn -EACCES;\n\t\t\t\t} else if (src_reg->type == SCALAR_VALUE) {\n\t\t\t\t\t*dst_reg = *src_reg;\n\t\t\t\t\tdst_reg->live |= REG_LIVE_WRITTEN;\n\t\t\t\t} else {\n\t\t\t\t\tmark_reg_unknown(env, regs,\n\t\t\t\t\t\t\t insn->dst_reg);\n\t\t\t\t}\n\t\t\t\tcoerce_reg_to_size(dst_reg, 4);\n\t\t\t}\n\t\t} else {\n\t\t\t/* case: R = imm\n\t\t\t * remember the value we stored into this reg\n\t\t\t */\n\t\t\t/* clear any state __mark_reg_known doesn't set */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tregs[insn->dst_reg].type = SCALAR_VALUE;\n\t\t\tif (BPF_CLASS(insn->code) == BPF_ALU64) {\n\t\t\t\t__mark_reg_known(regs + insn->dst_reg,\n\t\t\t\t\t\t insn->imm);\n\t\t\t} else {\n\t\t\t\t__mark_reg_known(regs + insn->dst_reg,\n\t\t\t\t\t\t (u32)insn->imm);\n\t\t\t}\n\t\t}\n\n\t} else if (opcode > BPF_END) {\n\t\tverbose(env, \"invalid BPF_ALU opcode %x\\n\", opcode);\n\t\treturn -EINVAL;\n\n\t} else {\t/* all other ALU ops: and, sub, xor, add, ... */\n\n\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\tif (insn->imm != 0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_ALU uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\t/* check src1 operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t} else {\n\t\t\tif (insn->src_reg != BPF_REG_0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_ALU uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check src2 operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif ((opcode == BPF_MOD || opcode == BPF_DIV) &&\n\t\t    BPF_SRC(insn->code) == BPF_K && insn->imm == 0) {\n\t\t\tverbose(env, \"div by zero\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif ((opcode == BPF_LSH || opcode == BPF_RSH ||\n\t\t     opcode == BPF_ARSH) && BPF_SRC(insn->code) == BPF_K) {\n\t\t\tint size = BPF_CLASS(insn->code) == BPF_ALU64 ? 64 : 32;\n\n\t\t\tif (insn->imm < 0 || insn->imm >= size) {\n\t\t\t\tverbose(env, \"invalid shift %d\\n\", insn->imm);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check dest operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP_NO_MARK);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\treturn adjust_reg_min_max_vals(env, insn);\n\t}\n\n\treturn 0;\n}\n\nstatic void find_good_pkt_pointers(struct bpf_verifier_state *vstate,\n\t\t\t\t   struct bpf_reg_state *dst_reg,\n\t\t\t\t   enum bpf_reg_type type,\n\t\t\t\t   bool range_right_open)\n{\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *reg;\n\tu16 new_range;\n\tint i, j;\n\n\tif (dst_reg->off < 0 ||\n\t    (dst_reg->off == 0 && range_right_open))\n\t\t/* This doesn't give us any range */\n\t\treturn;\n\n\tif (dst_reg->umax_value > MAX_PACKET_OFF ||\n\t    dst_reg->umax_value + dst_reg->off > MAX_PACKET_OFF)\n\t\t/* Risk of overflow.  For instance, ptr + (1<<63) may be less\n\t\t * than pkt_end, but that's because it's also less than pkt.\n\t\t */\n\t\treturn;\n\n\tnew_range = dst_reg->off;\n\tif (range_right_open)\n\t\tnew_range--;\n\n\t/* Examples for register markings:\n\t *\n\t * pkt_data in dst register:\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (r2 > pkt_end) goto <handle exception>\n\t *   <access okay>\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (r2 < pkt_end) goto <access okay>\n\t *   <handle exception>\n\t *\n\t *   Where:\n\t *     r2 == dst_reg, pkt_end == src_reg\n\t *     r2=pkt(id=n,off=8,r=0)\n\t *     r3=pkt(id=n,off=0,r=0)\n\t *\n\t * pkt_data in src register:\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (pkt_end >= r2) goto <access okay>\n\t *   <handle exception>\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (pkt_end <= r2) goto <handle exception>\n\t *   <access okay>\n\t *\n\t *   Where:\n\t *     pkt_end == dst_reg, r2 == src_reg\n\t *     r2=pkt(id=n,off=8,r=0)\n\t *     r3=pkt(id=n,off=0,r=0)\n\t *\n\t * Find register r3 and mark its range as r3=pkt(id=n,off=0,r=8)\n\t * or r3=pkt(id=n,off=0,r=8-1), so that range of bytes [r3, r3 + 8)\n\t * and [r3, r3 + 8-1) respectively is safe to access depending on\n\t * the check.\n\t */\n\n\t/* If our ids match, then we must have the same max_value.  And we\n\t * don't care about the other reg's fixed offset, since if it's too big\n\t * the range won't allow anything.\n\t * dst_reg->off is known < MAX_PACKET_OFF, therefore it fits in a u16.\n\t */\n\tfor (i = 0; i < MAX_BPF_REG; i++)\n\t\tif (regs[i].type == type && regs[i].id == dst_reg->id)\n\t\t\t/* keep the maximum range already checked */\n\t\t\tregs[i].range = max(regs[i].range, new_range);\n\n\tfor (j = 0; j <= vstate->curframe; j++) {\n\t\tstate = vstate->frame[j];\n\t\tbpf_for_each_spilled_reg(i, state, reg) {\n\t\t\tif (!reg)\n\t\t\t\tcontinue;\n\t\t\tif (reg->type == type && reg->id == dst_reg->id)\n\t\t\t\treg->range = max(reg->range, new_range);\n\t\t}\n\t}\n}\n\n/* compute branch direction of the expression \"if (reg opcode val) goto target;\"\n * and return:\n *  1 - branch will be taken and \"goto target\" will be executed\n *  0 - branch will not be taken and fall-through to next insn\n * -1 - unknown. Example: \"if (reg < 5)\" is unknown when register value range [0,10]\n */\nstatic int is_branch_taken(struct bpf_reg_state *reg, u64 val, u8 opcode)\n{\n\tif (__is_pointer_value(false, reg))\n\t\treturn -1;\n\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\tif (tnum_is_const(reg->var_off))\n\t\t\treturn !!tnum_equals_const(reg->var_off, val);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\tif (tnum_is_const(reg->var_off))\n\t\t\treturn !tnum_equals_const(reg->var_off, val);\n\t\tbreak;\n\tcase BPF_JSET:\n\t\tif ((~reg->var_off.mask & reg->var_off.value) & val)\n\t\t\treturn 1;\n\t\tif (!((reg->var_off.mask | reg->var_off.value) & val))\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JGT:\n\t\tif (reg->umin_value > val)\n\t\t\treturn 1;\n\t\telse if (reg->umax_value <= val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSGT:\n\t\tif (reg->smin_value > (s64)val)\n\t\t\treturn 1;\n\t\telse if (reg->smax_value < (s64)val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JLT:\n\t\tif (reg->umax_value < val)\n\t\t\treturn 1;\n\t\telse if (reg->umin_value >= val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSLT:\n\t\tif (reg->smax_value < (s64)val)\n\t\t\treturn 1;\n\t\telse if (reg->smin_value >= (s64)val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JGE:\n\t\tif (reg->umin_value >= val)\n\t\t\treturn 1;\n\t\telse if (reg->umax_value < val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSGE:\n\t\tif (reg->smin_value >= (s64)val)\n\t\t\treturn 1;\n\t\telse if (reg->smax_value < (s64)val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JLE:\n\t\tif (reg->umax_value <= val)\n\t\t\treturn 1;\n\t\telse if (reg->umin_value > val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSLE:\n\t\tif (reg->smax_value <= (s64)val)\n\t\t\treturn 1;\n\t\telse if (reg->smin_value > (s64)val)\n\t\t\treturn 0;\n\t\tbreak;\n\t}\n\n\treturn -1;\n}\n\n/* Adjusts the register min/max values in the case that the dst_reg is the\n * variable register that we are working on, and src_reg is a constant or we're\n * simply doing a BPF_K check.\n * In JEQ/JNE cases we also adjust the var_off values.\n */\nstatic void reg_set_min_max(struct bpf_reg_state *true_reg,\n\t\t\t    struct bpf_reg_state *false_reg, u64 val,\n\t\t\t    u8 opcode)\n{\n\t/* If the dst_reg is a pointer, we can't learn anything about its\n\t * variable offset from the compare (unless src_reg were a pointer into\n\t * the same object, but we don't bother with that.\n\t * Since false_reg and true_reg have the same type by construction, we\n\t * only need to check one of them for pointerness.\n\t */\n\tif (__is_pointer_value(false, false_reg))\n\t\treturn;\n\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\t/* If this is false then we know nothing Jon Snow, but if it is\n\t\t * true then we know for sure.\n\t\t */\n\t\t__mark_reg_known(true_reg, val);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\t/* If this is true we know nothing Jon Snow, but if it is false\n\t\t * we know the value for sure;\n\t\t */\n\t\t__mark_reg_known(false_reg, val);\n\t\tbreak;\n\tcase BPF_JSET:\n\t\tfalse_reg->var_off = tnum_and(false_reg->var_off,\n\t\t\t\t\t      tnum_const(~val));\n\t\tif (is_power_of_2(val))\n\t\t\ttrue_reg->var_off = tnum_or(true_reg->var_off,\n\t\t\t\t\t\t    tnum_const(val));\n\t\tbreak;\n\tcase BPF_JGT:\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val);\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JSGT:\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val);\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JLT:\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val);\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val - 1);\n\t\tbreak;\n\tcase BPF_JSLT:\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val);\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val - 1);\n\t\tbreak;\n\tcase BPF_JGE:\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val - 1);\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val);\n\t\tbreak;\n\tcase BPF_JSGE:\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val - 1);\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val);\n\t\tbreak;\n\tcase BPF_JLE:\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val + 1);\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val);\n\t\tbreak;\n\tcase BPF_JSLE:\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val + 1);\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t__reg_deduce_bounds(false_reg);\n\t__reg_deduce_bounds(true_reg);\n\t/* We might have learned some bits from the bounds. */\n\t__reg_bound_offset(false_reg);\n\t__reg_bound_offset(true_reg);\n\t/* Intersecting with the old var_off might have improved our bounds\n\t * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),\n\t * then new var_off is (0; 0x7f...fc) which improves our umax.\n\t */\n\t__update_reg_bounds(false_reg);\n\t__update_reg_bounds(true_reg);\n}\n\n/* Same as above, but for the case that dst_reg holds a constant and src_reg is\n * the variable reg.\n */\nstatic void reg_set_min_max_inv(struct bpf_reg_state *true_reg,\n\t\t\t\tstruct bpf_reg_state *false_reg, u64 val,\n\t\t\t\tu8 opcode)\n{\n\tif (__is_pointer_value(false, false_reg))\n\t\treturn;\n\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\t/* If this is false then we know nothing Jon Snow, but if it is\n\t\t * true then we know for sure.\n\t\t */\n\t\t__mark_reg_known(true_reg, val);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\t/* If this is true we know nothing Jon Snow, but if it is false\n\t\t * we know the value for sure;\n\t\t */\n\t\t__mark_reg_known(false_reg, val);\n\t\tbreak;\n\tcase BPF_JSET:\n\t\tfalse_reg->var_off = tnum_and(false_reg->var_off,\n\t\t\t\t\t      tnum_const(~val));\n\t\tif (is_power_of_2(val))\n\t\t\ttrue_reg->var_off = tnum_or(true_reg->var_off,\n\t\t\t\t\t\t    tnum_const(val));\n\t\tbreak;\n\tcase BPF_JGT:\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val - 1);\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val);\n\t\tbreak;\n\tcase BPF_JSGT:\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val - 1);\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val);\n\t\tbreak;\n\tcase BPF_JLT:\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val + 1);\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val);\n\t\tbreak;\n\tcase BPF_JSLT:\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val + 1);\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val);\n\t\tbreak;\n\tcase BPF_JGE:\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val);\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JSGE:\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val);\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JLE:\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val);\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val - 1);\n\t\tbreak;\n\tcase BPF_JSLE:\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val);\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val - 1);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t__reg_deduce_bounds(false_reg);\n\t__reg_deduce_bounds(true_reg);\n\t/* We might have learned some bits from the bounds. */\n\t__reg_bound_offset(false_reg);\n\t__reg_bound_offset(true_reg);\n\t/* Intersecting with the old var_off might have improved our bounds\n\t * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),\n\t * then new var_off is (0; 0x7f...fc) which improves our umax.\n\t */\n\t__update_reg_bounds(false_reg);\n\t__update_reg_bounds(true_reg);\n}\n\n/* Regs are known to be equal, so intersect their min/max/var_off */\nstatic void __reg_combine_min_max(struct bpf_reg_state *src_reg,\n\t\t\t\t  struct bpf_reg_state *dst_reg)\n{\n\tsrc_reg->umin_value = dst_reg->umin_value = max(src_reg->umin_value,\n\t\t\t\t\t\t\tdst_reg->umin_value);\n\tsrc_reg->umax_value = dst_reg->umax_value = min(src_reg->umax_value,\n\t\t\t\t\t\t\tdst_reg->umax_value);\n\tsrc_reg->smin_value = dst_reg->smin_value = max(src_reg->smin_value,\n\t\t\t\t\t\t\tdst_reg->smin_value);\n\tsrc_reg->smax_value = dst_reg->smax_value = min(src_reg->smax_value,\n\t\t\t\t\t\t\tdst_reg->smax_value);\n\tsrc_reg->var_off = dst_reg->var_off = tnum_intersect(src_reg->var_off,\n\t\t\t\t\t\t\t     dst_reg->var_off);\n\t/* We might have learned new bounds from the var_off. */\n\t__update_reg_bounds(src_reg);\n\t__update_reg_bounds(dst_reg);\n\t/* We might have learned something about the sign bit. */\n\t__reg_deduce_bounds(src_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t/* We might have learned some bits from the bounds. */\n\t__reg_bound_offset(src_reg);\n\t__reg_bound_offset(dst_reg);\n\t/* Intersecting with the old var_off might have improved our bounds\n\t * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),\n\t * then new var_off is (0; 0x7f...fc) which improves our umax.\n\t */\n\t__update_reg_bounds(src_reg);\n\t__update_reg_bounds(dst_reg);\n}\n\nstatic void reg_combine_min_max(struct bpf_reg_state *true_src,\n\t\t\t\tstruct bpf_reg_state *true_dst,\n\t\t\t\tstruct bpf_reg_state *false_src,\n\t\t\t\tstruct bpf_reg_state *false_dst,\n\t\t\t\tu8 opcode)\n{\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\t__reg_combine_min_max(true_src, true_dst);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\t__reg_combine_min_max(false_src, false_dst);\n\t\tbreak;\n\t}\n}\n\nstatic void mark_ptr_or_null_reg(struct bpf_func_state *state,\n\t\t\t\t struct bpf_reg_state *reg, u32 id,\n\t\t\t\t bool is_null)\n{\n\tif (reg_type_may_be_null(reg->type) && reg->id == id) {\n\t\t/* Old offset (both fixed and variable parts) should\n\t\t * have been known-zero, because we don't allow pointer\n\t\t * arithmetic on pointers that might be NULL.\n\t\t */\n\t\tif (WARN_ON_ONCE(reg->smin_value || reg->smax_value ||\n\t\t\t\t !tnum_equals_const(reg->var_off, 0) ||\n\t\t\t\t reg->off)) {\n\t\t\t__mark_reg_known_zero(reg);\n\t\t\treg->off = 0;\n\t\t}\n\t\tif (is_null) {\n\t\t\treg->type = SCALAR_VALUE;\n\t\t} else if (reg->type == PTR_TO_MAP_VALUE_OR_NULL) {\n\t\t\tif (reg->map_ptr->inner_map_meta) {\n\t\t\t\treg->type = CONST_PTR_TO_MAP;\n\t\t\t\treg->map_ptr = reg->map_ptr->inner_map_meta;\n\t\t\t} else {\n\t\t\t\treg->type = PTR_TO_MAP_VALUE;\n\t\t\t}\n\t\t} else if (reg->type == PTR_TO_SOCKET_OR_NULL) {\n\t\t\treg->type = PTR_TO_SOCKET;\n\t\t}\n\t\tif (is_null || !reg_is_refcounted(reg)) {\n\t\t\t/* We don't need id from this point onwards anymore,\n\t\t\t * thus we should better reset it, so that state\n\t\t\t * pruning has chances to take effect.\n\t\t\t */\n\t\t\treg->id = 0;\n\t\t}\n\t}\n}\n\n/* The logic is similar to find_good_pkt_pointers(), both could eventually\n * be folded together at some point.\n */\nstatic void mark_ptr_or_null_regs(struct bpf_verifier_state *vstate, u32 regno,\n\t\t\t\t  bool is_null)\n{\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *reg, *regs = state->regs;\n\tu32 id = regs[regno].id;\n\tint i, j;\n\n\tif (reg_is_refcounted_or_null(&regs[regno]) && is_null)\n\t\t__release_reference_state(state, id);\n\n\tfor (i = 0; i < MAX_BPF_REG; i++)\n\t\tmark_ptr_or_null_reg(state, &regs[i], id, is_null);\n\n\tfor (j = 0; j <= vstate->curframe; j++) {\n\t\tstate = vstate->frame[j];\n\t\tbpf_for_each_spilled_reg(i, state, reg) {\n\t\t\tif (!reg)\n\t\t\t\tcontinue;\n\t\t\tmark_ptr_or_null_reg(state, reg, id, is_null);\n\t\t}\n\t}\n}\n\nstatic bool try_match_pkt_pointers(const struct bpf_insn *insn,\n\t\t\t\t   struct bpf_reg_state *dst_reg,\n\t\t\t\t   struct bpf_reg_state *src_reg,\n\t\t\t\t   struct bpf_verifier_state *this_branch,\n\t\t\t\t   struct bpf_verifier_state *other_branch)\n{\n\tif (BPF_SRC(insn->code) != BPF_X)\n\t\treturn false;\n\n\tswitch (BPF_OP(insn->code)) {\n\tcase BPF_JGT:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' > pkt_end, pkt_meta' > pkt_data */\n\t\t\tfind_good_pkt_pointers(this_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, false);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end > pkt_data', pkt_data > pkt_meta' */\n\t\t\tfind_good_pkt_pointers(other_branch, src_reg,\n\t\t\t\t\t       src_reg->type, true);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase BPF_JLT:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' < pkt_end, pkt_meta' < pkt_data */\n\t\t\tfind_good_pkt_pointers(other_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, true);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end < pkt_data', pkt_data > pkt_meta' */\n\t\t\tfind_good_pkt_pointers(this_branch, src_reg,\n\t\t\t\t\t       src_reg->type, false);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase BPF_JGE:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' >= pkt_end, pkt_meta' >= pkt_data */\n\t\t\tfind_good_pkt_pointers(this_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, true);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end >= pkt_data', pkt_data >= pkt_meta' */\n\t\t\tfind_good_pkt_pointers(other_branch, src_reg,\n\t\t\t\t\t       src_reg->type, false);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase BPF_JLE:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' <= pkt_end, pkt_meta' <= pkt_data */\n\t\t\tfind_good_pkt_pointers(other_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, false);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end <= pkt_data', pkt_data <= pkt_meta' */\n\t\t\tfind_good_pkt_pointers(this_branch, src_reg,\n\t\t\t\t\t       src_reg->type, true);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic int check_cond_jmp_op(struct bpf_verifier_env *env,\n\t\t\t     struct bpf_insn *insn, int *insn_idx)\n{\n\tstruct bpf_verifier_state *this_branch = env->cur_state;\n\tstruct bpf_verifier_state *other_branch;\n\tstruct bpf_reg_state *regs = this_branch->frame[this_branch->curframe]->regs;\n\tstruct bpf_reg_state *dst_reg, *other_branch_regs;\n\tu8 opcode = BPF_OP(insn->code);\n\tint err;\n\n\tif (opcode > BPF_JSLE) {\n\t\tverbose(env, \"invalid BPF_JMP opcode %x\\n\", opcode);\n\t\treturn -EINVAL;\n\t}\n\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tif (insn->imm != 0) {\n\t\t\tverbose(env, \"BPF_JMP uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/* check src1 operand */\n\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (is_pointer_value(env, insn->src_reg)) {\n\t\t\tverbose(env, \"R%d pointer comparison prohibited\\n\",\n\t\t\t\tinsn->src_reg);\n\t\t\treturn -EACCES;\n\t\t}\n\t} else {\n\t\tif (insn->src_reg != BPF_REG_0) {\n\t\t\tverbose(env, \"BPF_JMP uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t/* check src2 operand */\n\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tdst_reg = &regs[insn->dst_reg];\n\n\tif (BPF_SRC(insn->code) == BPF_K) {\n\t\tint pred = is_branch_taken(dst_reg, insn->imm, opcode);\n\n\t\tif (pred == 1) {\n\t\t\t /* only follow the goto, ignore fall-through */\n\t\t\t*insn_idx += insn->off;\n\t\t\treturn 0;\n\t\t} else if (pred == 0) {\n\t\t\t/* only follow fall-through branch, since\n\t\t\t * that's where the program will go\n\t\t\t */\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tother_branch = push_stack(env, *insn_idx + insn->off + 1, *insn_idx,\n\t\t\t\t  false);\n\tif (!other_branch)\n\t\treturn -EFAULT;\n\tother_branch_regs = other_branch->frame[other_branch->curframe]->regs;\n\n\t/* detect if we are comparing against a constant value so we can adjust\n\t * our min/max values for our dst register.\n\t * this is only legit if both are scalars (or pointers to the same\n\t * object, I suppose, but we don't support that right now), because\n\t * otherwise the different base pointers mean the offsets aren't\n\t * comparable.\n\t */\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tif (dst_reg->type == SCALAR_VALUE &&\n\t\t    regs[insn->src_reg].type == SCALAR_VALUE) {\n\t\t\tif (tnum_is_const(regs[insn->src_reg].var_off))\n\t\t\t\treg_set_min_max(&other_branch_regs[insn->dst_reg],\n\t\t\t\t\t\tdst_reg, regs[insn->src_reg].var_off.value,\n\t\t\t\t\t\topcode);\n\t\t\telse if (tnum_is_const(dst_reg->var_off))\n\t\t\t\treg_set_min_max_inv(&other_branch_regs[insn->src_reg],\n\t\t\t\t\t\t    &regs[insn->src_reg],\n\t\t\t\t\t\t    dst_reg->var_off.value, opcode);\n\t\t\telse if (opcode == BPF_JEQ || opcode == BPF_JNE)\n\t\t\t\t/* Comparing for equality, we can combine knowledge */\n\t\t\t\treg_combine_min_max(&other_branch_regs[insn->src_reg],\n\t\t\t\t\t\t    &other_branch_regs[insn->dst_reg],\n\t\t\t\t\t\t    &regs[insn->src_reg],\n\t\t\t\t\t\t    &regs[insn->dst_reg], opcode);\n\t\t}\n\t} else if (dst_reg->type == SCALAR_VALUE) {\n\t\treg_set_min_max(&other_branch_regs[insn->dst_reg],\n\t\t\t\t\tdst_reg, insn->imm, opcode);\n\t}\n\n\t/* detect if R == 0 where R is returned from bpf_map_lookup_elem() */\n\tif (BPF_SRC(insn->code) == BPF_K &&\n\t    insn->imm == 0 && (opcode == BPF_JEQ || opcode == BPF_JNE) &&\n\t    reg_type_may_be_null(dst_reg->type)) {\n\t\t/* Mark all identical registers in each branch as either\n\t\t * safe or unknown depending R == 0 or R != 0 conditional.\n\t\t */\n\t\tmark_ptr_or_null_regs(this_branch, insn->dst_reg,\n\t\t\t\t      opcode == BPF_JNE);\n\t\tmark_ptr_or_null_regs(other_branch, insn->dst_reg,\n\t\t\t\t      opcode == BPF_JEQ);\n\t} else if (!try_match_pkt_pointers(insn, dst_reg, &regs[insn->src_reg],\n\t\t\t\t\t   this_branch, other_branch) &&\n\t\t   is_pointer_value(env, insn->dst_reg)) {\n\t\tverbose(env, \"R%d pointer comparison prohibited\\n\",\n\t\t\tinsn->dst_reg);\n\t\treturn -EACCES;\n\t}\n\tif (env->log.level)\n\t\tprint_verifier_state(env, this_branch->frame[this_branch->curframe]);\n\treturn 0;\n}\n\n/* return the map pointer stored inside BPF_LD_IMM64 instruction */\nstatic struct bpf_map *ld_imm64_to_map_ptr(struct bpf_insn *insn)\n{\n\tu64 imm64 = ((u64) (u32) insn[0].imm) | ((u64) (u32) insn[1].imm) << 32;\n\n\treturn (struct bpf_map *) (unsigned long) imm64;\n}\n\n/* verify BPF_LD_IMM64 instruction */\nstatic int check_ld_imm(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tint err;\n\n\tif (BPF_SIZE(insn->code) != BPF_DW) {\n\t\tverbose(env, \"invalid BPF_LD_IMM insn\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (insn->off != 0) {\n\t\tverbose(env, \"BPF_LD_IMM64 uses reserved fields\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\terr = check_reg_arg(env, insn->dst_reg, DST_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (insn->src_reg == 0) {\n\t\tu64 imm = ((u64)(insn + 1)->imm << 32) | (u32)insn->imm;\n\n\t\tregs[insn->dst_reg].type = SCALAR_VALUE;\n\t\t__mark_reg_known(&regs[insn->dst_reg], imm);\n\t\treturn 0;\n\t}\n\n\t/* replace_map_fd_with_map_ptr() should have caught bad ld_imm64 */\n\tBUG_ON(insn->src_reg != BPF_PSEUDO_MAP_FD);\n\n\tregs[insn->dst_reg].type = CONST_PTR_TO_MAP;\n\tregs[insn->dst_reg].map_ptr = ld_imm64_to_map_ptr(insn);\n\treturn 0;\n}\n\nstatic bool may_access_skb(enum bpf_prog_type type)\n{\n\tswitch (type) {\n\tcase BPF_PROG_TYPE_SOCKET_FILTER:\n\tcase BPF_PROG_TYPE_SCHED_CLS:\n\tcase BPF_PROG_TYPE_SCHED_ACT:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n/* verify safety of LD_ABS|LD_IND instructions:\n * - they can only appear in the programs where ctx == skb\n * - since they are wrappers of function calls, they scratch R1-R5 registers,\n *   preserve R6-R9, and store return value into R0\n *\n * Implicit input:\n *   ctx == skb == R6 == CTX\n *\n * Explicit input:\n *   SRC == any register\n *   IMM == 32-bit immediate\n *\n * Output:\n *   R0 - 8/16/32-bit skb data converted to cpu endianness\n */\nstatic int check_ld_abs(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 mode = BPF_MODE(insn->code);\n\tint i, err;\n\n\tif (!may_access_skb(env->prog->type)) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] instructions not allowed for this program type\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!env->ops->gen_ld_abs) {\n\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (env->subprog_cnt > 1) {\n\t\t/* when program has LD_ABS insn JITs and interpreter assume\n\t\t * that r1 == ctx == skb which is not the case for callees\n\t\t * that can have arbitrary arguments. It's problematic\n\t\t * for main prog as well since JITs would need to analyze\n\t\t * all functions in order to make proper register save/restore\n\t\t * decisions in the main prog. Hence disallow LD_ABS with calls\n\t\t */\n\t\tverbose(env, \"BPF_LD_[ABS|IND] instructions cannot be mixed with bpf-to-bpf calls\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (insn->dst_reg != BPF_REG_0 || insn->off != 0 ||\n\t    BPF_SIZE(insn->code) == BPF_DW ||\n\t    (mode == BPF_ABS && insn->src_reg != BPF_REG_0)) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] uses reserved fields\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* check whether implicit source operand (register R6) is readable */\n\terr = check_reg_arg(env, BPF_REG_6, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\t/* Disallow usage of BPF_LD_[ABS|IND] with reference tracking, as\n\t * gen_ld_abs() may terminate the program at runtime, leading to\n\t * reference leak.\n\t */\n\terr = check_reference_leak(env);\n\tif (err) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] cannot be mixed with socket references\\n\");\n\t\treturn err;\n\t}\n\n\tif (regs[BPF_REG_6].type != PTR_TO_CTX) {\n\t\tverbose(env,\n\t\t\t\"at the time of BPF_LD_ABS|IND R6 != pointer to skb\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (mode == BPF_IND) {\n\t\t/* check explicit source operand */\n\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\t/* reset caller saved regs to unreadable */\n\tfor (i = 0; i < CALLER_SAVED_REGS; i++) {\n\t\tmark_reg_not_init(env, regs, caller_saved[i]);\n\t\tcheck_reg_arg(env, caller_saved[i], DST_OP_NO_MARK);\n\t}\n\n\t/* mark destination R0 register as readable, since it contains\n\t * the value fetched from the packet.\n\t * Already marked as written above.\n\t */\n\tmark_reg_unknown(env, regs, BPF_REG_0);\n\treturn 0;\n}\n\nstatic int check_return_code(struct bpf_verifier_env *env)\n{\n\tstruct bpf_reg_state *reg;\n\tstruct tnum range = tnum_range(0, 1);\n\n\tswitch (env->prog->type) {\n\tcase BPF_PROG_TYPE_CGROUP_SKB:\n\tcase BPF_PROG_TYPE_CGROUP_SOCK:\n\tcase BPF_PROG_TYPE_CGROUP_SOCK_ADDR:\n\tcase BPF_PROG_TYPE_SOCK_OPS:\n\tcase BPF_PROG_TYPE_CGROUP_DEVICE:\n\t\tbreak;\n\tdefault:\n\t\treturn 0;\n\t}\n\n\treg = cur_regs(env) + BPF_REG_0;\n\tif (reg->type != SCALAR_VALUE) {\n\t\tverbose(env, \"At program exit the register R0 is not a known value (%s)\\n\",\n\t\t\treg_type_str[reg->type]);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!tnum_in(range, reg->var_off)) {\n\t\tverbose(env, \"At program exit the register R0 \");\n\t\tif (!tnum_is_unknown(reg->var_off)) {\n\t\t\tchar tn_buf[48];\n\n\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\tverbose(env, \"has value %s\", tn_buf);\n\t\t} else {\n\t\t\tverbose(env, \"has unknown scalar value\");\n\t\t}\n\t\tverbose(env, \" should have been 0 or 1\\n\");\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\n/* non-recursive DFS pseudo code\n * 1  procedure DFS-iterative(G,v):\n * 2      label v as discovered\n * 3      let S be a stack\n * 4      S.push(v)\n * 5      while S is not empty\n * 6            t <- S.pop()\n * 7            if t is what we're looking for:\n * 8                return t\n * 9            for all edges e in G.adjacentEdges(t) do\n * 10               if edge e is already labelled\n * 11                   continue with the next edge\n * 12               w <- G.adjacentVertex(t,e)\n * 13               if vertex w is not discovered and not explored\n * 14                   label e as tree-edge\n * 15                   label w as discovered\n * 16                   S.push(w)\n * 17                   continue at 5\n * 18               else if vertex w is discovered\n * 19                   label e as back-edge\n * 20               else\n * 21                   // vertex w is explored\n * 22                   label e as forward- or cross-edge\n * 23           label t as explored\n * 24           S.pop()\n *\n * convention:\n * 0x10 - discovered\n * 0x11 - discovered and fall-through edge labelled\n * 0x12 - discovered and fall-through and branch edges labelled\n * 0x20 - explored\n */\n\nenum {\n\tDISCOVERED = 0x10,\n\tEXPLORED = 0x20,\n\tFALLTHROUGH = 1,\n\tBRANCH = 2,\n};\n\n#define STATE_LIST_MARK ((struct bpf_verifier_state_list *) -1L)\n\nstatic int *insn_stack;\t/* stack of insns to process */\nstatic int cur_stack;\t/* current stack index */\nstatic int *insn_state;\n\n/* t, w, e - match pseudo-code above:\n * t - index of current instruction\n * w - next instruction\n * e - edge\n */\nstatic int push_insn(int t, int w, int e, struct bpf_verifier_env *env)\n{\n\tif (e == FALLTHROUGH && insn_state[t] >= (DISCOVERED | FALLTHROUGH))\n\t\treturn 0;\n\n\tif (e == BRANCH && insn_state[t] >= (DISCOVERED | BRANCH))\n\t\treturn 0;\n\n\tif (w < 0 || w >= env->prog->len) {\n\t\tverbose_linfo(env, t, \"%d: \", t);\n\t\tverbose(env, \"jump out of range from insn %d to %d\\n\", t, w);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e == BRANCH)\n\t\t/* mark branch target for state pruning */\n\t\tenv->explored_states[w] = STATE_LIST_MARK;\n\n\tif (insn_state[w] == 0) {\n\t\t/* tree-edge */\n\t\tinsn_state[t] = DISCOVERED | e;\n\t\tinsn_state[w] = DISCOVERED;\n\t\tif (cur_stack >= env->prog->len)\n\t\t\treturn -E2BIG;\n\t\tinsn_stack[cur_stack++] = w;\n\t\treturn 1;\n\t} else if ((insn_state[w] & 0xF0) == DISCOVERED) {\n\t\tverbose_linfo(env, t, \"%d: \", t);\n\t\tverbose_linfo(env, w, \"%d: \", w);\n\t\tverbose(env, \"back-edge from insn %d to %d\\n\", t, w);\n\t\treturn -EINVAL;\n\t} else if (insn_state[w] == EXPLORED) {\n\t\t/* forward- or cross-edge */\n\t\tinsn_state[t] = DISCOVERED | e;\n\t} else {\n\t\tverbose(env, \"insn state internal bug\\n\");\n\t\treturn -EFAULT;\n\t}\n\treturn 0;\n}\n\n/* non-recursive depth-first-search to detect loops in BPF program\n * loop == back-edge in directed graph\n */\nstatic int check_cfg(struct bpf_verifier_env *env)\n{\n\tstruct bpf_insn *insns = env->prog->insnsi;\n\tint insn_cnt = env->prog->len;\n\tint ret = 0;\n\tint i, t;\n\n\tinsn_state = kcalloc(insn_cnt, sizeof(int), GFP_KERNEL);\n\tif (!insn_state)\n\t\treturn -ENOMEM;\n\n\tinsn_stack = kcalloc(insn_cnt, sizeof(int), GFP_KERNEL);\n\tif (!insn_stack) {\n\t\tkfree(insn_state);\n\t\treturn -ENOMEM;\n\t}\n\n\tinsn_state[0] = DISCOVERED; /* mark 1st insn as discovered */\n\tinsn_stack[0] = 0; /* 0 is the first instruction */\n\tcur_stack = 1;\n\npeek_stack:\n\tif (cur_stack == 0)\n\t\tgoto check_state;\n\tt = insn_stack[cur_stack - 1];\n\n\tif (BPF_CLASS(insns[t].code) == BPF_JMP) {\n\t\tu8 opcode = BPF_OP(insns[t].code);\n\n\t\tif (opcode == BPF_EXIT) {\n\t\t\tgoto mark_explored;\n\t\t} else if (opcode == BPF_CALL) {\n\t\t\tret = push_insn(t, t + 1, FALLTHROUGH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t\tif (t + 1 < insn_cnt)\n\t\t\t\tenv->explored_states[t + 1] = STATE_LIST_MARK;\n\t\t\tif (insns[t].src_reg == BPF_PSEUDO_CALL) {\n\t\t\t\tenv->explored_states[t] = STATE_LIST_MARK;\n\t\t\t\tret = push_insn(t, t + insns[t].imm + 1, BRANCH, env);\n\t\t\t\tif (ret == 1)\n\t\t\t\t\tgoto peek_stack;\n\t\t\t\telse if (ret < 0)\n\t\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t} else if (opcode == BPF_JA) {\n\t\t\tif (BPF_SRC(insns[t].code) != BPF_K) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t\t/* unconditional jump with single edge */\n\t\t\tret = push_insn(t, t + insns[t].off + 1,\n\t\t\t\t\tFALLTHROUGH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t\t/* tell verifier to check for equivalent states\n\t\t\t * after every call and jump\n\t\t\t */\n\t\t\tif (t + 1 < insn_cnt)\n\t\t\t\tenv->explored_states[t + 1] = STATE_LIST_MARK;\n\t\t} else {\n\t\t\t/* conditional jump with two edges */\n\t\t\tenv->explored_states[t] = STATE_LIST_MARK;\n\t\t\tret = push_insn(t, t + 1, FALLTHROUGH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\n\t\t\tret = push_insn(t, t + insns[t].off + 1, BRANCH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t}\n\t} else {\n\t\t/* all other non-branch instructions with single\n\t\t * fall-through edge\n\t\t */\n\t\tret = push_insn(t, t + 1, FALLTHROUGH, env);\n\t\tif (ret == 1)\n\t\t\tgoto peek_stack;\n\t\telse if (ret < 0)\n\t\t\tgoto err_free;\n\t}\n\nmark_explored:\n\tinsn_state[t] = EXPLORED;\n\tif (cur_stack-- <= 0) {\n\t\tverbose(env, \"pop stack internal bug\\n\");\n\t\tret = -EFAULT;\n\t\tgoto err_free;\n\t}\n\tgoto peek_stack;\n\ncheck_state:\n\tfor (i = 0; i < insn_cnt; i++) {\n\t\tif (insn_state[i] != EXPLORED) {\n\t\t\tverbose(env, \"unreachable insn %d\\n\", i);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\t}\n\tret = 0; /* cfg looks good */\n\nerr_free:\n\tkfree(insn_state);\n\tkfree(insn_stack);\n\treturn ret;\n}\n\n/* The minimum supported BTF func info size */\n#define MIN_BPF_FUNCINFO_SIZE\t8\n#define MAX_FUNCINFO_REC_SIZE\t252\n\nstatic int check_btf_func(struct bpf_verifier_env *env,\n\t\t\t  const union bpf_attr *attr,\n\t\t\t  union bpf_attr __user *uattr)\n{\n\tu32 i, nfuncs, urec_size, min_size, prev_offset;\n\tu32 krec_size = sizeof(struct bpf_func_info);\n\tstruct bpf_func_info *krecord;\n\tconst struct btf_type *type;\n\tstruct bpf_prog *prog;\n\tconst struct btf *btf;\n\tvoid __user *urecord;\n\tint ret = 0;\n\n\tnfuncs = attr->func_info_cnt;\n\tif (!nfuncs)\n\t\treturn 0;\n\n\tif (nfuncs != env->subprog_cnt) {\n\t\tverbose(env, \"number of funcs in func_info doesn't match number of subprogs\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\turec_size = attr->func_info_rec_size;\n\tif (urec_size < MIN_BPF_FUNCINFO_SIZE ||\n\t    urec_size > MAX_FUNCINFO_REC_SIZE ||\n\t    urec_size % sizeof(u32)) {\n\t\tverbose(env, \"invalid func info rec size %u\\n\", urec_size);\n\t\treturn -EINVAL;\n\t}\n\n\tprog = env->prog;\n\tbtf = prog->aux->btf;\n\n\turecord = u64_to_user_ptr(attr->func_info);\n\tmin_size = min_t(u32, krec_size, urec_size);\n\n\tkrecord = kvcalloc(nfuncs, krec_size, GFP_KERNEL | __GFP_NOWARN);\n\tif (!krecord)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < nfuncs; i++) {\n\t\tret = bpf_check_uarg_tail_zero(urecord, krec_size, urec_size);\n\t\tif (ret) {\n\t\t\tif (ret == -E2BIG) {\n\t\t\t\tverbose(env, \"nonzero tailing record in func info\");\n\t\t\t\t/* set the size kernel expects so loader can zero\n\t\t\t\t * out the rest of the record.\n\t\t\t\t */\n\t\t\t\tif (put_user(min_size, &uattr->func_info_rec_size))\n\t\t\t\t\tret = -EFAULT;\n\t\t\t}\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (copy_from_user(&krecord[i], urecord, min_size)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/* check insn_off */\n\t\tif (i == 0) {\n\t\t\tif (krecord[i].insn_off) {\n\t\t\t\tverbose(env,\n\t\t\t\t\t\"nonzero insn_off %u for the first func info record\",\n\t\t\t\t\tkrecord[i].insn_off);\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t} else if (krecord[i].insn_off <= prev_offset) {\n\t\t\tverbose(env,\n\t\t\t\t\"same or smaller insn offset (%u) than previous func info record (%u)\",\n\t\t\t\tkrecord[i].insn_off, prev_offset);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (env->subprog_info[i].start != krecord[i].insn_off) {\n\t\t\tverbose(env, \"func_info BTF section doesn't match subprog layout in BPF program\\n\");\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/* check type_id */\n\t\ttype = btf_type_by_id(btf, krecord[i].type_id);\n\t\tif (!type || BTF_INFO_KIND(type->info) != BTF_KIND_FUNC) {\n\t\t\tverbose(env, \"invalid type id %d in func info\",\n\t\t\t\tkrecord[i].type_id);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tprev_offset = krecord[i].insn_off;\n\t\turecord += urec_size;\n\t}\n\n\tprog->aux->func_info = krecord;\n\tprog->aux->func_info_cnt = nfuncs;\n\treturn 0;\n\nerr_free:\n\tkvfree(krecord);\n\treturn ret;\n}\n\nstatic void adjust_btf_func(struct bpf_verifier_env *env)\n{\n\tint i;\n\n\tif (!env->prog->aux->func_info)\n\t\treturn;\n\n\tfor (i = 0; i < env->subprog_cnt; i++)\n\t\tenv->prog->aux->func_info[i].insn_off = env->subprog_info[i].start;\n}\n\n#define MIN_BPF_LINEINFO_SIZE\t(offsetof(struct bpf_line_info, line_col) + \\\n\t\tsizeof(((struct bpf_line_info *)(0))->line_col))\n#define MAX_LINEINFO_REC_SIZE\tMAX_FUNCINFO_REC_SIZE\n\nstatic int check_btf_line(struct bpf_verifier_env *env,\n\t\t\t  const union bpf_attr *attr,\n\t\t\t  union bpf_attr __user *uattr)\n{\n\tu32 i, s, nr_linfo, ncopy, expected_size, rec_size, prev_offset = 0;\n\tstruct bpf_subprog_info *sub;\n\tstruct bpf_line_info *linfo;\n\tstruct bpf_prog *prog;\n\tconst struct btf *btf;\n\tvoid __user *ulinfo;\n\tint err;\n\n\tnr_linfo = attr->line_info_cnt;\n\tif (!nr_linfo)\n\t\treturn 0;\n\n\trec_size = attr->line_info_rec_size;\n\tif (rec_size < MIN_BPF_LINEINFO_SIZE ||\n\t    rec_size > MAX_LINEINFO_REC_SIZE ||\n\t    rec_size & (sizeof(u32) - 1))\n\t\treturn -EINVAL;\n\n\t/* Need to zero it in case the userspace may\n\t * pass in a smaller bpf_line_info object.\n\t */\n\tlinfo = kvcalloc(nr_linfo, sizeof(struct bpf_line_info),\n\t\t\t GFP_KERNEL | __GFP_NOWARN);\n\tif (!linfo)\n\t\treturn -ENOMEM;\n\n\tprog = env->prog;\n\tbtf = prog->aux->btf;\n\n\ts = 0;\n\tsub = env->subprog_info;\n\tulinfo = u64_to_user_ptr(attr->line_info);\n\texpected_size = sizeof(struct bpf_line_info);\n\tncopy = min_t(u32, expected_size, rec_size);\n\tfor (i = 0; i < nr_linfo; i++) {\n\t\terr = bpf_check_uarg_tail_zero(ulinfo, expected_size, rec_size);\n\t\tif (err) {\n\t\t\tif (err == -E2BIG) {\n\t\t\t\tverbose(env, \"nonzero tailing record in line_info\");\n\t\t\t\tif (put_user(expected_size,\n\t\t\t\t\t     &uattr->line_info_rec_size))\n\t\t\t\t\terr = -EFAULT;\n\t\t\t}\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (copy_from_user(&linfo[i], ulinfo, ncopy)) {\n\t\t\terr = -EFAULT;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/*\n\t\t * Check insn_off to ensure\n\t\t * 1) strictly increasing AND\n\t\t * 2) bounded by prog->len\n\t\t *\n\t\t * The linfo[0].insn_off == 0 check logically falls into\n\t\t * the later \"missing bpf_line_info for func...\" case\n\t\t * because the first linfo[0].insn_off must be the\n\t\t * first sub also and the first sub must have\n\t\t * subprog_info[0].start == 0.\n\t\t */\n\t\tif ((i && linfo[i].insn_off <= prev_offset) ||\n\t\t    linfo[i].insn_off >= prog->len) {\n\t\t\tverbose(env, \"Invalid line_info[%u].insn_off:%u (prev_offset:%u prog->len:%u)\\n\",\n\t\t\t\ti, linfo[i].insn_off, prev_offset,\n\t\t\t\tprog->len);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (!prog->insnsi[linfo[i].insn_off].code) {\n\t\t\tverbose(env,\n\t\t\t\t\"Invalid insn code at line_info[%u].insn_off\\n\",\n\t\t\t\ti);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (!btf_name_by_offset(btf, linfo[i].line_off) ||\n\t\t    !btf_name_by_offset(btf, linfo[i].file_name_off)) {\n\t\t\tverbose(env, \"Invalid line_info[%u].line_off or .file_name_off\\n\", i);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (s != env->subprog_cnt) {\n\t\t\tif (linfo[i].insn_off == sub[s].start) {\n\t\t\t\tsub[s].linfo_idx = i;\n\t\t\t\ts++;\n\t\t\t} else if (sub[s].start < linfo[i].insn_off) {\n\t\t\t\tverbose(env, \"missing bpf_line_info for func#%u\\n\", s);\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t}\n\n\t\tprev_offset = linfo[i].insn_off;\n\t\tulinfo += rec_size;\n\t}\n\n\tif (s != env->subprog_cnt) {\n\t\tverbose(env, \"missing bpf_line_info for %u funcs starting from func#%u\\n\",\n\t\t\tenv->subprog_cnt - s, s);\n\t\terr = -EINVAL;\n\t\tgoto err_free;\n\t}\n\n\tprog->aux->linfo = linfo;\n\tprog->aux->nr_linfo = nr_linfo;\n\n\treturn 0;\n\nerr_free:\n\tkvfree(linfo);\n\treturn err;\n}\n\nstatic int check_btf_info(struct bpf_verifier_env *env,\n\t\t\t  const union bpf_attr *attr,\n\t\t\t  union bpf_attr __user *uattr)\n{\n\tstruct btf *btf;\n\tint err;\n\n\tif (!attr->func_info_cnt && !attr->line_info_cnt)\n\t\treturn 0;\n\n\tbtf = btf_get_by_fd(attr->prog_btf_fd);\n\tif (IS_ERR(btf))\n\t\treturn PTR_ERR(btf);\n\tenv->prog->aux->btf = btf;\n\n\terr = check_btf_func(env, attr, uattr);\n\tif (err)\n\t\treturn err;\n\n\terr = check_btf_line(env, attr, uattr);\n\tif (err)\n\t\treturn err;\n\n\treturn 0;\n}\n\n/* check %cur's range satisfies %old's */\nstatic bool range_within(struct bpf_reg_state *old,\n\t\t\t struct bpf_reg_state *cur)\n{\n\treturn old->umin_value <= cur->umin_value &&\n\t       old->umax_value >= cur->umax_value &&\n\t       old->smin_value <= cur->smin_value &&\n\t       old->smax_value >= cur->smax_value;\n}\n\n/* Maximum number of register states that can exist at once */\n#define ID_MAP_SIZE\t(MAX_BPF_REG + MAX_BPF_STACK / BPF_REG_SIZE)\nstruct idpair {\n\tu32 old;\n\tu32 cur;\n};\n\n/* If in the old state two registers had the same id, then they need to have\n * the same id in the new state as well.  But that id could be different from\n * the old state, so we need to track the mapping from old to new ids.\n * Once we have seen that, say, a reg with old id 5 had new id 9, any subsequent\n * regs with old id 5 must also have new id 9 for the new state to be safe.  But\n * regs with a different old id could still have new id 9, we don't care about\n * that.\n * So we look through our idmap to see if this old id has been seen before.  If\n * so, we require the new id to match; otherwise, we add the id pair to the map.\n */\nstatic bool check_ids(u32 old_id, u32 cur_id, struct idpair *idmap)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < ID_MAP_SIZE; i++) {\n\t\tif (!idmap[i].old) {\n\t\t\t/* Reached an empty slot; haven't seen this id before */\n\t\t\tidmap[i].old = old_id;\n\t\t\tidmap[i].cur = cur_id;\n\t\t\treturn true;\n\t\t}\n\t\tif (idmap[i].old == old_id)\n\t\t\treturn idmap[i].cur == cur_id;\n\t}\n\t/* We ran out of idmap slots, which should be impossible */\n\tWARN_ON_ONCE(1);\n\treturn false;\n}",
    "diff": "+\tu32 dst = insn->dst_reg;\n+\t\t\tverbose(env, \"R%d tried to add from different pointers or scalars\\n\", dst);\n \t\tif (signed_add_overflows(dst_reg->smin_value, smin_val) ||\n \t\t    signed_add_overflows(dst_reg->smax_value, smax_val)) {\n \t\t\tdst_reg->smin_value = S64_MIN;\n \t\tdst_reg->var_off = tnum_add(dst_reg->var_off, src_reg.var_off);\n+\t\t\tverbose(env, \"R%d tried to sub from different pointers or scalars\\n\", dst);\n \t\tif (signed_sub_overflows(dst_reg->smin_value, smax_val) ||\n \t\t    signed_sub_overflows(dst_reg->smax_value, smin_val)) {",
    "critical_vars": [
      "dst"
    ],
    "variable_definitions": {
      "dst": "u32 dst = insn->dst_reg;"
    },
    "variable_types": {
      "dst": "integer"
    },
    "type_mapping": {
      "dst": "Integer"
    },
    "vulnerable_line": "if (signed_add_overflows(dst_reg->smin_value, smin_val) || signed_add_overflows(dst_reg->smax_value, smax_val))",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The function signed_add_overflows checks for overflow when adding values to dst_reg's min and max value states. If the operation does overflow, it may lead to unforeseen behaviors, such as executing invalid paths in the program or triggering assertion failures, compromising control flow."
  },
  {
    "fix_code": "static inline void x86_assign_hw_event(struct perf_event *event,\n\t\t\t\tstruct cpu_hw_events *cpuc, int i)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\thwc->idx = cpuc->assign[i];\n\thwc->last_cpu = smp_processor_id();\n\thwc->last_tag = ++cpuc->tags[i];\n\n\tif (hwc->idx == X86_PMC_IDX_FIXED_BTS) {\n\t\thwc->config_base = 0;\n\t\thwc->event_base\t= 0;\n\t} else if (hwc->idx >= X86_PMC_IDX_FIXED) {\n\t\thwc->config_base = MSR_ARCH_PERFMON_FIXED_CTR_CTRL;\n\t\thwc->event_base = MSR_ARCH_PERFMON_FIXED_CTR0 + (hwc->idx - X86_PMC_IDX_FIXED);\n\t} else {\n\t\thwc->config_base = x86_pmu_config_addr(hwc->idx);\n\t\thwc->event_base  = x86_pmu_event_addr(hwc->idx);\n\t}\n}",
    "diff": " \t\thwc->event_base\t= 0;\n-\t\thwc->event_base = MSR_ARCH_PERFMON_FIXED_CTR0;\n+\t\thwc->event_base = MSR_ARCH_PERFMON_FIXED_CTR0 + (hwc->idx - X86_PMC_IDX_FIXED);\n \t\thwc->event_base  = x86_pmu_event_addr(hwc->idx);",
    "critical_vars": [
      "hwc->event_base"
    ],
    "variable_definitions": {
      "hwc->event_base": "Definition not found"
    },
    "variable_types": {
      "hwc->event_base": "struct pointer_integer"
    },
    "type_mapping": {
      "hwc->event_base": "sp_integer"
    },
    "vulnerable_line": "hwc->event_base = MSR_ARCH_PERFMON_FIXED_CTR0 + (hwc->idx - X86_PMC_IDX_FIXED);",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The expression (hwc->idx - X86_PMC_IDX_FIXED) can lead to an integer overflow if hwc->idx is sufficiently large, resulting in hwc->event_base being assigned an incorrect value. This can cause a denial of service (panic) as indicated in the CVE description."
  },
  {
    "fix_code": "static struct sk_buff *udp6_ufo_fragment(struct sk_buff *skb,\n\t\t\t\t\t netdev_features_t features)\n{\n\tstruct sk_buff *segs = ERR_PTR(-EINVAL);\n\tunsigned int mss;\n\tunsigned int unfrag_ip6hlen, unfrag_len;\n\tstruct frag_hdr *fptr;\n\tu8 *packet_start, *prevhdr;\n\tu8 nexthdr;\n\tu8 frag_hdr_sz = sizeof(struct frag_hdr);\n\tint offset;\n\t__wsum csum;\n\tint tnl_hlen;\n\n\tmss = skb_shinfo(skb)->gso_size;\n\tif (unlikely(skb->len <= mss))\n\t\tgoto out;\n\n\tif (skb_gso_ok(skb, features | NETIF_F_GSO_ROBUST)) {\n\t\t/* Packet is from an untrusted source, reset gso_segs. */\n\t\tint type = skb_shinfo(skb)->gso_type;\n\n\t\tif (unlikely(type & ~(SKB_GSO_UDP |\n\t\t\t\t      SKB_GSO_DODGY |\n\t\t\t\t      SKB_GSO_UDP_TUNNEL |\n\t\t\t\t      SKB_GSO_GRE |\n\t\t\t\t      SKB_GSO_IPIP |\n\t\t\t\t      SKB_GSO_SIT |\n\t\t\t\t      SKB_GSO_MPLS) ||\n\t\t\t     !(type & (SKB_GSO_UDP))))\n\t\t\tgoto out;\n\n\t\tskb_shinfo(skb)->gso_segs = DIV_ROUND_UP(skb->len, mss);\n\n\t\tsegs = NULL;\n\t\tgoto out;\n\t}\n\n\tif (skb->encapsulation && skb_shinfo(skb)->gso_type & SKB_GSO_UDP_TUNNEL)\n\t\tsegs = skb_udp_tunnel_segment(skb, features);\n\telse {\n\t\t/* Do software UFO. Complete and fill in the UDP checksum as HW cannot\n\t\t * do checksum of UDP packets sent as multiple IP fragments.\n\t\t */\n\t\toffset = skb_checksum_start_offset(skb);\n\t\tcsum = skb_checksum(skb, offset, skb->len - offset, 0);\n\t\toffset += skb->csum_offset;\n\t\t*(__sum16 *)(skb->data + offset) = csum_fold(csum);\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\n\t\t/* Check if there is enough headroom to insert fragment header. */\n\t\ttnl_hlen = skb_tnl_header_len(skb);\n\t\tif (skb->mac_header < (tnl_hlen + frag_hdr_sz)) {\n\t\t\tif (gso_pskb_expand_head(skb, tnl_hlen + frag_hdr_sz))\n\t\t\t\tgoto out;\n\t\t}\n\n\t\t/* Find the unfragmentable header and shift it left by frag_hdr_sz\n\t\t * bytes to insert fragment header.\n\t\t */\n\t\tunfrag_ip6hlen = ip6_find_1stfragopt(skb, &prevhdr);\n\t\tnexthdr = *prevhdr;\n\t\t*prevhdr = NEXTHDR_FRAGMENT;\n\t\tunfrag_len = (skb_network_header(skb) - skb_mac_header(skb)) +\n\t\t\t     unfrag_ip6hlen + tnl_hlen;\n\t\tpacket_start = (u8 *) skb->head + SKB_GSO_CB(skb)->mac_offset;\n\t\tmemmove(packet_start-frag_hdr_sz, packet_start, unfrag_len);\n\n\t\tSKB_GSO_CB(skb)->mac_offset -= frag_hdr_sz;\n\t\tskb->mac_header -= frag_hdr_sz;\n\t\tskb->network_header -= frag_hdr_sz;\n\n\t\tfptr = (struct frag_hdr *)(skb_network_header(skb) + unfrag_ip6hlen);\n\t\tfptr->nexthdr = nexthdr;\n\t\tfptr->reserved = 0;\n\t\tipv6_select_ident(fptr, (struct rt6_info *)skb_dst(skb));\n\n\t\t/* Fragment the skb. ipv6 header and the remaining fields of the\n\t\t * fragment header are updated in ipv6_gso_segment()\n\t\t */\n\t\tsegs = skb_segment(skb, features);\n\t}\n\nout:\n\treturn segs;\n}",
    "diff": "@@ -90,7 +90,7 @@ static struct sk_buff *udp6_ufo_fragment(struct sk_buff *skb,\n \t\ttnl_hlen = skb_tnl_header_len(skb);\n-\t\tif (skb_headroom(skb) < (tnl_hlen + frag_hdr_sz)) {\n+\t\tif (skb->mac_header < (tnl_hlen + frag_hdr_sz)) {\n \t\t\tif (gso_pskb_expand_head(skb, tnl_hlen + frag_hdr_sz))",
    "critical_vars": [
      "skb"
    ],
    "variable_definitions": {
      "skb": "struct sk_buff *skb"
    },
    "variable_types": {
      "skb": "struct pointer"
    },
    "type_mapping": {
      "skb": "struct pointer"
    },
    "vulnerable_line": "if (skb->mac_header < (tnl_hlen + frag_hdr_sz)) {",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The comparison does not correctly validate the headroom available due to potential overflow when large UDP packets are processed, leading to a denial of service."
  },
  {
    "fix_code": "static struct sk_buff *udp6_ufo_fragment(struct sk_buff *skb,\n\t\t\t\t\t netdev_features_t features)\n{\n\tstruct sk_buff *segs = ERR_PTR(-EINVAL);\n\tunsigned int mss;\n\tunsigned int unfrag_ip6hlen, unfrag_len;\n\tstruct frag_hdr *fptr;\n\tu8 *packet_start, *prevhdr;\n\tu8 nexthdr;\n\tu8 frag_hdr_sz = sizeof(struct frag_hdr);\n\tint offset;\n\t__wsum csum;\n\tint tnl_hlen;\n\n\tmss = skb_shinfo(skb)->gso_size;\n\tif (unlikely(skb->len <= mss))\n\t\tgoto out;\n\n\tif (skb_gso_ok(skb, features | NETIF_F_GSO_ROBUST)) {\n\t\t/* Packet is from an untrusted source, reset gso_segs. */\n\t\tint type = skb_shinfo(skb)->gso_type;\n\n\t\tif (unlikely(type & ~(SKB_GSO_UDP |\n\t\t\t\t      SKB_GSO_DODGY |\n\t\t\t\t      SKB_GSO_UDP_TUNNEL |\n\t\t\t\t      SKB_GSO_GRE |\n\t\t\t\t      SKB_GSO_IPIP |\n\t\t\t\t      SKB_GSO_SIT |\n\t\t\t\t      SKB_GSO_MPLS) ||\n\t\t\t     !(type & (SKB_GSO_UDP))))\n\t\t\tgoto out;\n\n\t\tskb_shinfo(skb)->gso_segs = DIV_ROUND_UP(skb->len, mss);\n\n\t\tsegs = NULL;\n\t\tgoto out;\n\t}\n\n\tif (skb->encapsulation && skb_shinfo(skb)->gso_type & SKB_GSO_UDP_TUNNEL)\n\t\tsegs = skb_udp_tunnel_segment(skb, features);\n\telse {\n\t\t/* Do software UFO. Complete and fill in the UDP checksum as HW cannot\n\t\t * do checksum of UDP packets sent as multiple IP fragments.\n\t\t */\n\t\toffset = skb_checksum_start_offset(skb);\n\t\tcsum = skb_checksum(skb, offset, skb->len - offset, 0);\n\t\toffset += skb->csum_offset;\n\t\t*(__sum16 *)(skb->data + offset) = csum_fold(csum);\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\n\t\t/* Check if there is enough headroom to insert fragment header. */\n\t\ttnl_hlen = skb_tnl_header_len(skb);\n\t\tif (skb->mac_header < (tnl_hlen + frag_hdr_sz)) {\n\t\t\tif (gso_pskb_expand_head(skb, tnl_hlen + frag_hdr_sz))\n\t\t\t\tgoto out;\n\t\t}\n\n\t\t/* Find the unfragmentable header and shift it left by frag_hdr_sz\n\t\t * bytes to insert fragment header.\n\t\t */\n\t\tunfrag_ip6hlen = ip6_find_1stfragopt(skb, &prevhdr);\n\t\tnexthdr = *prevhdr;\n\t\t*prevhdr = NEXTHDR_FRAGMENT;\n\t\tunfrag_len = (skb_network_header(skb) - skb_mac_header(skb)) +\n\t\t\t     unfrag_ip6hlen + tnl_hlen;\n\t\tpacket_start = (u8 *) skb->head + SKB_GSO_CB(skb)->mac_offset;\n\t\tmemmove(packet_start-frag_hdr_sz, packet_start, unfrag_len);\n\n\t\tSKB_GSO_CB(skb)->mac_offset -= frag_hdr_sz;\n\t\tskb->mac_header -= frag_hdr_sz;\n\t\tskb->network_header -= frag_hdr_sz;\n\n\t\tfptr = (struct frag_hdr *)(skb_network_header(skb) + unfrag_ip6hlen);\n\t\tfptr->nexthdr = nexthdr;\n\t\tfptr->reserved = 0;\n\t\tipv6_select_ident(fptr, (struct rt6_info *)skb_dst(skb));\n\n\t\t/* Fragment the skb. ipv6 header and the remaining fields of the\n\t\t * fragment header are updated in ipv6_gso_segment()\n\t\t */\n\t\tsegs = skb_segment(skb, features);\n\t}\n\nout:\n\treturn segs;\n}",
    "diff": "+\t\tif (skb->mac_header < (tnl_hlen + frag_hdr_sz)) {",
    "critical_vars": [
      "skb->mac_header"
    ],
    "variable_definitions": {
      "skb->mac_header": "Definition not found"
    },
    "variable_types": {
      "skb->mac_header": "struct pointer_integer"
    },
    "type_mapping": {
      "skb->mac_header": "sp_integer"
    },
    "vulnerable_line": "if (skb->mac_header < (tnl_hlen + frag_hdr_sz)) {",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The comparison of skb->mac_header could result in an integer overflow, leading to the incorrect insertion of the fragment header when large IPv6 UDP packets are processed, enabling denial of service via panic."
  },
  {
    "fix_code": "static void tcp_cwnd_reduction(struct sock *sk, const int prior_unsacked,\n\t\t\t       int fast_rexmit, int flag)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint sndcnt = 0;\n\tint delta = tp->snd_ssthresh - tcp_packets_in_flight(tp);\n\tint newly_acked_sacked = prior_unsacked -\n\t\t\t\t (tp->packets_out - tp->sacked_out);\n\n\tif (newly_acked_sacked <= 0 || WARN_ON_ONCE(!tp->prior_cwnd))\n\t\treturn;\n\n\ttp->prr_delivered += newly_acked_sacked;\n\tif (delta < 0) {\n\t\tu64 dividend = (u64)tp->snd_ssthresh * tp->prr_delivered +\n\t\t\t       tp->prior_cwnd - 1;\n\t\tsndcnt = div_u64(dividend, tp->prior_cwnd) - tp->prr_out;\n\t} else if ((flag & FLAG_RETRANS_DATA_ACKED) &&\n\t\t   !(flag & FLAG_LOST_RETRANS)) {\n\t\tsndcnt = min_t(int, delta,\n\t\t\t       max_t(int, tp->prr_delivered - tp->prr_out,\n\t\t\t\t     newly_acked_sacked) + 1);\n\t} else {\n\t\tsndcnt = min(delta, newly_acked_sacked);\n\t}\n\tsndcnt = max(sndcnt, (fast_rexmit ? 1 : 0));\n\ttp->snd_cwnd = tcp_packets_in_flight(tp) + sndcnt;\n}",
    "diff": " \tint newly_acked_sacked = prior_unsacked -\n+\tif (newly_acked_sacked <= 0 || WARN_ON_ONCE(!tp->prior_cwnd))\n \ttp->prr_delivered += newly_acked_sacked;",
    "critical_vars": [
      "newly_acked_sacked",
      "tp->prior_cwnd"
    ],
    "variable_definitions": {
      "newly_acked_sacked": "int newly_acked_sacked = prior_unsacked - (tp->packets_out - tp->sacked_out);",
      "tp->prior_cwnd": "Definition not found"
    },
    "variable_types": {
      "newly_acked_sacked": "integer",
      "tp->prior_cwnd": "struct pointer_integer"
    },
    "type_mapping": {
      "newly_acked_sacked": "Integer",
      "tp->prior_cwnd": "sp_integer"
    },
    "vulnerable_line": "if (newly_acked_sacked <= 0 || WARN_ON_ONCE(!tp->prior_cwnd))",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Division by Zero",
    "reasoning": "The check for new packets being acknowledged fails when newly_acked_sacked is zero or negative, potentially causing a division by zero in subsequent calculations where tp->prior_cwnd is used."
  },
  {
    "fix_code": "static u32 apic_get_tmcct(struct kvm_lapic *apic)\n{\n\tktime_t remaining;\n\ts64 ns;\n\tu32 tmcct;\n\n\tASSERT(apic != NULL);\n\n\t/* if initial count is 0, current count should also be 0 */\n\tif (kvm_apic_get_reg(apic, APIC_TMICT) == 0 ||\n\t\tapic->lapic_timer.period == 0)\n\t\treturn 0;\n\n\tremaining = hrtimer_get_remaining(&apic->lapic_timer.timer);\n\tif (ktime_to_ns(remaining) < 0)\n\t\tremaining = ktime_set(0, 0);\n\n\tns = mod_64(ktime_to_ns(remaining), apic->lapic_timer.period);\n\ttmcct = div64_u64(ns,\n\t\t\t (APIC_BUS_CYCLE_NS * apic->divide_count));\n\n\treturn tmcct;\n}",
    "diff": "@@ -841,7 +841,8 @@ static u32 apic_get_tmcct(struct kvm_lapic *apic)\n \tASSERT(apic != NULL);\n-\tif (kvm_apic_get_reg(apic, APIC_TMICT) == 0)\n+\tif (kvm_apic_get_reg(apic, APIC_TMICT) == 0 ||\n+\t\tapic->lapic_timer.period == 0)\n \tremaining = hrtimer_get_remaining(&apic->lapic_timer.timer);",
    "critical_vars": [
      "apic"
    ],
    "variable_definitions": {
      "apic": "struct kvm_lapic *apic"
    },
    "variable_types": {
      "apic": "struct pointer"
    },
    "type_mapping": {
      "apic": "struct pointer"
    },
    "vulnerable_line": "if (kvm_apic_get_reg(apic, APIC_TMICT) == 0 || apic->lapic_timer.period == 0)",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Division by Zero",
    "reasoning": "The check for zero in the lapic_timer.period variable leads to a division by zero in the subsequent computation if it is not properly validated, causing a denial of service."
  },
  {
    "fix_code": "static u32 apic_get_tmcct(struct kvm_lapic *apic)\n{\n\tktime_t remaining;\n\ts64 ns;\n\tu32 tmcct;\n\n\tASSERT(apic != NULL);\n\n\t/* if initial count is 0, current count should also be 0 */\n\tif (kvm_apic_get_reg(apic, APIC_TMICT) == 0 ||\n\t\tapic->lapic_timer.period == 0)\n\t\treturn 0;\n\n\tremaining = hrtimer_get_remaining(&apic->lapic_timer.timer);\n\tif (ktime_to_ns(remaining) < 0)\n\t\tremaining = ktime_set(0, 0);\n\n\tns = mod_64(ktime_to_ns(remaining), apic->lapic_timer.period);\n\ttmcct = div64_u64(ns,\n\t\t\t (APIC_BUS_CYCLE_NS * apic->divide_count));\n\n\treturn tmcct;\n}",
    "diff": "@@ -841,7 +841,8 @@ static u32 apic_get_tmcct(struct kvm_lapic *apic)\n \tASSERT(apic != NULL);\n-\tif (kvm_apic_get_reg(apic, APIC_TMICT) == 0)\n+\tif (kvm_apic_get_reg(apic, APIC_TMICT) == 0 ||\n+\t\tapic->lapic_timer.period == 0)\n \tremaining = hrtimer_get_remaining(&apic->lapic_timer.timer);",
    "critical_vars": [
      "apic->lapic_timer.period",
      "apic"
    ],
    "variable_definitions": {
      "apic->lapic_timer.period": "Definition not found",
      "apic": "struct kvm_lapic *apic"
    },
    "variable_types": {
      "apic->lapic_timer.period": "struct pointer_integer",
      "apic": "struct pointer"
    },
    "type_mapping": {
      "apic->lapic_timer.period": "sp_integer",
      "apic": "struct pointer"
    },
    "vulnerable_line": "if (kvm_apic_get_reg(apic, APIC_TMICT) == 0 || apic->lapic_timer.period == 0)",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Division by Zero",
    "reasoning": "The original check did not account for the scenario when apic->lapic_timer.period is zero, leading to a divide-by-zero error when calculating tmcct."
  },
  {
    "fix_code": "void oz_hcd_get_desc_cnf(void *hport, u8 req_id, u8 status, const u8 *desc,\n\t\t\tu8 length, u16 offset, u16 total_size)\n{\n\tstruct oz_port *port = hport;\n\tstruct urb *urb;\n\tint err = 0;\n\n\toz_dbg(ON, \"oz_hcd_get_desc_cnf length = %d offs = %d tot_size = %d\\n\",\n\t       length, offset, total_size);\n\turb = oz_find_urb_by_id(port, 0, req_id);\n\tif (!urb)\n\t\treturn;\n\tif (status == 0) {\n\t\tunsigned int copy_len;\n\t\tunsigned int required_size = urb->transfer_buffer_length;\n\n\t\tif (required_size > total_size)\n\t\t\trequired_size = total_size;\n\t\tcopy_len = required_size-offset;\n\t\tif (length <= copy_len)\n\t\t\tcopy_len = length;\n\t\tmemcpy(urb->transfer_buffer+offset, desc, copy_len);\n\t\toffset += copy_len;\n\t\tif (offset < required_size) {\n\t\t\tstruct usb_ctrlrequest *setup =\n\t\t\t\t(struct usb_ctrlrequest *)urb->setup_packet;\n\t\t\tunsigned wvalue = le16_to_cpu(setup->wValue);\n\n\t\t\tif (oz_enqueue_ep_urb(port, 0, 0, urb, req_id))\n\t\t\t\terr = -ENOMEM;\n\t\t\telse if (oz_usb_get_desc_req(port->hpd, req_id,\n\t\t\t\t\tsetup->bRequestType, (u8)(wvalue>>8),\n\t\t\t\t\t(u8)wvalue, setup->wIndex, offset,\n\t\t\t\t\trequired_size-offset)) {\n\t\t\t\toz_dequeue_ep_urb(port, 0, 0, urb);\n\t\t\t\terr = -ENOMEM;\n\t\t\t}\n\t\t\tif (err == 0)\n\t\t\t\treturn;\n\t\t}\n\t}\n\turb->actual_length = total_size;\n\toz_complete_urb(port->ozhcd->hcd, urb, 0);\n}",
    "diff": "-\t\tint copy_len;\n+\t\tunsigned int copy_len;",
    "critical_vars": [
      "copy_len"
    ],
    "variable_definitions": {
      "copy_len": "unsigned int copy_len;"
    },
    "variable_types": {
      "copy_len": "integer"
    },
    "type_mapping": {
      "copy_len": "Integer"
    },
    "vulnerable_line": "copy_len = required_size-offset;",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Underflow",
    "reasoning": "If 'offset' is greater than 'required_size', 'copy_len' results in a negative value, leading to risky memory access during memcpy."
  },
  {
    "fix_code": "static int\ni915_gem_do_execbuffer(struct drm_device *dev, void *data,\n\t\t       struct drm_file *file,\n\t\t       struct drm_i915_gem_execbuffer2 *args,\n\t\t       struct drm_i915_gem_exec_object2 *exec)\n{\n\tdrm_i915_private_t *dev_priv = dev->dev_private;\n\tstruct list_head objects;\n\tstruct eb_objects *eb;\n\tstruct drm_i915_gem_object *batch_obj;\n\tstruct drm_clip_rect *cliprects = NULL;\n\tstruct intel_ring_buffer *ring;\n\tu32 exec_start, exec_len;\n\tu32 seqno;\n\tu32 mask;\n\tint ret, mode, i;\n\n\tif (!i915_gem_check_execbuffer(args)) {\n\t\tDRM_DEBUG(\"execbuf with invalid offset/length\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tret = validate_exec_list(exec, args->buffer_count);\n\tif (ret)\n\t\treturn ret;\n\n\tswitch (args->flags & I915_EXEC_RING_MASK) {\n\tcase I915_EXEC_DEFAULT:\n\tcase I915_EXEC_RENDER:\n\t\tring = &dev_priv->ring[RCS];\n\t\tbreak;\n\tcase I915_EXEC_BSD:\n\t\tif (!HAS_BSD(dev)) {\n\t\t\tDRM_DEBUG(\"execbuf with invalid ring (BSD)\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tring = &dev_priv->ring[VCS];\n\t\tbreak;\n\tcase I915_EXEC_BLT:\n\t\tif (!HAS_BLT(dev)) {\n\t\t\tDRM_DEBUG(\"execbuf with invalid ring (BLT)\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tring = &dev_priv->ring[BCS];\n\t\tbreak;\n\tdefault:\n\t\tDRM_DEBUG(\"execbuf with unknown ring: %d\\n\",\n\t\t\t  (int)(args->flags & I915_EXEC_RING_MASK));\n\t\treturn -EINVAL;\n\t}\n\n\tmode = args->flags & I915_EXEC_CONSTANTS_MASK;\n\tmask = I915_EXEC_CONSTANTS_MASK;\n\tswitch (mode) {\n\tcase I915_EXEC_CONSTANTS_REL_GENERAL:\n\tcase I915_EXEC_CONSTANTS_ABSOLUTE:\n\tcase I915_EXEC_CONSTANTS_REL_SURFACE:\n\t\tif (ring == &dev_priv->ring[RCS] &&\n\t\t    mode != dev_priv->relative_constants_mode) {\n\t\t\tif (INTEL_INFO(dev)->gen < 4)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tif (INTEL_INFO(dev)->gen > 5 &&\n\t\t\t    mode == I915_EXEC_CONSTANTS_REL_SURFACE)\n\t\t\t\treturn -EINVAL;\n\n\t\t\t/* The HW changed the meaning on this bit on gen6 */\n\t\t\tif (INTEL_INFO(dev)->gen >= 6)\n\t\t\t\tmask &= ~I915_EXEC_CONSTANTS_REL_SURFACE;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tDRM_DEBUG(\"execbuf with unknown constants: %d\\n\", mode);\n\t\treturn -EINVAL;\n\t}\n\n\tif (args->buffer_count < 1) {\n\t\tDRM_DEBUG(\"execbuf with %d buffers\\n\", args->buffer_count);\n\t\treturn -EINVAL;\n\t}\n\n\tif (args->num_cliprects != 0) {\n\t\tif (ring != &dev_priv->ring[RCS]) {\n\t\t\tDRM_DEBUG(\"clip rectangles are only valid with the render ring\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (args->num_cliprects > UINT_MAX / sizeof(*cliprects)) {\n\t\t\tDRM_DEBUG(\"execbuf with %u cliprects\\n\",\n\t\t\t\t  args->num_cliprects);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tcliprects = kmalloc(args->num_cliprects * sizeof(*cliprects),\n\t\t\t\t    GFP_KERNEL);\n\t\tif (cliprects == NULL) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto pre_mutex_err;\n\t\t}\n\n\t\tif (copy_from_user(cliprects,\n\t\t\t\t     (struct drm_clip_rect __user *)(uintptr_t)\n\t\t\t\t     args->cliprects_ptr,\n\t\t\t\t     sizeof(*cliprects)*args->num_cliprects)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto pre_mutex_err;\n\t\t}\n\t}\n\n\tret = i915_mutex_lock_interruptible(dev);\n\tif (ret)\n\t\tgoto pre_mutex_err;\n\n\tif (dev_priv->mm.suspended) {\n\t\tmutex_unlock(&dev->struct_mutex);\n\t\tret = -EBUSY;\n\t\tgoto pre_mutex_err;\n\t}\n\n\teb = eb_create(args->buffer_count);\n\tif (eb == NULL) {\n\t\tmutex_unlock(&dev->struct_mutex);\n\t\tret = -ENOMEM;\n\t\tgoto pre_mutex_err;\n\t}\n\n\t/* Look up object handles */\n\tINIT_LIST_HEAD(&objects);\n\tfor (i = 0; i < args->buffer_count; i++) {\n\t\tstruct drm_i915_gem_object *obj;\n\n\t\tobj = to_intel_bo(drm_gem_object_lookup(dev, file,\n\t\t\t\t\t\t\texec[i].handle));\n\t\tif (&obj->base == NULL) {\n\t\t\tDRM_DEBUG(\"Invalid object handle %d at index %d\\n\",\n\t\t\t\t   exec[i].handle, i);\n\t\t\t/* prevent error path from reading uninitialized data */\n\t\t\tret = -ENOENT;\n\t\t\tgoto err;\n\t\t}\n\n\t\tif (!list_empty(&obj->exec_list)) {\n\t\t\tDRM_DEBUG(\"Object %p [handle %d, index %d] appears more than once in object list\\n\",\n\t\t\t\t   obj, exec[i].handle, i);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err;\n\t\t}\n\n\t\tlist_add_tail(&obj->exec_list, &objects);\n\t\tobj->exec_handle = exec[i].handle;\n\t\tobj->exec_entry = &exec[i];\n\t\teb_add_object(eb, obj);\n\t}\n\n\t/* take note of the batch buffer before we might reorder the lists */\n\tbatch_obj = list_entry(objects.prev,\n\t\t\t       struct drm_i915_gem_object,\n\t\t\t       exec_list);\n\n\t/* Move the objects en-masse into the GTT, evicting if necessary. */\n\tret = i915_gem_execbuffer_reserve(ring, file, &objects);\n\tif (ret)\n\t\tgoto err;\n\n\t/* The objects are in their final locations, apply the relocations. */\n\tret = i915_gem_execbuffer_relocate(dev, eb, &objects);\n\tif (ret) {\n\t\tif (ret == -EFAULT) {\n\t\t\tret = i915_gem_execbuffer_relocate_slow(dev, file, ring,\n\t\t\t\t\t\t\t\t&objects, eb,\n\t\t\t\t\t\t\t\texec,\n\t\t\t\t\t\t\t\targs->buffer_count);\n\t\t\tBUG_ON(!mutex_is_locked(&dev->struct_mutex));\n\t\t}\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\t/* Set the pending read domains for the batch buffer to COMMAND */\n\tif (batch_obj->base.pending_write_domain) {\n\t\tDRM_DEBUG(\"Attempting to use self-modifying batch buffer\\n\");\n\t\tret = -EINVAL;\n\t\tgoto err;\n\t}\n\tbatch_obj->base.pending_read_domains |= I915_GEM_DOMAIN_COMMAND;\n\n\tret = i915_gem_execbuffer_move_to_gpu(ring, &objects);\n\tif (ret)\n\t\tgoto err;\n\n\tseqno = i915_gem_next_request_seqno(ring);\n\tfor (i = 0; i < ARRAY_SIZE(ring->sync_seqno); i++) {\n\t\tif (seqno < ring->sync_seqno[i]) {\n\t\t\t/* The GPU can not handle its semaphore value wrapping,\n\t\t\t * so every billion or so execbuffers, we need to stall\n\t\t\t * the GPU in order to reset the counters.\n\t\t\t */\n\t\t\tret = i915_gpu_idle(dev, true);\n\t\t\tif (ret)\n\t\t\t\tgoto err;\n\n\t\t\tBUG_ON(ring->sync_seqno[i]);\n\t\t}\n\t}\n\n\tif (ring == &dev_priv->ring[RCS] &&\n\t    mode != dev_priv->relative_constants_mode) {\n\t\tret = intel_ring_begin(ring, 4);\n\t\tif (ret)\n\t\t\t\tgoto err;\n\n\t\tintel_ring_emit(ring, MI_NOOP);\n\t\tintel_ring_emit(ring, MI_LOAD_REGISTER_IMM(1));\n\t\tintel_ring_emit(ring, INSTPM);\n\t\tintel_ring_emit(ring, mask << 16 | mode);\n\t\tintel_ring_advance(ring);\n\n\t\tdev_priv->relative_constants_mode = mode;\n\t}\n\n\tif (args->flags & I915_EXEC_GEN7_SOL_RESET) {\n\t\tret = i915_reset_gen7_sol_offsets(dev, ring);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\ttrace_i915_gem_ring_dispatch(ring, seqno);\n\n\texec_start = batch_obj->gtt_offset + args->batch_start_offset;\n\texec_len = args->batch_len;\n\tif (cliprects) {\n\t\tfor (i = 0; i < args->num_cliprects; i++) {\n\t\t\tret = i915_emit_box(dev, &cliprects[i],\n\t\t\t\t\t    args->DR1, args->DR4);\n\t\t\tif (ret)\n\t\t\t\tgoto err;\n\n\t\t\tret = ring->dispatch_execbuffer(ring,\n\t\t\t\t\t\t\texec_start, exec_len);\n\t\t\tif (ret)\n\t\t\t\tgoto err;\n\t\t}\n\t} else {\n\t\tret = ring->dispatch_execbuffer(ring, exec_start, exec_len);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\ti915_gem_execbuffer_move_to_active(&objects, ring, seqno);\n\ti915_gem_execbuffer_retire_commands(dev, file, ring);\n\nerr:\n\teb_destroy(eb);\n\twhile (!list_empty(&objects)) {\n\t\tstruct drm_i915_gem_object *obj;\n\n\t\tobj = list_first_entry(&objects,\n\t\t\t\t       struct drm_i915_gem_object,\n\t\t\t\t       exec_list);\n\t\tlist_del_init(&obj->exec_list);\n\t\tdrm_gem_object_unreference(&obj->base);\n\t}\n\n\tmutex_unlock(&dev->struct_mutex);\n\npre_mutex_err:\n\tkfree(cliprects);\n\treturn ret;\n}",
    "diff": "+\t\tif (args->num_cliprects > UINT_MAX / sizeof(*cliprects)) {\n+\t\t\t\t  args->num_cliprects);\n \t\tcliprects = kmalloc(args->num_cliprects * sizeof(*cliprects),",
    "critical_vars": [
      "args->num_cliprects"
    ],
    "variable_definitions": {
      "args->num_cliprects": "Definition not found"
    },
    "variable_types": {
      "args->num_cliprects": "struct pointer_integer"
    },
    "type_mapping": {
      "args->num_cliprects": "sp_integer"
    },
    "vulnerable_line": "if (args->num_cliprects > UINT_MAX / sizeof(*cliprects)) {",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The comparison allows for a potential integer overflow when calculating the size of allocated memory for cliprects, leading to an out-of-bounds write if args->num_cliprects is large enough."
  },
  {
    "fix_code": "static void sw_perf_event_destroy(struct perf_event *event)\n{\n\tu64 event_id = event->attr.config;\n\n\tWARN_ON(event->parent);\n\n\tstatic_key_slow_dec(&perf_swevent_enabled[event_id]);\n\tswevent_hlist_put(event);\n}",
    "diff": "-\tint event_id = event->attr.config;\n+\tu64 event_id = event->attr.config;",
    "critical_vars": [
      "event_id"
    ],
    "variable_definitions": {
      "event_id": "u64 event_id = event->attr.config;"
    },
    "variable_types": {
      "event_id": "integer"
    },
    "type_mapping": {
      "event_id": "Integer"
    },
    "vulnerable_line": "int event_id = event->attr.config;",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Signed/Unsigned Conversion",
    "reasoning": "event_id, defined as an integer, can inadvertently treat large unsigned values from event->attr.config as negative when converted, leading to privilege escalation vulnerabilities."
  },
  {
    "fix_code": "static int ext4_fill_flex_info(struct super_block *sb)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_group_desc *gdp = NULL;\n\text4_group_t flex_group_count;\n\text4_group_t flex_group;\n\tunsigned int groups_per_flex = 0;\n\tsize_t size;\n\tint i;\n\n\tsbi->s_log_groups_per_flex = sbi->s_es->s_log_groups_per_flex;\n\tif (sbi->s_log_groups_per_flex < 1 || sbi->s_log_groups_per_flex > 31) {\n\t\tsbi->s_log_groups_per_flex = 0;\n\t\treturn 1;\n\t}\n\tgroups_per_flex = 1 << sbi->s_log_groups_per_flex;\n\n\t/* We allocate both existing and potentially added groups */\n\tflex_group_count = ((sbi->s_groups_count + groups_per_flex - 1) +\n\t\t\t((le16_to_cpu(sbi->s_es->s_reserved_gdt_blocks) + 1) <<\n\t\t\t      EXT4_DESC_PER_BLOCK_BITS(sb))) / groups_per_flex;\n\tsize = flex_group_count * sizeof(struct flex_groups);\n\tsbi->s_flex_groups = ext4_kvzalloc(size, GFP_KERNEL);\n\tif (sbi->s_flex_groups == NULL) {\n\t\text4_msg(sb, KERN_ERR, \"not enough memory for %u flex groups\",\n\t\t\t flex_group_count);\n\t\tgoto failed;\n\t}\n\n\tfor (i = 0; i < sbi->s_groups_count; i++) {\n\t\tgdp = ext4_get_group_desc(sb, i, NULL);\n\n\t\tflex_group = ext4_flex_group(sbi, i);\n\t\tatomic_add(ext4_free_inodes_count(sb, gdp),\n\t\t\t   &sbi->s_flex_groups[flex_group].free_inodes);\n\t\tatomic_add(ext4_free_group_clusters(sb, gdp),\n\t\t\t   &sbi->s_flex_groups[flex_group].free_clusters);\n\t\tatomic_add(ext4_used_dirs_count(sb, gdp),\n\t\t\t   &sbi->s_flex_groups[flex_group].used_dirs);\n\t}\n\n\treturn 1;\nfailed:\n\treturn 0;\n}",
    "diff": "-\tint groups_per_flex = 0;\n+\tunsigned int groups_per_flex = 0;\n \tsbi->s_log_groups_per_flex = sbi->s_es->s_log_groups_per_flex;\n-\tgroups_per_flex = 1 << sbi->s_log_groups_per_flex;\n-\tif (groups_per_flex < 2) {\n+\tif (sbi->s_log_groups_per_flex < 1 || sbi->s_log_groups_per_flex > 31) {\n \t\tsbi->s_log_groups_per_flex = 0;\n+\tgroups_per_flex = 1 << sbi->s_log_groups_per_flex;\n \tflex_group_count = ((sbi->s_groups_count + groups_per_flex - 1) +",
    "critical_vars": [
      "groups_per_flex"
    ],
    "variable_definitions": {
      "groups_per_flex": "unsigned int groups_per_flex = 0;"
    },
    "variable_types": {
      "groups_per_flex": "integer"
    },
    "type_mapping": {
      "groups_per_flex": "Integer"
    },
    "vulnerable_line": "flex_group_count = ((sbi->s_groups_count + groups_per_flex - 1) + ((le16_to_cpu(sbi->s_es->s_reserved_gdt_blocks) + 1) << EXT4_DESC_PER_BLOCK_BITS(sb))) / groups_per_flex;",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The calculation for flex_group_count can overflow if sbi->s_groups_count and groups_per_flex are large enough, especially since groups_per_flex is derived from a left shift operation on sbi->s_log_groups_per_flex. This creates the risk of incorrect memory allocation or access, leading to a denial of service."
  },
  {
    "fix_code": "static int ext4_fill_flex_info(struct super_block *sb)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_group_desc *gdp = NULL;\n\text4_group_t flex_group_count;\n\text4_group_t flex_group;\n\tunsigned int groups_per_flex = 0;\n\tsize_t size;\n\tint i;\n\n\tsbi->s_log_groups_per_flex = sbi->s_es->s_log_groups_per_flex;\n\tif (sbi->s_log_groups_per_flex < 1 || sbi->s_log_groups_per_flex > 31) {\n\t\tsbi->s_log_groups_per_flex = 0;\n\t\treturn 1;\n\t}\n\tgroups_per_flex = 1 << sbi->s_log_groups_per_flex;\n\n\t/* We allocate both existing and potentially added groups */\n\tflex_group_count = ((sbi->s_groups_count + groups_per_flex - 1) +\n\t\t\t((le16_to_cpu(sbi->s_es->s_reserved_gdt_blocks) + 1) <<\n\t\t\t      EXT4_DESC_PER_BLOCK_BITS(sb))) / groups_per_flex;\n\tsize = flex_group_count * sizeof(struct flex_groups);\n\tsbi->s_flex_groups = ext4_kvzalloc(size, GFP_KERNEL);\n\tif (sbi->s_flex_groups == NULL) {\n\t\text4_msg(sb, KERN_ERR, \"not enough memory for %u flex groups\",\n\t\t\t flex_group_count);\n\t\tgoto failed;\n\t}\n\n\tfor (i = 0; i < sbi->s_groups_count; i++) {\n\t\tgdp = ext4_get_group_desc(sb, i, NULL);\n\n\t\tflex_group = ext4_flex_group(sbi, i);\n\t\tatomic_add(ext4_free_inodes_count(sb, gdp),\n\t\t\t   &sbi->s_flex_groups[flex_group].free_inodes);\n\t\tatomic_add(ext4_free_group_clusters(sb, gdp),\n\t\t\t   &sbi->s_flex_groups[flex_group].free_clusters);\n\t\tatomic_add(ext4_used_dirs_count(sb, gdp),\n\t\t\t   &sbi->s_flex_groups[flex_group].used_dirs);\n\t}\n\n\treturn 1;\nfailed:\n\treturn 0;\n}",
    "diff": "-\tint groups_per_flex = 0;\n+\tunsigned int groups_per_flex = 0;\n \tsbi->s_log_groups_per_flex = sbi->s_es->s_log_groups_per_flex;\n-\tgroups_per_flex = 1 << sbi->s_log_groups_per_flex;\n-\tif (groups_per_flex < 2) {\n+\tif (sbi->s_log_groups_per_flex < 1 || sbi->s_log_groups_per_flex > 31) {\n \t\tsbi->s_log_groups_per_flex = 0;\n+\tgroups_per_flex = 1 << sbi->s_log_groups_per_flex;\n \tflex_group_count = ((sbi->s_groups_count + groups_per_flex - 1) +",
    "critical_vars": [
      "groups_per_flex"
    ],
    "variable_definitions": {
      "groups_per_flex": "int groups_per_flex = 0;"
    },
    "variable_types": {
      "groups_per_flex": "integer"
    },
    "type_mapping": {
      "groups_per_flex": "Integer"
    },
    "vulnerable_line": "flex_group_count = ((sbi->s_groups_count + groups_per_flex - 1) + ((le16_to_cpu(sbi->s_es->s_reserved_gdt_blocks) + 1) << EXT4_DESC_PER_BLOCK_BITS(sb))) / groups_per_flex;",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The formula for flex_group_count can lead to an overflow when sbi->s_groups_count and groups_per_flex are sufficiently large, which could result in incorrect allocation and potential denial of service."
  },
  {
    "fix_code": "static void recalculate_apic_map(struct kvm *kvm)\n{\n\tstruct kvm_apic_map *new, *old = NULL;\n\tstruct kvm_vcpu *vcpu;\n\tint i;\n\n\tnew = kzalloc(sizeof(struct kvm_apic_map), GFP_KERNEL);\n\n\tmutex_lock(&kvm->arch.apic_map_lock);\n\n\tif (!new)\n\t\tgoto out;\n\n\tnew->ldr_bits = 8;\n\t/* flat mode is default */\n\tnew->cid_shift = 8;\n\tnew->cid_mask = 0;\n\tnew->lid_mask = 0xff;\n\n\tkvm_for_each_vcpu(i, vcpu, kvm) {\n\t\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\t\tu16 cid, lid;\n\t\tu32 ldr;\n\n\t\tif (!kvm_apic_present(vcpu))\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * All APICs have to be configured in the same mode by an OS.\n\t\t * We take advatage of this while building logical id loockup\n\t\t * table. After reset APICs are in xapic/flat mode, so if we\n\t\t * find apic with different setting we assume this is the mode\n\t\t * OS wants all apics to be in; build lookup table accordingly.\n\t\t */\n\t\tif (apic_x2apic_mode(apic)) {\n\t\t\tnew->ldr_bits = 32;\n\t\t\tnew->cid_shift = 16;\n\t\t\tnew->cid_mask = (1 << KVM_X2APIC_CID_BITS) - 1;\n\t\t\tnew->lid_mask = 0xffff;\n\t\t} else if (kvm_apic_sw_enabled(apic) &&\n\t\t\t\t!new->cid_mask /* flat mode */ &&\n\t\t\t\tkvm_apic_get_reg(apic, APIC_DFR) == APIC_DFR_CLUSTER) {\n\t\t\tnew->cid_shift = 4;\n\t\t\tnew->cid_mask = 0xf;\n\t\t\tnew->lid_mask = 0xf;\n\t\t}\n\n\t\tnew->phys_map[kvm_apic_id(apic)] = apic;\n\n\t\tldr = kvm_apic_get_reg(apic, APIC_LDR);\n\t\tcid = apic_cluster_id(new, ldr);\n\t\tlid = apic_logical_id(new, ldr);\n\n\t\tif (lid)\n\t\t\tnew->logical_map[cid][ffs(lid) - 1] = apic;\n\t}\nout:\n\told = rcu_dereference_protected(kvm->arch.apic_map,\n\t\t\tlockdep_is_held(&kvm->arch.apic_map_lock));\n\trcu_assign_pointer(kvm->arch.apic_map, new);\n\tmutex_unlock(&kvm->arch.apic_map_lock);\n\n\tif (old)\n\t\tkfree_rcu(old, rcu);\n\n\tkvm_vcpu_request_scan_ioapic(kvm);\n}",
    "diff": "-\t\t\tnew->cid_mask = new->lid_mask = 0xffff;\n+\t\t\tnew->cid_mask = (1 << KVM_X2APIC_CID_BITS) - 1;\n \t\t\t\t!new->cid_mask /* flat mode */ &&",
    "critical_vars": [
      "new->cid_mask"
    ],
    "variable_definitions": {
      "new->cid_mask": "Definition not found"
    },
    "variable_types": {
      "new->cid_mask": "struct pointer_integer"
    },
    "type_mapping": {
      "new->cid_mask": "sp_integer"
    },
    "vulnerable_line": "new->cid_mask = (1 << KVM_X2APIC_CID_BITS) - 1;",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The expression (1 << KVM_X2APIC_CID_BITS) can lead to an overflow if KVM_X2APIC_CID_BITS is set to a value greater than or equal to 32, causing unexpected behavior in mask calculations."
  },
  {
    "fix_code": "static struct vm_area_struct *vma_to_resize(unsigned long addr,\n\tunsigned long old_len, unsigned long new_len, unsigned long *p)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma = find_vma(mm, addr);\n\n\tif (!vma || vma->vm_start > addr)\n\t\tgoto Efault;\n\n\tif (is_vm_hugetlb_page(vma))\n\t\tgoto Einval;\n\n\t/* We can't remap across vm area boundaries */\n\tif (old_len > vma->vm_end - addr)\n\t\tgoto Efault;\n\n\t/* Need to be careful about a growing mapping */\n\tif (new_len > old_len) {\n\t\tunsigned long pgoff;\n\n\t\tif (vma->vm_flags & (VM_DONTEXPAND | VM_PFNMAP))\n\t\t\tgoto Efault;\n\t\tpgoff = (addr - vma->vm_start) >> PAGE_SHIFT;\n\t\tpgoff += vma->vm_pgoff;\n\t\tif (pgoff + (new_len >> PAGE_SHIFT) < pgoff)\n\t\t\tgoto Einval;\n\t}\n\n\tif (vma->vm_flags & VM_LOCKED) {\n\t\tunsigned long locked, lock_limit;\n\t\tlocked = mm->locked_vm << PAGE_SHIFT;\n\t\tlock_limit = rlimit(RLIMIT_MEMLOCK);\n\t\tlocked += new_len - old_len;\n\t\tif (locked > lock_limit && !capable(CAP_IPC_LOCK))\n\t\t\tgoto Eagain;\n\t}\n\n\tif (!may_expand_vm(mm, (new_len - old_len) >> PAGE_SHIFT))\n\t\tgoto Enomem;\n\n\tif (vma->vm_flags & VM_ACCOUNT) {\n\t\tunsigned long charged = (new_len - old_len) >> PAGE_SHIFT;\n\t\tif (security_vm_enough_memory(charged))\n\t\t\tgoto Efault;\n\t\t*p = charged;\n\t}\n\n\treturn vma;\n\nEfault:\t/* very odd choice for most of the cases, but... */\n\treturn ERR_PTR(-EFAULT);\nEinval:\n\treturn ERR_PTR(-EINVAL);\nEnomem:\n\treturn ERR_PTR(-ENOMEM);\nEagain:\n\treturn ERR_PTR(-EAGAIN);\n}\n\nstatic unsigned long mremap_to(unsigned long addr,\n\tunsigned long old_len, unsigned long new_addr,\n\tunsigned long new_len)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long ret = -EINVAL;\n\tunsigned long charged = 0;\n\tunsigned long map_flags;\n\n\tif (new_addr & ~PAGE_MASK)\n\t\tgoto out;\n\n\tif (new_len > TASK_SIZE || new_addr > TASK_SIZE - new_len)\n\t\tgoto out;\n\n\t/* Check if the location we're moving into overlaps the\n\t * old location at all, and fail if it does.\n\t */\n\tif ((new_addr <= addr) && (new_addr+new_len) > addr)\n\t\tgoto out;\n\n\tif ((addr <= new_addr) && (addr+old_len) > new_addr)\n\t\tgoto out;\n\n\tret = security_file_mmap(NULL, 0, 0, 0, new_addr, 1);\n\tif (ret)\n\t\tgoto out;\n\n\tret = do_munmap(mm, new_addr, new_len);\n\tif (ret)\n\t\tgoto out;\n\n\tif (old_len >= new_len) {\n\t\tret = do_munmap(mm, addr+new_len, old_len - new_len);\n\t\tif (ret && old_len != new_len)\n\t\t\tgoto out;\n\t\told_len = new_len;\n\t}\n\n\tvma = vma_to_resize(addr, old_len, new_len, &charged);\n\tif (IS_ERR(vma)) {\n\t\tret = PTR_ERR(vma);\n\t\tgoto out;\n\t}\n\n\tmap_flags = MAP_FIXED;\n\tif (vma->vm_flags & VM_MAYSHARE)\n\t\tmap_flags |= MAP_SHARED;\n\n\tret = get_unmapped_area(vma->vm_file, new_addr, new_len, vma->vm_pgoff +\n\t\t\t\t((addr - vma->vm_start) >> PAGE_SHIFT),\n\t\t\t\tmap_flags);\n\tif (ret & ~PAGE_MASK)\n\t\tgoto out1;\n\n\tret = move_vma(vma, addr, old_len, new_len, new_addr);\n\tif (!(ret & ~PAGE_MASK))\n\t\tgoto out;\nout1:\n\tvm_unacct_memory(charged);\n\nout:\n\treturn ret;\n}",
    "diff": "-\tif (vma->vm_flags & (VM_DONTEXPAND | VM_PFNMAP)) {\n+\t\tif (vma->vm_flags & (VM_DONTEXPAND | VM_PFNMAP))\n \tif (vma->vm_flags & VM_LOCKED) {",
    "critical_vars": [
      "vma->vm_flags"
    ],
    "variable_definitions": {
      "vma->vm_flags": "Definition not found"
    },
    "variable_types": {
      "vma->vm_flags": "struct pointer_integer"
    },
    "type_mapping": {
      "vma->vm_flags": "sp_integer"
    },
    "vulnerable_line": "if (new_len > old_len)",
    "critical_variable_in_vulnerable_line": false,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "If new_len is significantly larger than old_len, an integer overflow can occur when computing pgoff + (new_len >> PAGE_SHIFT), potentially causing invalid memory access or system crash."
  },
  {
    "fix_code": "static struct vm_area_struct *vma_to_resize(unsigned long addr,\n\tunsigned long old_len, unsigned long new_len, unsigned long *p)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma = find_vma(mm, addr);\n\n\tif (!vma || vma->vm_start > addr)\n\t\tgoto Efault;\n\n\tif (is_vm_hugetlb_page(vma))\n\t\tgoto Einval;\n\n\t/* We can't remap across vm area boundaries */\n\tif (old_len > vma->vm_end - addr)\n\t\tgoto Efault;\n\n\t/* Need to be careful about a growing mapping */\n\tif (new_len > old_len) {\n\t\tunsigned long pgoff;\n\n\t\tif (vma->vm_flags & (VM_DONTEXPAND | VM_PFNMAP))\n\t\t\tgoto Efault;\n\t\tpgoff = (addr - vma->vm_start) >> PAGE_SHIFT;\n\t\tpgoff += vma->vm_pgoff;\n\t\tif (pgoff + (new_len >> PAGE_SHIFT) < pgoff)\n\t\t\tgoto Einval;\n\t}\n\n\tif (vma->vm_flags & VM_LOCKED) {\n\t\tunsigned long locked, lock_limit;\n\t\tlocked = mm->locked_vm << PAGE_SHIFT;\n\t\tlock_limit = rlimit(RLIMIT_MEMLOCK);\n\t\tlocked += new_len - old_len;\n\t\tif (locked > lock_limit && !capable(CAP_IPC_LOCK))\n\t\t\tgoto Eagain;\n\t}\n\n\tif (!may_expand_vm(mm, (new_len - old_len) >> PAGE_SHIFT))\n\t\tgoto Enomem;\n\n\tif (vma->vm_flags & VM_ACCOUNT) {\n\t\tunsigned long charged = (new_len - old_len) >> PAGE_SHIFT;\n\t\tif (security_vm_enough_memory(charged))\n\t\t\tgoto Efault;\n\t\t*p = charged;\n\t}\n\n\treturn vma;\n\nEfault:\t/* very odd choice for most of the cases, but... */\n\treturn ERR_PTR(-EFAULT);\nEinval:\n\treturn ERR_PTR(-EINVAL);\nEnomem:\n\treturn ERR_PTR(-ENOMEM);\nEagain:\n\treturn ERR_PTR(-EAGAIN);\n}\n\nstatic unsigned long mremap_to(unsigned long addr,\n\tunsigned long old_len, unsigned long new_addr,\n\tunsigned long new_len)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long ret = -EINVAL;\n\tunsigned long charged = 0;\n\tunsigned long map_flags;\n\n\tif (new_addr & ~PAGE_MASK)\n\t\tgoto out;\n\n\tif (new_len > TASK_SIZE || new_addr > TASK_SIZE - new_len)\n\t\tgoto out;\n\n\t/* Check if the location we're moving into overlaps the\n\t * old location at all, and fail if it does.\n\t */\n\tif ((new_addr <= addr) && (new_addr+new_len) > addr)\n\t\tgoto out;\n\n\tif ((addr <= new_addr) && (addr+old_len) > new_addr)\n\t\tgoto out;\n\n\tret = security_file_mmap(NULL, 0, 0, 0, new_addr, 1);\n\tif (ret)\n\t\tgoto out;\n\n\tret = do_munmap(mm, new_addr, new_len);\n\tif (ret)\n\t\tgoto out;\n\n\tif (old_len >= new_len) {\n\t\tret = do_munmap(mm, addr+new_len, old_len - new_len);\n\t\tif (ret && old_len != new_len)\n\t\t\tgoto out;\n\t\told_len = new_len;\n\t}\n\n\tvma = vma_to_resize(addr, old_len, new_len, &charged);\n\tif (IS_ERR(vma)) {\n\t\tret = PTR_ERR(vma);\n\t\tgoto out;\n\t}\n\n\tmap_flags = MAP_FIXED;\n\tif (vma->vm_flags & VM_MAYSHARE)\n\t\tmap_flags |= MAP_SHARED;\n\n\tret = get_unmapped_area(vma->vm_file, new_addr, new_len, vma->vm_pgoff +\n\t\t\t\t((addr - vma->vm_start) >> PAGE_SHIFT),\n\t\t\t\tmap_flags);\n\tif (ret & ~PAGE_MASK)\n\t\tgoto out1;\n\n\tret = move_vma(vma, addr, old_len, new_len, new_addr);\n\tif (!(ret & ~PAGE_MASK))\n\t\tgoto out;\nout1:\n\tvm_unacct_memory(charged);\n\nout:\n\treturn ret;\n}",
    "diff": "-\t\tif (new_len > old_len)\n+\tif (new_len > old_len) {\n+\t\tif (pgoff + (new_len >> PAGE_SHIFT) < pgoff)",
    "critical_vars": [
      "new_len"
    ],
    "variable_definitions": {
      "new_len": "unsigned long new_len"
    },
    "variable_types": {
      "new_len": "integer"
    },
    "type_mapping": {
      "new_len": "Integer"
    },
    "vulnerable_line": "if (new_len > old_len) {",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The comparison between new_len and old_len can lead to an integer overflow if new_len is significantly larger than old_len, allowing a crafted mremap system call to cause a denial of service through memory expansion."
  },
  {
    "fix_code": "struct xt_table_info *xt_alloc_table_info(unsigned int size)\n{\n\tstruct xt_table_info *info = NULL;\n\tsize_t sz = sizeof(*info) + size;\n\n\tif (sz < sizeof(*info))\n\t\treturn NULL;\n\n\t/* Pedantry: prevent them from hitting BUG() in vmalloc.c --RR */\n\tif ((SMP_ALIGN(size) >> PAGE_SHIFT) + 2 > totalram_pages)\n\t\treturn NULL;\n\n\tif (sz <= (PAGE_SIZE << PAGE_ALLOC_COSTLY_ORDER))\n\t\tinfo = kmalloc(sz, GFP_KERNEL | __GFP_NOWARN | __GFP_NORETRY);\n\tif (!info) {\n\t\tinfo = vmalloc(sz);\n\t\tif (!info)\n\t\t\treturn NULL;\n\t}\n\tmemset(info, 0, sizeof(*info));\n\tinfo->size = size;\n\treturn info;\n}",
    "diff": " \tsize_t sz = sizeof(*info) + size;\n+\tif (sz < sizeof(*info))",
    "critical_vars": [
      "sz"
    ],
    "variable_definitions": {
      "sz": "size_t sz = sizeof(*info) + size;"
    },
    "variable_types": {
      "sz": "integer"
    },
    "type_mapping": {
      "sz": "Integer"
    },
    "vulnerable_line": "size_t sz = sizeof(*info) + size;",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The addition of sizeof(*info) and size can result in an overflow if size is large enough, causing sz to become smaller than sizeof(*info), leading to potential heap memory corruption."
  },
  {
    "fix_code": "static void oz_usb_handle_ep_data(struct oz_usb_ctx *usb_ctx,\n\tstruct oz_usb_hdr *usb_hdr, int len)\n{\n\tstruct oz_data *data_hdr = (struct oz_data *)usb_hdr;\n\n\tswitch (data_hdr->format) {\n\tcase OZ_DATA_F_MULTIPLE_FIXED: {\n\t\t\tstruct oz_multiple_fixed *body =\n\t\t\t\t(struct oz_multiple_fixed *)data_hdr;\n\t\t\tu8 *data = body->data;\n\t\t\tint n;\n\t\t\tif (!body->unit_size)\n\t\t\t\tbreak;\n\t\t\tn = (len - sizeof(struct oz_multiple_fixed)+1)\n\t\t\t\t/ body->unit_size;\n\t\t\twhile (n--) {\n\t\t\t\toz_hcd_data_ind(usb_ctx->hport, body->endpoint,\n\t\t\t\t\tdata, body->unit_size);\n\t\t\t\tdata += body->unit_size;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tcase OZ_DATA_F_ISOC_FIXED: {\n\t\t\tstruct oz_isoc_fixed *body =\n\t\t\t\t(struct oz_isoc_fixed *)data_hdr;\n\t\t\tint data_len = len-sizeof(struct oz_isoc_fixed)+1;\n\t\t\tint unit_size = body->unit_size;\n\t\t\tu8 *data = body->data;\n\t\t\tint count;\n\t\t\tint i;\n\n\t\t\tif (!unit_size)\n\t\t\t\tbreak;\n\t\t\tcount = data_len/unit_size;\n\t\t\tfor (i = 0; i < count; i++) {\n\t\t\t\toz_hcd_data_ind(usb_ctx->hport,\n\t\t\t\t\tbody->endpoint, data, unit_size);\n\t\t\t\tdata += unit_size;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\t}\n\n}",
    "diff": "@@ -326,7 +326,10 @@ static void oz_usb_handle_ep_data(struct oz_usb_ctx *usb_ctx,\n-\t\t\tint n = (len - sizeof(struct oz_multiple_fixed)+1)\n+\t\t\tint n;\n+\t\t\tif (!body->unit_size)\n+\t\t\tn = (len - sizeof(struct oz_multiple_fixed)+1)\n \t\t\t\t/ body->unit_size;\n \t\t\twhile (n--) {\n \t\t\t\toz_hcd_data_ind(usb_ctx->hport, body->endpoint,",
    "critical_vars": [
      "n"
    ],
    "variable_definitions": {
      "n": "int n = (len - sizeof(struct oz_multiple_fixed)+1) / body->unit_size;"
    },
    "variable_types": {
      "n": "integer"
    },
    "type_mapping": {
      "n": "Integer"
    },
    "vulnerable_line": "n = (len - sizeof(struct oz_multiple_fixed)+1) / body->unit_size;",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Division by Zero",
    "reasoning": "The division by body->unit_size occurs without checking if body->unit_size is zero, leading to a potential divide-by-zero error if unit_size is zero, causing a denial of service."
  },
  {
    "fix_code": "static void oz_usb_handle_ep_data(struct oz_usb_ctx *usb_ctx,\n\tstruct oz_usb_hdr *usb_hdr, int len)\n{\n\tstruct oz_data *data_hdr = (struct oz_data *)usb_hdr;\n\n\tswitch (data_hdr->format) {\n\tcase OZ_DATA_F_MULTIPLE_FIXED: {\n\t\t\tstruct oz_multiple_fixed *body =\n\t\t\t\t(struct oz_multiple_fixed *)data_hdr;\n\t\t\tu8 *data = body->data;\n\t\t\tint n;\n\t\t\tif (!body->unit_size)\n\t\t\t\tbreak;\n\t\t\tn = (len - sizeof(struct oz_multiple_fixed)+1)\n\t\t\t\t/ body->unit_size;\n\t\t\twhile (n--) {\n\t\t\t\toz_hcd_data_ind(usb_ctx->hport, body->endpoint,\n\t\t\t\t\tdata, body->unit_size);\n\t\t\t\tdata += body->unit_size;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tcase OZ_DATA_F_ISOC_FIXED: {\n\t\t\tstruct oz_isoc_fixed *body =\n\t\t\t\t(struct oz_isoc_fixed *)data_hdr;\n\t\t\tint data_len = len-sizeof(struct oz_isoc_fixed)+1;\n\t\t\tint unit_size = body->unit_size;\n\t\t\tu8 *data = body->data;\n\t\t\tint count;\n\t\t\tint i;\n\n\t\t\tif (!unit_size)\n\t\t\t\tbreak;\n\t\t\tcount = data_len/unit_size;\n\t\t\tfor (i = 0; i < count; i++) {\n\t\t\t\toz_hcd_data_ind(usb_ctx->hport,\n\t\t\t\t\tbody->endpoint, data, unit_size);\n\t\t\t\tdata += unit_size;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\t}\n\n}",
    "diff": "@@ -326,7 +326,10 @@ static void oz_usb_handle_ep_data(struct oz_usb_ctx *usb_ctx,\n-\t\t\tint n = (len - sizeof(struct oz_multiple_fixed)+1)\n+\t\t\tint n;\n+\t\t\tif (!body->unit_size)\n+\t\t\tn = (len - sizeof(struct oz_multiple_fixed)+1)\n \t\t\t\t/ body->unit_size;\n \t\t\twhile (n--) {\n \t\t\t\toz_hcd_data_ind(usb_ctx->hport, body->endpoint,",
    "critical_vars": [
      "n"
    ],
    "variable_definitions": {
      "n": "int n;"
    },
    "variable_types": {
      "n": "integer"
    },
    "type_mapping": {
      "n": "Integer"
    },
    "vulnerable_line": "n = (len - sizeof(struct oz_multiple_fixed)+1) / body->unit_size;",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Division by Zero",
    "reasoning": "If body->unit_size is zero, it leads to a division by zero, causing a system crash."
  },
  {
    "fix_code": "static int chk_mem_check(void*       mem,\n                         size_t*     allocated,\n                         const char* func)\n{\n    char*  buffer;\n    size_t offset, bytes;\n    int    i;\n    char*  buf;\n\n    /* first check the bytes in the sentinel header */\n    buf = (char*)mem - CHK_SENTINEL_HEAD_SIZE;\n    for (i=0 ; i<CHK_SENTINEL_HEAD_SIZE ; i++) {\n        if (buf[i] != CHK_SENTINEL_VALUE) {\n            assert_log_message(\n                \"*** %s CHECK: buffer %p \"\n                \"corrupted %d bytes before allocation\",\n                func, mem, CHK_SENTINEL_HEAD_SIZE-i);\n            return -1;\n        }\n    }\n\n    /* then the ones in the sentinel trailer */\n    buffer = (char*)mem - CHK_SENTINEL_HEAD_SIZE;\n    offset = dlmalloc_usable_size(buffer) - sizeof(size_t);\n    bytes  = *(size_t *)(buffer + offset);\n\n    buf = (char*)mem + bytes;\n    for (i=CHK_SENTINEL_TAIL_SIZE-1 ; i>=0 ; i--) {\n        if (buf[i] != CHK_SENTINEL_VALUE) {\n            assert_log_message(\n                \"*** %s CHECK: buffer %p, size=%lu, \"\n                \"corrupted %d bytes after allocation\",\n                func, buffer, bytes, i+1);\n            return -1;\n        }\n    }\n\n    *allocated = bytes;\n    return 0;\n}",
    "diff": "-    char* buffer = (char*)dlmalloc(bytes + CHK_OVERHEAD_SIZE);\n+    uint8_t* buffer = (uint8_t*) dlmalloc(size);\n     if (buffer) {\n         memset(buffer, CHK_SENTINEL_VALUE, bytes + CHK_OVERHEAD_SIZE);\n         size_t offset = dlmalloc_usable_size(buffer) - sizeof(size_t);",
    "critical_vars": [
      "buffer"
    ],
    "variable_definitions": {
      "buffer": "char*  buffer;"
    },
    "variable_types": {
      "buffer": "char pointer"
    },
    "type_mapping": {
      "buffer": "char pointer"
    },
    "vulnerable_line": "if (size < bytes) { // Overflow.",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The calculation of 'size' can overflow the 'bytes' variable, leading to a situation where insufficient memory is allocated (i.e., less than expected), allowing for potential memory-related attacks."
  },
  {
    "fix_code": "void* leak_malloc(size_t bytes)\n{\n    // allocate enough space infront of the allocation to store the pointer for\n    // the alloc structure. This will making free'ing the structer really fast!\n\n    // 1. allocate enough memory and include our header\n    // 2. set the base pointer to be right after our header\n\n    size_t size = bytes + sizeof(AllocationEntry);\n    if (size < bytes) { // Overflow.\n        return NULL;\n    }\n\n    void* base = dlmalloc(size);\n    if (base != NULL) {\n        pthread_mutex_lock(&gAllocationsMutex);\n\n            intptr_t backtrace[BACKTRACE_SIZE];\n            size_t numEntries = get_backtrace(backtrace, BACKTRACE_SIZE);\n\n            AllocationEntry* header = (AllocationEntry*)base;\n            header->entry = record_backtrace(backtrace, numEntries, bytes);\n            header->guard = GUARD;\n\n            // now increment base to point to after our header.\n            // this should just work since our header is 8 bytes.\n            base = (AllocationEntry*)base + 1;\n\n        pthread_mutex_unlock(&gAllocationsMutex);\n    }\n\n    return base;\n}\n\nvoid leak_free(void* mem)\n{\n    if (mem != NULL) {\n        pthread_mutex_lock(&gAllocationsMutex);\n\n        // check the guard to make sure it is valid\n        AllocationEntry* header = (AllocationEntry*)mem - 1;\n\n        if (header->guard != GUARD) {\n            // could be a memaligned block\n            if (((void**)mem)[-1] == MEMALIGN_GUARD) {\n                mem = ((void**)mem)[-2];\n                header = (AllocationEntry*)mem - 1;\n            }\n        }\n\n        if (header->guard == GUARD || is_valid_entry(header->entry)) {\n            // decrement the allocations\n            HashEntry* entry = header->entry;\n            entry->allocations--;\n            if (entry->allocations <= 0) {\n                remove_entry(entry);\n                dlfree(entry);\n            }\n\n            // now free the memory!\n            dlfree(header);\n        } else {\n            debug_log(\"WARNING bad header guard: '0x%x'! and invalid entry: %p\\n\",\n                    header->guard, header->entry);\n        }\n\n        pthread_mutex_unlock(&gAllocationsMutex);\n    }\n}\n\nvoid* leak_calloc(size_t n_elements, size_t elem_size)\n{\n    size_t  size;\n    void*   ptr;\n\n    /* Fail on overflow - just to be safe even though this code runs only\n     * within the debugging C library, not the production one */\n    if (n_elements && MAX_SIZE_T / n_elements < elem_size) {\n        return NULL;\n    }\n    size = n_elements * elem_size;\n    ptr  = leak_malloc(size);\n    if (ptr != NULL) {\n        memset(ptr, 0, size);\n    }\n    return ptr;\n}\n\nvoid* leak_realloc(void* oldMem, size_t bytes)\n{\n    if (oldMem == NULL) {\n        return leak_malloc(bytes);\n    }\n    void* newMem = NULL;\n    AllocationEntry* header = (AllocationEntry*)oldMem - 1;\n    if (header && header->guard == GUARD) {\n        size_t oldSize = header->entry->size & ~SIZE_FLAG_MASK;\n        newMem = leak_malloc(bytes);\n        if (newMem != NULL) {\n            size_t copySize = (oldSize <= bytes) ? oldSize : bytes;\n            memcpy(newMem, oldMem, copySize);\n            leak_free(oldMem);\n        }\n    } else {\n        newMem = dlrealloc(oldMem, bytes);\n    }\n    return newMem;\n}\n\nvoid* leak_memalign(size_t alignment, size_t bytes)\n{\n    // we can just use malloc\n    if (alignment <= MALLOC_ALIGNMENT)\n        return leak_malloc(bytes);\n\n    // need to make sure it's a power of two\n    if (alignment & (alignment-1))\n        alignment = 1L << (31 - __builtin_clz(alignment));\n\n    // here, aligment is at least MALLOC_ALIGNMENT<<1 bytes\n    // we will align by at least MALLOC_ALIGNMENT bytes\n    // and at most alignment-MALLOC_ALIGNMENT bytes\n    size_t size = (alignment-MALLOC_ALIGNMENT) + bytes;\n    if (size < bytes) { // Overflow.\n        return NULL;\n    }\n\n    void* base = leak_malloc(size);\n    if (base != NULL) {\n        intptr_t ptr = (intptr_t)base;\n        if ((ptr % alignment) == 0)\n            return base;\n\n        // align the pointer\n        ptr += ((-ptr) % alignment);\n\n        // there is always enough space for the base pointer and the guard\n        ((void**)ptr)[-1] = MEMALIGN_GUARD;\n        ((void**)ptr)[-2] = base;\n\n        return (void*)ptr;\n    }\n    return base;\n}",
    "diff": "     // 2. set the base pointer to be right after our header\n-    void* base = dlmalloc(bytes + sizeof(AllocationEntry));\n+    void* base = dlmalloc(size);\n     if (base != NULL) {\n     void* base = leak_malloc(size);\n     if (base != NULL) {\n         intptr_t ptr = (intptr_t)base;",
    "critical_vars": [
      "base"
    ],
    "variable_definitions": {
      "base": "void* base = dlmalloc(bytes + sizeof(AllocationEntry));"
    },
    "variable_types": {
      "base": "unknown"
    },
    "type_mapping": {
      "base": "unknown"
    },
    "vulnerable_line": "if (size < bytes) { // Overflow.",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "This line checks for overflow by comparing 'size' with 'bytes'; if 'size' (result of an arithmetic operation) is less than 'bytes', it indicates an overflow occurred, leading to a potential allocation of less memory than intended."
  },
  {
    "fix_code": "void* leak_malloc(size_t bytes)\n{\n    // allocate enough space infront of the allocation to store the pointer for\n    // the alloc structure. This will making free'ing the structer really fast!\n\n    // 1. allocate enough memory and include our header\n    // 2. set the base pointer to be right after our header\n\n    size_t size = bytes + sizeof(AllocationEntry);\n    if (size < bytes) { // Overflow.\n        return NULL;\n    }\n\n    void* base = dlmalloc(size);\n    if (base != NULL) {\n        pthread_mutex_lock(&gAllocationsMutex);\n\n            intptr_t backtrace[BACKTRACE_SIZE];\n            size_t numEntries = get_backtrace(backtrace, BACKTRACE_SIZE);\n\n            AllocationEntry* header = (AllocationEntry*)base;\n            header->entry = record_backtrace(backtrace, numEntries, bytes);\n            header->guard = GUARD;\n\n            // now increment base to point to after our header.\n            // this should just work since our header is 8 bytes.\n            base = (AllocationEntry*)base + 1;\n\n        pthread_mutex_unlock(&gAllocationsMutex);\n    }\n\n    return base;\n}\n\nvoid leak_free(void* mem)\n{\n    if (mem != NULL) {\n        pthread_mutex_lock(&gAllocationsMutex);\n\n        // check the guard to make sure it is valid\n        AllocationEntry* header = (AllocationEntry*)mem - 1;\n\n        if (header->guard != GUARD) {\n            // could be a memaligned block\n            if (((void**)mem)[-1] == MEMALIGN_GUARD) {\n                mem = ((void**)mem)[-2];\n                header = (AllocationEntry*)mem - 1;\n            }\n        }\n\n        if (header->guard == GUARD || is_valid_entry(header->entry)) {\n            // decrement the allocations\n            HashEntry* entry = header->entry;\n            entry->allocations--;\n            if (entry->allocations <= 0) {\n                remove_entry(entry);\n                dlfree(entry);\n            }\n\n            // now free the memory!\n            dlfree(header);\n        } else {\n            debug_log(\"WARNING bad header guard: '0x%x'! and invalid entry: %p\\n\",\n                    header->guard, header->entry);\n        }\n\n        pthread_mutex_unlock(&gAllocationsMutex);\n    }\n}\n\nvoid* leak_calloc(size_t n_elements, size_t elem_size)\n{\n    size_t  size;\n    void*   ptr;\n\n    /* Fail on overflow - just to be safe even though this code runs only\n     * within the debugging C library, not the production one */\n    if (n_elements && MAX_SIZE_T / n_elements < elem_size) {\n        return NULL;\n    }\n    size = n_elements * elem_size;\n    ptr  = leak_malloc(size);\n    if (ptr != NULL) {\n        memset(ptr, 0, size);\n    }\n    return ptr;\n}\n\nvoid* leak_realloc(void* oldMem, size_t bytes)\n{\n    if (oldMem == NULL) {\n        return leak_malloc(bytes);\n    }\n    void* newMem = NULL;\n    AllocationEntry* header = (AllocationEntry*)oldMem - 1;\n    if (header && header->guard == GUARD) {\n        size_t oldSize = header->entry->size & ~SIZE_FLAG_MASK;\n        newMem = leak_malloc(bytes);\n        if (newMem != NULL) {\n            size_t copySize = (oldSize <= bytes) ? oldSize : bytes;\n            memcpy(newMem, oldMem, copySize);\n            leak_free(oldMem);\n        }\n    } else {\n        newMem = dlrealloc(oldMem, bytes);\n    }\n    return newMem;\n}\n\nvoid* leak_memalign(size_t alignment, size_t bytes)\n{\n    // we can just use malloc\n    if (alignment <= MALLOC_ALIGNMENT)\n        return leak_malloc(bytes);\n\n    // need to make sure it's a power of two\n    if (alignment & (alignment-1))\n        alignment = 1L << (31 - __builtin_clz(alignment));\n\n    // here, aligment is at least MALLOC_ALIGNMENT<<1 bytes\n    // we will align by at least MALLOC_ALIGNMENT bytes\n    // and at most alignment-MALLOC_ALIGNMENT bytes\n    size_t size = (alignment-MALLOC_ALIGNMENT) + bytes;\n    if (size < bytes) { // Overflow.\n        return NULL;\n    }\n\n    void* base = leak_malloc(size);\n    if (base != NULL) {\n        intptr_t ptr = (intptr_t)base;\n        if ((ptr % alignment) == 0)\n            return base;\n\n        // align the pointer\n        ptr += ((-ptr) % alignment);\n\n        // there is always enough space for the base pointer and the guard\n        ((void**)ptr)[-1] = MEMALIGN_GUARD;\n        ((void**)ptr)[-2] = base;\n\n        return (void*)ptr;\n    }\n    return base;\n}",
    "diff": " void* chk_malloc(size_t bytes)\n+    size_t size = bytes + CHK_OVERHEAD_SIZE;\n+    if (size < bytes) { // Overflow.\n+    uint8_t* buffer = (uint8_t*) dlmalloc(size);\n         size_t offset = dlmalloc_usable_size(buffer) - sizeof(size_t);\n@@ -505,7 +509,12 @@ void* leak_malloc(size_t bytes)\n-    void* base = dlmalloc(bytes + sizeof(AllocationEntry));\n+    size_t size = bytes + sizeof(AllocationEntry);\n+    if (size < bytes) { // Overflow.\n+    void* base = dlmalloc(size);\n@@ -615,6 +624,10 @@ void* leak_memalign(size_t alignment, size_t bytes)\n     size_t size = (alignment-MALLOC_ALIGNMENT) + bytes;\n+    if (size < bytes) { // Overflow.\n     void* base = leak_malloc(size);",
    "critical_vars": [
      "size"
    ],
    "variable_definitions": {
      "size": "size_t size = bytes + sizeof(AllocationEntry);"
    },
    "variable_types": {
      "size": "integer"
    },
    "type_mapping": {
      "size": "Integer"
    },
    "vulnerable_line": "if (size < bytes) { // Overflow.",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The calculation of 'size' can result in an overflow when 'bytes' is a large number, causing 'size' to be less than 'bytes', leading to insufficient memory allocation and potential buffer overflows."
  },
  {
    "fix_code": "void* leak_memalign(size_t alignment, size_t bytes)\n{\n    // we can just use malloc\n    if (alignment <= MALLOC_ALIGNMENT)\n        return leak_malloc(bytes);\n\n    // need to make sure it's a power of two\n    if (alignment & (alignment-1))\n        alignment = 1L << (31 - __builtin_clz(alignment));\n\n    // here, aligment is at least MALLOC_ALIGNMENT<<1 bytes\n    // we will align by at least MALLOC_ALIGNMENT bytes\n    // and at most alignment-MALLOC_ALIGNMENT bytes\n    size_t size = (alignment-MALLOC_ALIGNMENT) + bytes;\n    if (size < bytes) { // Overflow.\n        return NULL;\n    }\n\n    void* base = leak_malloc(size);\n    if (base != NULL) {\n        intptr_t ptr = (intptr_t)base;\n        if ((ptr % alignment) == 0)\n            return base;\n\n        // align the pointer\n        ptr += ((-ptr) % alignment);\n\n        // there is always enough space for the base pointer and the guard\n        ((void**)ptr)[-1] = MEMALIGN_GUARD;\n        ((void**)ptr)[-2] = base;\n\n        return (void*)ptr;\n    }\n    return base;\n}\n\n/* Initializes malloc debugging framework.\n * See comments on MallocDebugInit in malloc_debug_common.h\n */\nint malloc_debug_initialize(void)\n{\n    // We don't really have anything that requires initialization here.\n    return 0;\n}",
    "diff": " void* chk_malloc(size_t bytes)\n+    size_t size = bytes + CHK_OVERHEAD_SIZE;\n+    if (size < bytes) { // Overflow.\n+    uint8_t* buffer = (uint8_t*) dlmalloc(size);\n         size_t offset = dlmalloc_usable_size(buffer) - sizeof(size_t);\n@@ -505,7 +509,12 @@ void* leak_malloc(size_t bytes)\n-    void* base = dlmalloc(bytes + sizeof(AllocationEntry));\n+    size_t size = bytes + sizeof(AllocationEntry);\n+    if (size < bytes) { // Overflow.\n+    void* base = dlmalloc(size);\n@@ -615,6 +624,10 @@ void* leak_memalign(size_t alignment, size_t bytes)\n     size_t size = (alignment-MALLOC_ALIGNMENT) + bytes;\n+    if (size < bytes) { // Overflow.\n     void* base = leak_malloc(size);",
    "critical_vars": [
      "size"
    ],
    "variable_definitions": {
      "size": "size_t size = (alignment-MALLOC_ALIGNMENT) + bytes;"
    },
    "variable_types": {
      "size": "integer"
    },
    "type_mapping": {
      "size": "Integer"
    },
    "vulnerable_line": "if (size < bytes) { // Overflow.",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The check size < bytes is intended to prevent overflow; however, if bytes is large enough, the sum can wrap around, causing incorrect memory allocation, which can lead to further issues such as buffer overflows."
  },
  {
    "fix_code": "PHPAPI zend_string *php_escape_shell_cmd(char *str)\n{\n\tregister int x, y, l = (int)strlen(str);\n\tsize_t estimate = (2 * l) + 1;\n\tzend_string *cmd;\n#ifndef PHP_WIN32\n\tchar *p = NULL;\n#endif\n\n\n\tcmd = zend_string_safe_alloc(2, l, 0, 0);\n\n\tfor (x = 0, y = 0; x < l; x++) {\n\t\tint mb_len = php_mblen(str + x, (l - x));\n\n\t\t/* skip non-valid multibyte characters */\n\t\tif (mb_len < 0) {\n\t\t\tcontinue;\n\t\t} else if (mb_len > 1) {\n\t\t\tmemcpy(ZSTR_VAL(cmd) + y, str + x, mb_len);\n\t\t\ty += mb_len;\n\t\t\tx += mb_len - 1;\n\t\t\tcontinue;\n\t\t}\n\n\t\tswitch (str[x]) {\n#ifndef PHP_WIN32\n\t\t\tcase '\"':\n\t\t\tcase '\\'':\n\t\t\t\tif (!p && (p = memchr(str + x + 1, str[x], l - x - 1))) {\n\t\t\t\t\t/* noop */\n\t\t\t\t} else if (p && *p == str[x]) {\n\t\t\t\t\tp = NULL;\n\t\t\t\t} else {\n\t\t\t\t\tZSTR_VAL(cmd)[y++] = '\\\\';\n\t\t\t\t}\n\t\t\t\tZSTR_VAL(cmd)[y++] = str[x];\n\t\t\t\tbreak;\n#else\n\t\t\t/* % is Windows specific for environmental variables, ^%PATH% will \n\t\t\t\toutput PATH while ^%PATH^% will not. escapeshellcmd->val will escape all % and !.\n\t\t\t*/\n\t\t\tcase '%':\n\t\t\tcase '!':\n\t\t\tcase '\"':\n\t\t\tcase '\\'':\n#endif\n\t\t\tcase '#': /* This is character-set independent */\n\t\t\tcase '&':\n\t\t\tcase ';':\n\t\t\tcase '`':\n\t\t\tcase '|':\n\t\t\tcase '*':\n\t\t\tcase '?':\n\t\t\tcase '~':\n\t\t\tcase '<':\n\t\t\tcase '>':\n\t\t\tcase '^':\n\t\t\tcase '(':\n\t\t\tcase ')':\n\t\t\tcase '[':\n\t\t\tcase ']':\n\t\t\tcase '{':\n\t\t\tcase '}':\n\t\t\tcase '$':\n\t\t\tcase '\\\\':\n\t\t\tcase '\\x0A': /* excluding these two */\n\t\t\tcase '\\xFF':\n#ifdef PHP_WIN32\n\t\t\t\tZSTR_VAL(cmd)[y++] = '^';\n#else\n\t\t\t\tZSTR_VAL(cmd)[y++] = '\\\\';\n#endif\n\t\t\t\t/* fall-through */\n\t\t\tdefault:\n\t\t\t\tZSTR_VAL(cmd)[y++] = str[x];\n\n\t\t}\n\t}\n\tZSTR_VAL(cmd)[y] = '\\0';\n\n\tif ((estimate - y) > 4096) {\n\t\t/* realloc if the estimate was way overill\n\t\t * Arbitrary cutoff point of 4096 */\n\t\tcmd = zend_string_truncate(cmd, y, 0);\n\t}\n\n\tZSTR_LEN(cmd) = y;\n\n\treturn cmd;\n}",
    "diff": "@@ -253,7 +253,7 @@ PHPAPI zend_string *php_escape_shell_cmd(char *str)\n-\tcmd = zend_string_alloc(2 * l, 0);\n+\tcmd = zend_string_safe_alloc(2, l, 0, 0);\n-\tcmd = zend_string_alloc(4 * l + 2, 0); /* worst case */\n+\tcmd = zend_string_safe_alloc(4, l, 2, 0); /* worst case */\n \tZSTR_VAL(cmd)[y++] = '\"';",
    "critical_vars": [
      "cmd"
    ],
    "variable_definitions": {
      "cmd": "zend_string *cmd;"
    },
    "variable_types": {
      "cmd": "struct pointer"
    },
    "type_mapping": {
      "cmd": "struct pointer"
    },
    "vulnerable_line": "cmd = zend_string_alloc(4 * l + 2, 0); /* worst case */",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The calculation (4 * l + 2) can overflow if 'l' is sufficiently large, leading to a potentially insufficient allocation for 'cmd', allowing for exploitation via heap-based buffer overflow."
  },
  {
    "fix_code": "PHPAPI zend_string *php_escape_shell_arg(char *str)\n{\n\tint x, y = 0, l = (int)strlen(str);\n\tzend_string *cmd;\n\tsize_t estimate = (4 * l) + 3;\n\n\n\tcmd = zend_string_safe_alloc(4, l, 2, 0); /* worst case */\n\n#ifdef PHP_WIN32\n\tZSTR_VAL(cmd)[y++] = '\"';\n#else\n\tZSTR_VAL(cmd)[y++] = '\\'';\n#endif\n\n\tfor (x = 0; x < l; x++) {\n\t\tint mb_len = php_mblen(str + x, (l - x));\n\n\t\t/* skip non-valid multibyte characters */\n\t\tif (mb_len < 0) {\n\t\t\tcontinue;\n\t\t} else if (mb_len > 1) {\n\t\t\tmemcpy(ZSTR_VAL(cmd) + y, str + x, mb_len);\n\t\t\ty += mb_len;\n\t\t\tx += mb_len - 1;\n\t\t\tcontinue;\n\t\t}\n\n\t\tswitch (str[x]) {\n#ifdef PHP_WIN32\n\t\tcase '\"':\n\t\tcase '%':\n\t\tcase '!':\n\t\t\tZSTR_VAL(cmd)[y++] = ' ';\n\t\t\tbreak;\n#else\n\t\tcase '\\'':\n\t\t\tZSTR_VAL(cmd)[y++] = '\\'';\n\t\t\tZSTR_VAL(cmd)[y++] = '\\\\';\n\t\t\tZSTR_VAL(cmd)[y++] = '\\'';\n#endif\n\t\t\t/* fall-through */\n\t\tdefault:\n\t\t\tZSTR_VAL(cmd)[y++] = str[x];\n\t\t}\n\t}\n#ifdef PHP_WIN32\n\tif (y > 0 && '\\\\' == ZSTR_VAL(cmd)[y - 1]) {\n\t\tint k = 0, n = y - 1;\n\t\tfor (; n >= 0 && '\\\\' == ZSTR_VAL(cmd)[n]; n--, k++);\n\t\tif (k % 2) {\n\t\t\tZSTR_VAL(cmd)[y++] = '\\\\';\n\t\t}\n\t}\n\n\tZSTR_VAL(cmd)[y++] = '\"';\n#else\n\tZSTR_VAL(cmd)[y++] = '\\'';\n#endif\n\tZSTR_VAL(cmd)[y] = '\\0';\n\n\tif ((estimate - y) > 4096) {\n\t\t/* realloc if the estimate was way overill\n\t\t * Arbitrary cutoff point of 4096 */\n\t\tcmd = zend_string_truncate(cmd, y, 0);\n\t}\n\tZSTR_LEN(cmd) = y;\n\treturn cmd;\n}",
    "diff": "@@ -253,7 +253,7 @@ PHPAPI zend_string *php_escape_shell_cmd(char *str)\n-\tcmd = zend_string_alloc(2 * l, 0);\n+\tcmd = zend_string_safe_alloc(2, l, 0, 0);\n-\tcmd = zend_string_alloc(4 * l + 2, 0); /* worst case */\n+\tcmd = zend_string_safe_alloc(4, l, 2, 0); /* worst case */\n \tZSTR_VAL(cmd)[y++] = '\"';",
    "critical_vars": [
      "cmd"
    ],
    "variable_definitions": {
      "cmd": "zend_string *cmd;"
    },
    "variable_types": {
      "cmd": "struct pointer"
    },
    "type_mapping": {
      "cmd": "struct pointer"
    },
    "vulnerable_line": "cmd = zend_string_alloc(4 * l + 2, 0); /* worst case */",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The calculation (4 * l + 2) can overflow if l is large enough, which may allocate insufficient memory for cmd, leading to a heap-based buffer overflow."
  },
  {
    "fix_code": "int\ncdf_read_property_info(const cdf_stream_t *sst, const cdf_header_t *h,\n    uint32_t offs, cdf_property_info_t **info, size_t *count, size_t *maxcount)\n{\n\tconst cdf_section_header_t *shp;\n\tcdf_section_header_t sh;\n\tconst uint8_t *p, *q, *e;\n\tint16_t s16;\n\tint32_t s32;\n\tuint32_t u32;\n\tint64_t s64;\n\tuint64_t u64;\n\tcdf_timestamp_t tp;\n\tsize_t i, o, o4, nelements, j;\n\tcdf_property_info_t *inp;\n\n\tif (offs > UINT32_MAX / 4) {\n\t\terrno = EFTYPE;\n\t\tgoto out;\n\t}\n\tshp = CAST(const cdf_section_header_t *, (const void *)\n\t    ((const char *)sst->sst_tab + offs));\n\tif (cdf_check_stream_offset(sst, h, shp, sizeof(*shp), __LINE__) == -1)\n\t\tgoto out;\n\tsh.sh_len = CDF_TOLE4(shp->sh_len);\n#define CDF_SHLEN_LIMIT (UINT32_MAX / 8)\n\tif (sh.sh_len > CDF_SHLEN_LIMIT) {\n\t\terrno = EFTYPE;\n\t\tgoto out;\n\t}\n\tsh.sh_properties = CDF_TOLE4(shp->sh_properties);\n#define CDF_PROP_LIMIT (UINT32_MAX / (4 * sizeof(*inp)))\n\tif (sh.sh_properties > CDF_PROP_LIMIT)\n\t\tgoto out;\n\tDPRINTF((\"section len: %u properties %u\\n\", sh.sh_len,\n\t    sh.sh_properties));\n\tif (*maxcount) {\n\t\tif (*maxcount > CDF_PROP_LIMIT)\n\t\t\tgoto out;\n\t\t*maxcount += sh.sh_properties;\n\t\tinp = CAST(cdf_property_info_t *,\n\t\t    realloc(*info, *maxcount * sizeof(*inp)));\n\t} else {\n\t\t*maxcount = sh.sh_properties;\n\t\tinp = CAST(cdf_property_info_t *,\n\t\t    malloc(*maxcount * sizeof(*inp)));\n\t}\n\tif (inp == NULL)\n\t\tgoto out;\n\t*info = inp;\n\tinp += *count;\n\t*count += sh.sh_properties;\n\tp = CAST(const uint8_t *, (const void *)\n\t    ((const char *)(const void *)sst->sst_tab +\n\t    offs + sizeof(sh)));\n\te = CAST(const uint8_t *, (const void *)\n\t    (((const char *)(const void *)shp) + sh.sh_len));\n\tif (cdf_check_stream_offset(sst, h, e, 0, __LINE__) == -1)\n\t\tgoto out;\n\tfor (i = 0; i < sh.sh_properties; i++) {\n\t\tsize_t ofs, tail = (i << 1) + 1;\n\t\tif (cdf_check_stream_offset(sst, h, p, tail * sizeof(uint32_t),\n\t\t    __LINE__) == -1)\n\t\t\tgoto out;\n\t\tofs = CDF_GETUINT32(p, tail);\n\t\tq = (const uint8_t *)(const void *)\n\t\t    ((const char *)(const void *)p + ofs\n\t\t    - 2 * sizeof(uint32_t));\n\t\tif (q < p || q > e) {\n\t\t\tDPRINTF((\"Ran of the end %p > %p\\n\", q, e));\n\t\t\tgoto out;\n\t\t}\n\t\tinp[i].pi_id = CDF_GETUINT32(p, i << 1);\n\t\tinp[i].pi_type = CDF_GETUINT32(q, 0);\n\t\tDPRINTF((\"%\" SIZE_T_FORMAT \"u) id=%x type=%x offs=0x%tx,0x%x\\n\",\n\t\t    i, inp[i].pi_id, inp[i].pi_type, q - p, offs));\n\t\tif (inp[i].pi_type & CDF_VECTOR) {\n\t\t\tnelements = CDF_GETUINT32(q, 1);\n\t\t\tif (nelements == 0) {\n\t\t\t\tDPRINTF((\"CDF_VECTOR with nelements == 0\\n\"));\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\to = 2;\n\t\t} else {\n\t\t\tnelements = 1;\n\t\t\to = 1;\n\t\t}\n\t\to4 = o * sizeof(uint32_t);\n\t\tif (inp[i].pi_type & (CDF_ARRAY|CDF_BYREF|CDF_RESERVED))\n\t\t\tgoto unknown;\n\t\tswitch (inp[i].pi_type & CDF_TYPEMASK) {\n\t\tcase CDF_NULL:\n\t\tcase CDF_EMPTY:\n\t\t\tbreak;\n\t\tcase CDF_SIGNED16:\n\t\t\tif (inp[i].pi_type & CDF_VECTOR)\n\t\t\t\tgoto unknown;\n\t\t\t(void)memcpy(&s16, &q[o4], sizeof(s16));\n\t\t\tinp[i].pi_s16 = CDF_TOLE2(s16);\n\t\t\tbreak;\n\t\tcase CDF_SIGNED32:\n\t\t\tif (inp[i].pi_type & CDF_VECTOR)\n\t\t\t\tgoto unknown;\n\t\t\t(void)memcpy(&s32, &q[o4], sizeof(s32));\n\t\t\tinp[i].pi_s32 = CDF_TOLE4((uint32_t)s32);\n\t\t\tbreak;\n\t\tcase CDF_BOOL:\n\t\tcase CDF_UNSIGNED32:\n\t\t\tif (inp[i].pi_type & CDF_VECTOR)\n\t\t\t\tgoto unknown;\n\t\t\t(void)memcpy(&u32, &q[o4], sizeof(u32));\n\t\t\tinp[i].pi_u32 = CDF_TOLE4(u32);\n\t\t\tbreak;\n\t\tcase CDF_SIGNED64:\n\t\t\tif (inp[i].pi_type & CDF_VECTOR)\n\t\t\t\tgoto unknown;\n\t\t\t(void)memcpy(&s64, &q[o4], sizeof(s64));\n\t\t\tinp[i].pi_s64 = CDF_TOLE8((uint64_t)s64);\n\t\t\tbreak;\n\t\tcase CDF_UNSIGNED64:\n\t\t\tif (inp[i].pi_type & CDF_VECTOR)\n\t\t\t\tgoto unknown;\n\t\t\t(void)memcpy(&u64, &q[o4], sizeof(u64));\n\t\t\tinp[i].pi_u64 = CDF_TOLE8((uint64_t)u64);\n\t\t\tbreak;\n\t\tcase CDF_FLOAT:\n\t\t\tif (inp[i].pi_type & CDF_VECTOR)\n\t\t\t\tgoto unknown;\n\t\t\t(void)memcpy(&u32, &q[o4], sizeof(u32));\n\t\t\tu32 = CDF_TOLE4(u32);\n\t\t\tmemcpy(&inp[i].pi_f, &u32, sizeof(inp[i].pi_f));\n\t\t\tbreak;\n\t\tcase CDF_DOUBLE:\n\t\t\tif (inp[i].pi_type & CDF_VECTOR)\n\t\t\t\tgoto unknown;\n\t\t\t(void)memcpy(&u64, &q[o4], sizeof(u64));\n\t\t\tu64 = CDF_TOLE8((uint64_t)u64);\n\t\t\tmemcpy(&inp[i].pi_d, &u64, sizeof(inp[i].pi_d));\n\t\t\tbreak;\n\t\tcase CDF_LENGTH32_STRING:\n\t\tcase CDF_LENGTH32_WSTRING:\n\t\t\tif (nelements > 1) {\n\t\t\t\tsize_t nelem = inp - *info;\n\t\t\t\tif (*maxcount > CDF_PROP_LIMIT\n\t\t\t\t    || nelements > CDF_PROP_LIMIT)\n\t\t\t\t\tgoto out;\n\t\t\t\t*maxcount += nelements;\n\t\t\t\tinp = CAST(cdf_property_info_t *,\n\t\t\t\t    realloc(*info, *maxcount * sizeof(*inp)));\n\t\t\t\tif (inp == NULL)\n\t\t\t\t\tgoto out;\n\t\t\t\t*info = inp;\n\t\t\t\tinp = *info + nelem;\n\t\t\t}\n\t\t\tDPRINTF((\"nelements = %\" SIZE_T_FORMAT \"u\\n\",\n\t\t\t    nelements));\n\t\t\tfor (j = 0; j < nelements && i < sh.sh_properties; \n\t\t\t    j++, i++) \n\t\t\t{\n\t\t\t\tuint32_t l = CDF_GETUINT32(q, o);\n\t\t\t\tinp[i].pi_str.s_len = l;\n\t\t\t\tinp[i].pi_str.s_buf = (const char *)\n\t\t\t\t    (const void *)(&q[o4 + sizeof(l)]);\n\t\t\t\tDPRINTF((\"l = %d, r = %\" SIZE_T_FORMAT\n\t\t\t\t    \"u, s = %s\\n\", l,\n\t\t\t\t    CDF_ROUND(l, sizeof(l)),\n\t\t\t\t    inp[i].pi_str.s_buf));\n\t\t\t\tif (l & 1)\n\t\t\t\t\tl++;\n\t\t\t\to += l >> 1;\n\t\t\t\tif (q + o >= e)\n\t\t\t\t\tgoto out;\n\t\t\t\to4 = o * sizeof(uint32_t);\n\t\t\t}\n\t\t\ti--;\n\t\t\tbreak;\n\t\tcase CDF_FILETIME:\n\t\t\tif (inp[i].pi_type & CDF_VECTOR)\n\t\t\t\tgoto unknown;\n\t\t\t(void)memcpy(&tp, &q[o4], sizeof(tp));\n\t\t\tinp[i].pi_tp = CDF_TOLE8((uint64_t)tp);\n\t\t\tbreak;\n\t\tcase CDF_CLIPBOARD:\n\t\t\tif (inp[i].pi_type & CDF_VECTOR)\n\t\t\t\tgoto unknown;\n\t\t\tbreak;\n\t\tdefault:\n\t\tunknown:\n\t\t\tDPRINTF((\"Don't know how to deal with %x\\n\",\n\t\t\t    inp[i].pi_type));\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn 0;\nout:\n\tfree(*info);\n\treturn -1;\n}",
    "diff": " \t\tq = (const uint8_t *)(const void *)\n-\t\tif (q > e) {\n+\t\tif (q < p || q > e) {\n \t\t\tDPRINTF((\"Ran of the end %p > %p\\n\", q, e));",
    "critical_vars": [
      "q"
    ],
    "variable_definitions": {
      "q": "const uint8_t *p, *q, *e;"
    },
    "variable_types": {
      "q": "integer pointer"
    },
    "type_mapping": {
      "q": "integer pointer"
    },
    "vulnerable_line": "if (q < p || q > e) {",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Array out of bounds",
    "reasoning": "The pointer q could be calculated incorrectly due to an integer overflow of 'ofs', leading to a valid pointer being treated as out of bounds, allowing potential denial of service or memory corruption."
  },
  {
    "fix_code": "GC_API void * GC_CALL GC_generic_malloc(size_t lb, int k)\n{\n    void * result;\n    DCL_LOCK_STATE;\n\n    if (EXPECT(GC_have_errors, FALSE))\n      GC_print_all_errors();\n    GC_INVOKE_FINALIZERS();\n    if (SMALL_OBJ(lb)) {\n        LOCK();\n        result = GC_generic_malloc_inner((word)lb, k);\n        UNLOCK();\n    } else {\n        size_t lg;\n        size_t lb_rounded;\n        word n_blocks;\n        GC_bool init;\n        lg = ROUNDED_UP_GRANULES(lb);\n        lb_rounded = GRANULES_TO_BYTES(lg);\n        if (lb_rounded < lb)\n            return((*GC_get_oom_fn())(lb));\n        n_blocks = OBJ_SZ_TO_BLOCKS(lb_rounded);\n        init = GC_obj_kinds[k].ok_init;\n        LOCK();\n        result = (ptr_t)GC_alloc_large(lb_rounded, k, 0);\n        if (0 != result) {\n          if (GC_debugging_started) {\n            BZERO(result, n_blocks * HBLKSIZE);\n          } else {\n#           ifdef THREADS\n              /* Clear any memory that might be used for GC descriptors */\n              /* before we release the lock.                            */\n                ((word *)result)[0] = 0;\n                ((word *)result)[1] = 0;\n                ((word *)result)[GRANULES_TO_WORDS(lg)-1] = 0;\n                ((word *)result)[GRANULES_TO_WORDS(lg)-2] = 0;\n#           endif\n          }\n        }\n        GC_bytes_allocd += lb_rounded;\n        UNLOCK();\n        if (init && !GC_debugging_started && 0 != result) {\n            BZERO(result, n_blocks * HBLKSIZE);\n        }\n    }\n    if (0 == result) {\n        return((*GC_get_oom_fn())(lb));\n    } else {\n        return(result);\n    }\n}",
    "diff": "         lb_rounded = GRANULES_TO_BYTES(lg);\n+        if (lb_rounded < lb)\n         n_blocks = OBJ_SZ_TO_BLOCKS(lb_rounded);",
    "critical_vars": [
      "lb_rounded"
    ],
    "variable_definitions": {
      "lb_rounded": "size_t lb_rounded;"
    },
    "variable_types": {
      "lb_rounded": "integer"
    },
    "type_mapping": {
      "lb_rounded": "Integer"
    },
    "vulnerable_line": "if (lb_rounded < lb)",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The check lb_rounded < lb indicates an integer overflow where the calculated rounded size (lb_rounded) is less than the original size (lb), leading to potential allocation of insufficient memory and causing memory-related vulnerabilities."
  },
  {
    "fix_code": "GC_INNER void * GC_generic_malloc_ignore_off_page(size_t lb, int k)\n{\n    void *result;\n    size_t lg;\n    size_t lb_rounded;\n    word n_blocks;\n    GC_bool init;\n    DCL_LOCK_STATE;\n\n    if (SMALL_OBJ(lb))\n        return(GC_generic_malloc((word)lb, k));\n    lg = ROUNDED_UP_GRANULES(lb);\n    lb_rounded = GRANULES_TO_BYTES(lg);\n    if (lb_rounded < lb)\n        return((*GC_get_oom_fn())(lb));\n    n_blocks = OBJ_SZ_TO_BLOCKS(lb_rounded);\n    init = GC_obj_kinds[k].ok_init;\n    if (EXPECT(GC_have_errors, FALSE))\n      GC_print_all_errors();\n    GC_INVOKE_FINALIZERS();\n    LOCK();\n    result = (ptr_t)GC_alloc_large(ADD_SLOP(lb), k, IGNORE_OFF_PAGE);\n    if (0 != result) {\n        if (GC_debugging_started) {\n            BZERO(result, n_blocks * HBLKSIZE);\n        } else {\n#           ifdef THREADS\n              /* Clear any memory that might be used for GC descriptors */\n              /* before we release the lock.                          */\n                ((word *)result)[0] = 0;\n                ((word *)result)[1] = 0;\n                ((word *)result)[GRANULES_TO_WORDS(lg)-1] = 0;\n                ((word *)result)[GRANULES_TO_WORDS(lg)-2] = 0;\n#           endif\n        }\n    }\n    GC_bytes_allocd += lb_rounded;\n    if (0 == result) {\n        GC_oom_func oom_fn = GC_oom_fn;\n        UNLOCK();\n        return((*oom_fn)(lb));\n    } else {\n        UNLOCK();\n        if (init && !GC_debugging_started) {\n            BZERO(result, n_blocks * HBLKSIZE);\n        }\n        return(result);\n    }\n}",
    "diff": "     lb_rounded = GRANULES_TO_BYTES(lg);\n+    if (lb_rounded < lb)\n     n_blocks = OBJ_SZ_TO_BLOCKS(lb_rounded);",
    "critical_vars": [
      "lb_rounded"
    ],
    "variable_definitions": {
      "lb_rounded": "size_t lb_rounded;"
    },
    "variable_types": {
      "lb_rounded": "integer"
    },
    "type_mapping": {
      "lb_rounded": "Integer"
    },
    "vulnerable_line": "if (lb_rounded < lb)",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The check for lb_rounded against lb may result from an integer overflow in the calculation of lb_rounded, potentially leading to allocating less memory than intended."
  },
  {
    "fix_code": "Function not found",
    "diff": "-\t  if (bits_left > 16) i_ptr -= 2;\n+\t  if (bits_left == 0) ENSURE_BITS(16);\n \t  bits_left = 0; bit_buffer = 0;",
    "critical_vars": [
      "bits_left"
    ],
    "variable_definitions": {
      "bits_left": "register int bits_left, i=0;"
    },
    "variable_types": {
      "bits_left": "integer"
    },
    "type_mapping": {
      "bits_left": "Integer"
    },
    "raw_response": "{\n  \"vulnerable_line\": \"if (bits_left > 16) i_ptr -= 2;\",\n  \"critical_variable_in_vulnerable_line\": true,\n  \"vulnerability_type\": \"Integer Underflow\",\n  \"reasoning\": \"The check for bits_left allows for the possibility that bits_left could be negative, causing i_ptr to be decremented incorrectly, leading to potential buffer under-read.\",\n}"
  },
  {
    "fix_code": "Function not found",
    "diff": "-\t  if (bits_left > 16) i_ptr -= 2;\n+\t  if (bits_left == 0) ENSURE_BITS(16);\n \t  bits_left = 0; bit_buffer = 0;",
    "critical_vars": [
      "bits_left"
    ],
    "variable_definitions": {
      "bits_left": "register int bits_left, i=0;"
    },
    "variable_types": {
      "bits_left": "integer"
    },
    "type_mapping": {
      "bits_left": "Integer"
    },
    "vulnerable_line": "if (bits_left > 16) i_ptr -= 2;",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Underflow",
    "reasoning": "The condition checks if bits_left exceeds 16, but if bits_left is 0, subtracting 2 from i_ptr can lead to an underflow and point to an invalid memory location, causing a potential denial of service."
  },
  {
    "fix_code": "static struct mobj *alloc_ta_mem(size_t size)\n{\n#ifdef CFG_PAGED_USER_TA\n\treturn mobj_paged_alloc(size);\n#else\n\tstruct mobj *mobj = mobj_mm_alloc(mobj_sec_ddr, size, &tee_mm_sec_ddr);\n\n\tif (mobj) {\n\t\tsize_t granularity = BIT(tee_mm_sec_ddr.shift);\n\n\t\t/* Round up to allocation granularity size */\n\t\tmemset(mobj_get_va(mobj, 0), 0, ROUNDUP(size, granularity));\n\t}\n\treturn mobj;\n#endif\n}",
    "diff": "@@ -197,8 +197,12 @@ static struct mobj *alloc_ta_mem(size_t size)\n \tstruct mobj *mobj = mobj_mm_alloc(mobj_sec_ddr, size, &tee_mm_sec_ddr);\n-\tif (mobj)\n-\t\tmemset(mobj_get_va(mobj, 0), 0, size);\n+\tif (mobj) {\n+\t\tsize_t granularity = BIT(tee_mm_sec_ddr.shift);\n+\t\t/* Round up to allocation granularity size */\n+\t\tmemset(mobj_get_va(mobj, 0), 0, ROUNDUP(size, granularity));\n \treturn mobj;",
    "critical_vars": [
      "mobj",
      "size"
    ],
    "variable_definitions": {
      "mobj": "struct mobj *mobj = mobj_mm_alloc(mobj_sec_ddr, size, &tee_mm_sec_ddr);",
      "size": "size_t size"
    },
    "variable_types": {
      "mobj": "struct pointer",
      "size": "integer"
    },
    "type_mapping": {
      "mobj": "struct pointer",
      "size": "Integer"
    },
    "vulnerable_line": "memset(mobj_get_va(mobj, 0), 0, ROUNDUP(size, granularity));",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The size being passed to ROUNDUP can exceed the maximum value of size_t, leading to an integer overflow which could potentially allocate insufficient memory and allow leakage of data/code from previous Trusted Applications."
  },
  {
    "fix_code": "static struct mobj *alloc_ta_mem(size_t size)\n{\n#ifdef CFG_PAGED_USER_TA\n\treturn mobj_paged_alloc(size);\n#else\n\tstruct mobj *mobj = mobj_mm_alloc(mobj_sec_ddr, size, &tee_mm_sec_ddr);\n\n\tif (mobj) {\n\t\tsize_t granularity = BIT(tee_mm_sec_ddr.shift);\n\n\t\t/* Round up to allocation granularity size */\n\t\tmemset(mobj_get_va(mobj, 0), 0, ROUNDUP(size, granularity));\n\t}\n\treturn mobj;\n#endif\n}",
    "diff": "+\t\tsize_t granularity = BIT(tee_mm_sec_ddr.shift);\n+\t\t/* Round up to allocation granularity size */\n+\t\tmemset(mobj_get_va(mobj, 0), 0, ROUNDUP(size, granularity));",
    "critical_vars": [
      "granularity"
    ],
    "variable_definitions": {
      "granularity": "size_t granularity = BIT(tee_mm_sec_ddr.shift);"
    },
    "variable_types": {
      "granularity": "integer"
    },
    "type_mapping": {
      "granularity": "Integer"
    },
    "vulnerable_line": "memset(mobj_get_va(mobj, 0), 0, ROUNDUP(size, granularity));",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "ROUNDUP(size, granularity) could cause an integer overflow if 'size' is large enough, potentially leading to a memory allocation that exceeds available memory, which risks leaking sensitive data."
  },
  {
    "fix_code": "Function not found",
    "diff": "+\tif (TSQUERY_TOO_BIG(list_length(state.polstr), state.sumlen))\n \tcommonlen = COMPUTESIZE(list_length(state.polstr), state.sumlen);\n \tquery->size = list_length(state.polstr);",
    "critical_vars": [
      "state.sumlen",
      "state.polstr"
    ],
    "variable_definitions": {
      "state.sumlen": "Definition not found",
      "state.polstr": "Definition not found"
    },
    "variable_types": {
      "state.sumlen": "struct.integer",
      "state.polstr": "struct.struct pointer"
    },
    "type_mapping": {
      "state.sumlen": "st.i",
      "state.polstr": "st.sp"
    },
    "vulnerable_line": "commonlen = COMPUTESIZE(list_length(state.polstr), state.sumlen);",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The computation of commonlen from list_length(state.polstr) and state.sumlen can exceed the maximum value for the integer type, leading to overflow and potentially causing a buffer overflow in subsequent operations."
  },
  {
    "fix_code": "TSQuery\nQTN2QT(QTNode *in)\n{\n\tTSQuery\t\tout;\n\tint\t\t\tlen;\n\tint\t\t\tsumlen = 0,\n\t\t\t\tnnode = 0;\n\tQTN2QTState state;\n\n\tcntsize(in, &sumlen, &nnode);\n\n\tif (TSQUERY_TOO_BIG(nnode, sumlen))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t\t errmsg(\"tsquery is too large\")));\n\tlen = COMPUTESIZE(nnode, sumlen);\n\n\tout = (TSQuery) palloc0(len);\n\tSET_VARSIZE(out, len);\n\tout->size = nnode;\n\n\tstate.curitem = GETQUERY(out);\n\tstate.operand = state.curoperand = GETOPERAND(out);\n\n\tfillQT(&state, in);\n\treturn out;\n}",
    "diff": " \tcntsize(in, &sumlen, &nnode);\n+\tif (TSQUERY_TOO_BIG(nnode, sumlen))\n \tlen = COMPUTESIZE(nnode, sumlen);",
    "critical_vars": [
      "nnode",
      "sumlen"
    ],
    "variable_definitions": {
      "nnode": "int sumlen = 0, nnode = 0;",
      "sumlen": "int sumlen = 0, nnode = 0;"
    },
    "variable_types": {
      "nnode": "integer",
      "sumlen": "integer"
    },
    "type_mapping": {
      "nnode": "Integer",
      "sumlen": "Integer"
    },
    "vulnerable_line": "len = COMPUTESIZE(nnode, sumlen);",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The COMPUTESIZE function may result in an integer overflow if nnode and sumlen are large, causing allocation of insufficient memory leading to a buffer overflow."
  },
  {
    "fix_code": "Datum\t\thstore_recv(PG_FUNCTION_ARGS);\nDatum\nhstore_recv(PG_FUNCTION_ARGS)\n{\n\tint32\t\tbuflen;\n\tHStore\t   *out;\n\tPairs\t   *pairs;\n\tint32\t\ti;\n\tint32\t\tpcount;\n\tStringInfo\tbuf = (StringInfo) PG_GETARG_POINTER(0);\n\n\tpcount = pq_getmsgint(buf, 4);\n\n\tif (pcount == 0)\n\t{\n\t\tout = hstorePairs(NULL, 0, 0);\n\t\tPG_RETURN_POINTER(out);\n\t}\n\n\tif (pcount < 0 || pcount > MaxAllocSize / sizeof(Pairs))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t  errmsg(\"number of pairs (%d) exceeds the maximum allowed (%d)\",\n\t\t\t\t\t pcount, (int) (MaxAllocSize / sizeof(Pairs)))));\n\tpairs = palloc(pcount * sizeof(Pairs));\n\n\tfor (i = 0; i < pcount; ++i)\n\t{\n\t\tint\t\t\trawlen = pq_getmsgint(buf, 4);\n\t\tint\t\t\tlen;\n\n\t\tif (rawlen < 0)\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_NULL_VALUE_NOT_ALLOWED),\n\t\t\t\t\t errmsg(\"null value not allowed for hstore key\")));\n\n\t\tpairs[i].key = pq_getmsgtext(buf, rawlen, &len);\n\t\tpairs[i].keylen = hstoreCheckKeyLen(len);\n\t\tpairs[i].needfree = true;\n\n\t\trawlen = pq_getmsgint(buf, 4);\n\t\tif (rawlen < 0)\n\t\t{\n\t\t\tpairs[i].val = NULL;\n\t\t\tpairs[i].vallen = 0;\n\t\t\tpairs[i].isnull = true;\n\t\t}\n\t\telse\n\t\t{\n\t\t\tpairs[i].val = pq_getmsgtext(buf, rawlen, &len);\n\t\t\tpairs[i].vallen = hstoreCheckValLen(len);\n\t\t\tpairs[i].isnull = false;\n\t\t}\n\t}\n\n\tpcount = hstoreUniquePairs(pairs, pcount, &buflen);\n\n\tout = hstorePairs(pairs, pcount, buflen);\n\n\tPG_RETURN_POINTER(out);\n}",
    "diff": "+\tif (pcount < 0 || pcount > MaxAllocSize / sizeof(Pairs))\n+\t\t\t\t\t pcount, (int) (MaxAllocSize / sizeof(Pairs)))));\n \tpairs = palloc(pcount * sizeof(Pairs));\n \tfor (i = 0; i < pcount; ++i)",
    "critical_vars": [
      "pcount"
    ],
    "variable_definitions": {
      "pcount": "int32\t\tpcount;"
    },
    "variable_types": {
      "pcount": "integer"
    },
    "type_mapping": {
      "pcount": "Integer"
    },
    "vulnerable_line": "if (pcount < 0 || pcount > MaxAllocSize / sizeof(Pairs))",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The value of pcount can exceed the allocated maximum size, leading to potential integer overflow when allocating memory for pairs, thus causing a buffer overflow."
  },
  {
    "fix_code": "Datum\t\thstore_from_arrays(PG_FUNCTION_ARGS);\nDatum\nhstore_from_arrays(PG_FUNCTION_ARGS)\n{\n\tint32\t\tbuflen;\n\tHStore\t   *out;\n\tPairs\t   *pairs;\n\tDatum\t   *key_datums;\n\tbool\t   *key_nulls;\n\tint\t\t\tkey_count;\n\tDatum\t   *value_datums;\n\tbool\t   *value_nulls;\n\tint\t\t\tvalue_count;\n\tArrayType  *key_array;\n\tArrayType  *value_array;\n\tint\t\t\ti;\n\n\tif (PG_ARGISNULL(0))\n\t\tPG_RETURN_NULL();\n\n\tkey_array = PG_GETARG_ARRAYTYPE_P(0);\n\n\tAssert(ARR_ELEMTYPE(key_array) == TEXTOID);\n\n\t/*\n\t * must check >1 rather than != 1 because empty arrays have 0 dimensions,\n\t * not 1\n\t */\n\n\tif (ARR_NDIM(key_array) > 1)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_ARRAY_SUBSCRIPT_ERROR),\n\t\t\t\t errmsg(\"wrong number of array subscripts\")));\n\n\tdeconstruct_array(key_array,\n\t\t\t\t\t  TEXTOID, -1, false, 'i',\n\t\t\t\t\t  &key_datums, &key_nulls, &key_count);\n\n\t/* see discussion in hstoreArrayToPairs() */\n\tif (key_count > MaxAllocSize / sizeof(Pairs))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t  errmsg(\"number of pairs (%d) exceeds the maximum allowed (%d)\",\n\t\t\t\t\t key_count, (int) (MaxAllocSize / sizeof(Pairs)))));\n\n\t/* value_array might be NULL */\n\n\tif (PG_ARGISNULL(1))\n\t{\n\t\tvalue_array = NULL;\n\t\tvalue_count = key_count;\n\t\tvalue_datums = NULL;\n\t\tvalue_nulls = NULL;\n\t}\n\telse\n\t{\n\t\tvalue_array = PG_GETARG_ARRAYTYPE_P(1);\n\n\t\tAssert(ARR_ELEMTYPE(value_array) == TEXTOID);\n\n\t\tif (ARR_NDIM(value_array) > 1)\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_ARRAY_SUBSCRIPT_ERROR),\n\t\t\t\t\t errmsg(\"wrong number of array subscripts\")));\n\n\t\tif ((ARR_NDIM(key_array) > 0 || ARR_NDIM(value_array) > 0) &&\n\t\t\t(ARR_NDIM(key_array) != ARR_NDIM(value_array) ||\n\t\t\t ARR_DIMS(key_array)[0] != ARR_DIMS(value_array)[0] ||\n\t\t\t ARR_LBOUND(key_array)[0] != ARR_LBOUND(value_array)[0]))\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_ARRAY_SUBSCRIPT_ERROR),\n\t\t\t\t\t errmsg(\"arrays must have same bounds\")));\n\n\t\tdeconstruct_array(value_array,\n\t\t\t\t\t\t  TEXTOID, -1, false, 'i',\n\t\t\t\t\t\t  &value_datums, &value_nulls, &value_count);\n\n\t\tAssert(key_count == value_count);\n\t}\n\n\tpairs = palloc(key_count * sizeof(Pairs));\n\n\tfor (i = 0; i < key_count; ++i)\n\t{\n\t\tif (key_nulls[i])\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_NULL_VALUE_NOT_ALLOWED),\n\t\t\t\t\t errmsg(\"null value not allowed for hstore key\")));\n\n\t\tif (!value_nulls || value_nulls[i])\n\t\t{\n\t\t\tpairs[i].key = VARDATA_ANY(key_datums[i]);\n\t\t\tpairs[i].val = NULL;\n\t\t\tpairs[i].keylen = hstoreCheckKeyLen(VARSIZE_ANY_EXHDR(key_datums[i]));\n\t\t\tpairs[i].vallen = 4;\n\t\t\tpairs[i].isnull = true;\n\t\t\tpairs[i].needfree = false;\n\t\t}\n\t\telse\n\t\t{\n\t\t\tpairs[i].key = VARDATA_ANY(key_datums[i]);\n\t\t\tpairs[i].val = VARDATA_ANY(value_datums[i]);\n\t\t\tpairs[i].keylen = hstoreCheckKeyLen(VARSIZE_ANY_EXHDR(key_datums[i]));\n\t\t\tpairs[i].vallen = hstoreCheckValLen(VARSIZE_ANY_EXHDR(value_datums[i]));\n\t\t\tpairs[i].isnull = false;\n\t\t\tpairs[i].needfree = false;\n\t\t}\n\t}\n\n\tkey_count = hstoreUniquePairs(pairs, key_count, &buflen);\n\n\tout = hstorePairs(pairs, key_count, buflen);\n\n\tPG_RETURN_POINTER(out);\n}",
    "diff": " \t\t\t\t\t  &key_datums, &key_nulls, &key_count);\n+\tif (key_count > MaxAllocSize / sizeof(Pairs))\n+\t\t\t\t\t key_count, (int) (MaxAllocSize / sizeof(Pairs)))));",
    "critical_vars": [
      "key_count"
    ],
    "variable_definitions": {
      "key_count": "int\t\t\tkey_count;"
    },
    "variable_types": {
      "key_count": "integer"
    },
    "type_mapping": {
      "key_count": "Integer"
    },
    "vulnerable_line": "if (key_count > MaxAllocSize / sizeof(Pairs))",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "If key_count is set to a value that causes the multiplication with sizeof(Pairs) to exceed MaxAllocSize, it can result in an integer overflow, potentially leading to insufficient memory allocation when palloc is called."
  },
  {
    "fix_code": "Datum\t\thstore_from_array(PG_FUNCTION_ARGS);\nDatum\nhstore_from_array(PG_FUNCTION_ARGS)\n{\n\tArrayType  *in_array = PG_GETARG_ARRAYTYPE_P(0);\n\tint\t\t\tndims = ARR_NDIM(in_array);\n\tint\t\t\tcount;\n\tint32\t\tbuflen;\n\tHStore\t   *out;\n\tPairs\t   *pairs;\n\tDatum\t   *in_datums;\n\tbool\t   *in_nulls;\n\tint\t\t\tin_count;\n\tint\t\t\ti;\n\n\tAssert(ARR_ELEMTYPE(in_array) == TEXTOID);\n\n\tswitch (ndims)\n\t{\n\t\tcase 0:\n\t\t\tout = hstorePairs(NULL, 0, 0);\n\t\t\tPG_RETURN_POINTER(out);\n\n\t\tcase 1:\n\t\t\tif ((ARR_DIMS(in_array)[0]) % 2)\n\t\t\t\tereport(ERROR,\n\t\t\t\t\t\t(errcode(ERRCODE_ARRAY_SUBSCRIPT_ERROR),\n\t\t\t\t\t\t errmsg(\"array must have even number of elements\")));\n\t\t\tbreak;\n\n\t\tcase 2:\n\t\t\tif ((ARR_DIMS(in_array)[1]) != 2)\n\t\t\t\tereport(ERROR,\n\t\t\t\t\t\t(errcode(ERRCODE_ARRAY_SUBSCRIPT_ERROR),\n\t\t\t\t\t\t errmsg(\"array must have two columns\")));\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_ARRAY_SUBSCRIPT_ERROR),\n\t\t\t\t\t errmsg(\"wrong number of array subscripts\")));\n\t}\n\n\tdeconstruct_array(in_array,\n\t\t\t\t\t  TEXTOID, -1, false, 'i',\n\t\t\t\t\t  &in_datums, &in_nulls, &in_count);\n\n\tcount = in_count / 2;\n\n\t/* see discussion in hstoreArrayToPairs() */\n\tif (count > MaxAllocSize / sizeof(Pairs))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t  errmsg(\"number of pairs (%d) exceeds the maximum allowed (%d)\",\n\t\t\t\t\t count, (int) (MaxAllocSize / sizeof(Pairs)))));\n\n\tpairs = palloc(count * sizeof(Pairs));\n\n\tfor (i = 0; i < count; ++i)\n\t{\n\t\tif (in_nulls[i * 2])\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_NULL_VALUE_NOT_ALLOWED),\n\t\t\t\t\t errmsg(\"null value not allowed for hstore key\")));\n\n\t\tif (in_nulls[i * 2 + 1])\n\t\t{\n\t\t\tpairs[i].key = VARDATA_ANY(in_datums[i * 2]);\n\t\t\tpairs[i].val = NULL;\n\t\t\tpairs[i].keylen = hstoreCheckKeyLen(VARSIZE_ANY_EXHDR(in_datums[i * 2]));\n\t\t\tpairs[i].vallen = 4;\n\t\t\tpairs[i].isnull = true;\n\t\t\tpairs[i].needfree = false;\n\t\t}\n\t\telse\n\t\t{\n\t\t\tpairs[i].key = VARDATA_ANY(in_datums[i * 2]);\n\t\t\tpairs[i].val = VARDATA_ANY(in_datums[i * 2 + 1]);\n\t\t\tpairs[i].keylen = hstoreCheckKeyLen(VARSIZE_ANY_EXHDR(in_datums[i * 2]));\n\t\t\tpairs[i].vallen = hstoreCheckValLen(VARSIZE_ANY_EXHDR(in_datums[i * 2 + 1]));\n\t\t\tpairs[i].isnull = false;\n\t\t\tpairs[i].needfree = false;\n\t\t}\n\t}\n\n\tcount = hstoreUniquePairs(pairs, count, &buflen);\n\n\tout = hstorePairs(pairs, count, buflen);\n\n\tPG_RETURN_POINTER(out);\n}",
    "diff": "+\tif (pcount < 0 || pcount > MaxAllocSize / sizeof(Pairs))\n+\t\t\t\t\t pcount, (int) (MaxAllocSize / sizeof(Pairs)))));\n \tpairs = palloc(pcount * sizeof(Pairs));\n \tfor (i = 0; i < pcount; ++i)\n \t\t\t\t\t  &key_datums, &key_nulls, &key_count);\n+\tif (key_count > MaxAllocSize / sizeof(Pairs))\n+\t\t\t\t\t key_count, (int) (MaxAllocSize / sizeof(Pairs)))));\n \tcount = in_count / 2;\n+\tif (count > MaxAllocSize / sizeof(Pairs))\n+\t\t\t\t\t count, (int) (MaxAllocSize / sizeof(Pairs)))));\n \tpairs = palloc(count * sizeof(Pairs));\n \tfor (i = 0; i < count; ++i)",
    "critical_vars": [
      "count"
    ],
    "variable_definitions": {
      "count": "int\t\t\tcount;"
    },
    "variable_types": {
      "count": "integer"
    },
    "type_mapping": {
      "count": "Integer"
    },
    "vulnerable_line": "if (count > MaxAllocSize / sizeof(Pairs))",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The calculation of 'count' may exceed MaxAllocSize due to integer overflow during the division, leading to potential allocation of insufficient memory and possible buffer overflow."
  },
  {
    "fix_code": "Function not found",
    "diff": " \t\tmy_extra->ncolumns = ncolumns;\n+\tAssert(ncolumns <= MaxTupleAttributeNumber);\t\t/* thus, no overflow */\n \tpairs = palloc(ncolumns * sizeof(Pairs));",
    "critical_vars": [
      "ncolumns"
    ],
    "variable_definitions": {
      "ncolumns": "int\t\t\tncolumns;"
    },
    "variable_types": {
      "ncolumns": "integer"
    },
    "type_mapping": {
      "ncolumns": "Integer"
    },
    "vulnerable_line": "if (pcount < 0 || pcount > MaxAllocSize / sizeof(Pairs))",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The check for pcount exceeding MaxAllocSize allows for a potential integer overflow if pcount is a large positive value, leading to memory allocation issues."
  },
  {
    "fix_code": "* txid_snapshot_recv(internal) returns txid_snapshot\n *\n *\t\tbinary input function for type txid_snapshot\n *\n *\t\tformat: int4 nxip, int8 xmin, int8 xmax, int8 xip\n */\nDatum\ntxid_snapshot_recv(PG_FUNCTION_ARGS)\n{\n\tStringInfo\tbuf = (StringInfo) PG_GETARG_POINTER(0);\n\tTxidSnapshot *snap;\n\ttxid\t\tlast = 0;\n\tint\t\t\tnxip;\n\tint\t\t\ti;\n\ttxid\t\txmin,\n\t\t\t\txmax;\n\n\t/* load and validate nxip */\n\tnxip = pq_getmsgint(buf, 4);\n\tif (nxip < 0 || nxip > TXID_SNAPSHOT_MAX_NXIP)\n\t\tgoto bad_format;\n\n\txmin = pq_getmsgint64(buf);\n\txmax = pq_getmsgint64(buf);\n\tif (xmin == 0 || xmax == 0 || xmin > xmax || xmax > MAX_TXID)\n\t\tgoto bad_format;\n\n\tsnap = palloc(TXID_SNAPSHOT_SIZE(nxip));\n\tsnap->xmin = xmin;\n\tsnap->xmax = xmax;\n\tsnap->nxip = nxip;\n\tSET_VARSIZE(snap, TXID_SNAPSHOT_SIZE(nxip));\n\n\tfor (i = 0; i < nxip; i++)\n\t{\n\t\ttxid\t\tcur = pq_getmsgint64(buf);\n\n\t\tif (cur <= last || cur < xmin || cur >= xmax)\n\t\t\tgoto bad_format;\n\t\tsnap->xip[i] = cur;\n\t\tlast = cur;\n\t}\n\tPG_RETURN_POINTER(snap);\n\nbad_format:\n\telog(ERROR, \"invalid snapshot data\");\n\treturn (Datum) NULL;\n}",
    "diff": "-\tint\t\t\tavail;\n-\t * (nxip > avail) check is against int overflows in 'expect'.\n-\tavail = buf->len - buf->cursor;\n-\tif (nxip < 0 || nxip > avail || expect > avail)",
    "critical_vars": [
      "avail"
    ],
    "variable_definitions": {
      "avail": "int\t\t\tavail;"
    },
    "variable_types": {
      "avail": "integer"
    },
    "type_mapping": {
      "avail": "Integer"
    },
    "vulnerable_line": "if (nxip < 0 || nxip > TXID_SNAPSHOT_MAX_NXIP)",
    "critical_variable_in_vulnerable_line": false,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The check on nxip for being less than 0 and greater than TXID_SNAPSHOT_MAX_NXIP does not account for potential overflows when calculating expected sizes, which could lead to buffer allocation errors when nxip is large."
  },
  {
    "fix_code": "* txid_snapshot_recv(internal) returns txid_snapshot\n *\n *\t\tbinary input function for type txid_snapshot\n *\n *\t\tformat: int4 nxip, int8 xmin, int8 xmax, int8 xip\n */\nDatum\ntxid_snapshot_recv(PG_FUNCTION_ARGS)\n{\n\tStringInfo\tbuf = (StringInfo) PG_GETARG_POINTER(0);\n\tTxidSnapshot *snap;\n\ttxid\t\tlast = 0;\n\tint\t\t\tnxip;\n\tint\t\t\ti;\n\ttxid\t\txmin,\n\t\t\t\txmax;\n\n\t/* load and validate nxip */\n\tnxip = pq_getmsgint(buf, 4);\n\tif (nxip < 0 || nxip > TXID_SNAPSHOT_MAX_NXIP)\n\t\tgoto bad_format;\n\n\txmin = pq_getmsgint64(buf);\n\txmax = pq_getmsgint64(buf);\n\tif (xmin == 0 || xmax == 0 || xmin > xmax || xmax > MAX_TXID)\n\t\tgoto bad_format;\n\n\tsnap = palloc(TXID_SNAPSHOT_SIZE(nxip));\n\tsnap->xmin = xmin;\n\tsnap->xmax = xmax;\n\tsnap->nxip = nxip;\n\tSET_VARSIZE(snap, TXID_SNAPSHOT_SIZE(nxip));\n\n\tfor (i = 0; i < nxip; i++)\n\t{\n\t\ttxid\t\tcur = pq_getmsgint64(buf);\n\n\t\tif (cur <= last || cur < xmin || cur >= xmax)\n\t\t\tgoto bad_format;\n\t\tsnap->xip[i] = cur;\n\t\tlast = cur;\n\t}\n\tPG_RETURN_POINTER(snap);\n\nbad_format:\n\telog(ERROR, \"invalid snapshot data\");\n\treturn (Datum) NULL;\n}",
    "diff": " #define TXID_SNAPSHOT_SIZE(nxip) \\\n \t(offsetof(TxidSnapshot, xip) + sizeof(txid) * (nxip))\n+\t * MAX_BACKENDS prepared transactions) guarantee nxip won't be too large.\n \tnxip = cur->xcnt;\n \tsize = TXID_SNAPSHOT_SIZE(nxip);\n \tint\t\t\tnxip;\n-\t * load nxip and check for nonsense.\n-\t * (nxip > avail) check is against int overflows in 'expect'.\n+\t/* load and validate nxip */\n \tnxip = pq_getmsgint(buf, 4);\n-\texpect = 8 + 8 + nxip * 8;\n-\tif (nxip < 0 || nxip > avail || expect > avail)\n+\tif (nxip < 0 || nxip > TXID_SNAPSHOT_MAX_NXIP)",
    "critical_vars": [
      "nxip"
    ],
    "variable_definitions": {
      "nxip": "int\t\t\tnxip;"
    },
    "variable_types": {
      "nxip": "integer"
    },
    "type_mapping": {
      "nxip": "Integer"
    },
    "vulnerable_line": "if (nxip < 0 || nxip > TXID_SNAPSHOT_MAX_NXIP)",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The condition checks if nxip exceeds the calculated maximum size, but without proper validation during its assignment or how it's received, nxip could lead to an overflow, allowing for attacks. The fix introduces a static assertion to ensure nxip won't exceed these bounds during compile time."
  },
  {
    "fix_code": "Pairs *\nhstoreArrayToPairs(ArrayType *a, int *npairs)\n{\n\tDatum\t   *key_datums;\n\tbool\t   *key_nulls;\n\tint\t\t\tkey_count;\n\tPairs\t   *key_pairs;\n\tint\t\t\tbufsiz;\n\tint\t\t\ti,\n\t\t\t\tj;\n\n\tdeconstruct_array(a,\n\t\t\t\t\t  TEXTOID, -1, false, 'i',\n\t\t\t\t\t  &key_datums, &key_nulls, &key_count);\n\n\tif (key_count == 0)\n\t{\n\t\t*npairs = 0;\n\t\treturn NULL;\n\t}\n\n\t/*\n\t * A text array uses at least eight bytes per element, so any overflow in\n\t * \"key_count * sizeof(Pairs)\" is small enough for palloc() to catch.\n\t * However, credible improvements to the array format could invalidate\n\t * that assumption.  Therefore, use an explicit check rather than relying\n\t * on palloc() to complain.\n\t */\n\tif (key_count > MaxAllocSize / sizeof(Pairs))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t  errmsg(\"number of pairs (%d) exceeds the maximum allowed (%d)\",\n\t\t\t\t\t key_count, (int) (MaxAllocSize / sizeof(Pairs)))));\n\n\tkey_pairs = palloc(sizeof(Pairs) * key_count);\n\n\tfor (i = 0, j = 0; i < key_count; i++)\n\t{\n\t\tif (!key_nulls[i])\n\t\t{\n\t\t\tkey_pairs[j].key = VARDATA(key_datums[i]);\n\t\t\tkey_pairs[j].keylen = VARSIZE(key_datums[i]) - VARHDRSZ;\n\t\t\tkey_pairs[j].val = NULL;\n\t\t\tkey_pairs[j].vallen = 0;\n\t\t\tkey_pairs[j].needfree = 0;\n\t\t\tkey_pairs[j].isnull = 1;\n\t\t\tj++;\n\t\t}\n\t}\n\n\t*npairs = hstoreUniquePairs(key_pairs, j, &bufsiz);\n\n\treturn key_pairs;\n}",
    "diff": "+\t * \"key_count * sizeof(Pairs)\" is small enough for palloc() to catch.\n+\tif (key_count > MaxAllocSize / sizeof(Pairs))\n+\t\t\t\t\t key_count, (int) (MaxAllocSize / sizeof(Pairs)))));\n \tkey_pairs = palloc(sizeof(Pairs) * key_count);\n \tfor (i = 0, j = 0; i < key_count; i++)",
    "critical_vars": [
      "key_count"
    ],
    "variable_definitions": {
      "key_count": "int\t\t\tkey_count;"
    },
    "variable_types": {
      "key_count": "integer"
    },
    "type_mapping": {
      "key_count": "Integer"
    },
    "vulnerable_line": "if (key_count > MaxAllocSize / sizeof(Pairs))",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The condition checks if key_count results in an overflow when multiplied by sizeof(Pairs), which could allow for allocation beyond the intended limits when palloc is called."
  },
  {
    "fix_code": "Datum\nbit_in(PG_FUNCTION_ARGS)\n{\n\tchar\t   *input_string = PG_GETARG_CSTRING(0);\n\n#ifdef NOT_USED\n\tOid\t\t\ttypelem = PG_GETARG_OID(1);\n#endif\n\tint32\t\tatttypmod = PG_GETARG_INT32(2);\n\tVarBit\t   *result;\t\t\t/* The resulting bit string\t\t\t  */\n\tchar\t   *sp;\t\t\t\t/* pointer into the character string  */\n\tbits8\t   *r;\t\t\t\t/* pointer into the result */\n\tint\t\t\tlen,\t\t\t/* Length of the whole data structure */\n\t\t\t\tbitlen,\t\t\t/* Number of bits in the bit string   */\n\t\t\t\tslen;\t\t\t/* Length of the input string\t\t  */\n\tbool\t\tbit_not_hex;\t/* false = hex string  true = bit string */\n\tint\t\t\tbc;\n\tbits8\t\tx = 0;\n\n\t/* Check that the first character is a b or an x */\n\tif (input_string[0] == 'b' || input_string[0] == 'B')\n\t{\n\t\tbit_not_hex = true;\n\t\tsp = input_string + 1;\n\t}\n\telse if (input_string[0] == 'x' || input_string[0] == 'X')\n\t{\n\t\tbit_not_hex = false;\n\t\tsp = input_string + 1;\n\t}\n\telse\n\t{\n\t\t/*\n\t\t * Otherwise it's binary.  This allows things like cast('1001' as bit)\n\t\t * to work transparently.\n\t\t */\n\t\tbit_not_hex = true;\n\t\tsp = input_string;\n\t}\n\n\t/*\n\t * Determine bitlength from input string.  MaxAllocSize ensures a regular\n\t * input is small enough, but we must check hex input.\n\t */\n\tslen = strlen(sp);\n\tif (bit_not_hex)\n\t\tbitlen = slen;\n\telse\n\t{\n\t\tif (slen > VARBITMAXLEN / 4)\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t\t errmsg(\"bit string length exceeds the maximum allowed (%d)\",\n\t\t\t\t\t\tVARBITMAXLEN)));\n\t\tbitlen = slen * 4;\n\t}\n\n\t/*\n\t * Sometimes atttypmod is not supplied. If it is supplied we need to make\n\t * sure that the bitstring fits.\n\t */\n\tif (atttypmod <= 0)\n\t\tatttypmod = bitlen;\n\telse if (bitlen != atttypmod)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_STRING_DATA_LENGTH_MISMATCH),\n\t\t\t\t errmsg(\"bit string length %d does not match type bit(%d)\",\n\t\t\t\t\t\tbitlen, atttypmod)));\n\n\tlen = VARBITTOTALLEN(atttypmod);\n\t/* set to 0 so that *r is always initialised and string is zero-padded */\n\tresult = (VarBit *) palloc0(len);\n\tSET_VARSIZE(result, len);\n\tVARBITLEN(result) = atttypmod;\n\n\tr = VARBITS(result);\n\tif (bit_not_hex)\n\t{\n\t\t/* Parse the bit representation of the string */\n\t\t/* We know it fits, as bitlen was compared to atttypmod */\n\t\tx = HIGHBIT;\n\t\tfor (; *sp; sp++)\n\t\t{\n\t\t\tif (*sp == '1')\n\t\t\t\t*r |= x;\n\t\t\telse if (*sp != '0')\n\t\t\t\tereport(ERROR,\n\t\t\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t\t\t\t errmsg(\"\\\"%c\\\" is not a valid binary digit\",\n\t\t\t\t\t\t\t\t*sp)));\n\n\t\t\tx >>= 1;\n\t\t\tif (x == 0)\n\t\t\t{\n\t\t\t\tx = HIGHBIT;\n\t\t\t\tr++;\n\t\t\t}\n\t\t}\n\t}\n\telse\n\t{\n\t\t/* Parse the hex representation of the string */\n\t\tfor (bc = 0; *sp; sp++)\n\t\t{\n\t\t\tif (*sp >= '0' && *sp <= '9')\n\t\t\t\tx = (bits8) (*sp - '0');\n\t\t\telse if (*sp >= 'A' && *sp <= 'F')\n\t\t\t\tx = (bits8) (*sp - 'A') + 10;\n\t\t\telse if (*sp >= 'a' && *sp <= 'f')\n\t\t\t\tx = (bits8) (*sp - 'a') + 10;\n\t\t\telse\n\t\t\t\tereport(ERROR,\n\t\t\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t\t\t\t errmsg(\"\\\"%c\\\" is not a valid hexadecimal digit\",\n\t\t\t\t\t\t\t\t*sp)));\n\n\t\t\tif (bc)\n\t\t\t{\n\t\t\t\t*r++ |= x;\n\t\t\t\tbc = 0;\n\t\t\t}\n\t\t\telse\n\t\t\t{\n\t\t\t\t*r = x << 4;\n\t\t\t\tbc = 1;\n\t\t\t}\n\t\t}\n\t}\n\n\tPG_RETURN_VARBIT_P(result);\n}\n\n\nDatum\nbit_out(PG_FUNCTION_ARGS)\n{\n#if 1\n\t/* same as varbit output */\n\treturn varbit_out(fcinfo);\n#else\n\n\t/*\n\t * This is how one would print a hex string, in case someone wants to\n\t * write a formatting function.\n\t */\n\tVarBit\t   *s = PG_GETARG_VARBIT_P(0);\n\tchar\t   *result,\n\t\t\t   *r;\n\tbits8\t   *sp;\n\tint\t\t\ti,\n\t\t\t\tlen,\n\t\t\t\tbitlen;\n\n\tbitlen = VARBITLEN(s);\n\tlen = (bitlen + 3) / 4;\n\tresult = (char *) palloc(len + 2);\n\tsp = VARBITS(s);\n\tr = result;\n\t*r++ = 'X';\n\t/* we cheat by knowing that we store full bytes zero padded */\n\tfor (i = 0; i < len; i += 2, sp++)\n\t{\n\t\t*r++ = HEXDIG((*sp) >> 4);\n\t\t*r++ = HEXDIG((*sp) & 0xF);\n\t}\n\n\t/*\n\t * Go back one step if we printed a hex number that was not part of the\n\t * bitstring anymore\n\t */\n\tif (i > len)\n\t\tr--;\n\t*r = '\\0';\n\n\tPG_RETURN_CSTRING(result);\n#endif\n}\n\n/*\n *\t\tbit_recv\t\t\t- converts external binary format to bit\n */\nDatum\nbit_recv(PG_FUNCTION_ARGS)\n{\n\tStringInfo\tbuf = (StringInfo) PG_GETARG_POINTER(0);\n\n#ifdef NOT_USED\n\tOid\t\t\ttypelem = PG_GETARG_OID(1);\n#endif\n\tint32\t\tatttypmod = PG_GETARG_INT32(2);\n\tVarBit\t   *result;\n\tint\t\t\tlen,\n\t\t\t\tbitlen;\n\tint\t\t\tipad;\n\tbits8\t\tmask;\n\n\tbitlen = pq_getmsgint(buf, sizeof(int32));\n\tif (bitlen < 0)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_BINARY_REPRESENTATION),\n\t\t\t\t errmsg(\"invalid length in external bit string\")));\n\n\t/*\n\t * Sometimes atttypmod is not supplied. If it is supplied we need to make\n\t * sure that the bitstring fits.\n\t */\n\tif (atttypmod > 0 && bitlen != atttypmod)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_STRING_DATA_LENGTH_MISMATCH),\n\t\t\t\t errmsg(\"bit string length %d does not match type bit(%d)\",\n\t\t\t\t\t\tbitlen, atttypmod)));\n\n\tlen = VARBITTOTALLEN(bitlen);\n\tresult = (VarBit *) palloc(len);\n\tSET_VARSIZE(result, len);\n\tVARBITLEN(result) = bitlen;\n\n\tpq_copymsgbytes(buf, (char *) VARBITS(result), VARBITBYTES(result));\n\n\t/* Make sure last byte is zero-padded if needed */\n\tipad = VARBITPAD(result);\n\tif (ipad > 0)\n\t{\n\t\tmask = BITMASK << ipad;\n\t\t*(VARBITS(result) + VARBITBYTES(result) - 1) &= mask;\n\t}\n\n\tPG_RETURN_VARBIT_P(result);\n}\n\n/*\n *\t\tbit_send\t\t\t- converts bit to binary format\n */\nDatum\nbit_send(PG_FUNCTION_ARGS)\n{\n\t/* Exactly the same as varbit_send, so share code */\n\treturn varbit_send(fcinfo);\n}\n\n/*\n * bit()\n * Converts a bit() type to a specific internal length.\n * len is the bitlength specified in the column definition.\n *\n * If doing implicit cast, raise error when source data is wrong length.\n * If doing explicit cast, silently truncate or zero-pad to specified length.\n */\nDatum\nbit(PG_FUNCTION_ARGS)\n{\n\tVarBit\t   *arg = PG_GETARG_VARBIT_P(0);\n\tint32\t\tlen = PG_GETARG_INT32(1);\n\tbool\t\tisExplicit = PG_GETARG_BOOL(2);\n\tVarBit\t   *result;\n\tint\t\t\trlen;\n\tint\t\t\tipad;\n\tbits8\t\tmask;\n\n\t/* No work if typmod is invalid or supplied data matches it already */\n\tif (len <= 0 || len == VARBITLEN(arg))\n\t\tPG_RETURN_VARBIT_P(arg);\n\n\tif (!isExplicit)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_STRING_DATA_LENGTH_MISMATCH),\n\t\t\t\t errmsg(\"bit string length %d does not match type bit(%d)\",\n\t\t\t\t\t\tVARBITLEN(arg), len)));\n\n\trlen = VARBITTOTALLEN(len);\n\t/* set to 0 so that string is zero-padded */\n\tresult = (VarBit *) palloc0(rlen);\n\tSET_VARSIZE(result, rlen);\n\tVARBITLEN(result) = len;\n\n\tmemcpy(VARBITS(result), VARBITS(arg),\n\t\t   Min(VARBITBYTES(result), VARBITBYTES(arg)));\n\n\t/*\n\t * Make sure last byte is zero-padded if needed.  This is useless but safe\n\t * if source data was shorter than target length (we assume the last byte\n\t * of the source data was itself correctly zero-padded).\n\t */\n\tipad = VARBITPAD(result);\n\tif (ipad > 0)\n\t{\n\t\tmask = BITMASK << ipad;\n\t\t*(VARBITS(result) + VARBITBYTES(result) - 1) &= mask;\n\t}\n\n\tPG_RETURN_VARBIT_P(result);\n}\n\nDatum\nbittypmodin(PG_FUNCTION_ARGS)\n{\n\tArrayType  *ta = PG_GETARG_ARRAYTYPE_P(0);\n\n\tPG_RETURN_INT32(anybit_typmodin(ta, \"bit\"));\n}\n\nDatum\nbittypmodout(PG_FUNCTION_ARGS)\n{\n\tint32\t\ttypmod = PG_GETARG_INT32(0);\n\n\tPG_RETURN_CSTRING(anybit_typmodout(typmod));\n}\n\n\n/*\n * varbit_in -\n *\t  converts a string to the internal representation of a bitstring.\n *\t\tThis is the same as bit_in except that atttypmod is taken as\n *\t\tthe maximum length, not the exact length to force the bitstring to.\n */\nDatum\nvarbit_in(PG_FUNCTION_ARGS)\n{\n\tchar\t   *input_string = PG_GETARG_CSTRING(0);\n\n#ifdef NOT_USED\n\tOid\t\t\ttypelem = PG_GETARG_OID(1);\n#endif\n\tint32\t\tatttypmod = PG_GETARG_INT32(2);\n\tVarBit\t   *result;\t\t\t/* The resulting bit string\t\t\t  */\n\tchar\t   *sp;\t\t\t\t/* pointer into the character string  */\n\tbits8\t   *r;\t\t\t\t/* pointer into the result */\n\tint\t\t\tlen,\t\t\t/* Length of the whole data structure */\n\t\t\t\tbitlen,\t\t\t/* Number of bits in the bit string   */\n\t\t\t\tslen;\t\t\t/* Length of the input string\t\t  */\n\tbool\t\tbit_not_hex;\t/* false = hex string  true = bit string */\n\tint\t\t\tbc;\n\tbits8\t\tx = 0;\n\n\t/* Check that the first character is a b or an x */\n\tif (input_string[0] == 'b' || input_string[0] == 'B')\n\t{\n\t\tbit_not_hex = true;\n\t\tsp = input_string + 1;\n\t}\n\telse if (input_string[0] == 'x' || input_string[0] == 'X')\n\t{\n\t\tbit_not_hex = false;\n\t\tsp = input_string + 1;\n\t}\n\telse\n\t{\n\t\tbit_not_hex = true;\n\t\tsp = input_string;\n\t}\n\n\t/*\n\t * Determine bitlength from input string.  MaxAllocSize ensures a regular\n\t * input is small enough, but we must check hex input.\n\t */\n\tslen = strlen(sp);\n\tif (bit_not_hex)\n\t\tbitlen = slen;\n\telse\n\t{\n\t\tif (slen > VARBITMAXLEN / 4)\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t\t errmsg(\"bit string length exceeds the maximum allowed (%d)\",\n\t\t\t\t\t\tVARBITMAXLEN)));\n\t\tbitlen = slen * 4;\n\t}\n\n\t/*\n\t * Sometimes atttypmod is not supplied. If it is supplied we need to make\n\t * sure that the bitstring fits.\n\t */\n\tif (atttypmod <= 0)\n\t\tatttypmod = bitlen;\n\telse if (bitlen > atttypmod)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_STRING_DATA_RIGHT_TRUNCATION),\n\t\t\t\t errmsg(\"bit string too long for type bit varying(%d)\",\n\t\t\t\t\t\tatttypmod)));\n\n\tlen = VARBITTOTALLEN(bitlen);\n\t/* set to 0 so that *r is always initialised and string is zero-padded */\n\tresult = (VarBit *) palloc0(len);\n\tSET_VARSIZE(result, len);\n\tVARBITLEN(result) = Min(bitlen, atttypmod);\n\n\tr = VARBITS(result);\n\tif (bit_not_hex)\n\t{\n\t\t/* Parse the bit representation of the string */\n\t\t/* We know it fits, as bitlen was compared to atttypmod */\n\t\tx = HIGHBIT;\n\t\tfor (; *sp; sp++)\n\t\t{\n\t\t\tif (*sp == '1')\n\t\t\t\t*r |= x;\n\t\t\telse if (*sp != '0')\n\t\t\t\tereport(ERROR,\n\t\t\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t\t\t\t errmsg(\"\\\"%c\\\" is not a valid binary digit\",\n\t\t\t\t\t\t\t\t*sp)));\n\n\t\t\tx >>= 1;\n\t\t\tif (x == 0)\n\t\t\t{\n\t\t\t\tx = HIGHBIT;\n\t\t\t\tr++;\n\t\t\t}\n\t\t}\n\t}\n\telse\n\t{\n\t\t/* Parse the hex representation of the string */\n\t\tfor (bc = 0; *sp; sp++)\n\t\t{\n\t\t\tif (*sp >= '0' && *sp <= '9')\n\t\t\t\tx = (bits8) (*sp - '0');\n\t\t\telse if (*sp >= 'A' && *sp <= 'F')\n\t\t\t\tx = (bits8) (*sp - 'A') + 10;\n\t\t\telse if (*sp >= 'a' && *sp <= 'f')\n\t\t\t\tx = (bits8) (*sp - 'a') + 10;\n\t\t\telse\n\t\t\t\tereport(ERROR,\n\t\t\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t\t\t\t errmsg(\"\\\"%c\\\" is not a valid hexadecimal digit\",\n\t\t\t\t\t\t\t\t*sp)));\n\n\t\t\tif (bc)\n\t\t\t{\n\t\t\t\t*r++ |= x;\n\t\t\t\tbc = 0;\n\t\t\t}\n\t\t\telse\n\t\t\t{\n\t\t\t\t*r = x << 4;\n\t\t\t\tbc = 1;\n\t\t\t}\n\t\t}\n\t}\n\n\tPG_RETURN_VARBIT_P(result);\n}\n\n/*\n * varbit_out -\n *\t  Prints the string as bits to preserve length accurately\n *\n * XXX varbit_recv() and hex input to varbit_in() can load a value that this\n * cannot emit.  Consider using hex output for such values.\n */\nDatum\nvarbit_out(PG_FUNCTION_ARGS)\n{\n\tVarBit\t   *s = PG_GETARG_VARBIT_P(0);\n\tchar\t   *result,\n\t\t\t   *r;\n\tbits8\t   *sp;\n\tbits8\t\tx;\n\tint\t\t\ti,\n\t\t\t\tk,\n\t\t\t\tlen;\n\n\tlen = VARBITLEN(s);\n\tresult = (char *) palloc(len + 1);\n\tsp = VARBITS(s);\n\tr = result;\n\tfor (i = 0; i <= len - BITS_PER_BYTE; i += BITS_PER_BYTE, sp++)\n\t{\n\t\t/* print full bytes */\n\t\tx = *sp;\n\t\tfor (k = 0; k < BITS_PER_BYTE; k++)\n\t\t{\n\t\t\t*r++ = IS_HIGHBIT_SET(x) ? '1' : '0';\n\t\t\tx <<= 1;\n\t\t}\n\t}\n\tif (i < len)\n\t{\n\t\t/* print the last partial byte */\n\t\tx = *sp;\n\t\tfor (k = i; k < len; k++)\n\t\t{\n\t\t\t*r++ = IS_HIGHBIT_SET(x) ? '1' : '0';\n\t\t\tx <<= 1;\n\t\t}\n\t}\n\t*r = '\\0';\n\n\tPG_RETURN_CSTRING(result);\n}\n\n/*\n *\t\tvarbit_recv\t\t\t- converts external binary format to varbit\n *\n * External format is the bitlen as an int32, then the byte array.\n */\nDatum\nvarbit_recv(PG_FUNCTION_ARGS)\n{\n\tStringInfo\tbuf = (StringInfo) PG_GETARG_POINTER(0);\n\n#ifdef NOT_USED\n\tOid\t\t\ttypelem = PG_GETARG_OID(1);\n#endif\n\tint32\t\tatttypmod = PG_GETARG_INT32(2);\n\tVarBit\t   *result;\n\tint\t\t\tlen,\n\t\t\t\tbitlen;\n\tint\t\t\tipad;\n\tbits8\t\tmask;\n\n\tbitlen = pq_getmsgint(buf, sizeof(int32));\n\tif (bitlen < 0)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_BINARY_REPRESENTATION),\n\t\t\t\t errmsg(\"invalid length in external bit string\")));\n\n\t/*\n\t * Sometimes atttypmod is not supplied. If it is supplied we need to make\n\t * sure that the bitstring fits.\n\t */\n\tif (atttypmod > 0 && bitlen > atttypmod)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_STRING_DATA_RIGHT_TRUNCATION),\n\t\t\t\t errmsg(\"bit string too long for type bit varying(%d)\",\n\t\t\t\t\t\tatttypmod)));\n\n\tlen = VARBITTOTALLEN(bitlen);\n\tresult = (VarBit *) palloc(len);\n\tSET_VARSIZE(result, len);\n\tVARBITLEN(result) = bitlen;\n\n\tpq_copymsgbytes(buf, (char *) VARBITS(result), VARBITBYTES(result));\n\n\t/* Make sure last byte is zero-padded if needed */\n\tipad = VARBITPAD(result);\n\tif (ipad > 0)\n\t{\n\t\tmask = BITMASK << ipad;\n\t\t*(VARBITS(result) + VARBITBYTES(result) - 1) &= mask;\n\t}\n\n\tPG_RETURN_VARBIT_P(result);\n}\n\n/*\n *\t\tvarbit_send\t\t\t- converts varbit to binary format\n */\nDatum\nvarbit_send(PG_FUNCTION_ARGS)\n{\n\tVarBit\t   *s = PG_GETARG_VARBIT_P(0);\n\tStringInfoData buf;\n\n\tpq_begintypsend(&buf);\n\tpq_sendint(&buf, VARBITLEN(s), sizeof(int32));\n\tpq_sendbytes(&buf, (char *) VARBITS(s), VARBITBYTES(s));\n\tPG_RETURN_BYTEA_P(pq_endtypsend(&buf));\n}\n\n/*\n * varbit_transform()\n * Flatten calls to varbit's length coercion function that set the new maximum\n * length >= the previous maximum length.  We can ignore the isExplicit\n * argument, since that only affects truncation cases.\n */\nDatum\nvarbit_transform(PG_FUNCTION_ARGS)\n{\n\tFuncExpr   *expr = (FuncExpr *) PG_GETARG_POINTER(0);\n\tNode\t   *ret = NULL;\n\tNode\t   *typmod;\n\n\tAssert(IsA(expr, FuncExpr));\n\tAssert(list_length(expr->args) >= 2);\n\n\ttypmod = (Node *) lsecond(expr->args);\n\n\tif (IsA(typmod, Const) &&!((Const *) typmod)->constisnull)\n\t{\n\t\tNode\t   *source = (Node *) linitial(expr->args);\n\t\tint32\t\tnew_typmod = DatumGetInt32(((Const *) typmod)->constvalue);\n\t\tint32\t\told_max = exprTypmod(source);\n\t\tint32\t\tnew_max = new_typmod;\n\n\t\t/* Note: varbit() treats typmod 0 as invalid, so we do too */\n\t\tif (new_max <= 0 || (old_max > 0 && old_max <= new_max))\n\t\t\tret = relabel_to_typmod(source, new_typmod);\n\t}\n\n\tPG_RETURN_POINTER(ret);\n}\n\n/*\n * varbit()\n * Converts a varbit() type to a specific internal length.\n * len is the maximum bitlength specified in the column definition.\n *\n * If doing implicit cast, raise error when source data is too long.\n * If doing explicit cast, silently truncate to max length.\n */\nDatum\nvarbit(PG_FUNCTION_ARGS)\n{\n\tVarBit\t   *arg = PG_GETARG_VARBIT_P(0);\n\tint32\t\tlen = PG_GETARG_INT32(1);\n\tbool\t\tisExplicit = PG_GETARG_BOOL(2);\n\tVarBit\t   *result;\n\tint\t\t\trlen;\n\tint\t\t\tipad;\n\tbits8\t\tmask;\n\n\t/* No work if typmod is invalid or supplied data matches it already */\n\tif (len <= 0 || len >= VARBITLEN(arg))\n\t\tPG_RETURN_VARBIT_P(arg);\n\n\tif (!isExplicit)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_STRING_DATA_RIGHT_TRUNCATION),\n\t\t\t\t errmsg(\"bit string too long for type bit varying(%d)\",\n\t\t\t\t\t\tlen)));\n\n\trlen = VARBITTOTALLEN(len);\n\tresult = (VarBit *) palloc(rlen);\n\tSET_VARSIZE(result, rlen);\n\tVARBITLEN(result) = len;\n\n\tmemcpy(VARBITS(result), VARBITS(arg), VARBITBYTES(result));\n\n\t/* Make sure last byte is zero-padded if needed */\n\tipad = VARBITPAD(result);\n\tif (ipad > 0)\n\t{\n\t\tmask = BITMASK << ipad;\n\t\t*(VARBITS(result) + VARBITBYTES(result) - 1) &= mask;\n\t}\n\n\tPG_RETURN_VARBIT_P(result);\n}\n\nDatum\nvarbittypmodin(PG_FUNCTION_ARGS)\n{\n\tArrayType  *ta = PG_GETARG_ARRAYTYPE_P(0);\n\n\tPG_RETURN_INT32(anybit_typmodin(ta, \"varbit\"));\n}\n\nDatum\nvarbittypmodout(PG_FUNCTION_ARGS)\n{\n\tint32\t\ttypmod = PG_GETARG_INT32(0);\n\n\tPG_RETURN_CSTRING(anybit_typmodout(typmod));\n}\n\n\n/*\n * Comparison operators\n *\n * We only need one set of comparison operators for bitstrings, as the lengths\n * are stored in the same way for zero-padded and varying bit strings.\n *\n * Note that the standard is not unambiguous about the comparison between\n * zero-padded bit strings and varying bitstrings. If the same value is written\n * into a zero padded bitstring as into a varying bitstring, but the zero\n * padded bitstring has greater length, it will be bigger.\n *\n * Zeros from the beginning of a bitstring cannot simply be ignored, as they\n * may be part of a bit string and may be significant.\n *\n * Note: btree indexes need these routines not to leak memory; therefore,\n * be careful to free working copies of toasted datums.  Most places don't\n * need to be so careful.\n */\n\n/*\n * bit_cmp\n *\n * Compares two bitstrings and returns <0, 0, >0 depending on whether the first\n * string is smaller, equal, or bigger than the second. All bits are considered\n * and additional zero bits may make one string smaller/larger than the other,\n * even if their zero-padded values would be the same.\n */\nstatic int32\nbit_cmp(VarBit *arg1, VarBit *arg2)\n{\n\tint\t\t\tbitlen1,\n\t\t\t\tbytelen1,\n\t\t\t\tbitlen2,\n\t\t\t\tbytelen2;\n\tint32\t\tcmp;\n\n\tbytelen1 = VARBITBYTES(arg1);\n\tbytelen2 = VARBITBYTES(arg2);\n\n\tcmp = memcmp(VARBITS(arg1), VARBITS(arg2), Min(bytelen1, bytelen2));\n\tif (cmp == 0)\n\t{\n\t\tbitlen1 = VARBITLEN(arg1);\n\t\tbitlen2 = VARBITLEN(arg2);\n\t\tif (bitlen1 != bitlen2)\n\t\t\tcmp = (bitlen1 < bitlen2) ? -1 : 1;\n\t}\n\treturn cmp;\n}\n\nDatum\nbiteq(PG_FUNCTION_ARGS)\n{\n\tVarBit\t   *arg1 = PG_GETARG_VARBIT_P(0);\n\tVarBit\t   *arg2 = PG_GETARG_VARBIT_P(1);\n\tbool\t\tresult;\n\tint\t\t\tbitlen1,\n\t\t\t\tbitlen2;\n\n\tbitlen1 = VARBITLEN(arg1);\n\tbitlen2 = VARBITLEN(arg2);\n\n\t/* fast path for different-length inputs */\n\tif (bitlen1 != bitlen2)\n\t\tresult = false;\n\telse\n\t\tresult = (bit_cmp(arg1, arg2) == 0);\n\n\tPG_FREE_IF_COPY(arg1, 0);\n\tPG_FREE_IF_COPY(arg2, 1);\n\n\tPG_RETURN_BOOL(result);\n}\n\nDatum\nbitne(PG_FUNCTION_ARGS)\n{\n\tVarBit\t   *arg1 = PG_GETARG_VARBIT_P(0);\n\tVarBit\t   *arg2 = PG_GETARG_VARBIT_P(1);\n\tbool\t\tresult;\n\tint\t\t\tbitlen1,\n\t\t\t\tbitlen2;\n\n\tbitlen1 = VARBITLEN(arg1);\n\tbitlen2 = VARBITLEN(arg2);\n\n\t/* fast path for different-length inputs */\n\tif (bitlen1 != bitlen2)\n\t\tresult = true;\n\telse\n\t\tresult = (bit_cmp(arg1, arg2) != 0);\n\n\tPG_FREE_IF_COPY(arg1, 0);\n\tPG_FREE_IF_COPY(arg2, 1);\n\n\tPG_RETURN_BOOL(result);\n}\n\nDatum\nbitlt(PG_FUNCTION_ARGS)\n{\n\tVarBit\t   *arg1 = PG_GETARG_VARBIT_P(0);\n\tVarBit\t   *arg2 = PG_GETARG_VARBIT_P(1);\n\tbool\t\tresult;\n\n\tresult = (bit_cmp(arg1, arg2) < 0);\n\n\tPG_FREE_IF_COPY(arg1, 0);\n\tPG_FREE_IF_COPY(arg2, 1);\n\n\tPG_RETURN_BOOL(result);\n}\n\nDatum\nbitle(PG_FUNCTION_ARGS)\n{\n\tVarBit\t   *arg1 = PG_GETARG_VARBIT_P(0);\n\tVarBit\t   *arg2 = PG_GETARG_VARBIT_P(1);\n\tbool\t\tresult;\n\n\tresult = (bit_cmp(arg1, arg2) <= 0);\n\n\tPG_FREE_IF_COPY(arg1, 0);\n\tPG_FREE_IF_COPY(arg2, 1);\n\n\tPG_RETURN_BOOL(result);\n}\n\nDatum\nbitgt(PG_FUNCTION_ARGS)\n{\n\tVarBit\t   *arg1 = PG_GETARG_VARBIT_P(0);\n\tVarBit\t   *arg2 = PG_GETARG_VARBIT_P(1);\n\tbool\t\tresult;\n\n\tresult = (bit_cmp(arg1, arg2) > 0);\n\n\tPG_FREE_IF_COPY(arg1, 0);\n\tPG_FREE_IF_COPY(arg2, 1);\n\n\tPG_RETURN_BOOL(result);\n}\n\nDatum\nbitge(PG_FUNCTION_ARGS)\n{\n\tVarBit\t   *arg1 = PG_GETARG_VARBIT_P(0);\n\tVarBit\t   *arg2 = PG_GETARG_VARBIT_P(1);\n\tbool\t\tresult;\n\n\tresult = (bit_cmp(arg1, arg2) >= 0);\n\n\tPG_FREE_IF_COPY(arg1, 0);\n\tPG_FREE_IF_COPY(arg2, 1);\n\n\tPG_RETURN_BOOL(result);\n}\n\nDatum\nbitcmp(PG_FUNCTION_ARGS)\n{\n\tVarBit\t   *arg1 = PG_GETARG_VARBIT_P(0);\n\tVarBit\t   *arg2 = PG_GETARG_VARBIT_P(1);\n\tint32\t\tresult;\n\n\tresult = bit_cmp(arg1, arg2);\n\n\tPG_FREE_IF_COPY(arg1, 0);\n\tPG_FREE_IF_COPY(arg2, 1);\n\n\tPG_RETURN_INT32(result);\n}\n\n/*\n * bitcat\n * Concatenation of bit strings\n */\nDatum\nbitcat(PG_FUNCTION_ARGS)\n{\n\tVarBit\t   *arg1 = PG_GETARG_VARBIT_P(0);\n\tVarBit\t   *arg2 = PG_GETARG_VARBIT_P(1);\n\n\tPG_RETURN_VARBIT_P(bit_catenate(arg1, arg2));\n}\n\nstatic VarBit *\nbit_catenate(VarBit *arg1, VarBit *arg2)\n{\n\tVarBit\t   *result;\n\tint\t\t\tbitlen1,\n\t\t\t\tbitlen2,\n\t\t\t\tbytelen,\n\t\t\t\tbit1pad,\n\t\t\t\tbit2shift;\n\tbits8\t   *pr,\n\t\t\t   *pa;\n\n\tbitlen1 = VARBITLEN(arg1);\n\tbitlen2 = VARBITLEN(arg2);\n\n\tif (bitlen1 > VARBITMAXLEN - bitlen2)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t\t errmsg(\"bit string length exceeds the maximum allowed (%d)\",\n\t\t\t\t\t\tVARBITMAXLEN)));\n\tbytelen = VARBITTOTALLEN(bitlen1 + bitlen2);\n\n\tresult = (VarBit *) palloc(bytelen);\n\tSET_VARSIZE(result, bytelen);\n\tVARBITLEN(result) = bitlen1 + bitlen2;\n\n\t/* Copy the first bitstring in */\n\tmemcpy(VARBITS(result), VARBITS(arg1), VARBITBYTES(arg1));\n\n\t/* Copy the second bit string */\n\tbit1pad = VARBITPAD(arg1);\n\tif (bit1pad == 0)\n\t{\n\t\tmemcpy(VARBITS(result) + VARBITBYTES(arg1), VARBITS(arg2),\n\t\t\t   VARBITBYTES(arg2));\n\t}\n\telse if (bitlen2 > 0)\n\t{\n\t\t/* We need to shift all the bits to fit */\n\t\tbit2shift = BITS_PER_BYTE - bit1pad;\n\t\tpr = VARBITS(result) + VARBITBYTES(arg1) - 1;\n\t\tfor (pa = VARBITS(arg2); pa < VARBITEND(arg2); pa++)\n\t\t{\n\t\t\t*pr |= ((*pa >> bit2shift) & BITMASK);\n\t\t\tpr++;\n\t\t\tif (pr < VARBITEND(result))\n\t\t\t\t*pr = (*pa << bit1pad) & BITMASK;\n\t\t}\n\t}\n\n\treturn result;\n}\n\n/*\n * bitsubstr\n * retrieve a substring from the bit string.\n * Note, s is 1-based.\n * SQL draft 6.10 9)\n */\nDatum\nbitsubstr(PG_FUNCTION_ARGS)\n{\n\tPG_RETURN_VARBIT_P(bitsubstring(PG_GETARG_VARBIT_P(0),\n\t\t\t\t\t\t\t\t\tPG_GETARG_INT32(1),\n\t\t\t\t\t\t\t\t\tPG_GETARG_INT32(2),\n\t\t\t\t\t\t\t\t\tfalse));\n}\n\nDatum\nbitsubstr_no_len(PG_FUNCTION_ARGS)\n{\n\tPG_RETURN_VARBIT_P(bitsubstring(PG_GETARG_VARBIT_P(0),\n\t\t\t\t\t\t\t\t\tPG_GETARG_INT32(1),\n\t\t\t\t\t\t\t\t\t-1, true));\n}\n\nstatic VarBit *\nbitsubstring(VarBit *arg, int32 s, int32 l, bool length_not_specified)\n{\n\tVarBit\t   *result;\n\tint\t\t\tbitlen,\n\t\t\t\trbitlen,\n\t\t\t\tlen,\n\t\t\t\tipad = 0,\n\t\t\t\tishift,\n\t\t\t\ti;\n\tint\t\t\te,\n\t\t\t\ts1,\n\t\t\t\te1;\n\tbits8\t\tmask,\n\t\t\t   *r,\n\t\t\t   *ps;\n\n\tbitlen = VARBITLEN(arg);\n\ts1 = Max(s, 1);\n\t/* If we do not have an upper bound, use end of string */\n\tif (length_not_specified)\n\t{\n\t\te1 = bitlen + 1;\n\t}\n\telse\n\t{\n\t\te = s + l;\n\n\t\t/*\n\t\t * A negative value for L is the only way for the end position to be\n\t\t * before the start. SQL99 says to throw an error.\n\t\t */\n\t\tif (e < s)\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_SUBSTRING_ERROR),\n\t\t\t\t\t errmsg(\"negative substring length not allowed\")));\n\t\te1 = Min(e, bitlen + 1);\n\t}\n\tif (s1 > bitlen || e1 <= s1)\n\t{\n\t\t/* Need to return a zero-length bitstring */\n\t\tlen = VARBITTOTALLEN(0);\n\t\tresult = (VarBit *) palloc(len);\n\t\tSET_VARSIZE(result, len);\n\t\tVARBITLEN(result) = 0;\n\t}\n\telse\n\t{\n\t\t/*\n\t\t * OK, we've got a true substring starting at position s1-1 and ending\n\t\t * at position e1-1\n\t\t */\n\t\trbitlen = e1 - s1;\n\t\tlen = VARBITTOTALLEN(rbitlen);\n\t\tresult = (VarBit *) palloc(len);\n\t\tSET_VARSIZE(result, len);\n\t\tVARBITLEN(result) = rbitlen;\n\t\tlen -= VARHDRSZ + VARBITHDRSZ;\n\t\t/* Are we copying from a byte boundary? */\n\t\tif ((s1 - 1) % BITS_PER_BYTE == 0)\n\t\t{\n\t\t\t/* Yep, we are copying bytes */\n\t\t\tmemcpy(VARBITS(result), VARBITS(arg) + (s1 - 1) / BITS_PER_BYTE,\n\t\t\t\t   len);\n\t\t}\n\t\telse\n\t\t{\n\t\t\t/* Figure out how much we need to shift the sequence by */\n\t\t\tishift = (s1 - 1) % BITS_PER_BYTE;\n\t\t\tr = VARBITS(result);\n\t\t\tps = VARBITS(arg) + (s1 - 1) / BITS_PER_BYTE;\n\t\t\tfor (i = 0; i < len; i++)\n\t\t\t{\n\t\t\t\t*r = (*ps << ishift) & BITMASK;\n\t\t\t\tif ((++ps) < VARBITEND(arg))\n\t\t\t\t\t*r |= *ps >> (BITS_PER_BYTE - ishift);\n\t\t\t\tr++;\n\t\t\t}\n\t\t}\n\t\t/* Do we need to pad at the end? */\n\t\tipad = VARBITPAD(result);\n\t\tif (ipad > 0)\n\t\t{\n\t\t\tmask = BITMASK << ipad;\n\t\t\t*(VARBITS(result) + len - 1) &= mask;\n\t\t}\n\t}\n\n\treturn result;\n}",
    "diff": " \tslen = strlen(sp);\n \t\tbitlen = slen;\n+\t\tif (slen > VARBITMAXLEN / 4)\n \t\tbitlen = slen * 4;\n \tslen = strlen(sp);\n \t\tbitlen = slen;\n+\t\tif (slen > VARBITMAXLEN / 4)\n \t\tbitlen = slen * 4;",
    "critical_vars": [
      "slen"
    ],
    "variable_definitions": {
      "slen": "int\t\t\tlen,\t\t\t/* Length of the whole data structure */"
    },
    "variable_types": {
      "slen": "integer"
    },
    "type_mapping": {
      "slen": "Integer"
    },
    "vulnerable_line": "if (slen > VARBITMAXLEN / 4)",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The calculation of 'bitlen' based on 'slen' could exceed maximum allowed value during the multiplication, leading to an integer overflow. The fix ensures that 'slen' is checked against VARBITMAXLEN / 4 before proceeding with further calculations, preventing overflow."
  },
  {
    "fix_code": "Datum\nvarbit_in(PG_FUNCTION_ARGS)\n{\n\tchar\t   *input_string = PG_GETARG_CSTRING(0);\n\n#ifdef NOT_USED\n\tOid\t\t\ttypelem = PG_GETARG_OID(1);\n#endif\n\tint32\t\tatttypmod = PG_GETARG_INT32(2);\n\tVarBit\t   *result;\t\t\t/* The resulting bit string\t\t\t  */\n\tchar\t   *sp;\t\t\t\t/* pointer into the character string  */\n\tbits8\t   *r;\t\t\t\t/* pointer into the result */\n\tint\t\t\tlen,\t\t\t/* Length of the whole data structure */\n\t\t\t\tbitlen,\t\t\t/* Number of bits in the bit string   */\n\t\t\t\tslen;\t\t\t/* Length of the input string\t\t  */\n\tbool\t\tbit_not_hex;\t/* false = hex string  true = bit string */\n\tint\t\t\tbc;\n\tbits8\t\tx = 0;\n\n\t/* Check that the first character is a b or an x */\n\tif (input_string[0] == 'b' || input_string[0] == 'B')\n\t{\n\t\tbit_not_hex = true;\n\t\tsp = input_string + 1;\n\t}\n\telse if (input_string[0] == 'x' || input_string[0] == 'X')\n\t{\n\t\tbit_not_hex = false;\n\t\tsp = input_string + 1;\n\t}\n\telse\n\t{\n\t\tbit_not_hex = true;\n\t\tsp = input_string;\n\t}\n\n\t/*\n\t * Determine bitlength from input string.  MaxAllocSize ensures a regular\n\t * input is small enough, but we must check hex input.\n\t */\n\tslen = strlen(sp);\n\tif (bit_not_hex)\n\t\tbitlen = slen;\n\telse\n\t{\n\t\tif (slen > VARBITMAXLEN / 4)\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t\t errmsg(\"bit string length exceeds the maximum allowed (%d)\",\n\t\t\t\t\t\tVARBITMAXLEN)));\n\t\tbitlen = slen * 4;\n\t}\n\n\t/*\n\t * Sometimes atttypmod is not supplied. If it is supplied we need to make\n\t * sure that the bitstring fits.\n\t */\n\tif (atttypmod <= 0)\n\t\tatttypmod = bitlen;\n\telse if (bitlen > atttypmod)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_STRING_DATA_RIGHT_TRUNCATION),\n\t\t\t\t errmsg(\"bit string too long for type bit varying(%d)\",\n\t\t\t\t\t\tatttypmod)));\n\n\tlen = VARBITTOTALLEN(bitlen);\n\t/* set to 0 so that *r is always initialised and string is zero-padded */\n\tresult = (VarBit *) palloc0(len);\n\tSET_VARSIZE(result, len);\n\tVARBITLEN(result) = Min(bitlen, atttypmod);\n\n\tr = VARBITS(result);\n\tif (bit_not_hex)\n\t{\n\t\t/* Parse the bit representation of the string */\n\t\t/* We know it fits, as bitlen was compared to atttypmod */\n\t\tx = HIGHBIT;\n\t\tfor (; *sp; sp++)\n\t\t{\n\t\t\tif (*sp == '1')\n\t\t\t\t*r |= x;\n\t\t\telse if (*sp != '0')\n\t\t\t\tereport(ERROR,\n\t\t\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t\t\t\t errmsg(\"\\\"%c\\\" is not a valid binary digit\",\n\t\t\t\t\t\t\t\t*sp)));\n\n\t\t\tx >>= 1;\n\t\t\tif (x == 0)\n\t\t\t{\n\t\t\t\tx = HIGHBIT;\n\t\t\t\tr++;\n\t\t\t}\n\t\t}\n\t}\n\telse\n\t{\n\t\t/* Parse the hex representation of the string */\n\t\tfor (bc = 0; *sp; sp++)\n\t\t{\n\t\t\tif (*sp >= '0' && *sp <= '9')\n\t\t\t\tx = (bits8) (*sp - '0');\n\t\t\telse if (*sp >= 'A' && *sp <= 'F')\n\t\t\t\tx = (bits8) (*sp - 'A') + 10;\n\t\t\telse if (*sp >= 'a' && *sp <= 'f')\n\t\t\t\tx = (bits8) (*sp - 'a') + 10;\n\t\t\telse\n\t\t\t\tereport(ERROR,\n\t\t\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t\t\t\t errmsg(\"\\\"%c\\\" is not a valid hexadecimal digit\",\n\t\t\t\t\t\t\t\t*sp)));\n\n\t\t\tif (bc)\n\t\t\t{\n\t\t\t\t*r++ |= x;\n\t\t\t\tbc = 0;\n\t\t\t}\n\t\t\telse\n\t\t\t{\n\t\t\t\t*r = x << 4;\n\t\t\t\tbc = 1;\n\t\t\t}\n\t\t}\n\t}\n\n\tPG_RETURN_VARBIT_P(result);\n}",
    "diff": " \tslen = strlen(sp);\n \t\tbitlen = slen;\n+\t\tif (slen > VARBITMAXLEN / 4)\n \t\tbitlen = slen * 4;\n \tslen = strlen(sp);\n \t\tbitlen = slen;\n+\t\tif (slen > VARBITMAXLEN / 4)\n \t\tbitlen = slen * 4;",
    "critical_vars": [
      "slen"
    ],
    "variable_definitions": {
      "slen": "int\t\t\tlen,\t\t\t/* Length of the whole data structure */"
    },
    "variable_types": {
      "slen": "integer"
    },
    "type_mapping": {
      "slen": "Integer"
    },
    "vulnerable_line": "if (slen > VARBITMAXLEN / 4)",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The calculation of bitlen as slen * 4 could overflow if slen is large enough, leading to possible buffer overflow scenarios."
  },
  {
    "fix_code": "Datum\nltree_in(PG_FUNCTION_ARGS)\n{\n\tchar\t   *buf = (char *) PG_GETARG_POINTER(0);\n\tchar\t   *ptr;\n\tnodeitem   *list,\n\t\t\t   *lptr;\n\tint\t\t\tnum = 0,\n\t\t\t\ttotallen = 0;\n\tint\t\t\tstate = LTPRS_WAITNAME;\n\tltree\t   *result;\n\tltree_level *curlevel;\n\tint\t\t\tcharlen;\n\tint\t\t\tpos = 0;\n\n\tptr = buf;\n\twhile (*ptr)\n\t{\n\t\tcharlen = pg_mblen(ptr);\n\t\tif (charlen == 1 && t_iseq(ptr, '.'))\n\t\t\tnum++;\n\t\tptr += charlen;\n\t}\n\n\tif (num + 1 > MaxAllocSize / sizeof(nodeitem))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t errmsg(\"number of levels (%d) exceeds the maximum allowed (%d)\",\n\t\t\t\t\tnum + 1, (int) (MaxAllocSize / sizeof(nodeitem)))));\n\tlist = lptr = (nodeitem *) palloc(sizeof(nodeitem) * (num + 1));\n\tptr = buf;\n\twhile (*ptr)\n\t{\n\t\tcharlen = pg_mblen(ptr);\n\n\t\tif (state == LTPRS_WAITNAME)\n\t\t{\n\t\t\tif (ISALNUM(ptr))\n\t\t\t{\n\t\t\t\tlptr->start = ptr;\n\t\t\t\tlptr->wlen = 0;\n\t\t\t\tstate = LTPRS_WAITDELIM;\n\t\t\t}\n\t\t\telse\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse if (state == LTPRS_WAITDELIM)\n\t\t{\n\t\t\tif (charlen == 1 && t_iseq(ptr, '.'))\n\t\t\t{\n\t\t\t\tlptr->len = ptr - lptr->start;\n\t\t\t\tif (lptr->wlen > 255)\n\t\t\t\t\tereport(ERROR,\n\t\t\t\t\t\t\t(errcode(ERRCODE_NAME_TOO_LONG),\n\t\t\t\t\t\t\t errmsg(\"name of level is too long\"),\n\t\t\t\t\t\t\t errdetail(\"Name length is %d, must \"\n\t\t\t\t\t\t\t\t\t   \"be < 256, in position %d.\",\n\t\t\t\t\t\t\t\t\t   lptr->wlen, pos)));\n\n\t\t\t\ttotallen += MAXALIGN(lptr->len + LEVEL_HDRSIZE);\n\t\t\t\tlptr++;\n\t\t\t\tstate = LTPRS_WAITNAME;\n\t\t\t}\n\t\t\telse if (!ISALNUM(ptr))\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse\n\t\t\t/* internal error */\n\t\t\telog(ERROR, \"internal error in parser\");\n\n\t\tptr += charlen;\n\t\tlptr->wlen++;\n\t\tpos++;\n\t}\n\n\tif (state == LTPRS_WAITDELIM)\n\t{\n\t\tlptr->len = ptr - lptr->start;\n\t\tif (lptr->wlen > 255)\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_NAME_TOO_LONG),\n\t\t\t\t\t errmsg(\"name of level is too long\"),\n\t\t\t\t\t errdetail(\"Name length is %d, must \"\n\t\t\t\t\t\t\t   \"be < 256, in position %d.\",\n\t\t\t\t\t\t\t   lptr->wlen, pos)));\n\n\t\ttotallen += MAXALIGN(lptr->len + LEVEL_HDRSIZE);\n\t\tlptr++;\n\t}\n\telse if (!(state == LTPRS_WAITNAME && lptr == list))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_SYNTAX_ERROR),\n\t\t\t\t errmsg(\"syntax error\"),\n\t\t\t\t errdetail(\"Unexpected end of line.\")));\n\n\tresult = (ltree *) palloc0(LTREE_HDRSIZE + totallen);\n\tSET_VARSIZE(result, LTREE_HDRSIZE + totallen);\n\tresult->numlevel = lptr - list;\n\tcurlevel = LTREE_FIRST(result);\n\tlptr = list;\n\twhile (lptr - list < result->numlevel)\n\t{\n\t\tcurlevel->len = (uint16) lptr->len;\n\t\tmemcpy(curlevel->name, lptr->start, lptr->len);\n\t\tcurlevel = LEVEL_NEXT(curlevel);\n\t\tlptr++;\n\t}\n\n\tpfree(list);\n\tPG_RETURN_POINTER(result);\n}",
    "diff": "+\tif (num + 1 > MaxAllocSize / sizeof(nodeitem))\n+\t\t\t errmsg(\"number of levels (%d) exceeds the maximum allowed (%d)\",\n+\t\t\t\t\tnum + 1, (int) (MaxAllocSize / sizeof(nodeitem)))));\n \tlist = lptr = (nodeitem *) palloc(sizeof(nodeitem) * (num + 1));\n \tnum++;\n+\tif (num > MaxAllocSize / ITEMSIZE)\n+\t\t\t errmsg(\"number of levels (%d) exceeds the maximum allowed (%d)\",\n+\t\t\t\t\tnum, (int) (MaxAllocSize / ITEMSIZE))));\n \tcurqlevel = tmpql = (lquery_level *) palloc0(ITEMSIZE * num);",
    "critical_vars": [
      "num"
    ],
    "variable_definitions": {
      "num": "int\t\t\tnum = 0,"
    },
    "variable_types": {
      "num": "integer"
    },
    "type_mapping": {
      "num": "Integer"
    },
    "vulnerable_line": "if (num + 1 > MaxAllocSize / sizeof(nodeitem))",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The addition of num + 1 can cause an integer overflow if num is at its maximum limit, potentially leading to a condition where the resulting value is mistakenly interpreted as valid, allowing for a buffer overflow when memory is allocated."
  },
  {
    "fix_code": "Datum\nlquery_in(PG_FUNCTION_ARGS)\n{\n\tchar\t   *buf = (char *) PG_GETARG_POINTER(0);\n\tchar\t   *ptr;\n\tint\t\t\tnum = 0,\n\t\t\t\ttotallen = 0,\n\t\t\t\tnumOR = 0;\n\tint\t\t\tstate = LQPRS_WAITLEVEL;\n\tlquery\t   *result;\n\tnodeitem   *lptr = NULL;\n\tlquery_level *cur,\n\t\t\t   *curqlevel,\n\t\t\t   *tmpql;\n\tlquery_variant *lrptr = NULL;\n\tbool\t\thasnot = false;\n\tbool\t\twasbad = false;\n\tint\t\t\tcharlen;\n\tint\t\t\tpos = 0;\n\n\tptr = buf;\n\twhile (*ptr)\n\t{\n\t\tcharlen = pg_mblen(ptr);\n\n\t\tif (charlen == 1)\n\t\t{\n\t\t\tif (t_iseq(ptr, '.'))\n\t\t\t\tnum++;\n\t\t\telse if (t_iseq(ptr, '|'))\n\t\t\t\tnumOR++;\n\t\t}\n\n\t\tptr += charlen;\n\t}\n\n\tnum++;\n\tif (num > MaxAllocSize / ITEMSIZE)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t errmsg(\"number of levels (%d) exceeds the maximum allowed (%d)\",\n\t\t\t\t\tnum, (int) (MaxAllocSize / ITEMSIZE))));\n\tcurqlevel = tmpql = (lquery_level *) palloc0(ITEMSIZE * num);\n\tptr = buf;\n\twhile (*ptr)\n\t{\n\t\tcharlen = pg_mblen(ptr);\n\n\t\tif (state == LQPRS_WAITLEVEL)\n\t\t{\n\t\t\tif (ISALNUM(ptr))\n\t\t\t{\n\t\t\t\tGETVAR(curqlevel) = lptr = (nodeitem *) palloc0(sizeof(nodeitem) * (numOR + 1));\n\t\t\t\tlptr->start = ptr;\n\t\t\t\tstate = LQPRS_WAITDELIM;\n\t\t\t\tcurqlevel->numvar = 1;\n\t\t\t}\n\t\t\telse if (charlen == 1 && t_iseq(ptr, '!'))\n\t\t\t{\n\t\t\t\tGETVAR(curqlevel) = lptr = (nodeitem *) palloc0(sizeof(nodeitem) * (numOR + 1));\n\t\t\t\tlptr->start = ptr + 1;\n\t\t\t\tstate = LQPRS_WAITDELIM;\n\t\t\t\tcurqlevel->numvar = 1;\n\t\t\t\tcurqlevel->flag |= LQL_NOT;\n\t\t\t\thasnot = true;\n\t\t\t}\n\t\t\telse if (charlen == 1 && t_iseq(ptr, '*'))\n\t\t\t\tstate = LQPRS_WAITOPEN;\n\t\t\telse\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse if (state == LQPRS_WAITVAR)\n\t\t{\n\t\t\tif (ISALNUM(ptr))\n\t\t\t{\n\t\t\t\tlptr++;\n\t\t\t\tlptr->start = ptr;\n\t\t\t\tstate = LQPRS_WAITDELIM;\n\t\t\t\tcurqlevel->numvar++;\n\t\t\t}\n\t\t\telse\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse if (state == LQPRS_WAITDELIM)\n\t\t{\n\t\t\tif (charlen == 1 && t_iseq(ptr, '@'))\n\t\t\t{\n\t\t\t\tif (lptr->start == ptr)\n\t\t\t\t\tUNCHAR;\n\t\t\t\tlptr->flag |= LVAR_INCASE;\n\t\t\t\tcurqlevel->flag |= LVAR_INCASE;\n\t\t\t}\n\t\t\telse if (charlen == 1 && t_iseq(ptr, '*'))\n\t\t\t{\n\t\t\t\tif (lptr->start == ptr)\n\t\t\t\t\tUNCHAR;\n\t\t\t\tlptr->flag |= LVAR_ANYEND;\n\t\t\t\tcurqlevel->flag |= LVAR_ANYEND;\n\t\t\t}\n\t\t\telse if (charlen == 1 && t_iseq(ptr, '%'))\n\t\t\t{\n\t\t\t\tif (lptr->start == ptr)\n\t\t\t\t\tUNCHAR;\n\t\t\t\tlptr->flag |= LVAR_SUBLEXEME;\n\t\t\t\tcurqlevel->flag |= LVAR_SUBLEXEME;\n\t\t\t}\n\t\t\telse if (charlen == 1 && t_iseq(ptr, '|'))\n\t\t\t{\n\t\t\t\tlptr->len = ptr - lptr->start -\n\t\t\t\t\t((lptr->flag & LVAR_SUBLEXEME) ? 1 : 0) -\n\t\t\t\t\t((lptr->flag & LVAR_INCASE) ? 1 : 0) -\n\t\t\t\t\t((lptr->flag & LVAR_ANYEND) ? 1 : 0);\n\t\t\t\tif (lptr->wlen > 255)\n\t\t\t\t\tereport(ERROR,\n\t\t\t\t\t\t\t(errcode(ERRCODE_NAME_TOO_LONG),\n\t\t\t\t\t\t\t errmsg(\"name of level is too long\"),\n\t\t\t\t\t\t\t errdetail(\"Name length is %d, must \"\n\t\t\t\t\t\t\t\t\t   \"be < 256, in position %d.\",\n\t\t\t\t\t\t\t\t\t   lptr->wlen, pos)));\n\n\t\t\t\tstate = LQPRS_WAITVAR;\n\t\t\t}\n\t\t\telse if (charlen == 1 && t_iseq(ptr, '.'))\n\t\t\t{\n\t\t\t\tlptr->len = ptr - lptr->start -\n\t\t\t\t\t((lptr->flag & LVAR_SUBLEXEME) ? 1 : 0) -\n\t\t\t\t\t((lptr->flag & LVAR_INCASE) ? 1 : 0) -\n\t\t\t\t\t((lptr->flag & LVAR_ANYEND) ? 1 : 0);\n\t\t\t\tif (lptr->wlen > 255)\n\t\t\t\t\tereport(ERROR,\n\t\t\t\t\t\t\t(errcode(ERRCODE_NAME_TOO_LONG),\n\t\t\t\t\t\t\t errmsg(\"name of level is too long\"),\n\t\t\t\t\t\t\t errdetail(\"Name length is %d, must \"\n\t\t\t\t\t\t\t\t\t   \"be < 256, in position %d.\",\n\t\t\t\t\t\t\t\t\t   lptr->wlen, pos)));\n\n\t\t\t\tstate = LQPRS_WAITLEVEL;\n\t\t\t\tcurqlevel = NEXTLEV(curqlevel);\n\t\t\t}\n\t\t\telse if (ISALNUM(ptr))\n\t\t\t{\n\t\t\t\tif (lptr->flag)\n\t\t\t\t\tUNCHAR;\n\t\t\t}\n\t\t\telse\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse if (state == LQPRS_WAITOPEN)\n\t\t{\n\t\t\tif (charlen == 1 && t_iseq(ptr, '{'))\n\t\t\t\tstate = LQPRS_WAITFNUM;\n\t\t\telse if (charlen == 1 && t_iseq(ptr, '.'))\n\t\t\t{\n\t\t\t\tcurqlevel->low = 0;\n\t\t\t\tcurqlevel->high = 0xffff;\n\t\t\t\tcurqlevel = NEXTLEV(curqlevel);\n\t\t\t\tstate = LQPRS_WAITLEVEL;\n\t\t\t}\n\t\t\telse\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse if (state == LQPRS_WAITFNUM)\n\t\t{\n\t\t\tif (charlen == 1 && t_iseq(ptr, ','))\n\t\t\t\tstate = LQPRS_WAITSNUM;\n\t\t\telse if (t_isdigit(ptr))\n\t\t\t{\n\t\t\t\tcurqlevel->low = atoi(ptr);\n\t\t\t\tstate = LQPRS_WAITND;\n\t\t\t}\n\t\t\telse\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse if (state == LQPRS_WAITSNUM)\n\t\t{\n\t\t\tif (t_isdigit(ptr))\n\t\t\t{\n\t\t\t\tcurqlevel->high = atoi(ptr);\n\t\t\t\tstate = LQPRS_WAITCLOSE;\n\t\t\t}\n\t\t\telse if (charlen == 1 && t_iseq(ptr, '}'))\n\t\t\t{\n\t\t\t\tcurqlevel->high = 0xffff;\n\t\t\t\tstate = LQPRS_WAITEND;\n\t\t\t}\n\t\t\telse\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse if (state == LQPRS_WAITCLOSE)\n\t\t{\n\t\t\tif (charlen == 1 && t_iseq(ptr, '}'))\n\t\t\t\tstate = LQPRS_WAITEND;\n\t\t\telse if (!t_isdigit(ptr))\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse if (state == LQPRS_WAITND)\n\t\t{\n\t\t\tif (charlen == 1 && t_iseq(ptr, '}'))\n\t\t\t{\n\t\t\t\tcurqlevel->high = curqlevel->low;\n\t\t\t\tstate = LQPRS_WAITEND;\n\t\t\t}\n\t\t\telse if (charlen == 1 && t_iseq(ptr, ','))\n\t\t\t\tstate = LQPRS_WAITSNUM;\n\t\t\telse if (!t_isdigit(ptr))\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse if (state == LQPRS_WAITEND)\n\t\t{\n\t\t\tif (charlen == 1 && t_iseq(ptr, '.'))\n\t\t\t{\n\t\t\t\tstate = LQPRS_WAITLEVEL;\n\t\t\t\tcurqlevel = NEXTLEV(curqlevel);\n\t\t\t}\n\t\t\telse\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse\n\t\t\t/* internal error */\n\t\t\telog(ERROR, \"internal error in parser\");\n\n\t\tptr += charlen;\n\t\tif (state == LQPRS_WAITDELIM)\n\t\t\tlptr->wlen++;\n\t\tpos++;\n\t}\n\n\tif (state == LQPRS_WAITDELIM)\n\t{\n\t\tif (lptr->start == ptr)\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_SYNTAX_ERROR),\n\t\t\t\t\t errmsg(\"syntax error\"),\n\t\t\t\t\t errdetail(\"Unexpected end of line.\")));\n\n\t\tlptr->len = ptr - lptr->start -\n\t\t\t((lptr->flag & LVAR_SUBLEXEME) ? 1 : 0) -\n\t\t\t((lptr->flag & LVAR_INCASE) ? 1 : 0) -\n\t\t\t((lptr->flag & LVAR_ANYEND) ? 1 : 0);\n\t\tif (lptr->len == 0)\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_SYNTAX_ERROR),\n\t\t\t\t\t errmsg(\"syntax error\"),\n\t\t\t\t\t errdetail(\"Unexpected end of line.\")));\n\n\t\tif (lptr->wlen > 255)\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_NAME_TOO_LONG),\n\t\t\t\t\t errmsg(\"name of level is too long\"),\n\t\t\t\t\t errdetail(\"Name length is %d, must \"\n\t\t\t\t\t\t\t   \"be < 256, in position %d.\",\n\t\t\t\t\t\t\t   lptr->wlen, pos)));\n\t}\n\telse if (state == LQPRS_WAITOPEN)\n\t\tcurqlevel->high = 0xffff;\n\telse if (state != LQPRS_WAITEND)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_SYNTAX_ERROR),\n\t\t\t\t errmsg(\"syntax error\"),\n\t\t\t\t errdetail(\"Unexpected end of line.\")));\n\n\tcurqlevel = tmpql;\n\ttotallen = LQUERY_HDRSIZE;\n\twhile ((char *) curqlevel - (char *) tmpql < num * ITEMSIZE)\n\t{\n\t\ttotallen += LQL_HDRSIZE;\n\t\tif (curqlevel->numvar)\n\t\t{\n\t\t\tlptr = GETVAR(curqlevel);\n\t\t\twhile (lptr - GETVAR(curqlevel) < curqlevel->numvar)\n\t\t\t{\n\t\t\t\ttotallen += MAXALIGN(LVAR_HDRSIZE + lptr->len);\n\t\t\t\tlptr++;\n\t\t\t}\n\t\t}\n\t\telse if (curqlevel->low > curqlevel->high)\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_SYNTAX_ERROR),\n\t\t\t\t\t errmsg(\"syntax error\"),\n\t\t\t\t\t errdetail(\"Low limit(%d) is greater than upper(%d).\",\n\t\t\t\t\t\t\t   curqlevel->low, curqlevel->high)));\n\n\t\tcurqlevel = NEXTLEV(curqlevel);\n\t}\n\n\tresult = (lquery *) palloc0(totallen);\n\tSET_VARSIZE(result, totallen);\n\tresult->numlevel = num;\n\tresult->firstgood = 0;\n\tresult->flag = 0;\n\tif (hasnot)\n\t\tresult->flag |= LQUERY_HASNOT;\n\tcur = LQUERY_FIRST(result);\n\tcurqlevel = tmpql;\n\twhile ((char *) curqlevel - (char *) tmpql < num * ITEMSIZE)\n\t{\n\t\tmemcpy(cur, curqlevel, LQL_HDRSIZE);\n\t\tcur->totallen = LQL_HDRSIZE;\n\t\tif (curqlevel->numvar)\n\t\t{\n\t\t\tlrptr = LQL_FIRST(cur);\n\t\t\tlptr = GETVAR(curqlevel);\n\t\t\twhile (lptr - GETVAR(curqlevel) < curqlevel->numvar)\n\t\t\t{\n\t\t\t\tcur->totallen += MAXALIGN(LVAR_HDRSIZE + lptr->len);\n\t\t\t\tlrptr->len = lptr->len;\n\t\t\t\tlrptr->flag = lptr->flag;\n\t\t\t\tlrptr->val = ltree_crc32_sz(lptr->start, lptr->len);\n\t\t\t\tmemcpy(lrptr->name, lptr->start, lptr->len);\n\t\t\t\tlptr++;\n\t\t\t\tlrptr = LVAR_NEXT(lrptr);\n\t\t\t}\n\t\t\tpfree(GETVAR(curqlevel));\n\t\t\tif (cur->numvar > 1 || cur->flag != 0)\n\t\t\t\twasbad = true;\n\t\t\telse if (wasbad == false)\n\t\t\t\t(result->firstgood)++;\n\t\t}\n\t\telse\n\t\t\twasbad = true;\n\t\tcurqlevel = NEXTLEV(curqlevel);\n\t\tcur = LQL_NEXT(cur);\n\t}\n\n\tpfree(tmpql);\n\tPG_RETURN_POINTER(result);\n}",
    "diff": "+\tif (num + 1 > MaxAllocSize / sizeof(nodeitem))\n+\t\t\t errmsg(\"number of levels (%d) exceeds the maximum allowed (%d)\",\n+\t\t\t\t\tnum + 1, (int) (MaxAllocSize / sizeof(nodeitem)))));\n \tlist = lptr = (nodeitem *) palloc(sizeof(nodeitem) * (num + 1));\n \tnum++;\n+\tif (num > MaxAllocSize / ITEMSIZE)\n+\t\t\t errmsg(\"number of levels (%d) exceeds the maximum allowed (%d)\",\n+\t\t\t\t\tnum, (int) (MaxAllocSize / ITEMSIZE))));\n \tcurqlevel = tmpql = (lquery_level *) palloc0(ITEMSIZE * num);",
    "critical_vars": [
      "num"
    ],
    "variable_definitions": {
      "num": "int\t\t\tnum = 0,"
    },
    "variable_types": {
      "num": "integer"
    },
    "type_mapping": {
      "num": "Integer"
    },
    "vulnerable_line": "if (num > MaxAllocSize / ITEMSIZE)",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The expression can result in an integer overflow if 'num' is large enough, leading to the wrong calculation of the allowable maximum size and possibly allocating a buffer larger than intended."
  },
  {
    "fix_code": "Datum\nbqarr_in(PG_FUNCTION_ARGS)\n{\n\tchar\t   *buf = (char *) PG_GETARG_POINTER(0);\n\tWORKSTATE\tstate;\n\tint32\t\ti;\n\tQUERYTYPE  *query;\n\tint32\t\tcommonlen;\n\tITEM\t   *ptr;\n\tNODE\t   *tmp;\n\tint32\t\tpos = 0;\n\n#ifdef BS_DEBUG\n\tStringInfoData pbuf;\n#endif\n\n\tstate.buf = buf;\n\tstate.state = WAITOPERAND;\n\tstate.count = 0;\n\tstate.num = 0;\n\tstate.str = NULL;\n\n\t/* make polish notation (postfix, but in reverse order) */\n\tmakepol(&state);\n\tif (!state.num)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_PARAMETER_VALUE),\n\t\t\t\t errmsg(\"empty query\")));\n\n\tif (state.num > QUERYTYPEMAXITEMS)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\terrmsg(\"number of query items (%d) exceeds the maximum allowed (%d)\",\n\t\t\t   state.num, (int) QUERYTYPEMAXITEMS)));\n\tcommonlen = COMPUTESIZE(state.num);\n\n\tquery = (QUERYTYPE *) palloc(commonlen);\n\tSET_VARSIZE(query, commonlen);\n\tquery->size = state.num;\n\tptr = GETQUERY(query);\n\n\tfor (i = state.num - 1; i >= 0; i--)\n\t{\n\t\tptr[i].type = state.str->type;\n\t\tptr[i].val = state.str->val;\n\t\ttmp = state.str->next;\n\t\tpfree(state.str);\n\t\tstate.str = tmp;\n\t}\n\n\tpos = query->size - 1;\n\tfindoprnd(ptr, &pos);\n#ifdef BS_DEBUG\n\tinitStringInfo(&pbuf);\n\tfor (i = 0; i < query->size; i++)\n\t{\n\t\tif (ptr[i].type == OPR)\n\t\t\tappendStringInfo(&pbuf, \"%c(%d) \", ptr[i].val, ptr[i].left);\n\t\telse\n\t\t\tappendStringInfo(&pbuf, \"%d \", ptr[i].val);\n\t}\n\telog(DEBUG3, \"POR: %s\", pbuf.data);\n\tpfree(pbuf.data);\n#endif\n\n\tPG_RETURN_POINTER(query);\n}",
    "diff": "+\tif (state.num > QUERYTYPEMAXITEMS)\n+\t\t\t   state.num, (int) QUERYTYPEMAXITEMS)));\n \tcommonlen = COMPUTESIZE(state.num);\n \tquery->size = state.num;",
    "critical_vars": [
      "state.num"
    ],
    "variable_definitions": {
      "state.num": "Definition not found"
    },
    "variable_types": {
      "state.num": "struct.integer"
    },
    "type_mapping": {
      "state.num": "st.i"
    },
    "vulnerable_line": "commonlen = COMPUTESIZE(state.num);",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The COMPUTESIZE function uses state.num for calculations and may lead to exceeding the maximum value for the arithmetic operations involved, causing an integer overflow."
  },
  {
    "fix_code": "Datum\npath_in(PG_FUNCTION_ARGS)\n{\n\tchar\t   *str = PG_GETARG_CSTRING(0);\n\tPATH\t   *path;\n\tint\t\t\tisopen;\n\tchar\t   *s;\n\tint\t\t\tnpts;\n\tint\t\t\tsize;\n\tint\t\t\tbase_size;\n\tint\t\t\tdepth = 0;\n\n\tif ((npts = pair_count(str, ',')) <= 0)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t\t errmsg(\"invalid input syntax for type path: \\\"%s\\\"\", str)));\n\n\ts = str;\n\twhile (isspace((unsigned char) *s))\n\t\ts++;\n\n\t/* skip single leading paren */\n\tif ((*s == LDELIM) && (strrchr(s, LDELIM) == s))\n\t{\n\t\ts++;\n\t\tdepth++;\n\t}\n\n\tbase_size = sizeof(path->p[0]) * npts;\n\tsize = offsetof(PATH, p[0]) + base_size;\n\n\t/* Check for integer overflow */\n\tif (base_size / npts != sizeof(path->p[0]) || size <= base_size)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t\t errmsg(\"too many points requested\")));\n\n\tpath = (PATH *) palloc(size);\n\n\tSET_VARSIZE(path, size);\n\tpath->npts = npts;\n\n\tif ((!path_decode(TRUE, npts, s, &isopen, &s, &(path->p[0])))\n\t&& (!((depth == 0) && (*s == '\\0'))) && !((depth >= 1) && (*s == RDELIM)))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t\t errmsg(\"invalid input syntax for type path: \\\"%s\\\"\", str)));\n\n\tpath->closed = (!isopen);\n\t/* prevent instability in unused pad bytes */\n\tpath->dummy = 0;\n\n\tPG_RETURN_PATH_P(path);\n}",
    "diff": " \tint\t\t\tsize;\n+\tint\t\t\tbase_size;\n-\tsize = offsetof(PATH, p[0]) +sizeof(path->p[0]) * npts;\n+\tbase_size = sizeof(path->p[0]) * npts;\n+\tsize = offsetof(PATH, p[0]) + base_size;\n+\tif (base_size / npts != sizeof(path->p[0]) || size <= base_size)\n \tpath = (PATH *) palloc(size);\n \tSET_VARSIZE(path, size);\n \tint\t\t\tsize;\n+\tint\t\t\tbase_size;\n-\tsize = offsetof(POLYGON, p[0]) +sizeof(poly->p[0]) * npts;\n+\tbase_size = sizeof(poly->p[0]) * npts;\n+\tsize = offsetof(POLYGON, p[0]) + base_size;\n+\tif (base_size / npts != sizeof(poly->p[0]) || size <= base_size)\n \tpoly = (POLYGON *) palloc0(size);\t/* zero any holes */\n \tSET_VARSIZE(poly, size);\n+\t * Never overflows: the old size fit in MaxAllocSize, and the new size is\n \tsize = offsetof(POLYGON, p[0]) +sizeof(poly->p[0]) * path->npts;\n \tpoly = (POLYGON *) palloc(size);\n \tint\t\t\tsize;\n+\t * Never overflows: the old size fit in MaxAllocSize, and the new size is\n \tsize = offsetof(PATH, p[0]) +sizeof(path->p[0]) * poly->npts;\n \tpath = (PATH *) palloc(size);",
    "critical_vars": [
      "size"
    ],
    "variable_definitions": {
      "size": "int\t\t\tsize;"
    },
    "variable_types": {
      "size": "integer"
    },
    "type_mapping": {
      "size": "Integer"
    },
    "vulnerable_line": "if (base_size / npts != sizeof(path->p[0]) || size <= base_size)",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The division and subsequent checks may miscalculate the size if base_size becomes too large due to an overflow when using npts, leading to insufficient memory allocation and potential buffer overflow vulnerabilities."
  },
  {
    "fix_code": "Datum\npath_in(PG_FUNCTION_ARGS)\n{\n\tchar\t   *str = PG_GETARG_CSTRING(0);\n\tPATH\t   *path;\n\tint\t\t\tisopen;\n\tchar\t   *s;\n\tint\t\t\tnpts;\n\tint\t\t\tsize;\n\tint\t\t\tbase_size;\n\tint\t\t\tdepth = 0;\n\n\tif ((npts = pair_count(str, ',')) <= 0)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t\t errmsg(\"invalid input syntax for type path: \\\"%s\\\"\", str)));\n\n\ts = str;\n\twhile (isspace((unsigned char) *s))\n\t\ts++;\n\n\t/* skip single leading paren */\n\tif ((*s == LDELIM) && (strrchr(s, LDELIM) == s))\n\t{\n\t\ts++;\n\t\tdepth++;\n\t}\n\n\tbase_size = sizeof(path->p[0]) * npts;\n\tsize = offsetof(PATH, p[0]) + base_size;\n\n\t/* Check for integer overflow */\n\tif (base_size / npts != sizeof(path->p[0]) || size <= base_size)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t\t errmsg(\"too many points requested\")));\n\n\tpath = (PATH *) palloc(size);\n\n\tSET_VARSIZE(path, size);\n\tpath->npts = npts;\n\n\tif ((!path_decode(TRUE, npts, s, &isopen, &s, &(path->p[0])))\n\t&& (!((depth == 0) && (*s == '\\0'))) && !((depth >= 1) && (*s == RDELIM)))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t\t errmsg(\"invalid input syntax for type path: \\\"%s\\\"\", str)));\n\n\tpath->closed = (!isopen);\n\t/* prevent instability in unused pad bytes */\n\tpath->dummy = 0;\n\n\tPG_RETURN_PATH_P(path);\n}",
    "diff": "+\tint\t\t\tbase_size;\n+\tbase_size = sizeof(path->p[0]) * npts;\n+\tsize = offsetof(PATH, p[0]) + base_size;\n+\tif (base_size / npts != sizeof(path->p[0]) || size <= base_size)\n+\tint\t\t\tbase_size;\n+\tbase_size = sizeof(poly->p[0]) * npts;\n+\tsize = offsetof(POLYGON, p[0]) + base_size;\n+\tif (base_size / npts != sizeof(poly->p[0]) || size <= base_size)",
    "critical_vars": [
      "base_size"
    ],
    "variable_definitions": {
      "base_size": "int\t\t\tbase_size;"
    },
    "variable_types": {
      "base_size": "integer"
    },
    "type_mapping": {
      "base_size": "Integer"
    },
    "vulnerable_line": "if (base_size / npts != sizeof(path->p[0]) || size <= base_size)",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The calculation of base_size could overflow if npts is a large value, causing the resulting size to be incorrect, leading to potential buffer overflow during memory allocation."
  },
  {
    "fix_code": "Datum\npoly_in(PG_FUNCTION_ARGS)\n{\n\tchar\t   *str = PG_GETARG_CSTRING(0);\n\tPOLYGON    *poly;\n\tint\t\t\tnpts;\n\tint\t\t\tsize;\n\tint\t\t\tbase_size;\n\tint\t\t\tisopen;\n\tchar\t   *s;\n\n\tif ((npts = pair_count(str, ',')) <= 0)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t  errmsg(\"invalid input syntax for type polygon: \\\"%s\\\"\", str)));\n\n\tbase_size = sizeof(poly->p[0]) * npts;\n\tsize = offsetof(POLYGON, p[0]) + base_size;\n\n\t/* Check for integer overflow */\n\tif (base_size / npts != sizeof(poly->p[0]) || size <= base_size)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t\t errmsg(\"too many points requested\")));\n\n\tpoly = (POLYGON *) palloc0(size);\t/* zero any holes */\n\n\tSET_VARSIZE(poly, size);\n\tpoly->npts = npts;\n\n\tif ((!path_decode(FALSE, npts, str, &isopen, &s, &(poly->p[0])))\n\t\t|| (*s != '\\0'))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t  errmsg(\"invalid input syntax for type polygon: \\\"%s\\\"\", str)));\n\n\tmake_bound_box(poly);\n\n\tPG_RETURN_POLYGON_P(poly);\n}",
    "diff": " \tint\t\t\tsize;\n+\tint\t\t\tbase_size;\n-\tsize = offsetof(PATH, p[0]) +sizeof(path->p[0]) * npts;\n+\tbase_size = sizeof(path->p[0]) * npts;\n+\tsize = offsetof(PATH, p[0]) + base_size;\n+\tif (base_size / npts != sizeof(path->p[0]) || size <= base_size)\n \tpath = (PATH *) palloc(size);\n \tSET_VARSIZE(path, size);\n \tint\t\t\tsize;\n+\tint\t\t\tbase_size;\n-\tsize = offsetof(POLYGON, p[0]) +sizeof(poly->p[0]) * npts;\n+\tbase_size = sizeof(poly->p[0]) * npts;\n+\tsize = offsetof(POLYGON, p[0]) + base_size;\n+\tif (base_size / npts != sizeof(poly->p[0]) || size <= base_size)\n \tpoly = (POLYGON *) palloc0(size);\t/* zero any holes */\n \tSET_VARSIZE(poly, size);\n+\t * Never overflows: the old size fit in MaxAllocSize, and the new size is\n \tsize = offsetof(POLYGON, p[0]) +sizeof(poly->p[0]) * path->npts;\n \tpoly = (POLYGON *) palloc(size);\n \tint\t\t\tsize;\n+\t * Never overflows: the old size fit in MaxAllocSize, and the new size is\n \tsize = offsetof(PATH, p[0]) +sizeof(path->p[0]) * poly->npts;\n \tpath = (PATH *) palloc(size);",
    "critical_vars": [
      "size"
    ],
    "variable_definitions": {
      "size": "int\t\t\tsize;"
    },
    "variable_types": {
      "size": "integer"
    },
    "type_mapping": {
      "size": "Integer"
    },
    "vulnerable_line": "size = offsetof(POLYGON, p[0]) + base_size;",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The calculation of 'size' adds 'offsetof(POLYGON, p[0])' and 'base_size', which can exceed the maximum value for an int, leading to overflow and potential buffer overflows when allocating memory."
  },
  {
    "fix_code": "Datum\npoly_in(PG_FUNCTION_ARGS)\n{\n\tchar\t   *str = PG_GETARG_CSTRING(0);\n\tPOLYGON    *poly;\n\tint\t\t\tnpts;\n\tint\t\t\tsize;\n\tint\t\t\tbase_size;\n\tint\t\t\tisopen;\n\tchar\t   *s;\n\n\tif ((npts = pair_count(str, ',')) <= 0)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t  errmsg(\"invalid input syntax for type polygon: \\\"%s\\\"\", str)));\n\n\tbase_size = sizeof(poly->p[0]) * npts;\n\tsize = offsetof(POLYGON, p[0]) + base_size;\n\n\t/* Check for integer overflow */\n\tif (base_size / npts != sizeof(poly->p[0]) || size <= base_size)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t\t errmsg(\"too many points requested\")));\n\n\tpoly = (POLYGON *) palloc0(size);\t/* zero any holes */\n\n\tSET_VARSIZE(poly, size);\n\tpoly->npts = npts;\n\n\tif ((!path_decode(FALSE, npts, str, &isopen, &s, &(poly->p[0])))\n\t\t|| (*s != '\\0'))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t  errmsg(\"invalid input syntax for type polygon: \\\"%s\\\"\", str)));\n\n\tmake_bound_box(poly);\n\n\tPG_RETURN_POLYGON_P(poly);\n}",
    "diff": "+\tint\t\t\tbase_size;\n+\tbase_size = sizeof(path->p[0]) * npts;\n+\tsize = offsetof(PATH, p[0]) + base_size;\n+\tif (base_size / npts != sizeof(path->p[0]) || size <= base_size)\n+\tint\t\t\tbase_size;\n+\tbase_size = sizeof(poly->p[0]) * npts;\n+\tsize = offsetof(POLYGON, p[0]) + base_size;\n+\tif (base_size / npts != sizeof(poly->p[0]) || size <= base_size)",
    "critical_vars": [
      "base_size"
    ],
    "variable_definitions": {
      "base_size": "int\t\t\tbase_size;"
    },
    "variable_types": {
      "base_size": "integer"
    },
    "type_mapping": {
      "base_size": "Integer"
    },
    "vulnerable_line": "if (base_size / npts != sizeof(poly->p[0]) || size <= base_size)",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "If npts is less than or equal to 0, base_size can lead to an incorrect calculation, causing potential integer overflow when calculating size, which may result in a buffer overflow during memory allocation."
  },
  {
    "fix_code": "Function not found",
    "diff": "+\tif (LTXTQUERY_TOO_BIG(state.num, state.sumlen))\n \tcommonlen = COMPUTESIZE(state.num, state.sumlen);\n \tquery->size = state.num;",
    "critical_vars": [
      "state.num",
      "state.sumlen"
    ],
    "variable_definitions": {
      "state.num": "Definition not found",
      "state.sumlen": "Definition not found"
    },
    "variable_types": {
      "state.num": "struct.integer",
      "state.sumlen": "struct.integer"
    },
    "type_mapping": {
      "state.num": "st.i",
      "state.sumlen": "st.i"
    },
    "vulnerable_line": "commonlen = COMPUTESIZE(state.num, state.sumlen);",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The computation for commonlen using state.num and state.sumlen may exceed the maximum allowable value, causing an overflow, which can lead to insufficient allocation and potential buffer overflow vulnerabilities."
  },
  {
    "fix_code": "String string_chunk_split(const char *src, int srclen, const char *end,\n                          int endlen, int chunklen) {\n  int chunks = srclen / chunklen; // complete chunks!\n  int restlen = srclen - chunks * chunklen; /* srclen % chunklen */\n\n  String ret(\n    safe_address(\n      chunks + 1,\n      endlen,\n      srclen\n    ),\n    ReserveString\n  );\n  char *dest = ret.bufferSlice().ptr;\n\n  const char *p; char *q;\n  const char *pMax = src + srclen - chunklen + 1;\n  for (p = src, q = dest; p < pMax; ) {\n    memcpy(q, p, chunklen);\n    q += chunklen;\n    memcpy(q, end, endlen);\n    q += endlen;\n    p += chunklen;\n  }\n\n  if (restlen) {\n    memcpy(q, p, restlen);\n    q += restlen;\n    memcpy(q, end, endlen);\n    q += endlen;\n  }\n\n  ret.setSize(q - dest);\n  return ret;\n}",
    "diff": "-  int out_len = (chunks + 1) * endlen + srclen;\n-  String ret(out_len, ReserveString);",
    "critical_vars": [
      "out_len"
    ],
    "variable_definitions": {
      "out_len": "int out_len = (chunks + 1) * endlen + srclen;"
    },
    "variable_types": {
      "out_len": "integer"
    },
    "type_mapping": {
      "out_len": "Integer"
    },
    "vulnerable_line": "int out_len = (chunks + 1) * endlen + srclen;",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The calculation for out_len may result in an integer overflow if chunks + 1 multiplied by endlen, combined with srclen, exceeds the maximum integer value."
  },
  {
    "fix_code": "String string_chunk_split(const char *src, int srclen, const char *end,\n                          int endlen, int chunklen) {\n  int chunks = srclen / chunklen; // complete chunks!\n  int restlen = srclen - chunks * chunklen; /* srclen % chunklen */\n\n  String ret(\n    safe_address(\n      chunks + 1,\n      endlen,\n      srclen\n    ),\n    ReserveString\n  );\n  char *dest = ret.bufferSlice().ptr;\n\n  const char *p; char *q;\n  const char *pMax = src + srclen - chunklen + 1;\n  for (p = src, q = dest; p < pMax; ) {\n    memcpy(q, p, chunklen);\n    q += chunklen;\n    memcpy(q, end, endlen);\n    q += endlen;\n    p += chunklen;\n  }\n\n  if (restlen) {\n    memcpy(q, p, restlen);\n    q += restlen;\n    memcpy(q, end, endlen);\n    q += endlen;\n  }\n\n  ret.setSize(q - dest);\n  return ret;\n}",
    "diff": "@@ -626,8 +626,14 @@ String string_chunk_split(const char *src, int srclen, const char *end,\n   int chunks = srclen / chunklen; // complete chunks!\n   int restlen = srclen - chunks * chunklen; /* srclen % chunklen */\n-  int out_len = (chunks + 1) * endlen + srclen;\n-  String ret(out_len, ReserveString);\n+      chunks + 1,\n+      endlen,\n+      srclen\n+    ReserveString",
    "critical_vars": [
      "ReserveString",
      "endlen",
      "chunks",
      "srclen"
    ],
    "variable_definitions": {
      "ReserveString": "Definition not found",
      "endlen": "int endlen",
      "chunks": "int chunks = srclen / chunklen; // complete chunks!",
      "srclen": "int srclen"
    },
    "variable_types": {
      "ReserveString": "struct",
      "endlen": "integer",
      "chunks": "integer",
      "srclen": "integer"
    },
    "type_mapping": {
      "ReserveString": "struct",
      "endlen": "Integer",
      "chunks": "Integer",
      "srclen": "Integer"
    },
    "vulnerable_line": "int out_len = (chunks + 1) * endlen + srclen;",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The calculation of out_len can lead to an integer overflow if chunks + 1 exceeds the maximum value of int, which can cause application crash or denial of service."
  },
  {
    "fix_code": "BOOL license_read_scope_list(wStream* s, SCOPE_LIST* scopeList)\n{\n\tUINT32 i;\n\tUINT32 scopeCount;\n\n\tif (Stream_GetRemainingLength(s) < 4)\n\t\treturn FALSE;\n\n\tStream_Read_UINT32(s, scopeCount); /* ScopeCount (4 bytes) */\n\n        if (Stream_GetRemainingLength(s) / sizeof(LICENSE_BLOB) < scopeCount)\n                return FALSE;  /* Avoid overflow in malloc */\n\n\tscopeList->count = scopeCount;\n\tscopeList->array = (LICENSE_BLOB*) malloc(sizeof(LICENSE_BLOB) * scopeCount);\n\n\t/* ScopeArray */\n\tfor (i = 0; i < scopeCount; i++)\n\t{\n\t\tscopeList->array[i].type = BB_SCOPE_BLOB;\n\n\t\tif (!license_read_binary_blob(s, &scopeList->array[i]))\n\t\t\treturn FALSE;\n\t}\n\n\treturn TRUE;\n}",
    "diff": "@@ -670,6 +670,9 @@ BOOL license_read_scope_list(wStream* s, SCOPE_LIST* scopeList)\n \tStream_Read_UINT32(s, scopeCount); /* ScopeCount (4 bytes) */\n+        if (Stream_GetRemainingLength(s) / sizeof(LICENSE_BLOB) < scopeCount)\n \tscopeList->count = scopeCount;\n \tscopeList->array = (LICENSE_BLOB*) malloc(sizeof(LICENSE_BLOB) * scopeCount);",
    "critical_vars": [
      "s"
    ],
    "variable_definitions": {
      "s": "wStream* s"
    },
    "variable_types": {
      "s": "struct pointer"
    },
    "type_mapping": {
      "s": "struct pointer"
    },
    "vulnerable_line": "scopeList->array = (LICENSE_BLOB*) malloc(sizeof(LICENSE_BLOB) * scopeCount);",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The calculation 'sizeof(LICENSE_BLOB) * scopeCount' can overflow if scopeCount is large, leading to allocation of insufficient memory or a crash."
  },
  {
    "fix_code": "void ff_set_cmp(DSPContext* c, me_cmp_func *cmp, int type){\n    int i;\n\n    memset(cmp, 0, sizeof(void*)*6);\n\n    for(i=0; i<6; i++){\n        switch(type&0xFF){\n        case FF_CMP_SAD:\n            cmp[i]= c->sad[i];\n            break;\n        case FF_CMP_SATD:\n            cmp[i]= c->hadamard8_diff[i];\n            break;\n        case FF_CMP_SSE:\n            cmp[i]= c->sse[i];\n            break;\n        case FF_CMP_DCT:\n            cmp[i]= c->dct_sad[i];\n            break;\n        case FF_CMP_DCT264:\n            cmp[i]= c->dct264_sad[i];\n            break;\n        case FF_CMP_DCTMAX:\n            cmp[i]= c->dct_max[i];\n            break;\n        case FF_CMP_PSNR:\n            cmp[i]= c->quant_psnr[i];\n            break;\n        case FF_CMP_BIT:\n            cmp[i]= c->bit[i];\n            break;\n        case FF_CMP_RD:\n            cmp[i]= c->rd[i];\n            break;\n        case FF_CMP_VSAD:\n            cmp[i]= c->vsad[i];\n            break;\n        case FF_CMP_VSSE:\n            cmp[i]= c->vsse[i];\n            break;\n        case FF_CMP_ZERO:\n            cmp[i]= zero_cmp;\n            break;\n        case FF_CMP_NSSE:\n            cmp[i]= c->nsse[i];\n            break;\n#if CONFIG_DWT\n        case FF_CMP_W53:\n            cmp[i]= c->w53[i];\n            break;\n        case FF_CMP_W97:\n            cmp[i]= c->w97[i];\n            break;\n#endif\n        default:\n            av_log(NULL, AV_LOG_ERROR,\"internal error in cmp function selection\\n\");\n        }\n    }\n}",
    "diff": "@@ -1931,7 +1931,7 @@ void ff_set_cmp(DSPContext* c, me_cmp_func *cmp, int type){\n static void add_bytes_c(uint8_t *dst, uint8_t *src, int w){\n     long i;\n-    for(i=0; i<=w-sizeof(long); i+=sizeof(long)){\n+    for(i=0; i<=w-(int)sizeof(long); i+=sizeof(long)){\n         long a = *(long*)(src+i);\n         long b = *(long*)(dst+i);\n         *(long*)(dst+i) = ((a&pb_7f) + (b&pb_7f)) ^ ((a^b)&pb_80);\n@@ -1956,7 +1956,7 @@ static void diff_bytes_c(uint8_t *dst, const uint8_t *src1, const uint8_t *src2,\n #endif\n-    for(i=0; i<=w-sizeof(long); i+=sizeof(long)){\n+    for(i=0; i<=w-(int)sizeof(long); i+=sizeof(long)){\n         long a = *(long*)(src1+i);\n         long b = *(long*)(src2+i);\n         *(long*)(dst+i) = ((a|pb_80) - (b&pb_7f)) ^ ((a^b^pb_80)&pb_80);",
    "critical_vars": [
      "i"
    ],
    "variable_definitions": {
      "i": "int i;"
    },
    "variable_types": {
      "i": "integer"
    },
    "type_mapping": {
      "i": "Integer"
    },
    "vulnerable_line": "for(i=0; i<=w-sizeof(long); i+=sizeof(long)){",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Array out of bounds",
    "reasoning": "The loop condition allows 'i' to exceed the bounds of the array if 'w' is not properly validated, leading to out-of-bounds access when 'w' is smaller than sizeof(long)."
  },
  {
    "fix_code": "static void diff_bytes_c(uint8_t *dst, const uint8_t *src1, const uint8_t *src2, int w){\n    long i;\n#if !HAVE_FAST_UNALIGNED\n    if((long)src2 & (sizeof(long)-1)){\n        for(i=0; i+7<w; i+=8){\n            dst[i+0] = src1[i+0]-src2[i+0];\n            dst[i+1] = src1[i+1]-src2[i+1];\n            dst[i+2] = src1[i+2]-src2[i+2];\n            dst[i+3] = src1[i+3]-src2[i+3];\n            dst[i+4] = src1[i+4]-src2[i+4];\n            dst[i+5] = src1[i+5]-src2[i+5];\n            dst[i+6] = src1[i+6]-src2[i+6];\n            dst[i+7] = src1[i+7]-src2[i+7];\n        }\n    }else\n#endif\n    for(i=0; i<=w-(int)sizeof(long); i+=sizeof(long)){\n        long a = *(long*)(src1+i);\n        long b = *(long*)(src2+i);\n        *(long*)(dst+i) = ((a|pb_80) - (b&pb_7f)) ^ ((a^b^pb_80)&pb_80);\n    }\n    for(; i<w; i++)\n        dst[i+0] = src1[i+0]-src2[i+0];\n}",
    "diff": "@@ -1931,7 +1931,7 @@ void ff_set_cmp(DSPContext* c, me_cmp_func *cmp, int type){\n static void add_bytes_c(uint8_t *dst, uint8_t *src, int w){\n     long i;\n-    for(i=0; i<=w-sizeof(long); i+=sizeof(long)){\n+    for(i=0; i<=w-(int)sizeof(long); i+=sizeof(long)){\n         long a = *(long*)(src+i);\n         long b = *(long*)(dst+i);\n         *(long*)(dst+i) = ((a&pb_7f) + (b&pb_7f)) ^ ((a^b)&pb_80);\n@@ -1956,7 +1956,7 @@ static void diff_bytes_c(uint8_t *dst, const uint8_t *src1, const uint8_t *src2,\n #endif\n-    for(i=0; i<=w-sizeof(long); i+=sizeof(long)){\n+    for(i=0; i<=w-(int)sizeof(long); i+=sizeof(long)){\n         long a = *(long*)(src1+i);\n         long b = *(long*)(src2+i);\n         *(long*)(dst+i) = ((a|pb_80) - (b&pb_7f)) ^ ((a^b^pb_80)&pb_80);",
    "critical_vars": [
      "i"
    ],
    "variable_definitions": {
      "i": "long i;"
    },
    "variable_types": {
      "i": "integer"
    },
    "type_mapping": {
      "i": "Integer"
    },
    "vulnerable_line": "for(i=0; i<=w-sizeof(long); i+=sizeof(long)){",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Array out of bounds",
    "reasoning": "The loop condition allows 'i' to be equal to 'w-sizeof(long)', which can exceed the bounds of the array, leading to potential out-of-bounds memory access."
  },
  {
    "fix_code": "static int g2m_init_buffers(G2MContext *c)\n{\n    int aligned_height;\n\n    if (!c->framebuf || c->old_width < c->width || c->old_height < c->height) {\n        c->framebuf_stride = FFALIGN(c->width * 3, 16);\n        aligned_height     = FFALIGN(c->height,    16);\n        av_free(c->framebuf);\n        c->framebuf = av_mallocz(c->framebuf_stride * aligned_height);\n        if (!c->framebuf)\n            return AVERROR(ENOMEM);\n    }\n    if (!c->synth_tile || !c->jpeg_tile ||\n        c->old_tile_w < c->tile_width ||\n        c->old_tile_h < c->tile_height) {\n        c->tile_stride = FFALIGN(c->tile_width, 16) * 3;\n        aligned_height = FFALIGN(c->tile_height,    16);\n        av_free(c->synth_tile);\n        av_free(c->jpeg_tile);\n        av_free(c->kempf_buf);\n        av_free(c->kempf_flags);\n        c->synth_tile  = av_mallocz(c->tile_stride      * aligned_height);\n        c->jpeg_tile   = av_mallocz(c->tile_stride      * aligned_height);\n        c->kempf_buf   = av_mallocz((c->tile_width + 1) * aligned_height\n                                    + FF_INPUT_BUFFER_PADDING_SIZE);\n        c->kempf_flags = av_mallocz( c->tile_width      * aligned_height);\n        if (!c->synth_tile || !c->jpeg_tile ||\n            !c->kempf_buf || !c->kempf_flags)\n            return AVERROR(ENOMEM);\n    }\n\n    return 0;\n}",
    "diff": "-        c->tile_stride = FFALIGN(c->tile_width * 3, 16);\n+        c->tile_stride = FFALIGN(c->tile_width, 16) * 3;",
    "critical_vars": [
      "c->tile_stride"
    ],
    "variable_definitions": {
      "c->tile_stride": "Definition not found"
    },
    "variable_types": {
      "c->tile_stride": "struct pointer_integer"
    },
    "type_mapping": {
      "c->tile_stride": "sp_integer"
    },
    "vulnerable_line": "c->tile_stride = FFALIGN(c->tile_width * 3, 16);",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Buffer Size Calculation Error",
    "reasoning": "The multiplication of c->tile_width * 3 can cause an overflow, leading to incorrect buffer allocation when FFALIGN is applied, potentially resulting in an out-of-bounds access."
  },
  {
    "fix_code": "static void lsr_read_extension(GF_LASeRCodec *lsr, const char *name)\n{\n\tu32 len = lsr_read_vluimsbf5(lsr, name);\n#if 0\n\t*out_data = gf_malloc(sizeof(char)*len);\n\tgf_bs_read_data(lsr->bs, *out_data, len);\n\t*out_len = len;\n#else\n\twhile (len && gf_bs_available(lsr->bs) ) {\n\t\tgf_bs_read_int(lsr->bs, 8);\n\t\tlen--;\n\t}\n\tif (len) lsr->last_error = GF_NON_COMPLIANT_BITSTREAM;\n#endif\n}",
    "diff": " static void lsr_read_extend_class(GF_LASeRCodec *lsr, char **out_data, u32 *out_len, const char *name)\n-\tu32 len;\n+\tu32 len, blen;\n \tGF_LSR_READ_INT(lsr, len, lsr->info->cfg.extensionIDBits, \"reserved\");\n \tlen = lsr_read_vluimsbf5(lsr, \"len\");\n-//\twhile (len) gf_bs_read_int(lsr->bs, 1);\n-\tgf_bs_read_long_int(lsr->bs, len);\n+\twhile (len && !gf_bs_is_align(lsr->bs)) {\n+\t\tgf_bs_read_int(lsr->bs, len);\n+\t\tlen--;\n+\tblen = len / 8;\n+\tgf_bs_skip_bytes(lsr->bs, blen);\n+\tlen -= blen*8;\n+\twhile (len) {\n+\t\tgf_bs_read_int(lsr->bs, 1);\n+\t\tlen--;\n \tif (out_len) *out_len = 0;",
    "critical_vars": [
      "lsr->bs",
      "len"
    ],
    "variable_definitions": {
      "lsr->bs": "Definition not found",
      "len": "u32 len = lsr_read_vluimsbf5(lsr, name);"
    },
    "variable_types": {
      "lsr->bs": "struct pointer_struct pointer",
      "len": "integer"
    },
    "type_mapping": {
      "lsr->bs": "sp_sp",
      "len": "Integer"
    },
    "vulnerable_line": "s64 neg = (s64) val - (0x00000001UL << nb_bits);",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The left shift operation on '1' with nb_bits can exceed the maximum value for 's64', causing an overflow in the subtraction operation with 'val'. This can lead to unexpected behavior and vulnerabilities."
  },
  {
    "fix_code": "static Fixed lsr_translate_coords(GF_LASeRCodec *lsr, u32 val, u32 nb_bits)\n{\n\tif (!nb_bits) return 0;\n\tif (nb_bits>=32) return 0;\n\n#ifdef GPAC_FIXED_POINT\n\tif (val >> (nb_bits-1) ) {\n\t\ts64 neg = (s64) val - (0x00000001UL << nb_bits);\n\t\tif (neg < -FIX_ONE / 2)\n\t\t\treturn 2 * gf_divfix(INT2FIX(neg/2), lsr->res_factor);\n\t\treturn gf_divfix(INT2FIX(neg), lsr->res_factor);\n\t} else {\n\t\tif (val > FIX_ONE / 2)\n\t\t\treturn 2 * gf_divfix(INT2FIX(val/2), lsr->res_factor);\n\t\treturn gf_divfix(INT2FIX(val), lsr->res_factor);\n\t}\n#else\n\tif (val >> (nb_bits-1) ) {\n\t\ts64 neg = (s64) val - (0x00000001UL << nb_bits);\n\t\treturn ((Fixed)neg) / lsr->res_factor;\n\t} else {\n\t\treturn ((Fixed)val) / lsr->res_factor;\n\t}\n#endif\n}",
    "diff": "-\t\ts32 neg = (s32) val - (1<<nb_bits);\n+\t\ts64 neg = (s64) val - (0x00000001UL << nb_bits);\n \t\tif (neg < -FIX_ONE / 2)\n \t\t\treturn 2 * gf_divfix(INT2FIX(neg/2), lsr->res_factor);\n \t\treturn gf_divfix(INT2FIX(neg), lsr->res_factor);\n-\t\ts32 neg = (s32) val - (1<<nb_bits);\n-\t\treturn gf_divfix(INT2FIX(neg), lsr->res_factor);\n+\t\ts64 neg = (s64) val - (0x00000001UL << nb_bits);\n+\t\treturn ((Fixed)neg) / lsr->res_factor;",
    "critical_vars": [
      "neg"
    ],
    "variable_definitions": {
      "neg": "s64 neg = (s64) val - (0x00000001UL << nb_bits);"
    },
    "variable_types": {
      "neg": "integer"
    },
    "type_mapping": {
      "neg": "Integer"
    },
    "vulnerable_line": "s64 neg = (s64) val - (0x00000001UL << nb_bits);",
    "critical_variable_in_vulnerable_line": true,
    "vulnerability_type": "Integer Overflow",
    "reasoning": "The shift operation (1 << nb_bits) can overflow when nb_bits is >= 32, leading to incorrect subtraction and potentially resulting in a negative value that doesn't correctly represent val, allowing for manipulation of the function's behavior."
  }
]