[
    {
        "patch_model": "Delete",
        "change_type": "Fun-Call",
        "line_old": 208,
        "critical_vars": [
            "&ipcp->lock"
        ],
        "function": "*sem_obtain_lock",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "False",
        "function_code": "static inline struct sem_array *sem_obtain_lock(struct ipc_namespace *ns, int id)\n{\n\tstruct kern_ipc_perm *ipcp;\n\tstruct sem_array *sma;\n\n\trcu_read_lock();\n\tipcp = ipc_obtain_object(&sem_ids(ns), id);\n\tif (IS_ERR(ipcp)) {\n\t\tsma = ERR_CAST(ipcp);\n\t\tgoto err;\n\t}\n\n\tspin_lock(&ipcp->lock);\n\n\t/* ipc_rmid() may have already freed the ID while sem_lock\n\t * was spinning: verify that the structure is still valid\n\t */\n\tif (!ipcp->deleted)\n\t\treturn container_of(ipcp, struct sem_array, sem_perm);\n\n\tspin_unlock(&ipcp->lock);\n\tsma = ERR_PTR(-EINVAL);\nerr:\n\trcu_read_unlock();\n\treturn sma;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 287,
        "critical_vars": [
            "sma"
        ],
        "function": "*sem_obtain_lock",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "True",
        "function_code": "static inline struct sem_array *sem_obtain_lock(struct ipc_namespace *ns,\n\t\t\tint id, struct sembuf *sops, int nsops, int *locknum)\n{\n\tstruct kern_ipc_perm *ipcp;\n\tstruct sem_array *sma;\n\n\trcu_read_lock();\n\tipcp = ipc_obtain_object(&sem_ids(ns), id);\n\tif (IS_ERR(ipcp)) {\n\t\tsma = ERR_CAST(ipcp);\n\t\tgoto err;\n\t}\n\n\tsma = container_of(ipcp, struct sem_array, sem_perm);\n\t*locknum = sem_lock(sma, sops, nsops);\n\n\t/* ipc_rmid() may have already freed the ID while sem_lock\n\t * was spinning: verify that the structure is still valid\n\t */\n\tif (!ipcp->deleted)\n\t\treturn container_of(ipcp, struct sem_array, sem_perm);\n\n\tsem_unlock(sma, *locknum);\n\tsma = ERR_PTR(-EINVAL);\nerr:\n\trcu_read_unlock();\n\treturn sma;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 288,
        "critical_vars": [
            "*locknum"
        ],
        "function": "*sem_obtain_lock",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "True",
        "function_code": "static inline struct sem_array *sem_obtain_lock(struct ipc_namespace *ns,\n\t\t\tint id, struct sembuf *sops, int nsops, int *locknum)\n{\n\tstruct kern_ipc_perm *ipcp;\n\tstruct sem_array *sma;\n\n\trcu_read_lock();\n\tipcp = ipc_obtain_object(&sem_ids(ns), id);\n\tif (IS_ERR(ipcp)) {\n\t\tsma = ERR_CAST(ipcp);\n\t\tgoto err;\n\t}\n\n\tsma = container_of(ipcp, struct sem_array, sem_perm);\n\t*locknum = sem_lock(sma, sops, nsops);\n\n\t/* ipc_rmid() may have already freed the ID while sem_lock\n\t * was spinning: verify that the structure is still valid\n\t */\n\tif (!ipcp->deleted)\n\t\treturn container_of(ipcp, struct sem_array, sem_perm);\n\n\tsem_unlock(sma, *locknum);\n\tsma = ERR_PTR(-EINVAL);\nerr:\n\trcu_read_unlock();\n\treturn sma;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Fun-Call",
        "line_old": 216,
        "critical_vars": [
            "&ipcp->lock"
        ],
        "function": "*sem_obtain_lock",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "False",
        "function_code": "static inline struct sem_array *sem_obtain_lock(struct ipc_namespace *ns, int id)\n{\n\tstruct kern_ipc_perm *ipcp;\n\tstruct sem_array *sma;\n\n\trcu_read_lock();\n\tipcp = ipc_obtain_object(&sem_ids(ns), id);\n\tif (IS_ERR(ipcp)) {\n\t\tsma = ERR_CAST(ipcp);\n\t\tgoto err;\n\t}\n\n\tspin_lock(&ipcp->lock);\n\n\t/* ipc_rmid() may have already freed the ID while sem_lock\n\t * was spinning: verify that the structure is still valid\n\t */\n\tif (!ipcp->deleted)\n\t\treturn container_of(ipcp, struct sem_array, sem_perm);\n\n\tspin_unlock(&ipcp->lock);\n\tsma = ERR_PTR(-EINVAL);\nerr:\n\trcu_read_unlock();\n\treturn sma;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 296,
        "critical_vars": [
            "sma",
            "locknum"
        ],
        "function": "*sem_obtain_lock",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "True",
        "function_code": "static inline struct sem_array *sem_obtain_lock(struct ipc_namespace *ns,\n\t\t\tint id, struct sembuf *sops, int nsops, int *locknum)\n{\n\tstruct kern_ipc_perm *ipcp;\n\tstruct sem_array *sma;\n\n\trcu_read_lock();\n\tipcp = ipc_obtain_object(&sem_ids(ns), id);\n\tif (IS_ERR(ipcp)) {\n\t\tsma = ERR_CAST(ipcp);\n\t\tgoto err;\n\t}\n\n\tsma = container_of(ipcp, struct sem_array, sem_perm);\n\t*locknum = sem_lock(sma, sops, nsops);\n\n\t/* ipc_rmid() may have already freed the ID while sem_lock\n\t * was spinning: verify that the structure is still valid\n\t */\n\tif (!ipcp->deleted)\n\t\treturn container_of(ipcp, struct sem_array, sem_perm);\n\n\tsem_unlock(sma, *locknum);\n\tsma = ERR_PTR(-EINVAL);\nerr:\n\trcu_read_unlock();\n\treturn sma;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Fun-Head",
        "line_old": 233,
        "critical_vars": [
            "id",
            "*ns"
        ],
        "function": "*sem_obtain_object",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "False",
        "function_code": "\nstatic inline struct sem_array *sem_obtain_object(struct ipc_namespace *ns, int id)\n{\n\tstruct kern_ipc_perm *ipcp = ipc_obtain_object(&sem_ids(ns), id);\n\n\tif (IS_ERR(ipcp))\n\t\treturn ERR_CAST(ipcp);\n\n\treturn container_of(ipcp, struct sem_array, sem_perm);\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 236,
        "critical_vars": [
            "*ipcp"
        ],
        "function": "*sem_obtain_object",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "False",
        "function_code": "\nstatic inline struct sem_array *sem_obtain_object(struct ipc_namespace *ns, int id)\n{\n\tstruct kern_ipc_perm *ipcp = ipc_obtain_object(&sem_ids(ns), id);\n\n\tif (IS_ERR(ipcp))\n\t\treturn ERR_CAST(ipcp);\n\n\treturn container_of(ipcp, struct sem_array, sem_perm);\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "if-Condition",
        "line_old": 238,
        "critical_vars": [
            "ipcp"
        ],
        "function": "*sem_obtain_object",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "False",
        "function_code": "\nstatic inline struct sem_array *sem_obtain_object(struct ipc_namespace *ns, int id)\n{\n\tstruct kern_ipc_perm *ipcp = ipc_obtain_object(&sem_ids(ns), id);\n\n\tif (IS_ERR(ipcp))\n\t\treturn ERR_CAST(ipcp);\n\n\treturn container_of(ipcp, struct sem_array, sem_perm);\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Fun-Call",
        "line_old": 257,
        "critical_vars": [
            "&sma->sem_perm"
        ],
        "function": "*sem_obtain_object_check",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "False",
        "function_code": "\nstatic inline struct sem_array *sem_obtain_object_check(struct ipc_namespace *ns,\n\t\t\t\t\t\t\tint id)\n{\n\tstruct kern_ipc_perm *ipcp = ipc_obtain_object_check(&sem_ids(ns), id);\n\n\tif (IS_ERR(ipcp))\n\t\treturn ERR_CAST(ipcp);\n\n\treturn container_of(ipcp, struct sem_array, sem_perm);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 327,
        "critical_vars": [
            "sma"
        ],
        "function": "*sem_obtain_object_check",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "True",
        "function_code": "\nstatic inline struct sem_array *sem_obtain_object_check(struct ipc_namespace *ns,\n\t\t\t\t\t\t\tint id)\n{\n\tstruct kern_ipc_perm *ipcp = ipc_obtain_object_check(&sem_ids(ns), id);\n\n\tif (IS_ERR(ipcp))\n\t\treturn ERR_CAST(ipcp);\n\n\treturn container_of(ipcp, struct sem_array, sem_perm);\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Fun-Call",
        "line_old": 263,
        "critical_vars": [
            "sma"
        ],
        "function": "*sem_obtain_object_check",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "False",
        "function_code": "\nstatic inline struct sem_array *sem_obtain_object_check(struct ipc_namespace *ns,\n\t\t\t\t\t\t\tint id)\n{\n\tstruct kern_ipc_perm *ipcp = ipc_obtain_object_check(&sem_ids(ns), id);\n\n\tif (IS_ERR(ipcp))\n\t\treturn ERR_CAST(ipcp);\n\n\treturn container_of(ipcp, struct sem_array, sem_perm);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 333,
        "critical_vars": [
            "sma"
        ],
        "function": "*sem_obtain_object_check",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "True",
        "function_code": "\nstatic inline struct sem_array *sem_obtain_object_check(struct ipc_namespace *ns,\n\t\t\t\t\t\t\tint id)\n{\n\tstruct kern_ipc_perm *ipcp = ipc_obtain_object_check(&sem_ids(ns), id);\n\n\tif (IS_ERR(ipcp))\n\t\treturn ERR_CAST(ipcp);\n\n\treturn container_of(ipcp, struct sem_array, sem_perm);\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Fun-Call",
        "line_old": 264,
        "critical_vars": [
            "sem_perm"
        ],
        "function": "*sem_obtain_object_check",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "False",
        "function_code": "\nstatic inline struct sem_array *sem_obtain_object_check(struct ipc_namespace *ns,\n\t\t\t\t\t\t\tint id)\n{\n\tstruct kern_ipc_perm *ipcp = ipc_obtain_object_check(&sem_ids(ns), id);\n\n\tif (IS_ERR(ipcp))\n\t\treturn ERR_CAST(ipcp);\n\n\treturn container_of(ipcp, struct sem_array, sem_perm);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 334,
        "critical_vars": [
            "sma"
        ],
        "function": "*sem_obtain_object_check",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "True",
        "function_code": "\nstatic inline struct sem_array *sem_obtain_object_check(struct ipc_namespace *ns,\n\t\t\t\t\t\t\tint id)\n{\n\tstruct kern_ipc_perm *ipcp = ipc_obtain_object_check(&sem_ids(ns), id);\n\n\tif (IS_ERR(ipcp))\n\t\treturn ERR_CAST(ipcp);\n\n\treturn container_of(ipcp, struct sem_array, sem_perm);\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Fun-Call",
        "line_old": 269,
        "critical_vars": [
            "&sma->sem_perm"
        ],
        "function": "*sem_obtain_object_check",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "False",
        "function_code": "\nstatic inline struct sem_array *sem_obtain_object_check(struct ipc_namespace *ns,\n\t\t\t\t\t\t\tint id)\n{\n\tstruct kern_ipc_perm *ipcp = ipc_obtain_object_check(&sem_ids(ns), id);\n\n\tif (IS_ERR(ipcp))\n\t\treturn ERR_CAST(ipcp);\n\n\treturn container_of(ipcp, struct sem_array, sem_perm);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 339,
        "critical_vars": [
            "sma"
        ],
        "function": "*sem_obtain_object_check",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "True",
        "function_code": "\nstatic inline struct sem_array *sem_obtain_object_check(struct ipc_namespace *ns,\n\t\t\t\t\t\t\tint id)\n{\n\tstruct kern_ipc_perm *ipcp = ipc_obtain_object_check(&sem_ids(ns), id);\n\n\tif (IS_ERR(ipcp))\n\t\treturn ERR_CAST(ipcp);\n\n\treturn container_of(ipcp, struct sem_array, sem_perm);\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Fun-Call",
        "line_old": 270,
        "critical_vars": [
            "sma"
        ],
        "function": "*sem_obtain_object_check",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "False",
        "function_code": "\nstatic inline struct sem_array *sem_obtain_object_check(struct ipc_namespace *ns,\n\t\t\t\t\t\t\tint id)\n{\n\tstruct kern_ipc_perm *ipcp = ipc_obtain_object_check(&sem_ids(ns), id);\n\n\tif (IS_ERR(ipcp))\n\t\treturn ERR_CAST(ipcp);\n\n\treturn container_of(ipcp, struct sem_array, sem_perm);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 340,
        "critical_vars": [
            "sma"
        ],
        "function": "*sem_obtain_object_check",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "True",
        "function_code": "\nstatic inline struct sem_array *sem_obtain_object_check(struct ipc_namespace *ns,\n\t\t\t\t\t\t\tint id)\n{\n\tstruct kern_ipc_perm *ipcp = ipc_obtain_object_check(&sem_ids(ns), id);\n\n\tif (IS_ERR(ipcp))\n\t\treturn ERR_CAST(ipcp);\n\n\treturn container_of(ipcp, struct sem_array, sem_perm);\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Fun-Call",
        "line_old": 271,
        "critical_vars": [
            "sem_perm"
        ],
        "function": "*sem_obtain_object_check",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "False",
        "function_code": "\nstatic inline struct sem_array *sem_obtain_object_check(struct ipc_namespace *ns,\n\t\t\t\t\t\t\tint id)\n{\n\tstruct kern_ipc_perm *ipcp = ipc_obtain_object_check(&sem_ids(ns), id);\n\n\tif (IS_ERR(ipcp))\n\t\treturn ERR_CAST(ipcp);\n\n\treturn container_of(ipcp, struct sem_array, sem_perm);\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Fun-Call",
        "line_old": 279,
        "critical_vars": [
            "sem_perm.lock"
        ],
        "function": "sem_putref",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "False",
        "function_code": "\nstatic inline void sem_putref(struct sem_array *sma)\n{\n\tipc_lock_by_ptr(&sma->sem_perm);\n\tipc_rcu_putref(sma);\n\tipc_unlock(&(sma)->sem_perm);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 348,
        "critical_vars": [
            "sma"
        ],
        "function": "sem_putref",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "True",
        "function_code": "\nstatic inline void sem_putref(struct sem_array *sma)\n{\n\tsem_lock_and_putref(sma);\n\tsem_unlock(sma, -1);\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Fun-Call",
        "line_old": 280,
        "critical_vars": [
            "sma"
        ],
        "function": "sem_putref",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "False",
        "function_code": "\nstatic inline void sem_putref(struct sem_array *sma)\n{\n\tipc_lock_by_ptr(&sma->sem_perm);\n\tipc_rcu_putref(sma);\n\tipc_unlock(&(sma)->sem_perm);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 349,
        "critical_vars": [
            "sma"
        ],
        "function": "sem_putref",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "True",
        "function_code": "\nstatic inline void sem_putref(struct sem_array *sma)\n{\n\tsem_lock_and_putref(sma);\n\tsem_unlock(sma, -1);\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Fun-Call",
        "line_old": 281,
        "critical_vars": [
            "sem_perm"
        ],
        "function": "sem_putref",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "False",
        "function_code": "\nstatic inline void sem_putref(struct sem_array *sma)\n{\n\tipc_lock_by_ptr(&sma->sem_perm);\n\tipc_rcu_putref(sma);\n\tipc_unlock(&(sma)->sem_perm);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 350,
        "critical_vars": [
            "sma"
        ],
        "function": "sem_putref",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "True",
        "function_code": "\nstatic inline void sem_putref(struct sem_array *sma)\n{\n\tsem_lock_and_putref(sma);\n\tsem_unlock(sma, -1);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 445,
        "critical_vars": [
            "&sma->sem_base[i].lock"
        ],
        "function": "newary",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "True",
        "function_code": "\nstatic int newary(struct ipc_namespace *ns, struct ipc_params *params)\n{\n\tint id;\n\tint retval;\n\tstruct sem_array *sma;\n\tint size;\n\tkey_t key = params->key;\n\tint nsems = params->u.nsems;\n\tint semflg = params->flg;\n\tint i;\n\n\tif (!nsems)\n\t\treturn -EINVAL;\n\tif (ns->used_sems + nsems > ns->sc_semmns)\n\t\treturn -ENOSPC;\n\n\tsize = sizeof (*sma) + nsems * sizeof (struct sem);\n\tsma = ipc_rcu_alloc(size);\n\tif (!sma) {\n\t\treturn -ENOMEM;\n\t}\n\tmemset (sma, 0, size);\n\n\tsma->sem_perm.mode = (semflg & S_IRWXUGO);\n\tsma->sem_perm.key = key;\n\n\tsma->sem_perm.security = NULL;\n\tretval = security_sem_alloc(sma);\n\tif (retval) {\n\t\tipc_rcu_putref(sma);\n\t\treturn retval;\n\t}\n\n\tid = ipc_addid(&sem_ids(ns), &sma->sem_perm, ns->sc_semmni);\n\tif (id < 0) {\n\t\tsecurity_sem_free(sma);\n\t\tipc_rcu_putref(sma);\n\t\treturn id;\n\t}\n\tns->used_sems += nsems;\n\n\tsma->sem_base = (struct sem *) &sma[1];\n\n\tfor (i = 0; i < nsems; i++) {\n\t\tINIT_LIST_HEAD(&sma->sem_base[i].sem_pending);\n\t\tspin_lock_init(&sma->sem_base[i].lock);\n\t}\n\n\tsma->complex_count = 0;\n\tINIT_LIST_HEAD(&sma->sem_pending);\n\tINIT_LIST_HEAD(&sma->list_id);\n\tsma->sem_nsems = nsems;\n\tsma->sem_ctime = get_seconds();\n\tsem_unlock(sma, -1);\n\n\treturn sma->sem_perm.id;\n}"
    },
    {
        "patch_model": "Replace",
        "change_type": "Fun-Call",
        "line_old": 382,
        "line_new": 453,
        "critical_vars": [
            "sma"
        ],
        "function": "newary",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "True",
        "function_code": "\nstatic int newary(struct ipc_namespace *ns, struct ipc_params *params)\n{\n\tint id;\n\tint retval;\n\tstruct sem_array *sma;\n\tint size;\n\tkey_t key = params->key;\n\tint nsems = params->u.nsems;\n\tint semflg = params->flg;\n\tint i;\n\n\tif (!nsems)\n\t\treturn -EINVAL;\n\tif (ns->used_sems + nsems > ns->sc_semmns)\n\t\treturn -ENOSPC;\n\n\tsize = sizeof (*sma) + nsems * sizeof (struct sem);\n\tsma = ipc_rcu_alloc(size);\n\tif (!sma) {\n\t\treturn -ENOMEM;\n\t}\n\tmemset (sma, 0, size);\n\n\tsma->sem_perm.mode = (semflg & S_IRWXUGO);\n\tsma->sem_perm.key = key;\n\n\tsma->sem_perm.security = NULL;\n\tretval = security_sem_alloc(sma);\n\tif (retval) {\n\t\tipc_rcu_putref(sma);\n\t\treturn retval;\n\t}\n\n\tid = ipc_addid(&sem_ids(ns), &sma->sem_perm, ns->sc_semmni);\n\tif (id < 0) {\n\t\tsecurity_sem_free(sma);\n\t\tipc_rcu_putref(sma);\n\t\treturn id;\n\t}\n\tns->used_sems += nsems;\n\n\tsma->sem_base = (struct sem *) &sma[1];\n\n\tfor (i = 0; i < nsems; i++) {\n\t\tINIT_LIST_HEAD(&sma->sem_base[i].sem_pending);\n\t\tspin_lock_init(&sma->sem_base[i].lock);\n\t}\n\n\tsma->complex_count = 0;\n\tINIT_LIST_HEAD(&sma->sem_pending);\n\tINIT_LIST_HEAD(&sma->list_id);\n\tsma->sem_nsems = nsems;\n\tsma->sem_ctime = get_seconds();\n\tsem_unlock(sma, -1);\n\n\treturn sma->sem_perm.id;\n}"
    },
    {
        "patch_model": "Replace",
        "change_type": "Fun-Call",
        "line_old": 821,
        "line_new": 892,
        "critical_vars": [
            "sma"
        ],
        "function": "freeary",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "True",
        "function_code": "static void freeary(struct ipc_namespace *ns, struct kern_ipc_perm *ipcp)\n{\n\tstruct sem_undo *un, *tu;\n\tstruct sem_queue *q, *tq;\n\tstruct sem_array *sma = container_of(ipcp, struct sem_array, sem_perm);\n\tstruct list_head tasks;\n\tint i;\n\n\t/* Free the existing undo structures for this semaphore set.  */\n\tassert_spin_locked(&sma->sem_perm.lock);\n\tlist_for_each_entry_safe(un, tu, &sma->list_id, list_id) {\n\t\tlist_del(&un->list_id);\n\t\tspin_lock(&un->ulp->lock);\n\t\tun->semid = -1;\n\t\tlist_del_rcu(&un->list_proc);\n\t\tspin_unlock(&un->ulp->lock);\n\t\tkfree_rcu(un, rcu);\n\t}\n\n\t/* Wake up all pending processes and let them fail with EIDRM. */\n\tINIT_LIST_HEAD(&tasks);\n\tlist_for_each_entry_safe(q, tq, &sma->sem_pending, list) {\n\t\tunlink_queue(sma, q);\n\t\twake_up_sem_queue_prepare(&tasks, q, -EIDRM);\n\t}\n\tfor (i = 0; i < sma->sem_nsems; i++) {\n\t\tstruct sem *sem = sma->sem_base + i;\n\t\tlist_for_each_entry_safe(q, tq, &sem->sem_pending, list) {\n\t\t\tunlink_queue(sma, q);\n\t\t\twake_up_sem_queue_prepare(&tasks, q, -EIDRM);\n\t\t}\n\t}\n\n\t/* Remove the semaphore set from the IDR */\n\tsem_rmid(ns, sma);\n\tsem_unlock(sma, -1);\n\n\twake_up_sem_queue_do(&tasks);\n\tns->used_sems -= sma->sem_nsems;\n\tsecurity_sem_free(sma);\n\tipc_rcu_putref(sma);\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Var-Declaration",
        "line_old": 950,
        "critical_vars": [
            "nsems"
        ],
        "function": "semctl_setval",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "False",
        "function_code": "\nstatic int semctl_setval(struct ipc_namespace *ns, int semid, int semnum,\n\t\tunsigned long arg)\n{\n\tstruct sem_undo *un;\n\tstruct sem_array *sma;\n\tstruct sem* curr;\n\tint err;\n\tint nsems;\n\tstruct list_head tasks;\n\tint val;\n#if defined(CONFIG_64BIT) && defined(__BIG_ENDIAN)\n\t/* big-endian 64bit */\n\tval = arg >> 32;\n#else\n\t/* 32bit or little-endian 64bit */\n\tval = arg;\n#endif\n\n\tsma = sem_lock_check(ns, semid);\n\tif (IS_ERR(sma))\n\t\treturn PTR_ERR(sma);\n\n\tINIT_LIST_HEAD(&tasks);\n\tnsems = sma->sem_nsems;\n\n\terr = -EACCES;\n\tif (ipcperms(ns, &sma->sem_perm, S_IWUGO))\n\t\tgoto out_unlock;\n\n\terr = security_sem_semctl(sma, SETVAL);\n\tif (err)\n\t\tgoto out_unlock;\n\n\terr = -EINVAL;\n\tif(semnum < 0 || semnum >= nsems)\n\t\tgoto out_unlock;\n\n\tcurr = &sma->sem_base[semnum];\n\n\terr = -ERANGE;\n\tif (val > SEMVMX || val < 0)\n\t\tgoto out_unlock;\n\n\tassert_spin_locked(&sma->sem_perm.lock);\n\tlist_for_each_entry(un, &sma->list_id, list_id)\n\t\tun->semadj[semnum] = 0;\n\n\tcurr->semval = val;\n\tcurr->sempid = task_tgid_vnr(current);\n\tsma->sem_ctime = get_seconds();\n\t/* maybe some queued-up processes were waiting for this */\n\tdo_smart_update(sma, NULL, 0, 0, &tasks);\n\terr = 0;\nout_unlock:\n\tsem_unlock(sma);\n\twake_up_sem_queue_do(&tasks);\n\treturn err;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 961,
        "critical_vars": [
            "sma"
        ],
        "function": "semctl_setval",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "False",
        "function_code": "\nstatic int semctl_setval(struct ipc_namespace *ns, int semid, int semnum,\n\t\tunsigned long arg)\n{\n\tstruct sem_undo *un;\n\tstruct sem_array *sma;\n\tstruct sem* curr;\n\tint err;\n\tint nsems;\n\tstruct list_head tasks;\n\tint val;\n#if defined(CONFIG_64BIT) && defined(__BIG_ENDIAN)\n\t/* big-endian 64bit */\n\tval = arg >> 32;\n#else\n\t/* 32bit or little-endian 64bit */\n\tval = arg;\n#endif\n\n\tsma = sem_lock_check(ns, semid);\n\tif (IS_ERR(sma))\n\t\treturn PTR_ERR(sma);\n\n\tINIT_LIST_HEAD(&tasks);\n\tnsems = sma->sem_nsems;\n\n\terr = -EACCES;\n\tif (ipcperms(ns, &sma->sem_perm, S_IWUGO))\n\t\tgoto out_unlock;\n\n\terr = security_sem_semctl(sma, SETVAL);\n\tif (err)\n\t\tgoto out_unlock;\n\n\terr = -EINVAL;\n\tif(semnum < 0 || semnum >= nsems)\n\t\tgoto out_unlock;\n\n\tcurr = &sma->sem_base[semnum];\n\n\terr = -ERANGE;\n\tif (val > SEMVMX || val < 0)\n\t\tgoto out_unlock;\n\n\tassert_spin_locked(&sma->sem_perm.lock);\n\tlist_for_each_entry(un, &sma->list_id, list_id)\n\t\tun->semadj[semnum] = 0;\n\n\tcurr->semval = val;\n\tcurr->sempid = task_tgid_vnr(current);\n\tsma->sem_ctime = get_seconds();\n\t/* maybe some queued-up processes were waiting for this */\n\tdo_smart_update(sma, NULL, 0, 0, &tasks);\n\terr = 0;\nout_unlock:\n\tsem_unlock(sma);\n\twake_up_sem_queue_do(&tasks);\n\treturn err;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 1031,
        "critical_vars": [
            "val"
        ],
        "function": "semctl_setval",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "True",
        "function_code": "\nstatic int semctl_setval(struct ipc_namespace *ns, int semid, int semnum,\n\t\tunsigned long arg)\n{\n\tstruct sem_undo *un;\n\tstruct sem_array *sma;\n\tstruct sem* curr;\n\tint err;\n\tstruct list_head tasks;\n\tint val;\n#if defined(CONFIG_64BIT) && defined(__BIG_ENDIAN)\n\t/* big-endian 64bit */\n\tval = arg >> 32;\n#else\n\t/* 32bit or little-endian 64bit */\n\tval = arg;\n#endif\n\n\tif (val > SEMVMX || val < 0)\n\t\treturn -ERANGE;\n\n\tINIT_LIST_HEAD(&tasks);\n\n\trcu_read_lock();\n\tsma = sem_obtain_object_check(ns, semid);\n\tif (IS_ERR(sma)) {\n\t\trcu_read_unlock();\n\t\treturn PTR_ERR(sma);\n\t}\n\n\tif (semnum < 0 || semnum >= sma->sem_nsems) {\n\t\trcu_read_unlock();\n\t\treturn -EINVAL;\n\t}\n\n\n\tif (ipcperms(ns, &sma->sem_perm, S_IWUGO)) {\n\t\trcu_read_unlock();\n\t\treturn -EACCES;\n\t}\n\n\terr = security_sem_semctl(sma, SETVAL);\n\tif (err) {\n\t\trcu_read_unlock();\n\t\treturn -EACCES;\n\t}\n\n\tsem_lock(sma, NULL, -1);\n\n\tcurr = &sma->sem_base[semnum];\n\n\tassert_spin_locked(&sma->sem_perm.lock);\n\tlist_for_each_entry(un, &sma->list_id, list_id)\n\t\tun->semadj[semnum] = 0;\n\n\tcurr->semval = val;\n\tcurr->sempid = task_tgid_vnr(current);\n\tsma->sem_ctime = get_seconds();\n\t/* maybe some queued-up processes were waiting for this */\n\tdo_smart_update(sma, NULL, 0, 0, &tasks);\n\tsem_unlock(sma, -1);\n\twake_up_sem_queue_do(&tasks);\n\treturn 0;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "if-Condition",
        "line_old": 962,
        "critical_vars": [
            "sma"
        ],
        "function": "semctl_setval",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "False",
        "function_code": "\nstatic int semctl_setval(struct ipc_namespace *ns, int semid, int semnum,\n\t\tunsigned long arg)\n{\n\tstruct sem_undo *un;\n\tstruct sem_array *sma;\n\tstruct sem* curr;\n\tint err;\n\tint nsems;\n\tstruct list_head tasks;\n\tint val;\n#if defined(CONFIG_64BIT) && defined(__BIG_ENDIAN)\n\t/* big-endian 64bit */\n\tval = arg >> 32;\n#else\n\t/* 32bit or little-endian 64bit */\n\tval = arg;\n#endif\n\n\tsma = sem_lock_check(ns, semid);\n\tif (IS_ERR(sma))\n\t\treturn PTR_ERR(sma);\n\n\tINIT_LIST_HEAD(&tasks);\n\tnsems = sma->sem_nsems;\n\n\terr = -EACCES;\n\tif (ipcperms(ns, &sma->sem_perm, S_IWUGO))\n\t\tgoto out_unlock;\n\n\terr = security_sem_semctl(sma, SETVAL);\n\tif (err)\n\t\tgoto out_unlock;\n\n\terr = -EINVAL;\n\tif(semnum < 0 || semnum >= nsems)\n\t\tgoto out_unlock;\n\n\tcurr = &sma->sem_base[semnum];\n\n\terr = -ERANGE;\n\tif (val > SEMVMX || val < 0)\n\t\tgoto out_unlock;\n\n\tassert_spin_locked(&sma->sem_perm.lock);\n\tlist_for_each_entry(un, &sma->list_id, list_id)\n\t\tun->semadj[semnum] = 0;\n\n\tcurr->semval = val;\n\tcurr->sempid = task_tgid_vnr(current);\n\tsma->sem_ctime = get_seconds();\n\t/* maybe some queued-up processes were waiting for this */\n\tdo_smart_update(sma, NULL, 0, 0, &tasks);\n\terr = 0;\nout_unlock:\n\tsem_unlock(sma);\n\twake_up_sem_queue_do(&tasks);\n\treturn err;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 966,
        "critical_vars": [
            "nsems"
        ],
        "function": "semctl_setval",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "False",
        "function_code": "\nstatic int semctl_setval(struct ipc_namespace *ns, int semid, int semnum,\n\t\tunsigned long arg)\n{\n\tstruct sem_undo *un;\n\tstruct sem_array *sma;\n\tstruct sem* curr;\n\tint err;\n\tint nsems;\n\tstruct list_head tasks;\n\tint val;\n#if defined(CONFIG_64BIT) && defined(__BIG_ENDIAN)\n\t/* big-endian 64bit */\n\tval = arg >> 32;\n#else\n\t/* 32bit or little-endian 64bit */\n\tval = arg;\n#endif\n\n\tsma = sem_lock_check(ns, semid);\n\tif (IS_ERR(sma))\n\t\treturn PTR_ERR(sma);\n\n\tINIT_LIST_HEAD(&tasks);\n\tnsems = sma->sem_nsems;\n\n\terr = -EACCES;\n\tif (ipcperms(ns, &sma->sem_perm, S_IWUGO))\n\t\tgoto out_unlock;\n\n\terr = security_sem_semctl(sma, SETVAL);\n\tif (err)\n\t\tgoto out_unlock;\n\n\terr = -EINVAL;\n\tif(semnum < 0 || semnum >= nsems)\n\t\tgoto out_unlock;\n\n\tcurr = &sma->sem_base[semnum];\n\n\terr = -ERANGE;\n\tif (val > SEMVMX || val < 0)\n\t\tgoto out_unlock;\n\n\tassert_spin_locked(&sma->sem_perm.lock);\n\tlist_for_each_entry(un, &sma->list_id, list_id)\n\t\tun->semadj[semnum] = 0;\n\n\tcurr->semval = val;\n\tcurr->sempid = task_tgid_vnr(current);\n\tsma->sem_ctime = get_seconds();\n\t/* maybe some queued-up processes were waiting for this */\n\tdo_smart_update(sma, NULL, 0, 0, &tasks);\n\terr = 0;\nout_unlock:\n\tsem_unlock(sma);\n\twake_up_sem_queue_do(&tasks);\n\treturn err;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 968,
        "critical_vars": [
            "err"
        ],
        "function": "semctl_setval",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "False",
        "function_code": "\nstatic int semctl_setval(struct ipc_namespace *ns, int semid, int semnum,\n\t\tunsigned long arg)\n{\n\tstruct sem_undo *un;\n\tstruct sem_array *sma;\n\tstruct sem* curr;\n\tint err;\n\tint nsems;\n\tstruct list_head tasks;\n\tint val;\n#if defined(CONFIG_64BIT) && defined(__BIG_ENDIAN)\n\t/* big-endian 64bit */\n\tval = arg >> 32;\n#else\n\t/* 32bit or little-endian 64bit */\n\tval = arg;\n#endif\n\n\tsma = sem_lock_check(ns, semid);\n\tif (IS_ERR(sma))\n\t\treturn PTR_ERR(sma);\n\n\tINIT_LIST_HEAD(&tasks);\n\tnsems = sma->sem_nsems;\n\n\terr = -EACCES;\n\tif (ipcperms(ns, &sma->sem_perm, S_IWUGO))\n\t\tgoto out_unlock;\n\n\terr = security_sem_semctl(sma, SETVAL);\n\tif (err)\n\t\tgoto out_unlock;\n\n\terr = -EINVAL;\n\tif(semnum < 0 || semnum >= nsems)\n\t\tgoto out_unlock;\n\n\tcurr = &sma->sem_base[semnum];\n\n\terr = -ERANGE;\n\tif (val > SEMVMX || val < 0)\n\t\tgoto out_unlock;\n\n\tassert_spin_locked(&sma->sem_perm.lock);\n\tlist_for_each_entry(un, &sma->list_id, list_id)\n\t\tun->semadj[semnum] = 0;\n\n\tcurr->semval = val;\n\tcurr->sempid = task_tgid_vnr(current);\n\tsma->sem_ctime = get_seconds();\n\t/* maybe some queued-up processes were waiting for this */\n\tdo_smart_update(sma, NULL, 0, 0, &tasks);\n\terr = 0;\nout_unlock:\n\tsem_unlock(sma);\n\twake_up_sem_queue_do(&tasks);\n\treturn err;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "if-Condition",
        "line_old": 969,
        "critical_vars": [
            "&sma->sem_perm",
            "ns"
        ],
        "function": "semctl_setval",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "False",
        "function_code": "\nstatic int semctl_setval(struct ipc_namespace *ns, int semid, int semnum,\n\t\tunsigned long arg)\n{\n\tstruct sem_undo *un;\n\tstruct sem_array *sma;\n\tstruct sem* curr;\n\tint err;\n\tint nsems;\n\tstruct list_head tasks;\n\tint val;\n#if defined(CONFIG_64BIT) && defined(__BIG_ENDIAN)\n\t/* big-endian 64bit */\n\tval = arg >> 32;\n#else\n\t/* 32bit or little-endian 64bit */\n\tval = arg;\n#endif\n\n\tsma = sem_lock_check(ns, semid);\n\tif (IS_ERR(sma))\n\t\treturn PTR_ERR(sma);\n\n\tINIT_LIST_HEAD(&tasks);\n\tnsems = sma->sem_nsems;\n\n\terr = -EACCES;\n\tif (ipcperms(ns, &sma->sem_perm, S_IWUGO))\n\t\tgoto out_unlock;\n\n\terr = security_sem_semctl(sma, SETVAL);\n\tif (err)\n\t\tgoto out_unlock;\n\n\terr = -EINVAL;\n\tif(semnum < 0 || semnum >= nsems)\n\t\tgoto out_unlock;\n\n\tcurr = &sma->sem_base[semnum];\n\n\terr = -ERANGE;\n\tif (val > SEMVMX || val < 0)\n\t\tgoto out_unlock;\n\n\tassert_spin_locked(&sma->sem_perm.lock);\n\tlist_for_each_entry(un, &sma->list_id, list_id)\n\t\tun->semadj[semnum] = 0;\n\n\tcurr->semval = val;\n\tcurr->sempid = task_tgid_vnr(current);\n\tsma->sem_ctime = get_seconds();\n\t/* maybe some queued-up processes were waiting for this */\n\tdo_smart_update(sma, NULL, 0, 0, &tasks);\n\terr = 0;\nout_unlock:\n\tsem_unlock(sma);\n\twake_up_sem_queue_do(&tasks);\n\treturn err;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 1037,
        "critical_vars": [
            "sma"
        ],
        "function": "semctl_setval",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "True",
        "function_code": "\nstatic int semctl_setval(struct ipc_namespace *ns, int semid, int semnum,\n\t\tunsigned long arg)\n{\n\tstruct sem_undo *un;\n\tstruct sem_array *sma;\n\tstruct sem* curr;\n\tint err;\n\tstruct list_head tasks;\n\tint val;\n#if defined(CONFIG_64BIT) && defined(__BIG_ENDIAN)\n\t/* big-endian 64bit */\n\tval = arg >> 32;\n#else\n\t/* 32bit or little-endian 64bit */\n\tval = arg;\n#endif\n\n\tif (val > SEMVMX || val < 0)\n\t\treturn -ERANGE;\n\n\tINIT_LIST_HEAD(&tasks);\n\n\trcu_read_lock();\n\tsma = sem_obtain_object_check(ns, semid);\n\tif (IS_ERR(sma)) {\n\t\trcu_read_unlock();\n\t\treturn PTR_ERR(sma);\n\t}\n\n\tif (semnum < 0 || semnum >= sma->sem_nsems) {\n\t\trcu_read_unlock();\n\t\treturn -EINVAL;\n\t}\n\n\n\tif (ipcperms(ns, &sma->sem_perm, S_IWUGO)) {\n\t\trcu_read_unlock();\n\t\treturn -EACCES;\n\t}\n\n\terr = security_sem_semctl(sma, SETVAL);\n\tif (err) {\n\t\trcu_read_unlock();\n\t\treturn -EACCES;\n\t}\n\n\tsem_lock(sma, NULL, -1);\n\n\tcurr = &sma->sem_base[semnum];\n\n\tassert_spin_locked(&sma->sem_perm.lock);\n\tlist_for_each_entry(un, &sma->list_id, list_id)\n\t\tun->semadj[semnum] = 0;\n\n\tcurr->semval = val;\n\tcurr->sempid = task_tgid_vnr(current);\n\tsma->sem_ctime = get_seconds();\n\t/* maybe some queued-up processes were waiting for this */\n\tdo_smart_update(sma, NULL, 0, 0, &tasks);\n\tsem_unlock(sma, -1);\n\twake_up_sem_queue_do(&tasks);\n\treturn 0;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 1038,
        "critical_vars": [
            "sma"
        ],
        "function": "semctl_setval",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "True",
        "function_code": "\nstatic int semctl_setval(struct ipc_namespace *ns, int semid, int semnum,\n\t\tunsigned long arg)\n{\n\tstruct sem_undo *un;\n\tstruct sem_array *sma;\n\tstruct sem* curr;\n\tint err;\n\tstruct list_head tasks;\n\tint val;\n#if defined(CONFIG_64BIT) && defined(__BIG_ENDIAN)\n\t/* big-endian 64bit */\n\tval = arg >> 32;\n#else\n\t/* 32bit or little-endian 64bit */\n\tval = arg;\n#endif\n\n\tif (val > SEMVMX || val < 0)\n\t\treturn -ERANGE;\n\n\tINIT_LIST_HEAD(&tasks);\n\n\trcu_read_lock();\n\tsma = sem_obtain_object_check(ns, semid);\n\tif (IS_ERR(sma)) {\n\t\trcu_read_unlock();\n\t\treturn PTR_ERR(sma);\n\t}\n\n\tif (semnum < 0 || semnum >= sma->sem_nsems) {\n\t\trcu_read_unlock();\n\t\treturn -EINVAL;\n\t}\n\n\n\tif (ipcperms(ns, &sma->sem_perm, S_IWUGO)) {\n\t\trcu_read_unlock();\n\t\treturn -EACCES;\n\t}\n\n\terr = security_sem_semctl(sma, SETVAL);\n\tif (err) {\n\t\trcu_read_unlock();\n\t\treturn -EACCES;\n\t}\n\n\tsem_lock(sma, NULL, -1);\n\n\tcurr = &sma->sem_base[semnum];\n\n\tassert_spin_locked(&sma->sem_perm.lock);\n\tlist_for_each_entry(un, &sma->list_id, list_id)\n\t\tun->semadj[semnum] = 0;\n\n\tcurr->semval = val;\n\tcurr->sempid = task_tgid_vnr(current);\n\tsma->sem_ctime = get_seconds();\n\t/* maybe some queued-up processes were waiting for this */\n\tdo_smart_update(sma, NULL, 0, 0, &tasks);\n\tsem_unlock(sma, -1);\n\twake_up_sem_queue_do(&tasks);\n\treturn 0;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 1043,
        "critical_vars": [
            "semnum"
        ],
        "function": "semctl_setval",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "True",
        "function_code": "\nstatic int semctl_setval(struct ipc_namespace *ns, int semid, int semnum,\n\t\tunsigned long arg)\n{\n\tstruct sem_undo *un;\n\tstruct sem_array *sma;\n\tstruct sem* curr;\n\tint err;\n\tstruct list_head tasks;\n\tint val;\n#if defined(CONFIG_64BIT) && defined(__BIG_ENDIAN)\n\t/* big-endian 64bit */\n\tval = arg >> 32;\n#else\n\t/* 32bit or little-endian 64bit */\n\tval = arg;\n#endif\n\n\tif (val > SEMVMX || val < 0)\n\t\treturn -ERANGE;\n\n\tINIT_LIST_HEAD(&tasks);\n\n\trcu_read_lock();\n\tsma = sem_obtain_object_check(ns, semid);\n\tif (IS_ERR(sma)) {\n\t\trcu_read_unlock();\n\t\treturn PTR_ERR(sma);\n\t}\n\n\tif (semnum < 0 || semnum >= sma->sem_nsems) {\n\t\trcu_read_unlock();\n\t\treturn -EINVAL;\n\t}\n\n\n\tif (ipcperms(ns, &sma->sem_perm, S_IWUGO)) {\n\t\trcu_read_unlock();\n\t\treturn -EACCES;\n\t}\n\n\terr = security_sem_semctl(sma, SETVAL);\n\tif (err) {\n\t\trcu_read_unlock();\n\t\treturn -EACCES;\n\t}\n\n\tsem_lock(sma, NULL, -1);\n\n\tcurr = &sma->sem_base[semnum];\n\n\tassert_spin_locked(&sma->sem_perm.lock);\n\tlist_for_each_entry(un, &sma->list_id, list_id)\n\t\tun->semadj[semnum] = 0;\n\n\tcurr->semval = val;\n\tcurr->sempid = task_tgid_vnr(current);\n\tsma->sem_ctime = get_seconds();\n\t/* maybe some queued-up processes were waiting for this */\n\tdo_smart_update(sma, NULL, 0, 0, &tasks);\n\tsem_unlock(sma, -1);\n\twake_up_sem_queue_do(&tasks);\n\treturn 0;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 1049,
        "critical_vars": [
            "&sma->sem_perm",
            "ns"
        ],
        "function": "semctl_setval",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "True",
        "function_code": "\nstatic int semctl_setval(struct ipc_namespace *ns, int semid, int semnum,\n\t\tunsigned long arg)\n{\n\tstruct sem_undo *un;\n\tstruct sem_array *sma;\n\tstruct sem* curr;\n\tint err;\n\tstruct list_head tasks;\n\tint val;\n#if defined(CONFIG_64BIT) && defined(__BIG_ENDIAN)\n\t/* big-endian 64bit */\n\tval = arg >> 32;\n#else\n\t/* 32bit or little-endian 64bit */\n\tval = arg;\n#endif\n\n\tif (val > SEMVMX || val < 0)\n\t\treturn -ERANGE;\n\n\tINIT_LIST_HEAD(&tasks);\n\n\trcu_read_lock();\n\tsma = sem_obtain_object_check(ns, semid);\n\tif (IS_ERR(sma)) {\n\t\trcu_read_unlock();\n\t\treturn PTR_ERR(sma);\n\t}\n\n\tif (semnum < 0 || semnum >= sma->sem_nsems) {\n\t\trcu_read_unlock();\n\t\treturn -EINVAL;\n\t}\n\n\n\tif (ipcperms(ns, &sma->sem_perm, S_IWUGO)) {\n\t\trcu_read_unlock();\n\t\treturn -EACCES;\n\t}\n\n\terr = security_sem_semctl(sma, SETVAL);\n\tif (err) {\n\t\trcu_read_unlock();\n\t\treturn -EACCES;\n\t}\n\n\tsem_lock(sma, NULL, -1);\n\n\tcurr = &sma->sem_base[semnum];\n\n\tassert_spin_locked(&sma->sem_perm.lock);\n\tlist_for_each_entry(un, &sma->list_id, list_id)\n\t\tun->semadj[semnum] = 0;\n\n\tcurr->semval = val;\n\tcurr->sempid = task_tgid_vnr(current);\n\tsma->sem_ctime = get_seconds();\n\t/* maybe some queued-up processes were waiting for this */\n\tdo_smart_update(sma, NULL, 0, 0, &tasks);\n\tsem_unlock(sma, -1);\n\twake_up_sem_queue_do(&tasks);\n\treturn 0;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 976,
        "critical_vars": [
            "err"
        ],
        "function": "semctl_setval",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "False",
        "function_code": "\nstatic int semctl_setval(struct ipc_namespace *ns, int semid, int semnum,\n\t\tunsigned long arg)\n{\n\tstruct sem_undo *un;\n\tstruct sem_array *sma;\n\tstruct sem* curr;\n\tint err;\n\tint nsems;\n\tstruct list_head tasks;\n\tint val;\n#if defined(CONFIG_64BIT) && defined(__BIG_ENDIAN)\n\t/* big-endian 64bit */\n\tval = arg >> 32;\n#else\n\t/* 32bit or little-endian 64bit */\n\tval = arg;\n#endif\n\n\tsma = sem_lock_check(ns, semid);\n\tif (IS_ERR(sma))\n\t\treturn PTR_ERR(sma);\n\n\tINIT_LIST_HEAD(&tasks);\n\tnsems = sma->sem_nsems;\n\n\terr = -EACCES;\n\tif (ipcperms(ns, &sma->sem_perm, S_IWUGO))\n\t\tgoto out_unlock;\n\n\terr = security_sem_semctl(sma, SETVAL);\n\tif (err)\n\t\tgoto out_unlock;\n\n\terr = -EINVAL;\n\tif(semnum < 0 || semnum >= nsems)\n\t\tgoto out_unlock;\n\n\tcurr = &sma->sem_base[semnum];\n\n\terr = -ERANGE;\n\tif (val > SEMVMX || val < 0)\n\t\tgoto out_unlock;\n\n\tassert_spin_locked(&sma->sem_perm.lock);\n\tlist_for_each_entry(un, &sma->list_id, list_id)\n\t\tun->semadj[semnum] = 0;\n\n\tcurr->semval = val;\n\tcurr->sempid = task_tgid_vnr(current);\n\tsma->sem_ctime = get_seconds();\n\t/* maybe some queued-up processes were waiting for this */\n\tdo_smart_update(sma, NULL, 0, 0, &tasks);\n\terr = 0;\nout_unlock:\n\tsem_unlock(sma);\n\twake_up_sem_queue_do(&tasks);\n\treturn err;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 1060,
        "critical_vars": [
            "sma"
        ],
        "function": "semctl_setval",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "True",
        "function_code": "\nstatic int semctl_setval(struct ipc_namespace *ns, int semid, int semnum,\n\t\tunsigned long arg)\n{\n\tstruct sem_undo *un;\n\tstruct sem_array *sma;\n\tstruct sem* curr;\n\tint err;\n\tstruct list_head tasks;\n\tint val;\n#if defined(CONFIG_64BIT) && defined(__BIG_ENDIAN)\n\t/* big-endian 64bit */\n\tval = arg >> 32;\n#else\n\t/* 32bit or little-endian 64bit */\n\tval = arg;\n#endif\n\n\tif (val > SEMVMX || val < 0)\n\t\treturn -ERANGE;\n\n\tINIT_LIST_HEAD(&tasks);\n\n\trcu_read_lock();\n\tsma = sem_obtain_object_check(ns, semid);\n\tif (IS_ERR(sma)) {\n\t\trcu_read_unlock();\n\t\treturn PTR_ERR(sma);\n\t}\n\n\tif (semnum < 0 || semnum >= sma->sem_nsems) {\n\t\trcu_read_unlock();\n\t\treturn -EINVAL;\n\t}\n\n\n\tif (ipcperms(ns, &sma->sem_perm, S_IWUGO)) {\n\t\trcu_read_unlock();\n\t\treturn -EACCES;\n\t}\n\n\terr = security_sem_semctl(sma, SETVAL);\n\tif (err) {\n\t\trcu_read_unlock();\n\t\treturn -EACCES;\n\t}\n\n\tsem_lock(sma, NULL, -1);\n\n\tcurr = &sma->sem_base[semnum];\n\n\tassert_spin_locked(&sma->sem_perm.lock);\n\tlist_for_each_entry(un, &sma->list_id, list_id)\n\t\tun->semadj[semnum] = 0;\n\n\tcurr->semval = val;\n\tcurr->sempid = task_tgid_vnr(current);\n\tsma->sem_ctime = get_seconds();\n\t/* maybe some queued-up processes were waiting for this */\n\tdo_smart_update(sma, NULL, 0, 0, &tasks);\n\tsem_unlock(sma, -1);\n\twake_up_sem_queue_do(&tasks);\n\treturn 0;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "if-Condition",
        "line_old": 977,
        "critical_vars": [
            "semnum"
        ],
        "function": "semctl_setval",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "False",
        "function_code": "\nstatic int semctl_setval(struct ipc_namespace *ns, int semid, int semnum,\n\t\tunsigned long arg)\n{\n\tstruct sem_undo *un;\n\tstruct sem_array *sma;\n\tstruct sem* curr;\n\tint err;\n\tint nsems;\n\tstruct list_head tasks;\n\tint val;\n#if defined(CONFIG_64BIT) && defined(__BIG_ENDIAN)\n\t/* big-endian 64bit */\n\tval = arg >> 32;\n#else\n\t/* 32bit or little-endian 64bit */\n\tval = arg;\n#endif\n\n\tsma = sem_lock_check(ns, semid);\n\tif (IS_ERR(sma))\n\t\treturn PTR_ERR(sma);\n\n\tINIT_LIST_HEAD(&tasks);\n\tnsems = sma->sem_nsems;\n\n\terr = -EACCES;\n\tif (ipcperms(ns, &sma->sem_perm, S_IWUGO))\n\t\tgoto out_unlock;\n\n\terr = security_sem_semctl(sma, SETVAL);\n\tif (err)\n\t\tgoto out_unlock;\n\n\terr = -EINVAL;\n\tif(semnum < 0 || semnum >= nsems)\n\t\tgoto out_unlock;\n\n\tcurr = &sma->sem_base[semnum];\n\n\terr = -ERANGE;\n\tif (val > SEMVMX || val < 0)\n\t\tgoto out_unlock;\n\n\tassert_spin_locked(&sma->sem_perm.lock);\n\tlist_for_each_entry(un, &sma->list_id, list_id)\n\t\tun->semadj[semnum] = 0;\n\n\tcurr->semval = val;\n\tcurr->sempid = task_tgid_vnr(current);\n\tsma->sem_ctime = get_seconds();\n\t/* maybe some queued-up processes were waiting for this */\n\tdo_smart_update(sma, NULL, 0, 0, &tasks);\n\terr = 0;\nout_unlock:\n\tsem_unlock(sma);\n\twake_up_sem_queue_do(&tasks);\n\treturn err;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 982,
        "critical_vars": [
            "err"
        ],
        "function": "semctl_setval",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "False",
        "function_code": "\nstatic int semctl_setval(struct ipc_namespace *ns, int semid, int semnum,\n\t\tunsigned long arg)\n{\n\tstruct sem_undo *un;\n\tstruct sem_array *sma;\n\tstruct sem* curr;\n\tint err;\n\tint nsems;\n\tstruct list_head tasks;\n\tint val;\n#if defined(CONFIG_64BIT) && defined(__BIG_ENDIAN)\n\t/* big-endian 64bit */\n\tval = arg >> 32;\n#else\n\t/* 32bit or little-endian 64bit */\n\tval = arg;\n#endif\n\n\tsma = sem_lock_check(ns, semid);\n\tif (IS_ERR(sma))\n\t\treturn PTR_ERR(sma);\n\n\tINIT_LIST_HEAD(&tasks);\n\tnsems = sma->sem_nsems;\n\n\terr = -EACCES;\n\tif (ipcperms(ns, &sma->sem_perm, S_IWUGO))\n\t\tgoto out_unlock;\n\n\terr = security_sem_semctl(sma, SETVAL);\n\tif (err)\n\t\tgoto out_unlock;\n\n\terr = -EINVAL;\n\tif(semnum < 0 || semnum >= nsems)\n\t\tgoto out_unlock;\n\n\tcurr = &sma->sem_base[semnum];\n\n\terr = -ERANGE;\n\tif (val > SEMVMX || val < 0)\n\t\tgoto out_unlock;\n\n\tassert_spin_locked(&sma->sem_perm.lock);\n\tlist_for_each_entry(un, &sma->list_id, list_id)\n\t\tun->semadj[semnum] = 0;\n\n\tcurr->semval = val;\n\tcurr->sempid = task_tgid_vnr(current);\n\tsma->sem_ctime = get_seconds();\n\t/* maybe some queued-up processes were waiting for this */\n\tdo_smart_update(sma, NULL, 0, 0, &tasks);\n\terr = 0;\nout_unlock:\n\tsem_unlock(sma);\n\twake_up_sem_queue_do(&tasks);\n\treturn err;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "if-Condition",
        "line_old": 983,
        "critical_vars": [
            "val"
        ],
        "function": "semctl_setval",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "False",
        "function_code": "\nstatic int semctl_setval(struct ipc_namespace *ns, int semid, int semnum,\n\t\tunsigned long arg)\n{\n\tstruct sem_undo *un;\n\tstruct sem_array *sma;\n\tstruct sem* curr;\n\tint err;\n\tint nsems;\n\tstruct list_head tasks;\n\tint val;\n#if defined(CONFIG_64BIT) && defined(__BIG_ENDIAN)\n\t/* big-endian 64bit */\n\tval = arg >> 32;\n#else\n\t/* 32bit or little-endian 64bit */\n\tval = arg;\n#endif\n\n\tsma = sem_lock_check(ns, semid);\n\tif (IS_ERR(sma))\n\t\treturn PTR_ERR(sma);\n\n\tINIT_LIST_HEAD(&tasks);\n\tnsems = sma->sem_nsems;\n\n\terr = -EACCES;\n\tif (ipcperms(ns, &sma->sem_perm, S_IWUGO))\n\t\tgoto out_unlock;\n\n\terr = security_sem_semctl(sma, SETVAL);\n\tif (err)\n\t\tgoto out_unlock;\n\n\terr = -EINVAL;\n\tif(semnum < 0 || semnum >= nsems)\n\t\tgoto out_unlock;\n\n\tcurr = &sma->sem_base[semnum];\n\n\terr = -ERANGE;\n\tif (val > SEMVMX || val < 0)\n\t\tgoto out_unlock;\n\n\tassert_spin_locked(&sma->sem_perm.lock);\n\tlist_for_each_entry(un, &sma->list_id, list_id)\n\t\tun->semadj[semnum] = 0;\n\n\tcurr->semval = val;\n\tcurr->sempid = task_tgid_vnr(current);\n\tsma->sem_ctime = get_seconds();\n\t/* maybe some queued-up processes were waiting for this */\n\tdo_smart_update(sma, NULL, 0, 0, &tasks);\n\terr = 0;\nout_unlock:\n\tsem_unlock(sma);\n\twake_up_sem_queue_do(&tasks);\n\treturn err;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 995,
        "critical_vars": [
            "err"
        ],
        "function": "semctl_setval",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "False",
        "function_code": "\nstatic int semctl_setval(struct ipc_namespace *ns, int semid, int semnum,\n\t\tunsigned long arg)\n{\n\tstruct sem_undo *un;\n\tstruct sem_array *sma;\n\tstruct sem* curr;\n\tint err;\n\tint nsems;\n\tstruct list_head tasks;\n\tint val;\n#if defined(CONFIG_64BIT) && defined(__BIG_ENDIAN)\n\t/* big-endian 64bit */\n\tval = arg >> 32;\n#else\n\t/* 32bit or little-endian 64bit */\n\tval = arg;\n#endif\n\n\tsma = sem_lock_check(ns, semid);\n\tif (IS_ERR(sma))\n\t\treturn PTR_ERR(sma);\n\n\tINIT_LIST_HEAD(&tasks);\n\tnsems = sma->sem_nsems;\n\n\terr = -EACCES;\n\tif (ipcperms(ns, &sma->sem_perm, S_IWUGO))\n\t\tgoto out_unlock;\n\n\terr = security_sem_semctl(sma, SETVAL);\n\tif (err)\n\t\tgoto out_unlock;\n\n\terr = -EINVAL;\n\tif(semnum < 0 || semnum >= nsems)\n\t\tgoto out_unlock;\n\n\tcurr = &sma->sem_base[semnum];\n\n\terr = -ERANGE;\n\tif (val > SEMVMX || val < 0)\n\t\tgoto out_unlock;\n\n\tassert_spin_locked(&sma->sem_perm.lock);\n\tlist_for_each_entry(un, &sma->list_id, list_id)\n\t\tun->semadj[semnum] = 0;\n\n\tcurr->semval = val;\n\tcurr->sempid = task_tgid_vnr(current);\n\tsma->sem_ctime = get_seconds();\n\t/* maybe some queued-up processes were waiting for this */\n\tdo_smart_update(sma, NULL, 0, 0, &tasks);\n\terr = 0;\nout_unlock:\n\tsem_unlock(sma);\n\twake_up_sem_queue_do(&tasks);\n\treturn err;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 1073,
        "critical_vars": [
            "sma"
        ],
        "function": "semctl_setval",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "True",
        "function_code": "\nstatic int semctl_setval(struct ipc_namespace *ns, int semid, int semnum,\n\t\tunsigned long arg)\n{\n\tstruct sem_undo *un;\n\tstruct sem_array *sma;\n\tstruct sem* curr;\n\tint err;\n\tstruct list_head tasks;\n\tint val;\n#if defined(CONFIG_64BIT) && defined(__BIG_ENDIAN)\n\t/* big-endian 64bit */\n\tval = arg >> 32;\n#else\n\t/* 32bit or little-endian 64bit */\n\tval = arg;\n#endif\n\n\tif (val > SEMVMX || val < 0)\n\t\treturn -ERANGE;\n\n\tINIT_LIST_HEAD(&tasks);\n\n\trcu_read_lock();\n\tsma = sem_obtain_object_check(ns, semid);\n\tif (IS_ERR(sma)) {\n\t\trcu_read_unlock();\n\t\treturn PTR_ERR(sma);\n\t}\n\n\tif (semnum < 0 || semnum >= sma->sem_nsems) {\n\t\trcu_read_unlock();\n\t\treturn -EINVAL;\n\t}\n\n\n\tif (ipcperms(ns, &sma->sem_perm, S_IWUGO)) {\n\t\trcu_read_unlock();\n\t\treturn -EACCES;\n\t}\n\n\terr = security_sem_semctl(sma, SETVAL);\n\tif (err) {\n\t\trcu_read_unlock();\n\t\treturn -EACCES;\n\t}\n\n\tsem_lock(sma, NULL, -1);\n\n\tcurr = &sma->sem_base[semnum];\n\n\tassert_spin_locked(&sma->sem_perm.lock);\n\tlist_for_each_entry(un, &sma->list_id, list_id)\n\t\tun->semadj[semnum] = 0;\n\n\tcurr->semval = val;\n\tcurr->sempid = task_tgid_vnr(current);\n\tsma->sem_ctime = get_seconds();\n\t/* maybe some queued-up processes were waiting for this */\n\tdo_smart_update(sma, NULL, 0, 0, &tasks);\n\tsem_unlock(sma, -1);\n\twake_up_sem_queue_do(&tasks);\n\treturn 0;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Fun-Call",
        "line_old": 996,
        "critical_vars": [
            "sma"
        ],
        "function": "semctl_setval",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "False",
        "function_code": "\nstatic int semctl_setval(struct ipc_namespace *ns, int semid, int semnum,\n\t\tunsigned long arg)\n{\n\tstruct sem_undo *un;\n\tstruct sem_array *sma;\n\tstruct sem* curr;\n\tint err;\n\tint nsems;\n\tstruct list_head tasks;\n\tint val;\n#if defined(CONFIG_64BIT) && defined(__BIG_ENDIAN)\n\t/* big-endian 64bit */\n\tval = arg >> 32;\n#else\n\t/* 32bit or little-endian 64bit */\n\tval = arg;\n#endif\n\n\tsma = sem_lock_check(ns, semid);\n\tif (IS_ERR(sma))\n\t\treturn PTR_ERR(sma);\n\n\tINIT_LIST_HEAD(&tasks);\n\tnsems = sma->sem_nsems;\n\n\terr = -EACCES;\n\tif (ipcperms(ns, &sma->sem_perm, S_IWUGO))\n\t\tgoto out_unlock;\n\n\terr = security_sem_semctl(sma, SETVAL);\n\tif (err)\n\t\tgoto out_unlock;\n\n\terr = -EINVAL;\n\tif(semnum < 0 || semnum >= nsems)\n\t\tgoto out_unlock;\n\n\tcurr = &sma->sem_base[semnum];\n\n\terr = -ERANGE;\n\tif (val > SEMVMX || val < 0)\n\t\tgoto out_unlock;\n\n\tassert_spin_locked(&sma->sem_perm.lock);\n\tlist_for_each_entry(un, &sma->list_id, list_id)\n\t\tun->semadj[semnum] = 0;\n\n\tcurr->semval = val;\n\tcurr->sempid = task_tgid_vnr(current);\n\tsma->sem_ctime = get_seconds();\n\t/* maybe some queued-up processes were waiting for this */\n\tdo_smart_update(sma, NULL, 0, 0, &tasks);\n\terr = 0;\nout_unlock:\n\tsem_unlock(sma);\n\twake_up_sem_queue_do(&tasks);\n\treturn err;\n}"
    },
    {
        "patch_model": "Replace",
        "change_type": "Fun-Call",
        "line_old": 1054,
        "line_new": 1130,
        "critical_vars": [
            "sma"
        ],
        "function": "semctl_main",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "True",
        "function_code": "\nstatic int semctl_main(struct ipc_namespace *ns, int semid, int semnum,\n\t\tint cmd, void __user *p)\n{\n\tstruct sem_array *sma;\n\tstruct sem* curr;\n\tint err, nsems;\n\tushort fast_sem_io[SEMMSL_FAST];\n\tushort* sem_io = fast_sem_io;\n\tstruct list_head tasks;\n\n\tINIT_LIST_HEAD(&tasks);\n\n\trcu_read_lock();\n\tsma = sem_obtain_object_check(ns, semid);\n\tif (IS_ERR(sma)) {\n\t\trcu_read_unlock();\n\t\treturn PTR_ERR(sma);\n\t}\n\n\tnsems = sma->sem_nsems;\n\n\terr = -EACCES;\n\tif (ipcperms(ns, &sma->sem_perm,\n\t\t\tcmd == SETALL ? S_IWUGO : S_IRUGO)) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terr = security_sem_semctl(sma, cmd);\n\tif (err) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terr = -EACCES;\n\tswitch (cmd) {\n\tcase GETALL:\n\t{\n\t\tushort __user *array = p;\n\t\tint i;\n\n\t\tif(nsems > SEMMSL_FAST) {\n\t\t\tsem_getref(sma);\n\n\t\t\tsem_io = ipc_alloc(sizeof(ushort)*nsems);\n\t\t\tif(sem_io == NULL) {\n\t\t\t\tsem_putref(sma);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\n\t\t\tsem_lock_and_putref(sma);\n\t\t\tif (sma->sem_perm.deleted) {\n\t\t\t\tsem_unlock(sma, -1);\n\t\t\t\terr = -EIDRM;\n\t\t\t\tgoto out_free;\n\t\t\t}\n\t\t} else\n\t\t\tsem_lock(sma, NULL, -1);\n\n\t\tfor (i = 0; i < sma->sem_nsems; i++)\n\t\t\tsem_io[i] = sma->sem_base[i].semval;\n\t\tsem_unlock(sma, -1);\n\t\terr = 0;\n\t\tif(copy_to_user(array, sem_io, nsems*sizeof(ushort)))\n\t\t\terr = -EFAULT;\n\t\tgoto out_free;\n\t}\n\tcase SETALL:\n\t{\n\t\tint i;\n\t\tstruct sem_undo *un;\n\n\t\tif (!ipc_rcu_getref(sma)) {\n\t\t\trcu_read_unlock();\n\t\t\treturn -EIDRM;\n\t\t}\n\t\trcu_read_unlock();\n\n\t\tif(nsems > SEMMSL_FAST) {\n\t\t\tsem_io = ipc_alloc(sizeof(ushort)*nsems);\n\t\t\tif(sem_io == NULL) {\n\t\t\t\tsem_putref(sma);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t}\n\n\t\tif (copy_from_user (sem_io, p, nsems*sizeof(ushort))) {\n\t\t\tsem_putref(sma);\n\t\t\terr = -EFAULT;\n\t\t\tgoto out_free;\n\t\t}\n\n\t\tfor (i = 0; i < nsems; i++) {\n\t\t\tif (sem_io[i] > SEMVMX) {\n\t\t\t\tsem_putref(sma);\n\t\t\t\terr = -ERANGE;\n\t\t\t\tgoto out_free;\n\t\t\t}\n\t\t}\n\t\tsem_lock_and_putref(sma);\n\t\tif (sma->sem_perm.deleted) {\n\t\t\tsem_unlock(sma, -1);\n\t\t\terr = -EIDRM;\n\t\t\tgoto out_free;\n\t\t}\n\n\t\tfor (i = 0; i < nsems; i++)\n\t\t\tsma->sem_base[i].semval = sem_io[i];\n\n\t\tassert_spin_locked(&sma->sem_perm.lock);\n\t\tlist_for_each_entry(un, &sma->list_id, list_id) {\n\t\t\tfor (i = 0; i < nsems; i++)\n\t\t\t\tun->semadj[i] = 0;\n\t\t}\n\t\tsma->sem_ctime = get_seconds();\n\t\t/* maybe some queued-up processes were waiting for this */\n\t\tdo_smart_update(sma, NULL, 0, 0, &tasks);\n\t\terr = 0;\n\t\tgoto out_unlock;\n\t}\n\t/* GETVAL, GETPID, GETNCTN, GETZCNT: fall-through */\n\t}\n\terr = -EINVAL;\n\tif (semnum < 0 || semnum >= nsems) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\tsem_lock(sma, NULL, -1);\n\tcurr = &sma->sem_base[semnum];\n\n\tswitch (cmd) {\n\tcase GETVAL:\n\t\terr = curr->semval;\n\t\tgoto out_unlock;\n\tcase GETPID:\n\t\terr = curr->sempid;\n\t\tgoto out_unlock;\n\tcase GETNCNT:\n\t\terr = count_semncnt(sma,semnum);\n\t\tgoto out_unlock;\n\tcase GETZCNT:\n\t\terr = count_semzcnt(sma,semnum);\n\t\tgoto out_unlock;\n\t}\n\nout_unlock:\n\tsem_unlock(sma, -1);\nout_wakeup:\n\twake_up_sem_queue_do(&tasks);\nout_free:\n\tif(sem_io != fast_sem_io)\n\t\tipc_free(sem_io, sizeof(ushort)*nsems);\n\treturn err;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 1135,
        "critical_vars": [
            "sma"
        ],
        "function": "semctl_main",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "True",
        "function_code": "\nstatic int semctl_main(struct ipc_namespace *ns, int semid, int semnum,\n\t\tint cmd, void __user *p)\n{\n\tstruct sem_array *sma;\n\tstruct sem* curr;\n\tint err, nsems;\n\tushort fast_sem_io[SEMMSL_FAST];\n\tushort* sem_io = fast_sem_io;\n\tstruct list_head tasks;\n\n\tINIT_LIST_HEAD(&tasks);\n\n\trcu_read_lock();\n\tsma = sem_obtain_object_check(ns, semid);\n\tif (IS_ERR(sma)) {\n\t\trcu_read_unlock();\n\t\treturn PTR_ERR(sma);\n\t}\n\n\tnsems = sma->sem_nsems;\n\n\terr = -EACCES;\n\tif (ipcperms(ns, &sma->sem_perm,\n\t\t\tcmd == SETALL ? S_IWUGO : S_IRUGO)) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terr = security_sem_semctl(sma, cmd);\n\tif (err) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terr = -EACCES;\n\tswitch (cmd) {\n\tcase GETALL:\n\t{\n\t\tushort __user *array = p;\n\t\tint i;\n\n\t\tif(nsems > SEMMSL_FAST) {\n\t\t\tsem_getref(sma);\n\n\t\t\tsem_io = ipc_alloc(sizeof(ushort)*nsems);\n\t\t\tif(sem_io == NULL) {\n\t\t\t\tsem_putref(sma);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\n\t\t\tsem_lock_and_putref(sma);\n\t\t\tif (sma->sem_perm.deleted) {\n\t\t\t\tsem_unlock(sma, -1);\n\t\t\t\terr = -EIDRM;\n\t\t\t\tgoto out_free;\n\t\t\t}\n\t\t} else\n\t\t\tsem_lock(sma, NULL, -1);\n\n\t\tfor (i = 0; i < sma->sem_nsems; i++)\n\t\t\tsem_io[i] = sma->sem_base[i].semval;\n\t\tsem_unlock(sma, -1);\n\t\terr = 0;\n\t\tif(copy_to_user(array, sem_io, nsems*sizeof(ushort)))\n\t\t\terr = -EFAULT;\n\t\tgoto out_free;\n\t}\n\tcase SETALL:\n\t{\n\t\tint i;\n\t\tstruct sem_undo *un;\n\n\t\tif (!ipc_rcu_getref(sma)) {\n\t\t\trcu_read_unlock();\n\t\t\treturn -EIDRM;\n\t\t}\n\t\trcu_read_unlock();\n\n\t\tif(nsems > SEMMSL_FAST) {\n\t\t\tsem_io = ipc_alloc(sizeof(ushort)*nsems);\n\t\t\tif(sem_io == NULL) {\n\t\t\t\tsem_putref(sma);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t}\n\n\t\tif (copy_from_user (sem_io, p, nsems*sizeof(ushort))) {\n\t\t\tsem_putref(sma);\n\t\t\terr = -EFAULT;\n\t\t\tgoto out_free;\n\t\t}\n\n\t\tfor (i = 0; i < nsems; i++) {\n\t\t\tif (sem_io[i] > SEMVMX) {\n\t\t\t\tsem_putref(sma);\n\t\t\t\terr = -ERANGE;\n\t\t\t\tgoto out_free;\n\t\t\t}\n\t\t}\n\t\tsem_lock_and_putref(sma);\n\t\tif (sma->sem_perm.deleted) {\n\t\t\tsem_unlock(sma, -1);\n\t\t\terr = -EIDRM;\n\t\t\tgoto out_free;\n\t\t}\n\n\t\tfor (i = 0; i < nsems; i++)\n\t\t\tsma->sem_base[i].semval = sem_io[i];\n\n\t\tassert_spin_locked(&sma->sem_perm.lock);\n\t\tlist_for_each_entry(un, &sma->list_id, list_id) {\n\t\t\tfor (i = 0; i < nsems; i++)\n\t\t\t\tun->semadj[i] = 0;\n\t\t}\n\t\tsma->sem_ctime = get_seconds();\n\t\t/* maybe some queued-up processes were waiting for this */\n\t\tdo_smart_update(sma, NULL, 0, 0, &tasks);\n\t\terr = 0;\n\t\tgoto out_unlock;\n\t}\n\t/* GETVAL, GETPID, GETNCTN, GETZCNT: fall-through */\n\t}\n\terr = -EINVAL;\n\tif (semnum < 0 || semnum >= nsems) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\tsem_lock(sma, NULL, -1);\n\tcurr = &sma->sem_base[semnum];\n\n\tswitch (cmd) {\n\tcase GETVAL:\n\t\terr = curr->semval;\n\t\tgoto out_unlock;\n\tcase GETPID:\n\t\terr = curr->sempid;\n\t\tgoto out_unlock;\n\tcase GETNCNT:\n\t\terr = count_semncnt(sma,semnum);\n\t\tgoto out_unlock;\n\tcase GETZCNT:\n\t\terr = count_semzcnt(sma,semnum);\n\t\tgoto out_unlock;\n\t}\n\nout_unlock:\n\tsem_unlock(sma, -1);\nout_wakeup:\n\twake_up_sem_queue_do(&tasks);\nout_free:\n\tif(sem_io != fast_sem_io)\n\t\tipc_free(sem_io, sizeof(ushort)*nsems);\n\treturn err;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Fun-Call",
        "line_old": 1060,
        "critical_vars": [
            "&sma->sem_perm.lock"
        ],
        "function": "semctl_main",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "False",
        "function_code": "\nstatic int semctl_main(struct ipc_namespace *ns, int semid, int semnum,\n\t\tint cmd, void __user *p)\n{\n\tstruct sem_array *sma;\n\tstruct sem* curr;\n\tint err, nsems;\n\tushort fast_sem_io[SEMMSL_FAST];\n\tushort* sem_io = fast_sem_io;\n\tstruct list_head tasks;\n\n\tINIT_LIST_HEAD(&tasks);\n\n\trcu_read_lock();\n\tsma = sem_obtain_object_check(ns, semid);\n\tif (IS_ERR(sma)) {\n\t\trcu_read_unlock();\n\t\treturn PTR_ERR(sma);\n\t}\n\n\tnsems = sma->sem_nsems;\n\n\terr = -EACCES;\n\tif (ipcperms(ns, &sma->sem_perm,\n\t\t\tcmd == SETALL ? S_IWUGO : S_IRUGO)) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terr = security_sem_semctl(sma, cmd);\n\tif (err) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terr = -EACCES;\n\tswitch (cmd) {\n\tcase GETALL:\n\t{\n\t\tushort __user *array = p;\n\t\tint i;\n\n\t\tif(nsems > SEMMSL_FAST) {\n\t\t\tsem_getref(sma);\n\n\t\t\tsem_io = ipc_alloc(sizeof(ushort)*nsems);\n\t\t\tif(sem_io == NULL) {\n\t\t\t\tsem_putref(sma);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\n\t\t\tsem_lock_and_putref(sma);\n\t\t\tif (sma->sem_perm.deleted) {\n\t\t\t\tsem_unlock(sma);\n\t\t\t\terr = -EIDRM;\n\t\t\t\tgoto out_free;\n\t\t\t}\n\t\t}\n\n\t\tspin_lock(&sma->sem_perm.lock);\n\t\tfor (i = 0; i < sma->sem_nsems; i++)\n\t\t\tsem_io[i] = sma->sem_base[i].semval;\n\t\tsem_unlock(sma);\n\t\terr = 0;\n\t\tif(copy_to_user(array, sem_io, nsems*sizeof(ushort)))\n\t\t\terr = -EFAULT;\n\t\tgoto out_free;\n\t}\n\tcase SETALL:\n\t{\n\t\tint i;\n\t\tstruct sem_undo *un;\n\n\t\tipc_rcu_getref(sma);\n\t\trcu_read_unlock();\n\n\t\tif(nsems > SEMMSL_FAST) {\n\t\t\tsem_io = ipc_alloc(sizeof(ushort)*nsems);\n\t\t\tif(sem_io == NULL) {\n\t\t\t\tsem_putref(sma);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t}\n\n\t\tif (copy_from_user (sem_io, p, nsems*sizeof(ushort))) {\n\t\t\tsem_putref(sma);\n\t\t\terr = -EFAULT;\n\t\t\tgoto out_free;\n\t\t}\n\n\t\tfor (i = 0; i < nsems; i++) {\n\t\t\tif (sem_io[i] > SEMVMX) {\n\t\t\t\tsem_putref(sma);\n\t\t\t\terr = -ERANGE;\n\t\t\t\tgoto out_free;\n\t\t\t}\n\t\t}\n\t\tsem_lock_and_putref(sma);\n\t\tif (sma->sem_perm.deleted) {\n\t\t\tsem_unlock(sma);\n\t\t\terr = -EIDRM;\n\t\t\tgoto out_free;\n\t\t}\n\n\t\tfor (i = 0; i < nsems; i++)\n\t\t\tsma->sem_base[i].semval = sem_io[i];\n\n\t\tassert_spin_locked(&sma->sem_perm.lock);\n\t\tlist_for_each_entry(un, &sma->list_id, list_id) {\n\t\t\tfor (i = 0; i < nsems; i++)\n\t\t\t\tun->semadj[i] = 0;\n\t\t}\n\t\tsma->sem_ctime = get_seconds();\n\t\t/* maybe some queued-up processes were waiting for this */\n\t\tdo_smart_update(sma, NULL, 0, 0, &tasks);\n\t\terr = 0;\n\t\tgoto out_unlock;\n\t}\n\t/* GETVAL, GETPID, GETNCTN, GETZCNT: fall-through */\n\t}\n\terr = -EINVAL;\n\tif (semnum < 0 || semnum >= nsems) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\tspin_lock(&sma->sem_perm.lock);\n\tcurr = &sma->sem_base[semnum];\n\n\tswitch (cmd) {\n\tcase GETVAL:\n\t\terr = curr->semval;\n\t\tgoto out_unlock;\n\tcase GETPID:\n\t\terr = curr->sempid;\n\t\tgoto out_unlock;\n\tcase GETNCNT:\n\t\terr = count_semncnt(sma,semnum);\n\t\tgoto out_unlock;\n\tcase GETZCNT:\n\t\terr = count_semzcnt(sma,semnum);\n\t\tgoto out_unlock;\n\t}\n\nout_unlock:\n\tsem_unlock(sma);\nout_wakeup:\n\twake_up_sem_queue_do(&tasks);\nout_free:\n\tif(sem_io != fast_sem_io)\n\t\tipc_free(sem_io, sizeof(ushort)*nsems);\n\treturn err;\n}"
    },
    {
        "patch_model": "Replace",
        "change_type": "Fun-Call",
        "line_old": 1063,
        "line_new": 1139,
        "critical_vars": [
            "sma"
        ],
        "function": "semctl_main",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "True",
        "function_code": "\nstatic int semctl_main(struct ipc_namespace *ns, int semid, int semnum,\n\t\tint cmd, void __user *p)\n{\n\tstruct sem_array *sma;\n\tstruct sem* curr;\n\tint err, nsems;\n\tushort fast_sem_io[SEMMSL_FAST];\n\tushort* sem_io = fast_sem_io;\n\tstruct list_head tasks;\n\n\tINIT_LIST_HEAD(&tasks);\n\n\trcu_read_lock();\n\tsma = sem_obtain_object_check(ns, semid);\n\tif (IS_ERR(sma)) {\n\t\trcu_read_unlock();\n\t\treturn PTR_ERR(sma);\n\t}\n\n\tnsems = sma->sem_nsems;\n\n\terr = -EACCES;\n\tif (ipcperms(ns, &sma->sem_perm,\n\t\t\tcmd == SETALL ? S_IWUGO : S_IRUGO)) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terr = security_sem_semctl(sma, cmd);\n\tif (err) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terr = -EACCES;\n\tswitch (cmd) {\n\tcase GETALL:\n\t{\n\t\tushort __user *array = p;\n\t\tint i;\n\n\t\tif(nsems > SEMMSL_FAST) {\n\t\t\tsem_getref(sma);\n\n\t\t\tsem_io = ipc_alloc(sizeof(ushort)*nsems);\n\t\t\tif(sem_io == NULL) {\n\t\t\t\tsem_putref(sma);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\n\t\t\tsem_lock_and_putref(sma);\n\t\t\tif (sma->sem_perm.deleted) {\n\t\t\t\tsem_unlock(sma, -1);\n\t\t\t\terr = -EIDRM;\n\t\t\t\tgoto out_free;\n\t\t\t}\n\t\t} else\n\t\t\tsem_lock(sma, NULL, -1);\n\n\t\tfor (i = 0; i < sma->sem_nsems; i++)\n\t\t\tsem_io[i] = sma->sem_base[i].semval;\n\t\tsem_unlock(sma, -1);\n\t\terr = 0;\n\t\tif(copy_to_user(array, sem_io, nsems*sizeof(ushort)))\n\t\t\terr = -EFAULT;\n\t\tgoto out_free;\n\t}\n\tcase SETALL:\n\t{\n\t\tint i;\n\t\tstruct sem_undo *un;\n\n\t\tif (!ipc_rcu_getref(sma)) {\n\t\t\trcu_read_unlock();\n\t\t\treturn -EIDRM;\n\t\t}\n\t\trcu_read_unlock();\n\n\t\tif(nsems > SEMMSL_FAST) {\n\t\t\tsem_io = ipc_alloc(sizeof(ushort)*nsems);\n\t\t\tif(sem_io == NULL) {\n\t\t\t\tsem_putref(sma);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t}\n\n\t\tif (copy_from_user (sem_io, p, nsems*sizeof(ushort))) {\n\t\t\tsem_putref(sma);\n\t\t\terr = -EFAULT;\n\t\t\tgoto out_free;\n\t\t}\n\n\t\tfor (i = 0; i < nsems; i++) {\n\t\t\tif (sem_io[i] > SEMVMX) {\n\t\t\t\tsem_putref(sma);\n\t\t\t\terr = -ERANGE;\n\t\t\t\tgoto out_free;\n\t\t\t}\n\t\t}\n\t\tsem_lock_and_putref(sma);\n\t\tif (sma->sem_perm.deleted) {\n\t\t\tsem_unlock(sma, -1);\n\t\t\terr = -EIDRM;\n\t\t\tgoto out_free;\n\t\t}\n\n\t\tfor (i = 0; i < nsems; i++)\n\t\t\tsma->sem_base[i].semval = sem_io[i];\n\n\t\tassert_spin_locked(&sma->sem_perm.lock);\n\t\tlist_for_each_entry(un, &sma->list_id, list_id) {\n\t\t\tfor (i = 0; i < nsems; i++)\n\t\t\t\tun->semadj[i] = 0;\n\t\t}\n\t\tsma->sem_ctime = get_seconds();\n\t\t/* maybe some queued-up processes were waiting for this */\n\t\tdo_smart_update(sma, NULL, 0, 0, &tasks);\n\t\terr = 0;\n\t\tgoto out_unlock;\n\t}\n\t/* GETVAL, GETPID, GETNCTN, GETZCNT: fall-through */\n\t}\n\terr = -EINVAL;\n\tif (semnum < 0 || semnum >= nsems) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\tsem_lock(sma, NULL, -1);\n\tcurr = &sma->sem_base[semnum];\n\n\tswitch (cmd) {\n\tcase GETVAL:\n\t\terr = curr->semval;\n\t\tgoto out_unlock;\n\tcase GETPID:\n\t\terr = curr->sempid;\n\t\tgoto out_unlock;\n\tcase GETNCNT:\n\t\terr = count_semncnt(sma,semnum);\n\t\tgoto out_unlock;\n\tcase GETZCNT:\n\t\terr = count_semzcnt(sma,semnum);\n\t\tgoto out_unlock;\n\t}\n\nout_unlock:\n\tsem_unlock(sma, -1);\nout_wakeup:\n\twake_up_sem_queue_do(&tasks);\nout_free:\n\tif(sem_io != fast_sem_io)\n\t\tipc_free(sem_io, sizeof(ushort)*nsems);\n\treturn err;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Fun-Call",
        "line_old": 1074,
        "critical_vars": [
            "sma"
        ],
        "function": "semctl_main",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "False",
        "function_code": "\nstatic int semctl_main(struct ipc_namespace *ns, int semid, int semnum,\n\t\tint cmd, void __user *p)\n{\n\tstruct sem_array *sma;\n\tstruct sem* curr;\n\tint err, nsems;\n\tushort fast_sem_io[SEMMSL_FAST];\n\tushort* sem_io = fast_sem_io;\n\tstruct list_head tasks;\n\n\tINIT_LIST_HEAD(&tasks);\n\n\trcu_read_lock();\n\tsma = sem_obtain_object_check(ns, semid);\n\tif (IS_ERR(sma)) {\n\t\trcu_read_unlock();\n\t\treturn PTR_ERR(sma);\n\t}\n\n\tnsems = sma->sem_nsems;\n\n\terr = -EACCES;\n\tif (ipcperms(ns, &sma->sem_perm,\n\t\t\tcmd == SETALL ? S_IWUGO : S_IRUGO)) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terr = security_sem_semctl(sma, cmd);\n\tif (err) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terr = -EACCES;\n\tswitch (cmd) {\n\tcase GETALL:\n\t{\n\t\tushort __user *array = p;\n\t\tint i;\n\n\t\tif(nsems > SEMMSL_FAST) {\n\t\t\tsem_getref(sma);\n\n\t\t\tsem_io = ipc_alloc(sizeof(ushort)*nsems);\n\t\t\tif(sem_io == NULL) {\n\t\t\t\tsem_putref(sma);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\n\t\t\tsem_lock_and_putref(sma);\n\t\t\tif (sma->sem_perm.deleted) {\n\t\t\t\tsem_unlock(sma);\n\t\t\t\terr = -EIDRM;\n\t\t\t\tgoto out_free;\n\t\t\t}\n\t\t}\n\n\t\tspin_lock(&sma->sem_perm.lock);\n\t\tfor (i = 0; i < sma->sem_nsems; i++)\n\t\t\tsem_io[i] = sma->sem_base[i].semval;\n\t\tsem_unlock(sma);\n\t\terr = 0;\n\t\tif(copy_to_user(array, sem_io, nsems*sizeof(ushort)))\n\t\t\terr = -EFAULT;\n\t\tgoto out_free;\n\t}\n\tcase SETALL:\n\t{\n\t\tint i;\n\t\tstruct sem_undo *un;\n\n\t\tipc_rcu_getref(sma);\n\t\trcu_read_unlock();\n\n\t\tif(nsems > SEMMSL_FAST) {\n\t\t\tsem_io = ipc_alloc(sizeof(ushort)*nsems);\n\t\t\tif(sem_io == NULL) {\n\t\t\t\tsem_putref(sma);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t}\n\n\t\tif (copy_from_user (sem_io, p, nsems*sizeof(ushort))) {\n\t\t\tsem_putref(sma);\n\t\t\terr = -EFAULT;\n\t\t\tgoto out_free;\n\t\t}\n\n\t\tfor (i = 0; i < nsems; i++) {\n\t\t\tif (sem_io[i] > SEMVMX) {\n\t\t\t\tsem_putref(sma);\n\t\t\t\terr = -ERANGE;\n\t\t\t\tgoto out_free;\n\t\t\t}\n\t\t}\n\t\tsem_lock_and_putref(sma);\n\t\tif (sma->sem_perm.deleted) {\n\t\t\tsem_unlock(sma);\n\t\t\terr = -EIDRM;\n\t\t\tgoto out_free;\n\t\t}\n\n\t\tfor (i = 0; i < nsems; i++)\n\t\t\tsma->sem_base[i].semval = sem_io[i];\n\n\t\tassert_spin_locked(&sma->sem_perm.lock);\n\t\tlist_for_each_entry(un, &sma->list_id, list_id) {\n\t\t\tfor (i = 0; i < nsems; i++)\n\t\t\t\tun->semadj[i] = 0;\n\t\t}\n\t\tsma->sem_ctime = get_seconds();\n\t\t/* maybe some queued-up processes were waiting for this */\n\t\tdo_smart_update(sma, NULL, 0, 0, &tasks);\n\t\terr = 0;\n\t\tgoto out_unlock;\n\t}\n\t/* GETVAL, GETPID, GETNCTN, GETZCNT: fall-through */\n\t}\n\terr = -EINVAL;\n\tif (semnum < 0 || semnum >= nsems) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\tspin_lock(&sma->sem_perm.lock);\n\tcurr = &sma->sem_base[semnum];\n\n\tswitch (cmd) {\n\tcase GETVAL:\n\t\terr = curr->semval;\n\t\tgoto out_unlock;\n\tcase GETPID:\n\t\terr = curr->sempid;\n\t\tgoto out_unlock;\n\tcase GETNCNT:\n\t\terr = count_semncnt(sma,semnum);\n\t\tgoto out_unlock;\n\tcase GETZCNT:\n\t\terr = count_semzcnt(sma,semnum);\n\t\tgoto out_unlock;\n\t}\n\nout_unlock:\n\tsem_unlock(sma);\nout_wakeup:\n\twake_up_sem_queue_do(&tasks);\nout_free:\n\tif(sem_io != fast_sem_io)\n\t\tipc_free(sem_io, sizeof(ushort)*nsems);\n\treturn err;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 1150,
        "critical_vars": [
            "sma"
        ],
        "function": "semctl_main",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "True",
        "function_code": "\nstatic int semctl_main(struct ipc_namespace *ns, int semid, int semnum,\n\t\tint cmd, void __user *p)\n{\n\tstruct sem_array *sma;\n\tstruct sem* curr;\n\tint err, nsems;\n\tushort fast_sem_io[SEMMSL_FAST];\n\tushort* sem_io = fast_sem_io;\n\tstruct list_head tasks;\n\n\tINIT_LIST_HEAD(&tasks);\n\n\trcu_read_lock();\n\tsma = sem_obtain_object_check(ns, semid);\n\tif (IS_ERR(sma)) {\n\t\trcu_read_unlock();\n\t\treturn PTR_ERR(sma);\n\t}\n\n\tnsems = sma->sem_nsems;\n\n\terr = -EACCES;\n\tif (ipcperms(ns, &sma->sem_perm,\n\t\t\tcmd == SETALL ? S_IWUGO : S_IRUGO)) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terr = security_sem_semctl(sma, cmd);\n\tif (err) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terr = -EACCES;\n\tswitch (cmd) {\n\tcase GETALL:\n\t{\n\t\tushort __user *array = p;\n\t\tint i;\n\n\t\tif(nsems > SEMMSL_FAST) {\n\t\t\tsem_getref(sma);\n\n\t\t\tsem_io = ipc_alloc(sizeof(ushort)*nsems);\n\t\t\tif(sem_io == NULL) {\n\t\t\t\tsem_putref(sma);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\n\t\t\tsem_lock_and_putref(sma);\n\t\t\tif (sma->sem_perm.deleted) {\n\t\t\t\tsem_unlock(sma, -1);\n\t\t\t\terr = -EIDRM;\n\t\t\t\tgoto out_free;\n\t\t\t}\n\t\t} else\n\t\t\tsem_lock(sma, NULL, -1);\n\n\t\tfor (i = 0; i < sma->sem_nsems; i++)\n\t\t\tsem_io[i] = sma->sem_base[i].semval;\n\t\tsem_unlock(sma, -1);\n\t\terr = 0;\n\t\tif(copy_to_user(array, sem_io, nsems*sizeof(ushort)))\n\t\t\terr = -EFAULT;\n\t\tgoto out_free;\n\t}\n\tcase SETALL:\n\t{\n\t\tint i;\n\t\tstruct sem_undo *un;\n\n\t\tif (!ipc_rcu_getref(sma)) {\n\t\t\trcu_read_unlock();\n\t\t\treturn -EIDRM;\n\t\t}\n\t\trcu_read_unlock();\n\n\t\tif(nsems > SEMMSL_FAST) {\n\t\t\tsem_io = ipc_alloc(sizeof(ushort)*nsems);\n\t\t\tif(sem_io == NULL) {\n\t\t\t\tsem_putref(sma);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t}\n\n\t\tif (copy_from_user (sem_io, p, nsems*sizeof(ushort))) {\n\t\t\tsem_putref(sma);\n\t\t\terr = -EFAULT;\n\t\t\tgoto out_free;\n\t\t}\n\n\t\tfor (i = 0; i < nsems; i++) {\n\t\t\tif (sem_io[i] > SEMVMX) {\n\t\t\t\tsem_putref(sma);\n\t\t\t\terr = -ERANGE;\n\t\t\t\tgoto out_free;\n\t\t\t}\n\t\t}\n\t\tsem_lock_and_putref(sma);\n\t\tif (sma->sem_perm.deleted) {\n\t\t\tsem_unlock(sma, -1);\n\t\t\terr = -EIDRM;\n\t\t\tgoto out_free;\n\t\t}\n\n\t\tfor (i = 0; i < nsems; i++)\n\t\t\tsma->sem_base[i].semval = sem_io[i];\n\n\t\tassert_spin_locked(&sma->sem_perm.lock);\n\t\tlist_for_each_entry(un, &sma->list_id, list_id) {\n\t\t\tfor (i = 0; i < nsems; i++)\n\t\t\t\tun->semadj[i] = 0;\n\t\t}\n\t\tsma->sem_ctime = get_seconds();\n\t\t/* maybe some queued-up processes were waiting for this */\n\t\tdo_smart_update(sma, NULL, 0, 0, &tasks);\n\t\terr = 0;\n\t\tgoto out_unlock;\n\t}\n\t/* GETVAL, GETPID, GETNCTN, GETZCNT: fall-through */\n\t}\n\terr = -EINVAL;\n\tif (semnum < 0 || semnum >= nsems) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\tsem_lock(sma, NULL, -1);\n\tcurr = &sma->sem_base[semnum];\n\n\tswitch (cmd) {\n\tcase GETVAL:\n\t\terr = curr->semval;\n\t\tgoto out_unlock;\n\tcase GETPID:\n\t\terr = curr->sempid;\n\t\tgoto out_unlock;\n\tcase GETNCNT:\n\t\terr = count_semncnt(sma,semnum);\n\t\tgoto out_unlock;\n\tcase GETZCNT:\n\t\terr = count_semzcnt(sma,semnum);\n\t\tgoto out_unlock;\n\t}\n\nout_unlock:\n\tsem_unlock(sma, -1);\nout_wakeup:\n\twake_up_sem_queue_do(&tasks);\nout_free:\n\tif(sem_io != fast_sem_io)\n\t\tipc_free(sem_io, sizeof(ushort)*nsems);\n\treturn err;\n}"
    },
    {
        "patch_model": "Replace",
        "change_type": "Fun-Call",
        "line_old": 1100,
        "line_new": 1179,
        "critical_vars": [
            "sma"
        ],
        "function": "semctl_main",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "True",
        "function_code": "\nstatic int semctl_main(struct ipc_namespace *ns, int semid, int semnum,\n\t\tint cmd, void __user *p)\n{\n\tstruct sem_array *sma;\n\tstruct sem* curr;\n\tint err, nsems;\n\tushort fast_sem_io[SEMMSL_FAST];\n\tushort* sem_io = fast_sem_io;\n\tstruct list_head tasks;\n\n\tINIT_LIST_HEAD(&tasks);\n\n\trcu_read_lock();\n\tsma = sem_obtain_object_check(ns, semid);\n\tif (IS_ERR(sma)) {\n\t\trcu_read_unlock();\n\t\treturn PTR_ERR(sma);\n\t}\n\n\tnsems = sma->sem_nsems;\n\n\terr = -EACCES;\n\tif (ipcperms(ns, &sma->sem_perm,\n\t\t\tcmd == SETALL ? S_IWUGO : S_IRUGO)) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terr = security_sem_semctl(sma, cmd);\n\tif (err) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terr = -EACCES;\n\tswitch (cmd) {\n\tcase GETALL:\n\t{\n\t\tushort __user *array = p;\n\t\tint i;\n\n\t\tif(nsems > SEMMSL_FAST) {\n\t\t\tsem_getref(sma);\n\n\t\t\tsem_io = ipc_alloc(sizeof(ushort)*nsems);\n\t\t\tif(sem_io == NULL) {\n\t\t\t\tsem_putref(sma);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\n\t\t\tsem_lock_and_putref(sma);\n\t\t\tif (sma->sem_perm.deleted) {\n\t\t\t\tsem_unlock(sma, -1);\n\t\t\t\terr = -EIDRM;\n\t\t\t\tgoto out_free;\n\t\t\t}\n\t\t} else\n\t\t\tsem_lock(sma, NULL, -1);\n\n\t\tfor (i = 0; i < sma->sem_nsems; i++)\n\t\t\tsem_io[i] = sma->sem_base[i].semval;\n\t\tsem_unlock(sma, -1);\n\t\terr = 0;\n\t\tif(copy_to_user(array, sem_io, nsems*sizeof(ushort)))\n\t\t\terr = -EFAULT;\n\t\tgoto out_free;\n\t}\n\tcase SETALL:\n\t{\n\t\tint i;\n\t\tstruct sem_undo *un;\n\n\t\tif (!ipc_rcu_getref(sma)) {\n\t\t\trcu_read_unlock();\n\t\t\treturn -EIDRM;\n\t\t}\n\t\trcu_read_unlock();\n\n\t\tif(nsems > SEMMSL_FAST) {\n\t\t\tsem_io = ipc_alloc(sizeof(ushort)*nsems);\n\t\t\tif(sem_io == NULL) {\n\t\t\t\tsem_putref(sma);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t}\n\n\t\tif (copy_from_user (sem_io, p, nsems*sizeof(ushort))) {\n\t\t\tsem_putref(sma);\n\t\t\terr = -EFAULT;\n\t\t\tgoto out_free;\n\t\t}\n\n\t\tfor (i = 0; i < nsems; i++) {\n\t\t\tif (sem_io[i] > SEMVMX) {\n\t\t\t\tsem_putref(sma);\n\t\t\t\terr = -ERANGE;\n\t\t\t\tgoto out_free;\n\t\t\t}\n\t\t}\n\t\tsem_lock_and_putref(sma);\n\t\tif (sma->sem_perm.deleted) {\n\t\t\tsem_unlock(sma, -1);\n\t\t\terr = -EIDRM;\n\t\t\tgoto out_free;\n\t\t}\n\n\t\tfor (i = 0; i < nsems; i++)\n\t\t\tsma->sem_base[i].semval = sem_io[i];\n\n\t\tassert_spin_locked(&sma->sem_perm.lock);\n\t\tlist_for_each_entry(un, &sma->list_id, list_id) {\n\t\t\tfor (i = 0; i < nsems; i++)\n\t\t\t\tun->semadj[i] = 0;\n\t\t}\n\t\tsma->sem_ctime = get_seconds();\n\t\t/* maybe some queued-up processes were waiting for this */\n\t\tdo_smart_update(sma, NULL, 0, 0, &tasks);\n\t\terr = 0;\n\t\tgoto out_unlock;\n\t}\n\t/* GETVAL, GETPID, GETNCTN, GETZCNT: fall-through */\n\t}\n\terr = -EINVAL;\n\tif (semnum < 0 || semnum >= nsems) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\tsem_lock(sma, NULL, -1);\n\tcurr = &sma->sem_base[semnum];\n\n\tswitch (cmd) {\n\tcase GETVAL:\n\t\terr = curr->semval;\n\t\tgoto out_unlock;\n\tcase GETPID:\n\t\terr = curr->sempid;\n\t\tgoto out_unlock;\n\tcase GETNCNT:\n\t\terr = count_semncnt(sma,semnum);\n\t\tgoto out_unlock;\n\tcase GETZCNT:\n\t\terr = count_semzcnt(sma,semnum);\n\t\tgoto out_unlock;\n\t}\n\nout_unlock:\n\tsem_unlock(sma, -1);\nout_wakeup:\n\twake_up_sem_queue_do(&tasks);\nout_free:\n\tif(sem_io != fast_sem_io)\n\t\tipc_free(sem_io, sizeof(ushort)*nsems);\n\treturn err;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Fun-Call",
        "line_old": 1127,
        "critical_vars": [
            "&sma->sem_perm.lock"
        ],
        "function": "semctl_main",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "False",
        "function_code": "\nstatic int semctl_main(struct ipc_namespace *ns, int semid, int semnum,\n\t\tint cmd, void __user *p)\n{\n\tstruct sem_array *sma;\n\tstruct sem* curr;\n\tint err, nsems;\n\tushort fast_sem_io[SEMMSL_FAST];\n\tushort* sem_io = fast_sem_io;\n\tstruct list_head tasks;\n\n\tINIT_LIST_HEAD(&tasks);\n\n\trcu_read_lock();\n\tsma = sem_obtain_object_check(ns, semid);\n\tif (IS_ERR(sma)) {\n\t\trcu_read_unlock();\n\t\treturn PTR_ERR(sma);\n\t}\n\n\tnsems = sma->sem_nsems;\n\n\terr = -EACCES;\n\tif (ipcperms(ns, &sma->sem_perm,\n\t\t\tcmd == SETALL ? S_IWUGO : S_IRUGO)) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terr = security_sem_semctl(sma, cmd);\n\tif (err) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terr = -EACCES;\n\tswitch (cmd) {\n\tcase GETALL:\n\t{\n\t\tushort __user *array = p;\n\t\tint i;\n\n\t\tif(nsems > SEMMSL_FAST) {\n\t\t\tsem_getref(sma);\n\n\t\t\tsem_io = ipc_alloc(sizeof(ushort)*nsems);\n\t\t\tif(sem_io == NULL) {\n\t\t\t\tsem_putref(sma);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\n\t\t\tsem_lock_and_putref(sma);\n\t\t\tif (sma->sem_perm.deleted) {\n\t\t\t\tsem_unlock(sma);\n\t\t\t\terr = -EIDRM;\n\t\t\t\tgoto out_free;\n\t\t\t}\n\t\t}\n\n\t\tspin_lock(&sma->sem_perm.lock);\n\t\tfor (i = 0; i < sma->sem_nsems; i++)\n\t\t\tsem_io[i] = sma->sem_base[i].semval;\n\t\tsem_unlock(sma);\n\t\terr = 0;\n\t\tif(copy_to_user(array, sem_io, nsems*sizeof(ushort)))\n\t\t\terr = -EFAULT;\n\t\tgoto out_free;\n\t}\n\tcase SETALL:\n\t{\n\t\tint i;\n\t\tstruct sem_undo *un;\n\n\t\tipc_rcu_getref(sma);\n\t\trcu_read_unlock();\n\n\t\tif(nsems > SEMMSL_FAST) {\n\t\t\tsem_io = ipc_alloc(sizeof(ushort)*nsems);\n\t\t\tif(sem_io == NULL) {\n\t\t\t\tsem_putref(sma);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t}\n\n\t\tif (copy_from_user (sem_io, p, nsems*sizeof(ushort))) {\n\t\t\tsem_putref(sma);\n\t\t\terr = -EFAULT;\n\t\t\tgoto out_free;\n\t\t}\n\n\t\tfor (i = 0; i < nsems; i++) {\n\t\t\tif (sem_io[i] > SEMVMX) {\n\t\t\t\tsem_putref(sma);\n\t\t\t\terr = -ERANGE;\n\t\t\t\tgoto out_free;\n\t\t\t}\n\t\t}\n\t\tsem_lock_and_putref(sma);\n\t\tif (sma->sem_perm.deleted) {\n\t\t\tsem_unlock(sma);\n\t\t\terr = -EIDRM;\n\t\t\tgoto out_free;\n\t\t}\n\n\t\tfor (i = 0; i < nsems; i++)\n\t\t\tsma->sem_base[i].semval = sem_io[i];\n\n\t\tassert_spin_locked(&sma->sem_perm.lock);\n\t\tlist_for_each_entry(un, &sma->list_id, list_id) {\n\t\t\tfor (i = 0; i < nsems; i++)\n\t\t\t\tun->semadj[i] = 0;\n\t\t}\n\t\tsma->sem_ctime = get_seconds();\n\t\t/* maybe some queued-up processes were waiting for this */\n\t\tdo_smart_update(sma, NULL, 0, 0, &tasks);\n\t\terr = 0;\n\t\tgoto out_unlock;\n\t}\n\t/* GETVAL, GETPID, GETNCTN, GETZCNT: fall-through */\n\t}\n\terr = -EINVAL;\n\tif (semnum < 0 || semnum >= nsems) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\tspin_lock(&sma->sem_perm.lock);\n\tcurr = &sma->sem_base[semnum];\n\n\tswitch (cmd) {\n\tcase GETVAL:\n\t\terr = curr->semval;\n\t\tgoto out_unlock;\n\tcase GETPID:\n\t\terr = curr->sempid;\n\t\tgoto out_unlock;\n\tcase GETNCNT:\n\t\terr = count_semncnt(sma,semnum);\n\t\tgoto out_unlock;\n\tcase GETZCNT:\n\t\terr = count_semzcnt(sma,semnum);\n\t\tgoto out_unlock;\n\t}\n\nout_unlock:\n\tsem_unlock(sma);\nout_wakeup:\n\twake_up_sem_queue_do(&tasks);\nout_free:\n\tif(sem_io != fast_sem_io)\n\t\tipc_free(sem_io, sizeof(ushort)*nsems);\n\treturn err;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 1206,
        "critical_vars": [
            "sma"
        ],
        "function": "semctl_main",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "True",
        "function_code": "\nstatic int semctl_main(struct ipc_namespace *ns, int semid, int semnum,\n\t\tint cmd, void __user *p)\n{\n\tstruct sem_array *sma;\n\tstruct sem* curr;\n\tint err, nsems;\n\tushort fast_sem_io[SEMMSL_FAST];\n\tushort* sem_io = fast_sem_io;\n\tstruct list_head tasks;\n\n\tINIT_LIST_HEAD(&tasks);\n\n\trcu_read_lock();\n\tsma = sem_obtain_object_check(ns, semid);\n\tif (IS_ERR(sma)) {\n\t\trcu_read_unlock();\n\t\treturn PTR_ERR(sma);\n\t}\n\n\tnsems = sma->sem_nsems;\n\n\terr = -EACCES;\n\tif (ipcperms(ns, &sma->sem_perm,\n\t\t\tcmd == SETALL ? S_IWUGO : S_IRUGO)) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terr = security_sem_semctl(sma, cmd);\n\tif (err) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terr = -EACCES;\n\tswitch (cmd) {\n\tcase GETALL:\n\t{\n\t\tushort __user *array = p;\n\t\tint i;\n\n\t\tif(nsems > SEMMSL_FAST) {\n\t\t\tsem_getref(sma);\n\n\t\t\tsem_io = ipc_alloc(sizeof(ushort)*nsems);\n\t\t\tif(sem_io == NULL) {\n\t\t\t\tsem_putref(sma);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\n\t\t\tsem_lock_and_putref(sma);\n\t\t\tif (sma->sem_perm.deleted) {\n\t\t\t\tsem_unlock(sma, -1);\n\t\t\t\terr = -EIDRM;\n\t\t\t\tgoto out_free;\n\t\t\t}\n\t\t} else\n\t\t\tsem_lock(sma, NULL, -1);\n\n\t\tfor (i = 0; i < sma->sem_nsems; i++)\n\t\t\tsem_io[i] = sma->sem_base[i].semval;\n\t\tsem_unlock(sma, -1);\n\t\terr = 0;\n\t\tif(copy_to_user(array, sem_io, nsems*sizeof(ushort)))\n\t\t\terr = -EFAULT;\n\t\tgoto out_free;\n\t}\n\tcase SETALL:\n\t{\n\t\tint i;\n\t\tstruct sem_undo *un;\n\n\t\tif (!ipc_rcu_getref(sma)) {\n\t\t\trcu_read_unlock();\n\t\t\treturn -EIDRM;\n\t\t}\n\t\trcu_read_unlock();\n\n\t\tif(nsems > SEMMSL_FAST) {\n\t\t\tsem_io = ipc_alloc(sizeof(ushort)*nsems);\n\t\t\tif(sem_io == NULL) {\n\t\t\t\tsem_putref(sma);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t}\n\n\t\tif (copy_from_user (sem_io, p, nsems*sizeof(ushort))) {\n\t\t\tsem_putref(sma);\n\t\t\terr = -EFAULT;\n\t\t\tgoto out_free;\n\t\t}\n\n\t\tfor (i = 0; i < nsems; i++) {\n\t\t\tif (sem_io[i] > SEMVMX) {\n\t\t\t\tsem_putref(sma);\n\t\t\t\terr = -ERANGE;\n\t\t\t\tgoto out_free;\n\t\t\t}\n\t\t}\n\t\tsem_lock_and_putref(sma);\n\t\tif (sma->sem_perm.deleted) {\n\t\t\tsem_unlock(sma, -1);\n\t\t\terr = -EIDRM;\n\t\t\tgoto out_free;\n\t\t}\n\n\t\tfor (i = 0; i < nsems; i++)\n\t\t\tsma->sem_base[i].semval = sem_io[i];\n\n\t\tassert_spin_locked(&sma->sem_perm.lock);\n\t\tlist_for_each_entry(un, &sma->list_id, list_id) {\n\t\t\tfor (i = 0; i < nsems; i++)\n\t\t\t\tun->semadj[i] = 0;\n\t\t}\n\t\tsma->sem_ctime = get_seconds();\n\t\t/* maybe some queued-up processes were waiting for this */\n\t\tdo_smart_update(sma, NULL, 0, 0, &tasks);\n\t\terr = 0;\n\t\tgoto out_unlock;\n\t}\n\t/* GETVAL, GETPID, GETNCTN, GETZCNT: fall-through */\n\t}\n\terr = -EINVAL;\n\tif (semnum < 0 || semnum >= nsems) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\tsem_lock(sma, NULL, -1);\n\tcurr = &sma->sem_base[semnum];\n\n\tswitch (cmd) {\n\tcase GETVAL:\n\t\terr = curr->semval;\n\t\tgoto out_unlock;\n\tcase GETPID:\n\t\terr = curr->sempid;\n\t\tgoto out_unlock;\n\tcase GETNCNT:\n\t\terr = count_semncnt(sma,semnum);\n\t\tgoto out_unlock;\n\tcase GETZCNT:\n\t\terr = count_semzcnt(sma,semnum);\n\t\tgoto out_unlock;\n\t}\n\nout_unlock:\n\tsem_unlock(sma, -1);\nout_wakeup:\n\twake_up_sem_queue_do(&tasks);\nout_free:\n\tif(sem_io != fast_sem_io)\n\t\tipc_free(sem_io, sizeof(ushort)*nsems);\n\treturn err;\n}"
    },
    {
        "patch_model": "Replace",
        "change_type": "Fun-Call",
        "line_old": 1146,
        "line_new": 1225,
        "critical_vars": [
            "sma"
        ],
        "function": "semctl_main",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "True",
        "function_code": "\nstatic int semctl_main(struct ipc_namespace *ns, int semid, int semnum,\n\t\tint cmd, void __user *p)\n{\n\tstruct sem_array *sma;\n\tstruct sem* curr;\n\tint err, nsems;\n\tushort fast_sem_io[SEMMSL_FAST];\n\tushort* sem_io = fast_sem_io;\n\tstruct list_head tasks;\n\n\tINIT_LIST_HEAD(&tasks);\n\n\trcu_read_lock();\n\tsma = sem_obtain_object_check(ns, semid);\n\tif (IS_ERR(sma)) {\n\t\trcu_read_unlock();\n\t\treturn PTR_ERR(sma);\n\t}\n\n\tnsems = sma->sem_nsems;\n\n\terr = -EACCES;\n\tif (ipcperms(ns, &sma->sem_perm,\n\t\t\tcmd == SETALL ? S_IWUGO : S_IRUGO)) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terr = security_sem_semctl(sma, cmd);\n\tif (err) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terr = -EACCES;\n\tswitch (cmd) {\n\tcase GETALL:\n\t{\n\t\tushort __user *array = p;\n\t\tint i;\n\n\t\tif(nsems > SEMMSL_FAST) {\n\t\t\tsem_getref(sma);\n\n\t\t\tsem_io = ipc_alloc(sizeof(ushort)*nsems);\n\t\t\tif(sem_io == NULL) {\n\t\t\t\tsem_putref(sma);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\n\t\t\tsem_lock_and_putref(sma);\n\t\t\tif (sma->sem_perm.deleted) {\n\t\t\t\tsem_unlock(sma, -1);\n\t\t\t\terr = -EIDRM;\n\t\t\t\tgoto out_free;\n\t\t\t}\n\t\t} else\n\t\t\tsem_lock(sma, NULL, -1);\n\n\t\tfor (i = 0; i < sma->sem_nsems; i++)\n\t\t\tsem_io[i] = sma->sem_base[i].semval;\n\t\tsem_unlock(sma, -1);\n\t\terr = 0;\n\t\tif(copy_to_user(array, sem_io, nsems*sizeof(ushort)))\n\t\t\terr = -EFAULT;\n\t\tgoto out_free;\n\t}\n\tcase SETALL:\n\t{\n\t\tint i;\n\t\tstruct sem_undo *un;\n\n\t\tif (!ipc_rcu_getref(sma)) {\n\t\t\trcu_read_unlock();\n\t\t\treturn -EIDRM;\n\t\t}\n\t\trcu_read_unlock();\n\n\t\tif(nsems > SEMMSL_FAST) {\n\t\t\tsem_io = ipc_alloc(sizeof(ushort)*nsems);\n\t\t\tif(sem_io == NULL) {\n\t\t\t\tsem_putref(sma);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t}\n\n\t\tif (copy_from_user (sem_io, p, nsems*sizeof(ushort))) {\n\t\t\tsem_putref(sma);\n\t\t\terr = -EFAULT;\n\t\t\tgoto out_free;\n\t\t}\n\n\t\tfor (i = 0; i < nsems; i++) {\n\t\t\tif (sem_io[i] > SEMVMX) {\n\t\t\t\tsem_putref(sma);\n\t\t\t\terr = -ERANGE;\n\t\t\t\tgoto out_free;\n\t\t\t}\n\t\t}\n\t\tsem_lock_and_putref(sma);\n\t\tif (sma->sem_perm.deleted) {\n\t\t\tsem_unlock(sma, -1);\n\t\t\terr = -EIDRM;\n\t\t\tgoto out_free;\n\t\t}\n\n\t\tfor (i = 0; i < nsems; i++)\n\t\t\tsma->sem_base[i].semval = sem_io[i];\n\n\t\tassert_spin_locked(&sma->sem_perm.lock);\n\t\tlist_for_each_entry(un, &sma->list_id, list_id) {\n\t\t\tfor (i = 0; i < nsems; i++)\n\t\t\t\tun->semadj[i] = 0;\n\t\t}\n\t\tsma->sem_ctime = get_seconds();\n\t\t/* maybe some queued-up processes were waiting for this */\n\t\tdo_smart_update(sma, NULL, 0, 0, &tasks);\n\t\terr = 0;\n\t\tgoto out_unlock;\n\t}\n\t/* GETVAL, GETPID, GETNCTN, GETZCNT: fall-through */\n\t}\n\terr = -EINVAL;\n\tif (semnum < 0 || semnum >= nsems) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\tsem_lock(sma, NULL, -1);\n\tcurr = &sma->sem_base[semnum];\n\n\tswitch (cmd) {\n\tcase GETVAL:\n\t\terr = curr->semval;\n\t\tgoto out_unlock;\n\tcase GETPID:\n\t\terr = curr->sempid;\n\t\tgoto out_unlock;\n\tcase GETNCNT:\n\t\terr = count_semncnt(sma,semnum);\n\t\tgoto out_unlock;\n\tcase GETZCNT:\n\t\terr = count_semzcnt(sma,semnum);\n\t\tgoto out_unlock;\n\t}\n\nout_unlock:\n\tsem_unlock(sma, -1);\nout_wakeup:\n\twake_up_sem_queue_do(&tasks);\nout_free:\n\tif(sem_io != fast_sem_io)\n\t\tipc_free(sem_io, sizeof(ushort)*nsems);\n\treturn err;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Fun-Call",
        "line_old": 1214,
        "critical_vars": [
            "&sma->sem_perm"
        ],
        "function": "semctl_down",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "False",
        "function_code": "static int semctl_down(struct ipc_namespace *ns, int semid,\n\t\t       int cmd, int version, void __user *p)\n{\n\tstruct sem_array *sma;\n\tint err;\n\tstruct semid64_ds semid64;\n\tstruct kern_ipc_perm *ipcp;\n\n\tif(cmd == IPC_SET) {\n\t\tif (copy_semid_from_user(&semid64, p, version))\n\t\t\treturn -EFAULT;\n\t}\n\n\tipcp = ipcctl_pre_down_nolock(ns, &sem_ids(ns), semid, cmd,\n\t\t\t\t      &semid64.sem_perm, 0);\n\tif (IS_ERR(ipcp))\n\t\treturn PTR_ERR(ipcp);\n\n\tsma = container_of(ipcp, struct sem_array, sem_perm);\n\n\terr = security_sem_semctl(sma, cmd);\n\tif (err) {\n\t\trcu_read_unlock();\n\t\tgoto out_unlock;\n\t}\n\n\tswitch(cmd){\n\tcase IPC_RMID:\n\t\tipc_lock_object(&sma->sem_perm);\n\t\tfreeary(ns, ipcp);\n\t\tgoto out_up;\n\tcase IPC_SET:\n\t\tipc_lock_object(&sma->sem_perm);\n\t\terr = ipc_update_perm(&semid64.sem_perm, ipcp);\n\t\tif (err)\n\t\t\tgoto out_unlock;\n\t\tsma->sem_ctime = get_seconds();\n\t\tbreak;\n\tdefault:\n\t\trcu_read_unlock();\n\t\terr = -EINVAL;\n\t\tgoto out_up;\n\t}\n\nout_unlock:\n\tsem_unlock(sma);\nout_up:\n\tup_write(&sem_ids(ns).rw_mutex);\n\treturn err;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 1293,
        "critical_vars": [
            "sma"
        ],
        "function": "semctl_down",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "True",
        "function_code": "static int semctl_down(struct ipc_namespace *ns, int semid,\n\t\t       int cmd, int version, void __user *p)\n{\n\tstruct sem_array *sma;\n\tint err;\n\tstruct semid64_ds semid64;\n\tstruct kern_ipc_perm *ipcp;\n\n\tif(cmd == IPC_SET) {\n\t\tif (copy_semid_from_user(&semid64, p, version))\n\t\t\treturn -EFAULT;\n\t}\n\n\tipcp = ipcctl_pre_down_nolock(ns, &sem_ids(ns), semid, cmd,\n\t\t\t\t      &semid64.sem_perm, 0);\n\tif (IS_ERR(ipcp))\n\t\treturn PTR_ERR(ipcp);\n\n\tsma = container_of(ipcp, struct sem_array, sem_perm);\n\n\terr = security_sem_semctl(sma, cmd);\n\tif (err) {\n\t\trcu_read_unlock();\n\t\tgoto out_unlock;\n\t}\n\n\tswitch(cmd){\n\tcase IPC_RMID:\n\t\tsem_lock(sma, NULL, -1);\n\t\tfreeary(ns, ipcp);\n\t\tgoto out_up;\n\tcase IPC_SET:\n\t\tsem_lock(sma, NULL, -1);\n\t\terr = ipc_update_perm(&semid64.sem_perm, ipcp);\n\t\tif (err)\n\t\t\tgoto out_unlock;\n\t\tsma->sem_ctime = get_seconds();\n\t\tbreak;\n\tdefault:\n\t\trcu_read_unlock();\n\t\terr = -EINVAL;\n\t\tgoto out_up;\n\t}\n\nout_unlock:\n\tsem_unlock(sma, -1);\nout_up:\n\tup_write(&sem_ids(ns).rw_mutex);\n\treturn err;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Fun-Call",
        "line_old": 1218,
        "critical_vars": [
            "&sma->sem_perm"
        ],
        "function": "semctl_down",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "False",
        "function_code": "static int semctl_down(struct ipc_namespace *ns, int semid,\n\t\t       int cmd, int version, void __user *p)\n{\n\tstruct sem_array *sma;\n\tint err;\n\tstruct semid64_ds semid64;\n\tstruct kern_ipc_perm *ipcp;\n\n\tif(cmd == IPC_SET) {\n\t\tif (copy_semid_from_user(&semid64, p, version))\n\t\t\treturn -EFAULT;\n\t}\n\n\tipcp = ipcctl_pre_down_nolock(ns, &sem_ids(ns), semid, cmd,\n\t\t\t\t      &semid64.sem_perm, 0);\n\tif (IS_ERR(ipcp))\n\t\treturn PTR_ERR(ipcp);\n\n\tsma = container_of(ipcp, struct sem_array, sem_perm);\n\n\terr = security_sem_semctl(sma, cmd);\n\tif (err) {\n\t\trcu_read_unlock();\n\t\tgoto out_unlock;\n\t}\n\n\tswitch(cmd){\n\tcase IPC_RMID:\n\t\tipc_lock_object(&sma->sem_perm);\n\t\tfreeary(ns, ipcp);\n\t\tgoto out_up;\n\tcase IPC_SET:\n\t\tipc_lock_object(&sma->sem_perm);\n\t\terr = ipc_update_perm(&semid64.sem_perm, ipcp);\n\t\tif (err)\n\t\t\tgoto out_unlock;\n\t\tsma->sem_ctime = get_seconds();\n\t\tbreak;\n\tdefault:\n\t\trcu_read_unlock();\n\t\terr = -EINVAL;\n\t\tgoto out_up;\n\t}\n\nout_unlock:\n\tsem_unlock(sma);\nout_up:\n\tup_write(&sem_ids(ns).rw_mutex);\n\treturn err;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 1297,
        "critical_vars": [
            "sma"
        ],
        "function": "semctl_down",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "True",
        "function_code": "static int semctl_down(struct ipc_namespace *ns, int semid,\n\t\t       int cmd, int version, void __user *p)\n{\n\tstruct sem_array *sma;\n\tint err;\n\tstruct semid64_ds semid64;\n\tstruct kern_ipc_perm *ipcp;\n\n\tif(cmd == IPC_SET) {\n\t\tif (copy_semid_from_user(&semid64, p, version))\n\t\t\treturn -EFAULT;\n\t}\n\n\tipcp = ipcctl_pre_down_nolock(ns, &sem_ids(ns), semid, cmd,\n\t\t\t\t      &semid64.sem_perm, 0);\n\tif (IS_ERR(ipcp))\n\t\treturn PTR_ERR(ipcp);\n\n\tsma = container_of(ipcp, struct sem_array, sem_perm);\n\n\terr = security_sem_semctl(sma, cmd);\n\tif (err) {\n\t\trcu_read_unlock();\n\t\tgoto out_unlock;\n\t}\n\n\tswitch(cmd){\n\tcase IPC_RMID:\n\t\tsem_lock(sma, NULL, -1);\n\t\tfreeary(ns, ipcp);\n\t\tgoto out_up;\n\tcase IPC_SET:\n\t\tsem_lock(sma, NULL, -1);\n\t\terr = ipc_update_perm(&semid64.sem_perm, ipcp);\n\t\tif (err)\n\t\t\tgoto out_unlock;\n\t\tsma->sem_ctime = get_seconds();\n\t\tbreak;\n\tdefault:\n\t\trcu_read_unlock();\n\t\terr = -EINVAL;\n\t\tgoto out_up;\n\t}\n\nout_unlock:\n\tsem_unlock(sma, -1);\nout_up:\n\tup_write(&sem_ids(ns).rw_mutex);\n\treturn err;\n}"
    },
    {
        "patch_model": "Replace",
        "change_type": "Fun-Call",
        "line_old": 1231,
        "line_new": 1310,
        "critical_vars": [
            "sma"
        ],
        "function": "semctl_down",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "True",
        "function_code": "static int semctl_down(struct ipc_namespace *ns, int semid,\n\t\t       int cmd, int version, void __user *p)\n{\n\tstruct sem_array *sma;\n\tint err;\n\tstruct semid64_ds semid64;\n\tstruct kern_ipc_perm *ipcp;\n\n\tif(cmd == IPC_SET) {\n\t\tif (copy_semid_from_user(&semid64, p, version))\n\t\t\treturn -EFAULT;\n\t}\n\n\tipcp = ipcctl_pre_down_nolock(ns, &sem_ids(ns), semid, cmd,\n\t\t\t\t      &semid64.sem_perm, 0);\n\tif (IS_ERR(ipcp))\n\t\treturn PTR_ERR(ipcp);\n\n\tsma = container_of(ipcp, struct sem_array, sem_perm);\n\n\terr = security_sem_semctl(sma, cmd);\n\tif (err) {\n\t\trcu_read_unlock();\n\t\tgoto out_unlock;\n\t}\n\n\tswitch(cmd){\n\tcase IPC_RMID:\n\t\tsem_lock(sma, NULL, -1);\n\t\tfreeary(ns, ipcp);\n\t\tgoto out_up;\n\tcase IPC_SET:\n\t\tsem_lock(sma, NULL, -1);\n\t\terr = ipc_update_perm(&semid64.sem_perm, ipcp);\n\t\tif (err)\n\t\t\tgoto out_unlock;\n\t\tsma->sem_ctime = get_seconds();\n\t\tbreak;\n\tdefault:\n\t\trcu_read_unlock();\n\t\terr = -EINVAL;\n\t\tgoto out_up;\n\t}\n\nout_unlock:\n\tsem_unlock(sma, -1);\nout_up:\n\tup_write(&sem_ids(ns).rw_mutex);\n\treturn err;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Var-Declaration",
        "line_new": 1422,
        "critical_vars": [
            "error"
        ],
        "function": "*find_alloc_undo",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "True",
        "function_code": "static struct sem_undo *find_alloc_undo(struct ipc_namespace *ns, int semid)\n{\n\tstruct sem_array *sma;\n\tstruct sem_undo_list *ulp;\n\tstruct sem_undo *un, *new;\n\tint nsems, error;\n\n\terror = get_undo_list(&ulp);\n\tif (error)\n\t\treturn ERR_PTR(error);\n\n\trcu_read_lock();\n\tspin_lock(&ulp->lock);\n\tun = lookup_undo(ulp, semid);\n\tspin_unlock(&ulp->lock);\n\tif (likely(un!=NULL))\n\t\tgoto out;\n\n\t/* no undo structure around - allocate one. */\n\t/* step 1: figure out the size of the semaphore array */\n\tsma = sem_obtain_object_check(ns, semid);\n\tif (IS_ERR(sma)) {\n\t\trcu_read_unlock();\n\t\treturn ERR_CAST(sma);\n\t}\n\n\tnsems = sma->sem_nsems;\n\tif (!ipc_rcu_getref(sma)) {\n\t\trcu_read_unlock();\n\t\tun = ERR_PTR(-EIDRM);\n\t\tgoto out;\n\t}\n\trcu_read_unlock();\n\n\t/* step 2: allocate new undo structure */\n\tnew = kzalloc(sizeof(struct sem_undo) + sizeof(short)*nsems, GFP_KERNEL);\n\tif (!new) {\n\t\tsem_putref(sma);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\t/* step 3: Acquire the lock on semaphore array */\n\tsem_lock_and_putref(sma);\n\tif (sma->sem_perm.deleted) {\n\t\tsem_unlock(sma, -1);\n\t\tkfree(new);\n\t\tun = ERR_PTR(-EIDRM);\n\t\tgoto out;\n\t}\n\tspin_lock(&ulp->lock);\n\n\t/*\n\t * step 4: check for races: did someone else allocate the undo struct?\n\t */\n\tun = lookup_undo(ulp, semid);\n\tif (un) {\n\t\tkfree(new);\n\t\tgoto success;\n\t}\n\t/* step 5: initialize & link new undo structure */\n\tnew->semadj = (short *) &new[1];\n\tnew->ulp = ulp;\n\tnew->semid = semid;\n\tassert_spin_locked(&ulp->lock);\n\tlist_add_rcu(&new->list_proc, &ulp->list_proc);\n\tassert_spin_locked(&sma->sem_perm.lock);\n\tlist_add(&new->list_id, &sma->list_id);\n\tun = new;\n\nsuccess:\n\tspin_unlock(&ulp->lock);\n\trcu_read_lock();\n\tsem_unlock(sma, -1);\nout:\n\treturn un;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Var-Declaration",
        "line_old": 1344,
        "critical_vars": [
            "error"
        ],
        "function": "*find_alloc_undo",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "False",
        "function_code": "static struct sem_undo *find_alloc_undo(struct ipc_namespace *ns, int semid)\n{\n\tstruct sem_array *sma;\n\tstruct sem_undo_list *ulp;\n\tstruct sem_undo *un, *new;\n\tint nsems;\n\tint error;\n\n\terror = get_undo_list(&ulp);\n\tif (error)\n\t\treturn ERR_PTR(error);\n\n\trcu_read_lock();\n\tspin_lock(&ulp->lock);\n\tun = lookup_undo(ulp, semid);\n\tspin_unlock(&ulp->lock);\n\tif (likely(un!=NULL))\n\t\tgoto out;\n\n\t/* no undo structure around - allocate one. */\n\t/* step 1: figure out the size of the semaphore array */\n\tsma = sem_obtain_object_check(ns, semid);\n\tif (IS_ERR(sma)) {\n\t\trcu_read_unlock();\n\t\treturn ERR_CAST(sma);\n\t}\n\n\tnsems = sma->sem_nsems;\n\tipc_rcu_getref(sma);\n\trcu_read_unlock();\n\n\t/* step 2: allocate new undo structure */\n\tnew = kzalloc(sizeof(struct sem_undo) + sizeof(short)*nsems, GFP_KERNEL);\n\tif (!new) {\n\t\tsem_putref(sma);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\t/* step 3: Acquire the lock on semaphore array */\n\tsem_lock_and_putref(sma);\n\tif (sma->sem_perm.deleted) {\n\t\tsem_unlock(sma);\n\t\tkfree(new);\n\t\tun = ERR_PTR(-EIDRM);\n\t\tgoto out;\n\t}\n\tspin_lock(&ulp->lock);\n\n\t/*\n\t * step 4: check for races: did someone else allocate the undo struct?\n\t */\n\tun = lookup_undo(ulp, semid);\n\tif (un) {\n\t\tkfree(new);\n\t\tgoto success;\n\t}\n\t/* step 5: initialize & link new undo structure */\n\tnew->semadj = (short *) &new[1];\n\tnew->ulp = ulp;\n\tnew->semid = semid;\n\tassert_spin_locked(&ulp->lock);\n\tlist_add_rcu(&new->list_proc, &ulp->list_proc);\n\tassert_spin_locked(&sma->sem_perm.lock);\n\tlist_add(&new->list_id, &sma->list_id);\n\tun = new;\n\nsuccess:\n\tspin_unlock(&ulp->lock);\n\trcu_read_lock();\n\tsem_unlock(sma);\nout:\n\treturn un;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Fun-Call",
        "line_old": 1366,
        "critical_vars": [
            "sma"
        ],
        "function": "*find_alloc_undo",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "False",
        "function_code": "static struct sem_undo *find_alloc_undo(struct ipc_namespace *ns, int semid)\n{\n\tstruct sem_array *sma;\n\tstruct sem_undo_list *ulp;\n\tstruct sem_undo *un, *new;\n\tint nsems;\n\tint error;\n\n\terror = get_undo_list(&ulp);\n\tif (error)\n\t\treturn ERR_PTR(error);\n\n\trcu_read_lock();\n\tspin_lock(&ulp->lock);\n\tun = lookup_undo(ulp, semid);\n\tspin_unlock(&ulp->lock);\n\tif (likely(un!=NULL))\n\t\tgoto out;\n\n\t/* no undo structure around - allocate one. */\n\t/* step 1: figure out the size of the semaphore array */\n\tsma = sem_obtain_object_check(ns, semid);\n\tif (IS_ERR(sma)) {\n\t\trcu_read_unlock();\n\t\treturn ERR_CAST(sma);\n\t}\n\n\tnsems = sma->sem_nsems;\n\tipc_rcu_getref(sma);\n\trcu_read_unlock();\n\n\t/* step 2: allocate new undo structure */\n\tnew = kzalloc(sizeof(struct sem_undo) + sizeof(short)*nsems, GFP_KERNEL);\n\tif (!new) {\n\t\tsem_putref(sma);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\t/* step 3: Acquire the lock on semaphore array */\n\tsem_lock_and_putref(sma);\n\tif (sma->sem_perm.deleted) {\n\t\tsem_unlock(sma);\n\t\tkfree(new);\n\t\tun = ERR_PTR(-EIDRM);\n\t\tgoto out;\n\t}\n\tspin_lock(&ulp->lock);\n\n\t/*\n\t * step 4: check for races: did someone else allocate the undo struct?\n\t */\n\tun = lookup_undo(ulp, semid);\n\tif (un) {\n\t\tkfree(new);\n\t\tgoto success;\n\t}\n\t/* step 5: initialize & link new undo structure */\n\tnew->semadj = (short *) &new[1];\n\tnew->ulp = ulp;\n\tnew->semid = semid;\n\tassert_spin_locked(&ulp->lock);\n\tlist_add_rcu(&new->list_proc, &ulp->list_proc);\n\tassert_spin_locked(&sma->sem_perm.lock);\n\tlist_add(&new->list_id, &sma->list_id);\n\tun = new;\n\nsuccess:\n\tspin_unlock(&ulp->lock);\n\trcu_read_lock();\n\tsem_unlock(sma);\nout:\n\treturn un;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 1444,
        "critical_vars": [
            "sma"
        ],
        "function": "*find_alloc_undo",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "True",
        "function_code": "static struct sem_undo *find_alloc_undo(struct ipc_namespace *ns, int semid)\n{\n\tstruct sem_array *sma;\n\tstruct sem_undo_list *ulp;\n\tstruct sem_undo *un, *new;\n\tint nsems, error;\n\n\terror = get_undo_list(&ulp);\n\tif (error)\n\t\treturn ERR_PTR(error);\n\n\trcu_read_lock();\n\tspin_lock(&ulp->lock);\n\tun = lookup_undo(ulp, semid);\n\tspin_unlock(&ulp->lock);\n\tif (likely(un!=NULL))\n\t\tgoto out;\n\n\t/* no undo structure around - allocate one. */\n\t/* step 1: figure out the size of the semaphore array */\n\tsma = sem_obtain_object_check(ns, semid);\n\tif (IS_ERR(sma)) {\n\t\trcu_read_unlock();\n\t\treturn ERR_CAST(sma);\n\t}\n\n\tnsems = sma->sem_nsems;\n\tif (!ipc_rcu_getref(sma)) {\n\t\trcu_read_unlock();\n\t\tun = ERR_PTR(-EIDRM);\n\t\tgoto out;\n\t}\n\trcu_read_unlock();\n\n\t/* step 2: allocate new undo structure */\n\tnew = kzalloc(sizeof(struct sem_undo) + sizeof(short)*nsems, GFP_KERNEL);\n\tif (!new) {\n\t\tsem_putref(sma);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\t/* step 3: Acquire the lock on semaphore array */\n\tsem_lock_and_putref(sma);\n\tif (sma->sem_perm.deleted) {\n\t\tsem_unlock(sma, -1);\n\t\tkfree(new);\n\t\tun = ERR_PTR(-EIDRM);\n\t\tgoto out;\n\t}\n\tspin_lock(&ulp->lock);\n\n\t/*\n\t * step 4: check for races: did someone else allocate the undo struct?\n\t */\n\tun = lookup_undo(ulp, semid);\n\tif (un) {\n\t\tkfree(new);\n\t\tgoto success;\n\t}\n\t/* step 5: initialize & link new undo structure */\n\tnew->semadj = (short *) &new[1];\n\tnew->ulp = ulp;\n\tnew->semid = semid;\n\tassert_spin_locked(&ulp->lock);\n\tlist_add_rcu(&new->list_proc, &ulp->list_proc);\n\tassert_spin_locked(&sma->sem_perm.lock);\n\tlist_add(&new->list_id, &sma->list_id);\n\tun = new;\n\nsuccess:\n\tspin_unlock(&ulp->lock);\n\trcu_read_lock();\n\tsem_unlock(sma, -1);\nout:\n\treturn un;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 1446,
        "critical_vars": [
            "un"
        ],
        "function": "*find_alloc_undo",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "True",
        "function_code": "static struct sem_undo *find_alloc_undo(struct ipc_namespace *ns, int semid)\n{\n\tstruct sem_array *sma;\n\tstruct sem_undo_list *ulp;\n\tstruct sem_undo *un, *new;\n\tint nsems, error;\n\n\terror = get_undo_list(&ulp);\n\tif (error)\n\t\treturn ERR_PTR(error);\n\n\trcu_read_lock();\n\tspin_lock(&ulp->lock);\n\tun = lookup_undo(ulp, semid);\n\tspin_unlock(&ulp->lock);\n\tif (likely(un!=NULL))\n\t\tgoto out;\n\n\t/* no undo structure around - allocate one. */\n\t/* step 1: figure out the size of the semaphore array */\n\tsma = sem_obtain_object_check(ns, semid);\n\tif (IS_ERR(sma)) {\n\t\trcu_read_unlock();\n\t\treturn ERR_CAST(sma);\n\t}\n\n\tnsems = sma->sem_nsems;\n\tif (!ipc_rcu_getref(sma)) {\n\t\trcu_read_unlock();\n\t\tun = ERR_PTR(-EIDRM);\n\t\tgoto out;\n\t}\n\trcu_read_unlock();\n\n\t/* step 2: allocate new undo structure */\n\tnew = kzalloc(sizeof(struct sem_undo) + sizeof(short)*nsems, GFP_KERNEL);\n\tif (!new) {\n\t\tsem_putref(sma);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\t/* step 3: Acquire the lock on semaphore array */\n\tsem_lock_and_putref(sma);\n\tif (sma->sem_perm.deleted) {\n\t\tsem_unlock(sma, -1);\n\t\tkfree(new);\n\t\tun = ERR_PTR(-EIDRM);\n\t\tgoto out;\n\t}\n\tspin_lock(&ulp->lock);\n\n\t/*\n\t * step 4: check for races: did someone else allocate the undo struct?\n\t */\n\tun = lookup_undo(ulp, semid);\n\tif (un) {\n\t\tkfree(new);\n\t\tgoto success;\n\t}\n\t/* step 5: initialize & link new undo structure */\n\tnew->semadj = (short *) &new[1];\n\tnew->ulp = ulp;\n\tnew->semid = semid;\n\tassert_spin_locked(&ulp->lock);\n\tlist_add_rcu(&new->list_proc, &ulp->list_proc);\n\tassert_spin_locked(&sma->sem_perm.lock);\n\tlist_add(&new->list_id, &sma->list_id);\n\tun = new;\n\nsuccess:\n\tspin_unlock(&ulp->lock);\n\trcu_read_lock();\n\tsem_unlock(sma, -1);\nout:\n\treturn un;\n}"
    },
    {
        "patch_model": "Replace",
        "change_type": "Fun-Call",
        "line_old": 1379,
        "line_new": 1461,
        "critical_vars": [
            "sma"
        ],
        "function": "*find_alloc_undo",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "True",
        "function_code": "static struct sem_undo *find_alloc_undo(struct ipc_namespace *ns, int semid)\n{\n\tstruct sem_array *sma;\n\tstruct sem_undo_list *ulp;\n\tstruct sem_undo *un, *new;\n\tint nsems, error;\n\n\terror = get_undo_list(&ulp);\n\tif (error)\n\t\treturn ERR_PTR(error);\n\n\trcu_read_lock();\n\tspin_lock(&ulp->lock);\n\tun = lookup_undo(ulp, semid);\n\tspin_unlock(&ulp->lock);\n\tif (likely(un!=NULL))\n\t\tgoto out;\n\n\t/* no undo structure around - allocate one. */\n\t/* step 1: figure out the size of the semaphore array */\n\tsma = sem_obtain_object_check(ns, semid);\n\tif (IS_ERR(sma)) {\n\t\trcu_read_unlock();\n\t\treturn ERR_CAST(sma);\n\t}\n\n\tnsems = sma->sem_nsems;\n\tif (!ipc_rcu_getref(sma)) {\n\t\trcu_read_unlock();\n\t\tun = ERR_PTR(-EIDRM);\n\t\tgoto out;\n\t}\n\trcu_read_unlock();\n\n\t/* step 2: allocate new undo structure */\n\tnew = kzalloc(sizeof(struct sem_undo) + sizeof(short)*nsems, GFP_KERNEL);\n\tif (!new) {\n\t\tsem_putref(sma);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\t/* step 3: Acquire the lock on semaphore array */\n\tsem_lock_and_putref(sma);\n\tif (sma->sem_perm.deleted) {\n\t\tsem_unlock(sma, -1);\n\t\tkfree(new);\n\t\tun = ERR_PTR(-EIDRM);\n\t\tgoto out;\n\t}\n\tspin_lock(&ulp->lock);\n\n\t/*\n\t * step 4: check for races: did someone else allocate the undo struct?\n\t */\n\tun = lookup_undo(ulp, semid);\n\tif (un) {\n\t\tkfree(new);\n\t\tgoto success;\n\t}\n\t/* step 5: initialize & link new undo structure */\n\tnew->semadj = (short *) &new[1];\n\tnew->ulp = ulp;\n\tnew->semid = semid;\n\tassert_spin_locked(&ulp->lock);\n\tlist_add_rcu(&new->list_proc, &ulp->list_proc);\n\tassert_spin_locked(&sma->sem_perm.lock);\n\tlist_add(&new->list_id, &sma->list_id);\n\tun = new;\n\nsuccess:\n\tspin_unlock(&ulp->lock);\n\trcu_read_lock();\n\tsem_unlock(sma, -1);\nout:\n\treturn un;\n}"
    },
    {
        "patch_model": "Replace",
        "change_type": "Fun-Call",
        "line_old": 1407,
        "line_new": 1489,
        "critical_vars": [
            "sma"
        ],
        "function": "*find_alloc_undo",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "True",
        "function_code": "static struct sem_undo *find_alloc_undo(struct ipc_namespace *ns, int semid)\n{\n\tstruct sem_array *sma;\n\tstruct sem_undo_list *ulp;\n\tstruct sem_undo *un, *new;\n\tint nsems, error;\n\n\terror = get_undo_list(&ulp);\n\tif (error)\n\t\treturn ERR_PTR(error);\n\n\trcu_read_lock();\n\tspin_lock(&ulp->lock);\n\tun = lookup_undo(ulp, semid);\n\tspin_unlock(&ulp->lock);\n\tif (likely(un!=NULL))\n\t\tgoto out;\n\n\t/* no undo structure around - allocate one. */\n\t/* step 1: figure out the size of the semaphore array */\n\tsma = sem_obtain_object_check(ns, semid);\n\tif (IS_ERR(sma)) {\n\t\trcu_read_unlock();\n\t\treturn ERR_CAST(sma);\n\t}\n\n\tnsems = sma->sem_nsems;\n\tif (!ipc_rcu_getref(sma)) {\n\t\trcu_read_unlock();\n\t\tun = ERR_PTR(-EIDRM);\n\t\tgoto out;\n\t}\n\trcu_read_unlock();\n\n\t/* step 2: allocate new undo structure */\n\tnew = kzalloc(sizeof(struct sem_undo) + sizeof(short)*nsems, GFP_KERNEL);\n\tif (!new) {\n\t\tsem_putref(sma);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\t/* step 3: Acquire the lock on semaphore array */\n\tsem_lock_and_putref(sma);\n\tif (sma->sem_perm.deleted) {\n\t\tsem_unlock(sma, -1);\n\t\tkfree(new);\n\t\tun = ERR_PTR(-EIDRM);\n\t\tgoto out;\n\t}\n\tspin_lock(&ulp->lock);\n\n\t/*\n\t * step 4: check for races: did someone else allocate the undo struct?\n\t */\n\tun = lookup_undo(ulp, semid);\n\tif (un) {\n\t\tkfree(new);\n\t\tgoto success;\n\t}\n\t/* step 5: initialize & link new undo structure */\n\tnew->semadj = (short *) &new[1];\n\tnew->ulp = ulp;\n\tnew->semid = semid;\n\tassert_spin_locked(&ulp->lock);\n\tlist_add_rcu(&new->list_proc, &ulp->list_proc);\n\tassert_spin_locked(&sma->sem_perm.lock);\n\tlist_add(&new->list_id, &sma->list_id);\n\tun = new;\n\nsuccess:\n\tspin_unlock(&ulp->lock);\n\trcu_read_lock();\n\tsem_unlock(sma, -1);\nout:\n\treturn un;\n}"
    },
    {
        "patch_model": "Replace",
        "change_type": "Assignment",
        "line_old": 1447,
        "line_new": 1529,
        "critical_vars": [
            "undos"
        ],
        "function": "SYSCALL_DEFINE4",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "True",
        "function_code": "\n\nSYSCALL_DEFINE4(semtimedop, int, semid, struct sembuf __user *, tsops,\n\t\tunsigned, nsops, const struct timespec __user *, timeout)\n{\n\tint error = -EINVAL;\n\tstruct sem_array *sma;\n\tstruct sembuf fast_sops[SEMOPM_FAST];\n\tstruct sembuf* sops = fast_sops, *sop;\n\tstruct sem_undo *un;\n\tint undos = 0, alter = 0, max, locknum;\n\tstruct sem_queue queue;\n\tunsigned long jiffies_left = 0;\n\tstruct ipc_namespace *ns;\n\tstruct list_head tasks;\n\n\tns = current->nsproxy->ipc_ns;\n\n\tif (nsops < 1 || semid < 0)\n\t\treturn -EINVAL;\n\tif (nsops > ns->sc_semopm)\n\t\treturn -E2BIG;\n\tif(nsops > SEMOPM_FAST) {\n\t\tsops = kmalloc(sizeof(*sops)*nsops,GFP_KERNEL);\n\t\tif(sops==NULL)\n\t\t\treturn -ENOMEM;\n\t}\n\tif (copy_from_user (sops, tsops, nsops * sizeof(*tsops))) {\n\t\terror=-EFAULT;\n\t\tgoto out_free;\n\t}\n\tif (timeout) {\n\t\tstruct timespec _timeout;\n\t\tif (copy_from_user(&_timeout, timeout, sizeof(*timeout))) {\n\t\t\terror = -EFAULT;\n\t\t\tgoto out_free;\n\t\t}\n\t\tif (_timeout.tv_sec < 0 || _timeout.tv_nsec < 0 ||\n\t\t\t_timeout.tv_nsec >= 1000000000L) {\n\t\t\terror = -EINVAL;\n\t\t\tgoto out_free;\n\t\t}\n\t\tjiffies_left = timespec_to_jiffies(&_timeout);\n\t}\n\tmax = 0;\n\tfor (sop = sops; sop < sops + nsops; sop++) {\n\t\tif (sop->sem_num >= max)\n\t\t\tmax = sop->sem_num;\n\t\tif (sop->sem_flg & SEM_UNDO)\n\t\t\tundos = 1;\n\t\tif (sop->sem_op != 0)\n\t\t\talter = 1;\n\t}\n\n\tINIT_LIST_HEAD(&tasks);\n\n\tif (undos) {\n\t\t/* On success, find_alloc_undo takes the rcu_read_lock */\n\t\tun = find_alloc_undo(ns, semid);\n\t\tif (IS_ERR(un)) {\n\t\t\terror = PTR_ERR(un);\n\t\t\tgoto out_free;\n\t\t}\n\t} else {\n\t\tun = NULL;\n\t\trcu_read_lock();\n\t}\n\n\tsma = sem_obtain_object_check(ns, semid);\n\tif (IS_ERR(sma)) {\n\t\trcu_read_unlock();\n\t\terror = PTR_ERR(sma);\n\t\tgoto out_free;\n\t}\n\n\terror = -EFBIG;\n\tif (max >= sma->sem_nsems) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terror = -EACCES;\n\tif (ipcperms(ns, &sma->sem_perm, alter ? S_IWUGO : S_IRUGO)) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terror = security_sem_semop(sma, sops, nsops, alter);\n\tif (error) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\t/*\n\t * semid identifiers are not unique - find_alloc_undo may have\n\t * allocated an undo structure, it was invalidated by an RMID\n\t * and now a new array with received the same id. Check and fail.\n\t * This case can be detected checking un->semid. The existence of\n\t * \"un\" itself is guaranteed by rcu.\n\t */\n\terror = -EIDRM;\n\tlocknum = sem_lock(sma, sops, nsops);\n\tif (un && un->semid == -1)\n\t\tgoto out_unlock_free;\n\n\terror = try_atomic_semop (sma, sops, nsops, un, task_tgid_vnr(current));\n\tif (error <= 0) {\n\t\tif (alter && error == 0)\n\t\t\tdo_smart_update(sma, sops, nsops, 1, &tasks);\n\n\t\tgoto out_unlock_free;\n\t}\n\n\t/* We need to sleep on this operation, so we put the current\n\t * task into the pending queue and go to sleep.\n\t */\n\t\t\n\tqueue.sops = sops;\n\tqueue.nsops = nsops;\n\tqueue.undo = un;\n\tqueue.pid = task_tgid_vnr(current);\n\tqueue.alter = alter;\n\n\tif (nsops == 1) {\n\t\tstruct sem *curr;\n\t\tcurr = &sma->sem_base[sops->sem_num];\n\n\t\tif (alter)\n\t\t\tlist_add_tail(&queue.list, &curr->sem_pending);\n\t\telse\n\t\t\tlist_add(&queue.list, &curr->sem_pending);\n\t} else {\n\t\tif (alter)\n\t\t\tlist_add_tail(&queue.list, &sma->sem_pending);\n\t\telse\n\t\t\tlist_add(&queue.list, &sma->sem_pending);\n\t\tsma->complex_count++;\n\t}\n\n\tqueue.status = -EINTR;\n\tqueue.sleeper = current;\n\nsleep_again:\n\tcurrent->state = TASK_INTERRUPTIBLE;\n\tsem_unlock(sma, locknum);\n\n\tif (timeout)\n\t\tjiffies_left = schedule_timeout(jiffies_left);\n\telse\n\t\tschedule();\n\n\terror = get_queue_result(&queue);\n\n\tif (error != -EINTR) {\n\t\t/* fast path: update_queue already obtained all requested\n\t\t * resources.\n\t\t * Perform a smp_mb(): User space could assume that semop()\n\t\t * is a memory barrier: Without the mb(), the cpu could\n\t\t * speculatively read in user space stale data that was\n\t\t * overwritten by the previous owner of the semaphore.\n\t\t */\n\t\tsmp_mb();\n\n\t\tgoto out_free;\n\t}\n\n\tsma = sem_obtain_lock(ns, semid, sops, nsops, &locknum);\n\n\t/*\n\t * Wait until it's guaranteed that no wakeup_sem_queue_do() is ongoing.\n\t */\n\terror = get_queue_result(&queue);\n\n\t/*\n\t * Array removed? If yes, leave without sem_unlock().\n\t */\n\tif (IS_ERR(sma)) {\n\t\tgoto out_free;\n\t}\n\n\n\t/*\n\t * If queue.status != -EINTR we are woken up by another process.\n\t * Leave without unlink_queue(), but with sem_unlock().\n\t */\n\n\tif (error != -EINTR) {\n\t\tgoto out_unlock_free;\n\t}\n\n\t/*\n\t * If an interrupt occurred we have to clean up the queue\n\t */\n\tif (timeout && jiffies_left == 0)\n\t\terror = -EAGAIN;\n\n\t/*\n\t * If the wakeup was spurious, just retry\n\t */\n\tif (error == -EINTR && !signal_pending(current))\n\t\tgoto sleep_again;\n\n\tunlink_queue(sma, &queue);\n\nout_unlock_free:\n\tsem_unlock(sma, locknum);\nout_wakeup:\n\twake_up_sem_queue_do(&tasks);\nout_free:\n\tif(sops != fast_sops)\n\t\tkfree(sops);\n\treturn error;\n}\n\nSYSCALL_DEFINE3(semop, int, semid, struct sembuf __user *, tsops,\n\t\tunsigned, nsops)\n{\n\treturn sys_semtimedop(semid, tsops, nsops, NULL);\n}\n\n/* If CLONE_SYSVSEM is set, establish sharing of SEM_UNDO state between\n * parent and child tasks.\n */\n\nint copy_semundo(unsigned long clone_flags, struct task_struct *tsk)\n{\n\tstruct sem_undo_list *undo_list;\n\tint error;\n\n\tif (clone_flags & CLONE_SYSVSEM) {\n\t\terror = get_undo_list(&undo_list);\n\t\tif (error)\n\t\t\treturn error;\n\t\tatomic_inc(&undo_list->refcnt);\n\t\ttsk->sysvsem.undo_list = undo_list;\n\t} else \n\t\ttsk->sysvsem.undo_list = NULL;\n\n\treturn 0;\n}\n\n/*\n * add semadj values to semaphores, free undo structures.\n * undo structures are not freed when semaphore arrays are destroyed\n * so some of them may be out of date.\n * IMPLEMENTATION NOTE: There is some confusion over whether the\n * set of adjustments that needs to be done should be done in an atomic\n * manner or not. That is, if we are attempting to decrement the semval\n * should we queue up and wait until we can do so legally?\n * The original implementation attempted to do this (queue and wait).\n * The current implementation does not do so. The POSIX standard\n * and SVID should be consulted to determine what behavior is mandated.\n */\nvoid exit_sem(struct task_struct *tsk)\n{\n\tstruct sem_undo_list *ulp;\n\n\tulp = tsk->sysvsem.undo_list;\n\tif (!ulp)\n\t\treturn;\n\ttsk->sysvsem.undo_list = NULL;\n\n\tif (!atomic_dec_and_test(&ulp->refcnt))\n\t\treturn;\n\n\tfor (;;) {\n\t\tstruct sem_array *sma;\n\t\tstruct sem_undo *un;\n\t\tstruct list_head tasks;\n\t\tint semid, i;\n\n\t\trcu_read_lock();\n\t\tun = list_entry_rcu(ulp->list_proc.next,\n\t\t\t\t    struct sem_undo, list_proc);\n\t\tif (&un->list_proc == &ulp->list_proc)\n\t\t\tsemid = -1;\n\t\t else\n\t\t\tsemid = un->semid;\n\n\t\tif (semid == -1) {\n\t\t\trcu_read_unlock();\n\t\t\tbreak;\n\t\t}\n\n\t\tsma = sem_obtain_object_check(tsk->nsproxy->ipc_ns, un->semid);\n\t\t/* exit_sem raced with IPC_RMID, nothing to do */\n\t\tif (IS_ERR(sma)) {\n\t\t\trcu_read_unlock();\n\t\t\tcontinue;\n\t\t}\n\n\t\tsem_lock(sma, NULL, -1);\n\t\tun = __lookup_undo(ulp, semid);\n\t\tif (un == NULL) {\n\t\t\t/* exit_sem raced with IPC_RMID+semget() that created\n\t\t\t * exactly the same semid. Nothing to do.\n\t\t\t */\n\t\t\tsem_unlock(sma, -1);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* remove un from the linked lists */\n\t\tassert_spin_locked(&sma->sem_perm.lock);\n\t\tlist_del(&un->list_id);\n\n\t\tspin_lock(&ulp->lock);\n\t\tlist_del_rcu(&un->list_proc);\n\t\tspin_unlock(&ulp->lock);\n\n\t\t/* perform adjustments registered in un */\n\t\tfor (i = 0; i < sma->sem_nsems; i++) {\n\t\t\tstruct sem * semaphore = &sma->sem_base[i];\n\t\t\tif (un->semadj[i]) {\n\t\t\t\tsemaphore->semval += un->semadj[i];\n\t\t\t\t/*\n\t\t\t\t * Range checks of the new semaphore value,\n\t\t\t\t * not defined by sus:\n\t\t\t\t * - Some unices ignore the undo entirely\n\t\t\t\t *   (e.g. HP UX 11i 11.22, Tru64 V5.1)\n\t\t\t\t * - some cap the value (e.g. FreeBSD caps\n\t\t\t\t *   at 0, but doesn't enforce SEMVMX)\n\t\t\t\t *\n\t\t\t\t * Linux caps the semaphore value, both at 0\n\t\t\t\t * and at SEMVMX.\n\t\t\t\t *\n\t\t\t\t * \tManfred <manfred@colorfullife.com>\n\t\t\t\t */\n\t\t\t\tif (semaphore->semval < 0)\n\t\t\t\t\tsemaphore->semval = 0;\n\t\t\t\tif (semaphore->semval > SEMVMX)\n\t\t\t\t\tsemaphore->semval = SEMVMX;\n\t\t\t\tsemaphore->sempid = task_tgid_vnr(current);\n\t\t\t}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 1573,
        "critical_vars": [
            "&tasks"
        ],
        "function": "SYSCALL_DEFINE4",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "True",
        "function_code": "\n\nSYSCALL_DEFINE4(semtimedop, int, semid, struct sembuf __user *, tsops,\n\t\tunsigned, nsops, const struct timespec __user *, timeout)\n{\n\tint error = -EINVAL;\n\tstruct sem_array *sma;\n\tstruct sembuf fast_sops[SEMOPM_FAST];\n\tstruct sembuf* sops = fast_sops, *sop;\n\tstruct sem_undo *un;\n\tint undos = 0, alter = 0, max, locknum;\n\tstruct sem_queue queue;\n\tunsigned long jiffies_left = 0;\n\tstruct ipc_namespace *ns;\n\tstruct list_head tasks;\n\n\tns = current->nsproxy->ipc_ns;\n\n\tif (nsops < 1 || semid < 0)\n\t\treturn -EINVAL;\n\tif (nsops > ns->sc_semopm)\n\t\treturn -E2BIG;\n\tif(nsops > SEMOPM_FAST) {\n\t\tsops = kmalloc(sizeof(*sops)*nsops,GFP_KERNEL);\n\t\tif(sops==NULL)\n\t\t\treturn -ENOMEM;\n\t}\n\tif (copy_from_user (sops, tsops, nsops * sizeof(*tsops))) {\n\t\terror=-EFAULT;\n\t\tgoto out_free;\n\t}\n\tif (timeout) {\n\t\tstruct timespec _timeout;\n\t\tif (copy_from_user(&_timeout, timeout, sizeof(*timeout))) {\n\t\t\terror = -EFAULT;\n\t\t\tgoto out_free;\n\t\t}\n\t\tif (_timeout.tv_sec < 0 || _timeout.tv_nsec < 0 ||\n\t\t\t_timeout.tv_nsec >= 1000000000L) {\n\t\t\terror = -EINVAL;\n\t\t\tgoto out_free;\n\t\t}\n\t\tjiffies_left = timespec_to_jiffies(&_timeout);\n\t}\n\tmax = 0;\n\tfor (sop = sops; sop < sops + nsops; sop++) {\n\t\tif (sop->sem_num >= max)\n\t\t\tmax = sop->sem_num;\n\t\tif (sop->sem_flg & SEM_UNDO)\n\t\t\tundos = 1;\n\t\tif (sop->sem_op != 0)\n\t\t\talter = 1;\n\t}\n\n\tINIT_LIST_HEAD(&tasks);\n\n\tif (undos) {\n\t\t/* On success, find_alloc_undo takes the rcu_read_lock */\n\t\tun = find_alloc_undo(ns, semid);\n\t\tif (IS_ERR(un)) {\n\t\t\terror = PTR_ERR(un);\n\t\t\tgoto out_free;\n\t\t}\n\t} else {\n\t\tun = NULL;\n\t\trcu_read_lock();\n\t}\n\n\tsma = sem_obtain_object_check(ns, semid);\n\tif (IS_ERR(sma)) {\n\t\trcu_read_unlock();\n\t\terror = PTR_ERR(sma);\n\t\tgoto out_free;\n\t}\n\n\terror = -EFBIG;\n\tif (max >= sma->sem_nsems) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terror = -EACCES;\n\tif (ipcperms(ns, &sma->sem_perm, alter ? S_IWUGO : S_IRUGO)) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terror = security_sem_semop(sma, sops, nsops, alter);\n\tif (error) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\t/*\n\t * semid identifiers are not unique - find_alloc_undo may have\n\t * allocated an undo structure, it was invalidated by an RMID\n\t * and now a new array with received the same id. Check and fail.\n\t * This case can be detected checking un->semid. The existence of\n\t * \"un\" itself is guaranteed by rcu.\n\t */\n\terror = -EIDRM;\n\tlocknum = sem_lock(sma, sops, nsops);\n\tif (un && un->semid == -1)\n\t\tgoto out_unlock_free;\n\n\terror = try_atomic_semop (sma, sops, nsops, un, task_tgid_vnr(current));\n\tif (error <= 0) {\n\t\tif (alter && error == 0)\n\t\t\tdo_smart_update(sma, sops, nsops, 1, &tasks);\n\n\t\tgoto out_unlock_free;\n\t}\n\n\t/* We need to sleep on this operation, so we put the current\n\t * task into the pending queue and go to sleep.\n\t */\n\t\t\n\tqueue.sops = sops;\n\tqueue.nsops = nsops;\n\tqueue.undo = un;\n\tqueue.pid = task_tgid_vnr(current);\n\tqueue.alter = alter;\n\n\tif (nsops == 1) {\n\t\tstruct sem *curr;\n\t\tcurr = &sma->sem_base[sops->sem_num];\n\n\t\tif (alter)\n\t\t\tlist_add_tail(&queue.list, &curr->sem_pending);\n\t\telse\n\t\t\tlist_add(&queue.list, &curr->sem_pending);\n\t} else {\n\t\tif (alter)\n\t\t\tlist_add_tail(&queue.list, &sma->sem_pending);\n\t\telse\n\t\t\tlist_add(&queue.list, &sma->sem_pending);\n\t\tsma->complex_count++;\n\t}\n\n\tqueue.status = -EINTR;\n\tqueue.sleeper = current;\n\nsleep_again:\n\tcurrent->state = TASK_INTERRUPTIBLE;\n\tsem_unlock(sma, locknum);\n\n\tif (timeout)\n\t\tjiffies_left = schedule_timeout(jiffies_left);\n\telse\n\t\tschedule();\n\n\terror = get_queue_result(&queue);\n\n\tif (error != -EINTR) {\n\t\t/* fast path: update_queue already obtained all requested\n\t\t * resources.\n\t\t * Perform a smp_mb(): User space could assume that semop()\n\t\t * is a memory barrier: Without the mb(), the cpu could\n\t\t * speculatively read in user space stale data that was\n\t\t * overwritten by the previous owner of the semaphore.\n\t\t */\n\t\tsmp_mb();\n\n\t\tgoto out_free;\n\t}\n\n\tsma = sem_obtain_lock(ns, semid, sops, nsops, &locknum);\n\n\t/*\n\t * Wait until it's guaranteed that no wakeup_sem_queue_do() is ongoing.\n\t */\n\terror = get_queue_result(&queue);\n\n\t/*\n\t * Array removed? If yes, leave without sem_unlock().\n\t */\n\tif (IS_ERR(sma)) {\n\t\tgoto out_free;\n\t}\n\n\n\t/*\n\t * If queue.status != -EINTR we are woken up by another process.\n\t * Leave without unlink_queue(), but with sem_unlock().\n\t */\n\n\tif (error != -EINTR) {\n\t\tgoto out_unlock_free;\n\t}\n\n\t/*\n\t * If an interrupt occurred we have to clean up the queue\n\t */\n\tif (timeout && jiffies_left == 0)\n\t\terror = -EAGAIN;\n\n\t/*\n\t * If the wakeup was spurious, just retry\n\t */\n\tif (error == -EINTR && !signal_pending(current))\n\t\tgoto sleep_again;\n\n\tunlink_queue(sma, &queue);\n\nout_unlock_free:\n\tsem_unlock(sma, locknum);\nout_wakeup:\n\twake_up_sem_queue_do(&tasks);\nout_free:\n\tif(sops != fast_sops)\n\t\tkfree(sops);\n\treturn error;\n}\n\nSYSCALL_DEFINE3(semop, int, semid, struct sembuf __user *, tsops,\n\t\tunsigned, nsops)\n{\n\treturn sys_semtimedop(semid, tsops, nsops, NULL);\n}\n\n/* If CLONE_SYSVSEM is set, establish sharing of SEM_UNDO state between\n * parent and child tasks.\n */\n\nint copy_semundo(unsigned long clone_flags, struct task_struct *tsk)\n{\n\tstruct sem_undo_list *undo_list;\n\tint error;\n\n\tif (clone_flags & CLONE_SYSVSEM) {\n\t\terror = get_undo_list(&undo_list);\n\t\tif (error)\n\t\t\treturn error;\n\t\tatomic_inc(&undo_list->refcnt);\n\t\ttsk->sysvsem.undo_list = undo_list;\n\t} else \n\t\ttsk->sysvsem.undo_list = NULL;\n\n\treturn 0;\n}\n\n/*\n * add semadj values to semaphores, free undo structures.\n * undo structures are not freed when semaphore arrays are destroyed\n * so some of them may be out of date.\n * IMPLEMENTATION NOTE: There is some confusion over whether the\n * set of adjustments that needs to be done should be done in an atomic\n * manner or not. That is, if we are attempting to decrement the semval\n * should we queue up and wait until we can do so legally?\n * The original implementation attempted to do this (queue and wait).\n * The current implementation does not do so. The POSIX standard\n * and SVID should be consulted to determine what behavior is mandated.\n */\nvoid exit_sem(struct task_struct *tsk)\n{\n\tstruct sem_undo_list *ulp;\n\n\tulp = tsk->sysvsem.undo_list;\n\tif (!ulp)\n\t\treturn;\n\ttsk->sysvsem.undo_list = NULL;\n\n\tif (!atomic_dec_and_test(&ulp->refcnt))\n\t\treturn;\n\n\tfor (;;) {\n\t\tstruct sem_array *sma;\n\t\tstruct sem_undo *un;\n\t\tstruct list_head tasks;\n\t\tint semid, i;\n\n\t\trcu_read_lock();\n\t\tun = list_entry_rcu(ulp->list_proc.next,\n\t\t\t\t    struct sem_undo, list_proc);\n\t\tif (&un->list_proc == &ulp->list_proc)\n\t\t\tsemid = -1;\n\t\t else\n\t\t\tsemid = un->semid;\n\n\t\tif (semid == -1) {\n\t\t\trcu_read_unlock();\n\t\t\tbreak;\n\t\t}\n\n\t\tsma = sem_obtain_object_check(tsk->nsproxy->ipc_ns, un->semid);\n\t\t/* exit_sem raced with IPC_RMID, nothing to do */\n\t\tif (IS_ERR(sma)) {\n\t\t\trcu_read_unlock();\n\t\t\tcontinue;\n\t\t}\n\n\t\tsem_lock(sma, NULL, -1);\n\t\tun = __lookup_undo(ulp, semid);\n\t\tif (un == NULL) {\n\t\t\t/* exit_sem raced with IPC_RMID+semget() that created\n\t\t\t * exactly the same semid. Nothing to do.\n\t\t\t */\n\t\t\tsem_unlock(sma, -1);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* remove un from the linked lists */\n\t\tassert_spin_locked(&sma->sem_perm.lock);\n\t\tlist_del(&un->list_id);\n\n\t\tspin_lock(&ulp->lock);\n\t\tlist_del_rcu(&un->list_proc);\n\t\tspin_unlock(&ulp->lock);\n\n\t\t/* perform adjustments registered in un */\n\t\tfor (i = 0; i < sma->sem_nsems; i++) {\n\t\t\tstruct sem * semaphore = &sma->sem_base[i];\n\t\t\tif (un->semadj[i]) {\n\t\t\t\tsemaphore->semval += un->semadj[i];\n\t\t\t\t/*\n\t\t\t\t * Range checks of the new semaphore value,\n\t\t\t\t * not defined by sus:\n\t\t\t\t * - Some unices ignore the undo entirely\n\t\t\t\t *   (e.g. HP UX 11i 11.22, Tru64 V5.1)\n\t\t\t\t * - some cap the value (e.g. FreeBSD caps\n\t\t\t\t *   at 0, but doesn't enforce SEMVMX)\n\t\t\t\t *\n\t\t\t\t * Linux caps the semaphore value, both at 0\n\t\t\t\t * and at SEMVMX.\n\t\t\t\t *\n\t\t\t\t * \tManfred <manfred@colorfullife.com>\n\t\t\t\t */\n\t\t\t\tif (semaphore->semval < 0)\n\t\t\t\t\tsemaphore->semval = 0;\n\t\t\t\tif (semaphore->semval > SEMVMX)\n\t\t\t\t\tsemaphore->semval = SEMVMX;\n\t\t\t\tsemaphore->sempid = task_tgid_vnr(current);\n\t\t\t}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Fun-Call",
        "line_old": 1500,
        "critical_vars": [
            "&tasks"
        ],
        "function": "SYSCALL_DEFINE4",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "False",
        "function_code": "\n\nSYSCALL_DEFINE4(semtimedop, int, semid, struct sembuf __user *, tsops,\n\t\tunsigned, nsops, const struct timespec __user *, timeout)\n{\n\tint error = -EINVAL;\n\tstruct sem_array *sma;\n\tstruct sembuf fast_sops[SEMOPM_FAST];\n\tstruct sembuf* sops = fast_sops, *sop;\n\tstruct sem_undo *un;\n\tint undos = 0, alter = 0, max;\n\tstruct sem_queue queue;\n\tunsigned long jiffies_left = 0;\n\tstruct ipc_namespace *ns;\n\tstruct list_head tasks;\n\n\tns = current->nsproxy->ipc_ns;\n\n\tif (nsops < 1 || semid < 0)\n\t\treturn -EINVAL;\n\tif (nsops > ns->sc_semopm)\n\t\treturn -E2BIG;\n\tif(nsops > SEMOPM_FAST) {\n\t\tsops = kmalloc(sizeof(*sops)*nsops,GFP_KERNEL);\n\t\tif(sops==NULL)\n\t\t\treturn -ENOMEM;\n\t}\n\tif (copy_from_user (sops, tsops, nsops * sizeof(*tsops))) {\n\t\terror=-EFAULT;\n\t\tgoto out_free;\n\t}\n\tif (timeout) {\n\t\tstruct timespec _timeout;\n\t\tif (copy_from_user(&_timeout, timeout, sizeof(*timeout))) {\n\t\t\terror = -EFAULT;\n\t\t\tgoto out_free;\n\t\t}\n\t\tif (_timeout.tv_sec < 0 || _timeout.tv_nsec < 0 ||\n\t\t\t_timeout.tv_nsec >= 1000000000L) {\n\t\t\terror = -EINVAL;\n\t\t\tgoto out_free;\n\t\t}\n\t\tjiffies_left = timespec_to_jiffies(&_timeout);\n\t}\n\tmax = 0;\n\tfor (sop = sops; sop < sops + nsops; sop++) {\n\t\tif (sop->sem_num >= max)\n\t\t\tmax = sop->sem_num;\n\t\tif (sop->sem_flg & SEM_UNDO)\n\t\t\tundos = 1;\n\t\tif (sop->sem_op != 0)\n\t\t\talter = 1;\n\t}\n\n\tif (undos) {\n\t\tun = find_alloc_undo(ns, semid);\n\t\tif (IS_ERR(un)) {\n\t\t\terror = PTR_ERR(un);\n\t\t\tgoto out_free;\n\t\t}\n\t} else\n\t\tun = NULL;\n\n\tINIT_LIST_HEAD(&tasks);\n\n\trcu_read_lock();\n\tsma = sem_obtain_object_check(ns, semid);\n\tif (IS_ERR(sma)) {\n\t\tif (un)\n\t\t\trcu_read_unlock();\n\t\terror = PTR_ERR(sma);\n\t\tgoto out_free;\n\t}\n\n\terror = -EFBIG;\n\tif (max >= sma->sem_nsems) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terror = -EACCES;\n\tif (ipcperms(ns, &sma->sem_perm, alter ? S_IWUGO : S_IRUGO)) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terror = security_sem_semop(sma, sops, nsops, alter);\n\tif (error) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\t/*\n\t * semid identifiers are not unique - find_alloc_undo may have\n\t * allocated an undo structure, it was invalidated by an RMID\n\t * and now a new array with received the same id. Check and fail.\n\t * This case can be detected checking un->semid. The existence of\n\t * \"un\" itself is guaranteed by rcu.\n\t */\n\terror = -EIDRM;\n\tipc_lock_object(&sma->sem_perm);\n\tif (un) {\n\t\tif (un->semid == -1) {\n\t\t\trcu_read_unlock();\n\t\t\tgoto out_unlock_free;\n\t\t} else {\n\t\t\t/*\n\t\t\t * rcu lock can be released, \"un\" cannot disappear:\n\t\t\t * - sem_lock is acquired, thus IPC_RMID is\n\t\t\t *   impossible.\n\t\t\t * - exit_sem is impossible, it always operates on\n\t\t\t *   current (or a dead task).\n\t\t\t */\n\n\t\t\trcu_read_unlock();\n\t\t}\n\t}\n\n\terror = try_atomic_semop (sma, sops, nsops, un, task_tgid_vnr(current));\n\tif (error <= 0) {\n\t\tif (alter && error == 0)\n\t\t\tdo_smart_update(sma, sops, nsops, 1, &tasks);\n\n\t\tgoto out_unlock_free;\n\t}\n\n\t/* We need to sleep on this operation, so we put the current\n\t * task into the pending queue and go to sleep.\n\t */\n\t\t\n\tqueue.sops = sops;\n\tqueue.nsops = nsops;\n\tqueue.undo = un;\n\tqueue.pid = task_tgid_vnr(current);\n\tqueue.alter = alter;\n\n\tif (nsops == 1) {\n\t\tstruct sem *curr;\n\t\tcurr = &sma->sem_base[sops->sem_num];\n\n\t\tif (alter)\n\t\t\tlist_add_tail(&queue.list, &curr->sem_pending);\n\t\telse\n\t\t\tlist_add(&queue.list, &curr->sem_pending);\n\t} else {\n\t\tif (alter)\n\t\t\tlist_add_tail(&queue.list, &sma->sem_pending);\n\t\telse\n\t\t\tlist_add(&queue.list, &sma->sem_pending);\n\t\tsma->complex_count++;\n\t}\n\n\tqueue.status = -EINTR;\n\tqueue.sleeper = current;\n\nsleep_again:\n\tcurrent->state = TASK_INTERRUPTIBLE;\n\tsem_unlock(sma);\n\n\tif (timeout)\n\t\tjiffies_left = schedule_timeout(jiffies_left);\n\telse\n\t\tschedule();\n\n\terror = get_queue_result(&queue);\n\n\tif (error != -EINTR) {\n\t\t/* fast path: update_queue already obtained all requested\n\t\t * resources.\n\t\t * Perform a smp_mb(): User space could assume that semop()\n\t\t * is a memory barrier: Without the mb(), the cpu could\n\t\t * speculatively read in user space stale data that was\n\t\t * overwritten by the previous owner of the semaphore.\n\t\t */\n\t\tsmp_mb();\n\n\t\tgoto out_free;\n\t}\n\n\tsma = sem_obtain_lock(ns, semid);\n\n\t/*\n\t * Wait until it's guaranteed that no wakeup_sem_queue_do() is ongoing.\n\t */\n\terror = get_queue_result(&queue);\n\n\t/*\n\t * Array removed? If yes, leave without sem_unlock().\n\t */\n\tif (IS_ERR(sma)) {\n\t\tgoto out_free;\n\t}\n\n\n\t/*\n\t * If queue.status != -EINTR we are woken up by another process.\n\t * Leave without unlink_queue(), but with sem_unlock().\n\t */\n\n\tif (error != -EINTR) {\n\t\tgoto out_unlock_free;\n\t}\n\n\t/*\n\t * If an interrupt occurred we have to clean up the queue\n\t */\n\tif (timeout && jiffies_left == 0)\n\t\terror = -EAGAIN;\n\n\t/*\n\t * If the wakeup was spurious, just retry\n\t */\n\tif (error == -EINTR && !signal_pending(current))\n\t\tgoto sleep_again;\n\n\tunlink_queue(sma, &queue);\n\nout_unlock_free:\n\tsem_unlock(sma);\nout_wakeup:\n\twake_up_sem_queue_do(&tasks);\nout_free:\n\tif(sops != fast_sops)\n\t\tkfree(sops);\n\treturn error;\n}\n\nSYSCALL_DEFINE3(semop, int, semid, struct sembuf __user *, tsops,\n\t\tunsigned, nsops)\n{\n\treturn sys_semtimedop(semid, tsops, nsops, NULL);\n}\n\n/* If CLONE_SYSVSEM is set, establish sharing of SEM_UNDO state between\n * parent and child tasks.\n */\n\nint copy_semundo(unsigned long clone_flags, struct task_struct *tsk)\n{\n\tstruct sem_undo_list *undo_list;\n\tint error;\n\n\tif (clone_flags & CLONE_SYSVSEM) {\n\t\terror = get_undo_list(&undo_list);\n\t\tif (error)\n\t\t\treturn error;\n\t\tatomic_inc(&undo_list->refcnt);\n\t\ttsk->sysvsem.undo_list = undo_list;\n\t} else \n\t\ttsk->sysvsem.undo_list = NULL;\n\n\treturn 0;\n}\n\n/*\n * add semadj values to semaphores, free undo structures.\n * undo structures are not freed when semaphore arrays are destroyed\n * so some of them may be out of date.\n * IMPLEMENTATION NOTE: There is some confusion over whether the\n * set of adjustments that needs to be done should be done in an atomic\n * manner or not. That is, if we are attempting to decrement the semval\n * should we queue up and wait until we can do so legally?\n * The original implementation attempted to do this (queue and wait).\n * The current implementation does not do so. The POSIX standard\n * and SVID should be consulted to determine what behavior is mandated.\n */\nvoid exit_sem(struct task_struct *tsk)\n{\n\tstruct sem_undo_list *ulp;\n\n\tulp = tsk->sysvsem.undo_list;\n\tif (!ulp)\n\t\treturn;\n\ttsk->sysvsem.undo_list = NULL;\n\n\tif (!atomic_dec_and_test(&ulp->refcnt))\n\t\treturn;\n\n\tfor (;;) {\n\t\tstruct sem_array *sma;\n\t\tstruct sem_undo *un;\n\t\tstruct list_head tasks;\n\t\tint semid;\n\t\tint i;\n\n\t\trcu_read_lock();\n\t\tun = list_entry_rcu(ulp->list_proc.next,\n\t\t\t\t    struct sem_undo, list_proc);\n\t\tif (&un->list_proc == &ulp->list_proc)\n\t\t\tsemid = -1;\n\t\t else\n\t\t\tsemid = un->semid;\n\t\trcu_read_unlock();\n\n\t\tif (semid == -1)\n\t\t\tbreak;\n\n\t\tsma = sem_lock_check(tsk->nsproxy->ipc_ns, un->semid);\n\n\t\t/* exit_sem raced with IPC_RMID, nothing to do */\n\t\tif (IS_ERR(sma))\n\t\t\tcontinue;\n\n\t\tun = __lookup_undo(ulp, semid);\n\t\tif (un == NULL) {\n\t\t\t/* exit_sem raced with IPC_RMID+semget() that created\n\t\t\t * exactly the same semid. Nothing to do.\n\t\t\t */\n\t\t\tsem_unlock(sma);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* remove un from the linked lists */\n\t\tassert_spin_locked(&sma->sem_perm.lock);\n\t\tlist_del(&un->list_id);\n\n\t\tspin_lock(&ulp->lock);\n\t\tlist_del_rcu(&un->list_proc);\n\t\tspin_unlock(&ulp->lock);\n\n\t\t/* perform adjustments registered in un */\n\t\tfor (i = 0; i < sma->sem_nsems; i++) {\n\t\t\tstruct sem * semaphore = &sma->sem_base[i];\n\t\t\tif (un->semadj[i]) {\n\t\t\t\tsemaphore->semval += un->semadj[i];\n\t\t\t\t/*\n\t\t\t\t * Range checks of the new semaphore value,\n\t\t\t\t * not defined by sus:\n\t\t\t\t * - Some unices ignore the undo entirely\n\t\t\t\t *   (e.g. HP UX 11i 11.22, Tru64 V5.1)\n\t\t\t\t * - some cap the value (e.g. FreeBSD caps\n\t\t\t\t *   at 0, but doesn't enforce SEMVMX)\n\t\t\t\t *\n\t\t\t\t * Linux caps the semaphore value, both at 0\n\t\t\t\t * and at SEMVMX.\n\t\t\t\t *\n\t\t\t\t * \tManfred <manfred@colorfullife.com>\n\t\t\t\t */\n\t\t\t\tif (semaphore->semval < 0)\n\t\t\t\t\tsemaphore->semval = 0;\n\t\t\t\tif (semaphore->semval > SEMVMX)\n\t\t\t\t\tsemaphore->semval = SEMVMX;\n\t\t\t\tsemaphore->sempid = task_tgid_vnr(current);\n\t\t\t}"
    },
    {
        "patch_model": "Delete",
        "change_type": "if-Condition",
        "line_old": 1505,
        "critical_vars": [
            "un"
        ],
        "function": "SYSCALL_DEFINE4",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "False",
        "function_code": "\n\nSYSCALL_DEFINE4(semtimedop, int, semid, struct sembuf __user *, tsops,\n\t\tunsigned, nsops, const struct timespec __user *, timeout)\n{\n\tint error = -EINVAL;\n\tstruct sem_array *sma;\n\tstruct sembuf fast_sops[SEMOPM_FAST];\n\tstruct sembuf* sops = fast_sops, *sop;\n\tstruct sem_undo *un;\n\tint undos = 0, alter = 0, max;\n\tstruct sem_queue queue;\n\tunsigned long jiffies_left = 0;\n\tstruct ipc_namespace *ns;\n\tstruct list_head tasks;\n\n\tns = current->nsproxy->ipc_ns;\n\n\tif (nsops < 1 || semid < 0)\n\t\treturn -EINVAL;\n\tif (nsops > ns->sc_semopm)\n\t\treturn -E2BIG;\n\tif(nsops > SEMOPM_FAST) {\n\t\tsops = kmalloc(sizeof(*sops)*nsops,GFP_KERNEL);\n\t\tif(sops==NULL)\n\t\t\treturn -ENOMEM;\n\t}\n\tif (copy_from_user (sops, tsops, nsops * sizeof(*tsops))) {\n\t\terror=-EFAULT;\n\t\tgoto out_free;\n\t}\n\tif (timeout) {\n\t\tstruct timespec _timeout;\n\t\tif (copy_from_user(&_timeout, timeout, sizeof(*timeout))) {\n\t\t\terror = -EFAULT;\n\t\t\tgoto out_free;\n\t\t}\n\t\tif (_timeout.tv_sec < 0 || _timeout.tv_nsec < 0 ||\n\t\t\t_timeout.tv_nsec >= 1000000000L) {\n\t\t\terror = -EINVAL;\n\t\t\tgoto out_free;\n\t\t}\n\t\tjiffies_left = timespec_to_jiffies(&_timeout);\n\t}\n\tmax = 0;\n\tfor (sop = sops; sop < sops + nsops; sop++) {\n\t\tif (sop->sem_num >= max)\n\t\t\tmax = sop->sem_num;\n\t\tif (sop->sem_flg & SEM_UNDO)\n\t\t\tundos = 1;\n\t\tif (sop->sem_op != 0)\n\t\t\talter = 1;\n\t}\n\n\tif (undos) {\n\t\tun = find_alloc_undo(ns, semid);\n\t\tif (IS_ERR(un)) {\n\t\t\terror = PTR_ERR(un);\n\t\t\tgoto out_free;\n\t\t}\n\t} else\n\t\tun = NULL;\n\n\tINIT_LIST_HEAD(&tasks);\n\n\trcu_read_lock();\n\tsma = sem_obtain_object_check(ns, semid);\n\tif (IS_ERR(sma)) {\n\t\tif (un)\n\t\t\trcu_read_unlock();\n\t\terror = PTR_ERR(sma);\n\t\tgoto out_free;\n\t}\n\n\terror = -EFBIG;\n\tif (max >= sma->sem_nsems) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terror = -EACCES;\n\tif (ipcperms(ns, &sma->sem_perm, alter ? S_IWUGO : S_IRUGO)) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terror = security_sem_semop(sma, sops, nsops, alter);\n\tif (error) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\t/*\n\t * semid identifiers are not unique - find_alloc_undo may have\n\t * allocated an undo structure, it was invalidated by an RMID\n\t * and now a new array with received the same id. Check and fail.\n\t * This case can be detected checking un->semid. The existence of\n\t * \"un\" itself is guaranteed by rcu.\n\t */\n\terror = -EIDRM;\n\tipc_lock_object(&sma->sem_perm);\n\tif (un) {\n\t\tif (un->semid == -1) {\n\t\t\trcu_read_unlock();\n\t\t\tgoto out_unlock_free;\n\t\t} else {\n\t\t\t/*\n\t\t\t * rcu lock can be released, \"un\" cannot disappear:\n\t\t\t * - sem_lock is acquired, thus IPC_RMID is\n\t\t\t *   impossible.\n\t\t\t * - exit_sem is impossible, it always operates on\n\t\t\t *   current (or a dead task).\n\t\t\t */\n\n\t\t\trcu_read_unlock();\n\t\t}\n\t}\n\n\terror = try_atomic_semop (sma, sops, nsops, un, task_tgid_vnr(current));\n\tif (error <= 0) {\n\t\tif (alter && error == 0)\n\t\t\tdo_smart_update(sma, sops, nsops, 1, &tasks);\n\n\t\tgoto out_unlock_free;\n\t}\n\n\t/* We need to sleep on this operation, so we put the current\n\t * task into the pending queue and go to sleep.\n\t */\n\t\t\n\tqueue.sops = sops;\n\tqueue.nsops = nsops;\n\tqueue.undo = un;\n\tqueue.pid = task_tgid_vnr(current);\n\tqueue.alter = alter;\n\n\tif (nsops == 1) {\n\t\tstruct sem *curr;\n\t\tcurr = &sma->sem_base[sops->sem_num];\n\n\t\tif (alter)\n\t\t\tlist_add_tail(&queue.list, &curr->sem_pending);\n\t\telse\n\t\t\tlist_add(&queue.list, &curr->sem_pending);\n\t} else {\n\t\tif (alter)\n\t\t\tlist_add_tail(&queue.list, &sma->sem_pending);\n\t\telse\n\t\t\tlist_add(&queue.list, &sma->sem_pending);\n\t\tsma->complex_count++;\n\t}\n\n\tqueue.status = -EINTR;\n\tqueue.sleeper = current;\n\nsleep_again:\n\tcurrent->state = TASK_INTERRUPTIBLE;\n\tsem_unlock(sma);\n\n\tif (timeout)\n\t\tjiffies_left = schedule_timeout(jiffies_left);\n\telse\n\t\tschedule();\n\n\terror = get_queue_result(&queue);\n\n\tif (error != -EINTR) {\n\t\t/* fast path: update_queue already obtained all requested\n\t\t * resources.\n\t\t * Perform a smp_mb(): User space could assume that semop()\n\t\t * is a memory barrier: Without the mb(), the cpu could\n\t\t * speculatively read in user space stale data that was\n\t\t * overwritten by the previous owner of the semaphore.\n\t\t */\n\t\tsmp_mb();\n\n\t\tgoto out_free;\n\t}\n\n\tsma = sem_obtain_lock(ns, semid);\n\n\t/*\n\t * Wait until it's guaranteed that no wakeup_sem_queue_do() is ongoing.\n\t */\n\terror = get_queue_result(&queue);\n\n\t/*\n\t * Array removed? If yes, leave without sem_unlock().\n\t */\n\tif (IS_ERR(sma)) {\n\t\tgoto out_free;\n\t}\n\n\n\t/*\n\t * If queue.status != -EINTR we are woken up by another process.\n\t * Leave without unlink_queue(), but with sem_unlock().\n\t */\n\n\tif (error != -EINTR) {\n\t\tgoto out_unlock_free;\n\t}\n\n\t/*\n\t * If an interrupt occurred we have to clean up the queue\n\t */\n\tif (timeout && jiffies_left == 0)\n\t\terror = -EAGAIN;\n\n\t/*\n\t * If the wakeup was spurious, just retry\n\t */\n\tif (error == -EINTR && !signal_pending(current))\n\t\tgoto sleep_again;\n\n\tunlink_queue(sma, &queue);\n\nout_unlock_free:\n\tsem_unlock(sma);\nout_wakeup:\n\twake_up_sem_queue_do(&tasks);\nout_free:\n\tif(sops != fast_sops)\n\t\tkfree(sops);\n\treturn error;\n}\n\nSYSCALL_DEFINE3(semop, int, semid, struct sembuf __user *, tsops,\n\t\tunsigned, nsops)\n{\n\treturn sys_semtimedop(semid, tsops, nsops, NULL);\n}\n\n/* If CLONE_SYSVSEM is set, establish sharing of SEM_UNDO state between\n * parent and child tasks.\n */\n\nint copy_semundo(unsigned long clone_flags, struct task_struct *tsk)\n{\n\tstruct sem_undo_list *undo_list;\n\tint error;\n\n\tif (clone_flags & CLONE_SYSVSEM) {\n\t\terror = get_undo_list(&undo_list);\n\t\tif (error)\n\t\t\treturn error;\n\t\tatomic_inc(&undo_list->refcnt);\n\t\ttsk->sysvsem.undo_list = undo_list;\n\t} else \n\t\ttsk->sysvsem.undo_list = NULL;\n\n\treturn 0;\n}\n\n/*\n * add semadj values to semaphores, free undo structures.\n * undo structures are not freed when semaphore arrays are destroyed\n * so some of them may be out of date.\n * IMPLEMENTATION NOTE: There is some confusion over whether the\n * set of adjustments that needs to be done should be done in an atomic\n * manner or not. That is, if we are attempting to decrement the semval\n * should we queue up and wait until we can do so legally?\n * The original implementation attempted to do this (queue and wait).\n * The current implementation does not do so. The POSIX standard\n * and SVID should be consulted to determine what behavior is mandated.\n */\nvoid exit_sem(struct task_struct *tsk)\n{\n\tstruct sem_undo_list *ulp;\n\n\tulp = tsk->sysvsem.undo_list;\n\tif (!ulp)\n\t\treturn;\n\ttsk->sysvsem.undo_list = NULL;\n\n\tif (!atomic_dec_and_test(&ulp->refcnt))\n\t\treturn;\n\n\tfor (;;) {\n\t\tstruct sem_array *sma;\n\t\tstruct sem_undo *un;\n\t\tstruct list_head tasks;\n\t\tint semid;\n\t\tint i;\n\n\t\trcu_read_lock();\n\t\tun = list_entry_rcu(ulp->list_proc.next,\n\t\t\t\t    struct sem_undo, list_proc);\n\t\tif (&un->list_proc == &ulp->list_proc)\n\t\t\tsemid = -1;\n\t\t else\n\t\t\tsemid = un->semid;\n\t\trcu_read_unlock();\n\n\t\tif (semid == -1)\n\t\t\tbreak;\n\n\t\tsma = sem_lock_check(tsk->nsproxy->ipc_ns, un->semid);\n\n\t\t/* exit_sem raced with IPC_RMID, nothing to do */\n\t\tif (IS_ERR(sma))\n\t\t\tcontinue;\n\n\t\tun = __lookup_undo(ulp, semid);\n\t\tif (un == NULL) {\n\t\t\t/* exit_sem raced with IPC_RMID+semget() that created\n\t\t\t * exactly the same semid. Nothing to do.\n\t\t\t */\n\t\t\tsem_unlock(sma);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* remove un from the linked lists */\n\t\tassert_spin_locked(&sma->sem_perm.lock);\n\t\tlist_del(&un->list_id);\n\n\t\tspin_lock(&ulp->lock);\n\t\tlist_del_rcu(&un->list_proc);\n\t\tspin_unlock(&ulp->lock);\n\n\t\t/* perform adjustments registered in un */\n\t\tfor (i = 0; i < sma->sem_nsems; i++) {\n\t\t\tstruct sem * semaphore = &sma->sem_base[i];\n\t\t\tif (un->semadj[i]) {\n\t\t\t\tsemaphore->semval += un->semadj[i];\n\t\t\t\t/*\n\t\t\t\t * Range checks of the new semaphore value,\n\t\t\t\t * not defined by sus:\n\t\t\t\t * - Some unices ignore the undo entirely\n\t\t\t\t *   (e.g. HP UX 11i 11.22, Tru64 V5.1)\n\t\t\t\t * - some cap the value (e.g. FreeBSD caps\n\t\t\t\t *   at 0, but doesn't enforce SEMVMX)\n\t\t\t\t *\n\t\t\t\t * Linux caps the semaphore value, both at 0\n\t\t\t\t * and at SEMVMX.\n\t\t\t\t *\n\t\t\t\t * \tManfred <manfred@colorfullife.com>\n\t\t\t\t */\n\t\t\t\tif (semaphore->semval < 0)\n\t\t\t\t\tsemaphore->semval = 0;\n\t\t\t\tif (semaphore->semval > SEMVMX)\n\t\t\t\t\tsemaphore->semval = SEMVMX;\n\t\t\t\tsemaphore->sempid = task_tgid_vnr(current);\n\t\t\t}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Fun-Call",
        "line_old": 1537,
        "critical_vars": [
            "&sma->sem_perm"
        ],
        "function": "SYSCALL_DEFINE4",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "False",
        "function_code": "\n\nSYSCALL_DEFINE4(semtimedop, int, semid, struct sembuf __user *, tsops,\n\t\tunsigned, nsops, const struct timespec __user *, timeout)\n{\n\tint error = -EINVAL;\n\tstruct sem_array *sma;\n\tstruct sembuf fast_sops[SEMOPM_FAST];\n\tstruct sembuf* sops = fast_sops, *sop;\n\tstruct sem_undo *un;\n\tint undos = 0, alter = 0, max;\n\tstruct sem_queue queue;\n\tunsigned long jiffies_left = 0;\n\tstruct ipc_namespace *ns;\n\tstruct list_head tasks;\n\n\tns = current->nsproxy->ipc_ns;\n\n\tif (nsops < 1 || semid < 0)\n\t\treturn -EINVAL;\n\tif (nsops > ns->sc_semopm)\n\t\treturn -E2BIG;\n\tif(nsops > SEMOPM_FAST) {\n\t\tsops = kmalloc(sizeof(*sops)*nsops,GFP_KERNEL);\n\t\tif(sops==NULL)\n\t\t\treturn -ENOMEM;\n\t}\n\tif (copy_from_user (sops, tsops, nsops * sizeof(*tsops))) {\n\t\terror=-EFAULT;\n\t\tgoto out_free;\n\t}\n\tif (timeout) {\n\t\tstruct timespec _timeout;\n\t\tif (copy_from_user(&_timeout, timeout, sizeof(*timeout))) {\n\t\t\terror = -EFAULT;\n\t\t\tgoto out_free;\n\t\t}\n\t\tif (_timeout.tv_sec < 0 || _timeout.tv_nsec < 0 ||\n\t\t\t_timeout.tv_nsec >= 1000000000L) {\n\t\t\terror = -EINVAL;\n\t\t\tgoto out_free;\n\t\t}\n\t\tjiffies_left = timespec_to_jiffies(&_timeout);\n\t}\n\tmax = 0;\n\tfor (sop = sops; sop < sops + nsops; sop++) {\n\t\tif (sop->sem_num >= max)\n\t\t\tmax = sop->sem_num;\n\t\tif (sop->sem_flg & SEM_UNDO)\n\t\t\tundos = 1;\n\t\tif (sop->sem_op != 0)\n\t\t\talter = 1;\n\t}\n\n\tif (undos) {\n\t\tun = find_alloc_undo(ns, semid);\n\t\tif (IS_ERR(un)) {\n\t\t\terror = PTR_ERR(un);\n\t\t\tgoto out_free;\n\t\t}\n\t} else\n\t\tun = NULL;\n\n\tINIT_LIST_HEAD(&tasks);\n\n\trcu_read_lock();\n\tsma = sem_obtain_object_check(ns, semid);\n\tif (IS_ERR(sma)) {\n\t\tif (un)\n\t\t\trcu_read_unlock();\n\t\terror = PTR_ERR(sma);\n\t\tgoto out_free;\n\t}\n\n\terror = -EFBIG;\n\tif (max >= sma->sem_nsems) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terror = -EACCES;\n\tif (ipcperms(ns, &sma->sem_perm, alter ? S_IWUGO : S_IRUGO)) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terror = security_sem_semop(sma, sops, nsops, alter);\n\tif (error) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\t/*\n\t * semid identifiers are not unique - find_alloc_undo may have\n\t * allocated an undo structure, it was invalidated by an RMID\n\t * and now a new array with received the same id. Check and fail.\n\t * This case can be detected checking un->semid. The existence of\n\t * \"un\" itself is guaranteed by rcu.\n\t */\n\terror = -EIDRM;\n\tipc_lock_object(&sma->sem_perm);\n\tif (un) {\n\t\tif (un->semid == -1) {\n\t\t\trcu_read_unlock();\n\t\t\tgoto out_unlock_free;\n\t\t} else {\n\t\t\t/*\n\t\t\t * rcu lock can be released, \"un\" cannot disappear:\n\t\t\t * - sem_lock is acquired, thus IPC_RMID is\n\t\t\t *   impossible.\n\t\t\t * - exit_sem is impossible, it always operates on\n\t\t\t *   current (or a dead task).\n\t\t\t */\n\n\t\t\trcu_read_unlock();\n\t\t}\n\t}\n\n\terror = try_atomic_semop (sma, sops, nsops, un, task_tgid_vnr(current));\n\tif (error <= 0) {\n\t\tif (alter && error == 0)\n\t\t\tdo_smart_update(sma, sops, nsops, 1, &tasks);\n\n\t\tgoto out_unlock_free;\n\t}\n\n\t/* We need to sleep on this operation, so we put the current\n\t * task into the pending queue and go to sleep.\n\t */\n\t\t\n\tqueue.sops = sops;\n\tqueue.nsops = nsops;\n\tqueue.undo = un;\n\tqueue.pid = task_tgid_vnr(current);\n\tqueue.alter = alter;\n\n\tif (nsops == 1) {\n\t\tstruct sem *curr;\n\t\tcurr = &sma->sem_base[sops->sem_num];\n\n\t\tif (alter)\n\t\t\tlist_add_tail(&queue.list, &curr->sem_pending);\n\t\telse\n\t\t\tlist_add(&queue.list, &curr->sem_pending);\n\t} else {\n\t\tif (alter)\n\t\t\tlist_add_tail(&queue.list, &sma->sem_pending);\n\t\telse\n\t\t\tlist_add(&queue.list, &sma->sem_pending);\n\t\tsma->complex_count++;\n\t}\n\n\tqueue.status = -EINTR;\n\tqueue.sleeper = current;\n\nsleep_again:\n\tcurrent->state = TASK_INTERRUPTIBLE;\n\tsem_unlock(sma);\n\n\tif (timeout)\n\t\tjiffies_left = schedule_timeout(jiffies_left);\n\telse\n\t\tschedule();\n\n\terror = get_queue_result(&queue);\n\n\tif (error != -EINTR) {\n\t\t/* fast path: update_queue already obtained all requested\n\t\t * resources.\n\t\t * Perform a smp_mb(): User space could assume that semop()\n\t\t * is a memory barrier: Without the mb(), the cpu could\n\t\t * speculatively read in user space stale data that was\n\t\t * overwritten by the previous owner of the semaphore.\n\t\t */\n\t\tsmp_mb();\n\n\t\tgoto out_free;\n\t}\n\n\tsma = sem_obtain_lock(ns, semid);\n\n\t/*\n\t * Wait until it's guaranteed that no wakeup_sem_queue_do() is ongoing.\n\t */\n\terror = get_queue_result(&queue);\n\n\t/*\n\t * Array removed? If yes, leave without sem_unlock().\n\t */\n\tif (IS_ERR(sma)) {\n\t\tgoto out_free;\n\t}\n\n\n\t/*\n\t * If queue.status != -EINTR we are woken up by another process.\n\t * Leave without unlink_queue(), but with sem_unlock().\n\t */\n\n\tif (error != -EINTR) {\n\t\tgoto out_unlock_free;\n\t}\n\n\t/*\n\t * If an interrupt occurred we have to clean up the queue\n\t */\n\tif (timeout && jiffies_left == 0)\n\t\terror = -EAGAIN;\n\n\t/*\n\t * If the wakeup was spurious, just retry\n\t */\n\tif (error == -EINTR && !signal_pending(current))\n\t\tgoto sleep_again;\n\n\tunlink_queue(sma, &queue);\n\nout_unlock_free:\n\tsem_unlock(sma);\nout_wakeup:\n\twake_up_sem_queue_do(&tasks);\nout_free:\n\tif(sops != fast_sops)\n\t\tkfree(sops);\n\treturn error;\n}\n\nSYSCALL_DEFINE3(semop, int, semid, struct sembuf __user *, tsops,\n\t\tunsigned, nsops)\n{\n\treturn sys_semtimedop(semid, tsops, nsops, NULL);\n}\n\n/* If CLONE_SYSVSEM is set, establish sharing of SEM_UNDO state between\n * parent and child tasks.\n */\n\nint copy_semundo(unsigned long clone_flags, struct task_struct *tsk)\n{\n\tstruct sem_undo_list *undo_list;\n\tint error;\n\n\tif (clone_flags & CLONE_SYSVSEM) {\n\t\terror = get_undo_list(&undo_list);\n\t\tif (error)\n\t\t\treturn error;\n\t\tatomic_inc(&undo_list->refcnt);\n\t\ttsk->sysvsem.undo_list = undo_list;\n\t} else \n\t\ttsk->sysvsem.undo_list = NULL;\n\n\treturn 0;\n}\n\n/*\n * add semadj values to semaphores, free undo structures.\n * undo structures are not freed when semaphore arrays are destroyed\n * so some of them may be out of date.\n * IMPLEMENTATION NOTE: There is some confusion over whether the\n * set of adjustments that needs to be done should be done in an atomic\n * manner or not. That is, if we are attempting to decrement the semval\n * should we queue up and wait until we can do so legally?\n * The original implementation attempted to do this (queue and wait).\n * The current implementation does not do so. The POSIX standard\n * and SVID should be consulted to determine what behavior is mandated.\n */\nvoid exit_sem(struct task_struct *tsk)\n{\n\tstruct sem_undo_list *ulp;\n\n\tulp = tsk->sysvsem.undo_list;\n\tif (!ulp)\n\t\treturn;\n\ttsk->sysvsem.undo_list = NULL;\n\n\tif (!atomic_dec_and_test(&ulp->refcnt))\n\t\treturn;\n\n\tfor (;;) {\n\t\tstruct sem_array *sma;\n\t\tstruct sem_undo *un;\n\t\tstruct list_head tasks;\n\t\tint semid;\n\t\tint i;\n\n\t\trcu_read_lock();\n\t\tun = list_entry_rcu(ulp->list_proc.next,\n\t\t\t\t    struct sem_undo, list_proc);\n\t\tif (&un->list_proc == &ulp->list_proc)\n\t\t\tsemid = -1;\n\t\t else\n\t\t\tsemid = un->semid;\n\t\trcu_read_unlock();\n\n\t\tif (semid == -1)\n\t\t\tbreak;\n\n\t\tsma = sem_lock_check(tsk->nsproxy->ipc_ns, un->semid);\n\n\t\t/* exit_sem raced with IPC_RMID, nothing to do */\n\t\tif (IS_ERR(sma))\n\t\t\tcontinue;\n\n\t\tun = __lookup_undo(ulp, semid);\n\t\tif (un == NULL) {\n\t\t\t/* exit_sem raced with IPC_RMID+semget() that created\n\t\t\t * exactly the same semid. Nothing to do.\n\t\t\t */\n\t\t\tsem_unlock(sma);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* remove un from the linked lists */\n\t\tassert_spin_locked(&sma->sem_perm.lock);\n\t\tlist_del(&un->list_id);\n\n\t\tspin_lock(&ulp->lock);\n\t\tlist_del_rcu(&un->list_proc);\n\t\tspin_unlock(&ulp->lock);\n\n\t\t/* perform adjustments registered in un */\n\t\tfor (i = 0; i < sma->sem_nsems; i++) {\n\t\t\tstruct sem * semaphore = &sma->sem_base[i];\n\t\t\tif (un->semadj[i]) {\n\t\t\t\tsemaphore->semval += un->semadj[i];\n\t\t\t\t/*\n\t\t\t\t * Range checks of the new semaphore value,\n\t\t\t\t * not defined by sus:\n\t\t\t\t * - Some unices ignore the undo entirely\n\t\t\t\t *   (e.g. HP UX 11i 11.22, Tru64 V5.1)\n\t\t\t\t * - some cap the value (e.g. FreeBSD caps\n\t\t\t\t *   at 0, but doesn't enforce SEMVMX)\n\t\t\t\t *\n\t\t\t\t * Linux caps the semaphore value, both at 0\n\t\t\t\t * and at SEMVMX.\n\t\t\t\t *\n\t\t\t\t * \tManfred <manfred@colorfullife.com>\n\t\t\t\t */\n\t\t\t\tif (semaphore->semval < 0)\n\t\t\t\t\tsemaphore->semval = 0;\n\t\t\t\tif (semaphore->semval > SEMVMX)\n\t\t\t\t\tsemaphore->semval = SEMVMX;\n\t\t\t\tsemaphore->sempid = task_tgid_vnr(current);\n\t\t\t}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 1620,
        "critical_vars": [
            "locknum"
        ],
        "function": "SYSCALL_DEFINE4",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "True",
        "function_code": "\n\nSYSCALL_DEFINE4(semtimedop, int, semid, struct sembuf __user *, tsops,\n\t\tunsigned, nsops, const struct timespec __user *, timeout)\n{\n\tint error = -EINVAL;\n\tstruct sem_array *sma;\n\tstruct sembuf fast_sops[SEMOPM_FAST];\n\tstruct sembuf* sops = fast_sops, *sop;\n\tstruct sem_undo *un;\n\tint undos = 0, alter = 0, max, locknum;\n\tstruct sem_queue queue;\n\tunsigned long jiffies_left = 0;\n\tstruct ipc_namespace *ns;\n\tstruct list_head tasks;\n\n\tns = current->nsproxy->ipc_ns;\n\n\tif (nsops < 1 || semid < 0)\n\t\treturn -EINVAL;\n\tif (nsops > ns->sc_semopm)\n\t\treturn -E2BIG;\n\tif(nsops > SEMOPM_FAST) {\n\t\tsops = kmalloc(sizeof(*sops)*nsops,GFP_KERNEL);\n\t\tif(sops==NULL)\n\t\t\treturn -ENOMEM;\n\t}\n\tif (copy_from_user (sops, tsops, nsops * sizeof(*tsops))) {\n\t\terror=-EFAULT;\n\t\tgoto out_free;\n\t}\n\tif (timeout) {\n\t\tstruct timespec _timeout;\n\t\tif (copy_from_user(&_timeout, timeout, sizeof(*timeout))) {\n\t\t\terror = -EFAULT;\n\t\t\tgoto out_free;\n\t\t}\n\t\tif (_timeout.tv_sec < 0 || _timeout.tv_nsec < 0 ||\n\t\t\t_timeout.tv_nsec >= 1000000000L) {\n\t\t\terror = -EINVAL;\n\t\t\tgoto out_free;\n\t\t}\n\t\tjiffies_left = timespec_to_jiffies(&_timeout);\n\t}\n\tmax = 0;\n\tfor (sop = sops; sop < sops + nsops; sop++) {\n\t\tif (sop->sem_num >= max)\n\t\t\tmax = sop->sem_num;\n\t\tif (sop->sem_flg & SEM_UNDO)\n\t\t\tundos = 1;\n\t\tif (sop->sem_op != 0)\n\t\t\talter = 1;\n\t}\n\n\tINIT_LIST_HEAD(&tasks);\n\n\tif (undos) {\n\t\t/* On success, find_alloc_undo takes the rcu_read_lock */\n\t\tun = find_alloc_undo(ns, semid);\n\t\tif (IS_ERR(un)) {\n\t\t\terror = PTR_ERR(un);\n\t\t\tgoto out_free;\n\t\t}\n\t} else {\n\t\tun = NULL;\n\t\trcu_read_lock();\n\t}\n\n\tsma = sem_obtain_object_check(ns, semid);\n\tif (IS_ERR(sma)) {\n\t\trcu_read_unlock();\n\t\terror = PTR_ERR(sma);\n\t\tgoto out_free;\n\t}\n\n\terror = -EFBIG;\n\tif (max >= sma->sem_nsems) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terror = -EACCES;\n\tif (ipcperms(ns, &sma->sem_perm, alter ? S_IWUGO : S_IRUGO)) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terror = security_sem_semop(sma, sops, nsops, alter);\n\tif (error) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\t/*\n\t * semid identifiers are not unique - find_alloc_undo may have\n\t * allocated an undo structure, it was invalidated by an RMID\n\t * and now a new array with received the same id. Check and fail.\n\t * This case can be detected checking un->semid. The existence of\n\t * \"un\" itself is guaranteed by rcu.\n\t */\n\terror = -EIDRM;\n\tlocknum = sem_lock(sma, sops, nsops);\n\tif (un && un->semid == -1)\n\t\tgoto out_unlock_free;\n\n\terror = try_atomic_semop (sma, sops, nsops, un, task_tgid_vnr(current));\n\tif (error <= 0) {\n\t\tif (alter && error == 0)\n\t\t\tdo_smart_update(sma, sops, nsops, 1, &tasks);\n\n\t\tgoto out_unlock_free;\n\t}\n\n\t/* We need to sleep on this operation, so we put the current\n\t * task into the pending queue and go to sleep.\n\t */\n\t\t\n\tqueue.sops = sops;\n\tqueue.nsops = nsops;\n\tqueue.undo = un;\n\tqueue.pid = task_tgid_vnr(current);\n\tqueue.alter = alter;\n\n\tif (nsops == 1) {\n\t\tstruct sem *curr;\n\t\tcurr = &sma->sem_base[sops->sem_num];\n\n\t\tif (alter)\n\t\t\tlist_add_tail(&queue.list, &curr->sem_pending);\n\t\telse\n\t\t\tlist_add(&queue.list, &curr->sem_pending);\n\t} else {\n\t\tif (alter)\n\t\t\tlist_add_tail(&queue.list, &sma->sem_pending);\n\t\telse\n\t\t\tlist_add(&queue.list, &sma->sem_pending);\n\t\tsma->complex_count++;\n\t}\n\n\tqueue.status = -EINTR;\n\tqueue.sleeper = current;\n\nsleep_again:\n\tcurrent->state = TASK_INTERRUPTIBLE;\n\tsem_unlock(sma, locknum);\n\n\tif (timeout)\n\t\tjiffies_left = schedule_timeout(jiffies_left);\n\telse\n\t\tschedule();\n\n\terror = get_queue_result(&queue);\n\n\tif (error != -EINTR) {\n\t\t/* fast path: update_queue already obtained all requested\n\t\t * resources.\n\t\t * Perform a smp_mb(): User space could assume that semop()\n\t\t * is a memory barrier: Without the mb(), the cpu could\n\t\t * speculatively read in user space stale data that was\n\t\t * overwritten by the previous owner of the semaphore.\n\t\t */\n\t\tsmp_mb();\n\n\t\tgoto out_free;\n\t}\n\n\tsma = sem_obtain_lock(ns, semid, sops, nsops, &locknum);\n\n\t/*\n\t * Wait until it's guaranteed that no wakeup_sem_queue_do() is ongoing.\n\t */\n\terror = get_queue_result(&queue);\n\n\t/*\n\t * Array removed? If yes, leave without sem_unlock().\n\t */\n\tif (IS_ERR(sma)) {\n\t\tgoto out_free;\n\t}\n\n\n\t/*\n\t * If queue.status != -EINTR we are woken up by another process.\n\t * Leave without unlink_queue(), but with sem_unlock().\n\t */\n\n\tif (error != -EINTR) {\n\t\tgoto out_unlock_free;\n\t}\n\n\t/*\n\t * If an interrupt occurred we have to clean up the queue\n\t */\n\tif (timeout && jiffies_left == 0)\n\t\terror = -EAGAIN;\n\n\t/*\n\t * If the wakeup was spurious, just retry\n\t */\n\tif (error == -EINTR && !signal_pending(current))\n\t\tgoto sleep_again;\n\n\tunlink_queue(sma, &queue);\n\nout_unlock_free:\n\tsem_unlock(sma, locknum);\nout_wakeup:\n\twake_up_sem_queue_do(&tasks);\nout_free:\n\tif(sops != fast_sops)\n\t\tkfree(sops);\n\treturn error;\n}\n\nSYSCALL_DEFINE3(semop, int, semid, struct sembuf __user *, tsops,\n\t\tunsigned, nsops)\n{\n\treturn sys_semtimedop(semid, tsops, nsops, NULL);\n}\n\n/* If CLONE_SYSVSEM is set, establish sharing of SEM_UNDO state between\n * parent and child tasks.\n */\n\nint copy_semundo(unsigned long clone_flags, struct task_struct *tsk)\n{\n\tstruct sem_undo_list *undo_list;\n\tint error;\n\n\tif (clone_flags & CLONE_SYSVSEM) {\n\t\terror = get_undo_list(&undo_list);\n\t\tif (error)\n\t\t\treturn error;\n\t\tatomic_inc(&undo_list->refcnt);\n\t\ttsk->sysvsem.undo_list = undo_list;\n\t} else \n\t\ttsk->sysvsem.undo_list = NULL;\n\n\treturn 0;\n}\n\n/*\n * add semadj values to semaphores, free undo structures.\n * undo structures are not freed when semaphore arrays are destroyed\n * so some of them may be out of date.\n * IMPLEMENTATION NOTE: There is some confusion over whether the\n * set of adjustments that needs to be done should be done in an atomic\n * manner or not. That is, if we are attempting to decrement the semval\n * should we queue up and wait until we can do so legally?\n * The original implementation attempted to do this (queue and wait).\n * The current implementation does not do so. The POSIX standard\n * and SVID should be consulted to determine what behavior is mandated.\n */\nvoid exit_sem(struct task_struct *tsk)\n{\n\tstruct sem_undo_list *ulp;\n\n\tulp = tsk->sysvsem.undo_list;\n\tif (!ulp)\n\t\treturn;\n\ttsk->sysvsem.undo_list = NULL;\n\n\tif (!atomic_dec_and_test(&ulp->refcnt))\n\t\treturn;\n\n\tfor (;;) {\n\t\tstruct sem_array *sma;\n\t\tstruct sem_undo *un;\n\t\tstruct list_head tasks;\n\t\tint semid, i;\n\n\t\trcu_read_lock();\n\t\tun = list_entry_rcu(ulp->list_proc.next,\n\t\t\t\t    struct sem_undo, list_proc);\n\t\tif (&un->list_proc == &ulp->list_proc)\n\t\t\tsemid = -1;\n\t\t else\n\t\t\tsemid = un->semid;\n\n\t\tif (semid == -1) {\n\t\t\trcu_read_unlock();\n\t\t\tbreak;\n\t\t}\n\n\t\tsma = sem_obtain_object_check(tsk->nsproxy->ipc_ns, un->semid);\n\t\t/* exit_sem raced with IPC_RMID, nothing to do */\n\t\tif (IS_ERR(sma)) {\n\t\t\trcu_read_unlock();\n\t\t\tcontinue;\n\t\t}\n\n\t\tsem_lock(sma, NULL, -1);\n\t\tun = __lookup_undo(ulp, semid);\n\t\tif (un == NULL) {\n\t\t\t/* exit_sem raced with IPC_RMID+semget() that created\n\t\t\t * exactly the same semid. Nothing to do.\n\t\t\t */\n\t\t\tsem_unlock(sma, -1);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* remove un from the linked lists */\n\t\tassert_spin_locked(&sma->sem_perm.lock);\n\t\tlist_del(&un->list_id);\n\n\t\tspin_lock(&ulp->lock);\n\t\tlist_del_rcu(&un->list_proc);\n\t\tspin_unlock(&ulp->lock);\n\n\t\t/* perform adjustments registered in un */\n\t\tfor (i = 0; i < sma->sem_nsems; i++) {\n\t\t\tstruct sem * semaphore = &sma->sem_base[i];\n\t\t\tif (un->semadj[i]) {\n\t\t\t\tsemaphore->semval += un->semadj[i];\n\t\t\t\t/*\n\t\t\t\t * Range checks of the new semaphore value,\n\t\t\t\t * not defined by sus:\n\t\t\t\t * - Some unices ignore the undo entirely\n\t\t\t\t *   (e.g. HP UX 11i 11.22, Tru64 V5.1)\n\t\t\t\t * - some cap the value (e.g. FreeBSD caps\n\t\t\t\t *   at 0, but doesn't enforce SEMVMX)\n\t\t\t\t *\n\t\t\t\t * Linux caps the semaphore value, both at 0\n\t\t\t\t * and at SEMVMX.\n\t\t\t\t *\n\t\t\t\t * \tManfred <manfred@colorfullife.com>\n\t\t\t\t */\n\t\t\t\tif (semaphore->semval < 0)\n\t\t\t\t\tsemaphore->semval = 0;\n\t\t\t\tif (semaphore->semval > SEMVMX)\n\t\t\t\t\tsemaphore->semval = SEMVMX;\n\t\t\t\tsemaphore->sempid = task_tgid_vnr(current);\n\t\t\t}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 1621,
        "critical_vars": [
            "un->semid"
        ],
        "function": "SYSCALL_DEFINE4",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "True",
        "function_code": "\n\nSYSCALL_DEFINE4(semtimedop, int, semid, struct sembuf __user *, tsops,\n\t\tunsigned, nsops, const struct timespec __user *, timeout)\n{\n\tint error = -EINVAL;\n\tstruct sem_array *sma;\n\tstruct sembuf fast_sops[SEMOPM_FAST];\n\tstruct sembuf* sops = fast_sops, *sop;\n\tstruct sem_undo *un;\n\tint undos = 0, alter = 0, max, locknum;\n\tstruct sem_queue queue;\n\tunsigned long jiffies_left = 0;\n\tstruct ipc_namespace *ns;\n\tstruct list_head tasks;\n\n\tns = current->nsproxy->ipc_ns;\n\n\tif (nsops < 1 || semid < 0)\n\t\treturn -EINVAL;\n\tif (nsops > ns->sc_semopm)\n\t\treturn -E2BIG;\n\tif(nsops > SEMOPM_FAST) {\n\t\tsops = kmalloc(sizeof(*sops)*nsops,GFP_KERNEL);\n\t\tif(sops==NULL)\n\t\t\treturn -ENOMEM;\n\t}\n\tif (copy_from_user (sops, tsops, nsops * sizeof(*tsops))) {\n\t\terror=-EFAULT;\n\t\tgoto out_free;\n\t}\n\tif (timeout) {\n\t\tstruct timespec _timeout;\n\t\tif (copy_from_user(&_timeout, timeout, sizeof(*timeout))) {\n\t\t\terror = -EFAULT;\n\t\t\tgoto out_free;\n\t\t}\n\t\tif (_timeout.tv_sec < 0 || _timeout.tv_nsec < 0 ||\n\t\t\t_timeout.tv_nsec >= 1000000000L) {\n\t\t\terror = -EINVAL;\n\t\t\tgoto out_free;\n\t\t}\n\t\tjiffies_left = timespec_to_jiffies(&_timeout);\n\t}\n\tmax = 0;\n\tfor (sop = sops; sop < sops + nsops; sop++) {\n\t\tif (sop->sem_num >= max)\n\t\t\tmax = sop->sem_num;\n\t\tif (sop->sem_flg & SEM_UNDO)\n\t\t\tundos = 1;\n\t\tif (sop->sem_op != 0)\n\t\t\talter = 1;\n\t}\n\n\tINIT_LIST_HEAD(&tasks);\n\n\tif (undos) {\n\t\t/* On success, find_alloc_undo takes the rcu_read_lock */\n\t\tun = find_alloc_undo(ns, semid);\n\t\tif (IS_ERR(un)) {\n\t\t\terror = PTR_ERR(un);\n\t\t\tgoto out_free;\n\t\t}\n\t} else {\n\t\tun = NULL;\n\t\trcu_read_lock();\n\t}\n\n\tsma = sem_obtain_object_check(ns, semid);\n\tif (IS_ERR(sma)) {\n\t\trcu_read_unlock();\n\t\terror = PTR_ERR(sma);\n\t\tgoto out_free;\n\t}\n\n\terror = -EFBIG;\n\tif (max >= sma->sem_nsems) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terror = -EACCES;\n\tif (ipcperms(ns, &sma->sem_perm, alter ? S_IWUGO : S_IRUGO)) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terror = security_sem_semop(sma, sops, nsops, alter);\n\tif (error) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\t/*\n\t * semid identifiers are not unique - find_alloc_undo may have\n\t * allocated an undo structure, it was invalidated by an RMID\n\t * and now a new array with received the same id. Check and fail.\n\t * This case can be detected checking un->semid. The existence of\n\t * \"un\" itself is guaranteed by rcu.\n\t */\n\terror = -EIDRM;\n\tlocknum = sem_lock(sma, sops, nsops);\n\tif (un && un->semid == -1)\n\t\tgoto out_unlock_free;\n\n\terror = try_atomic_semop (sma, sops, nsops, un, task_tgid_vnr(current));\n\tif (error <= 0) {\n\t\tif (alter && error == 0)\n\t\t\tdo_smart_update(sma, sops, nsops, 1, &tasks);\n\n\t\tgoto out_unlock_free;\n\t}\n\n\t/* We need to sleep on this operation, so we put the current\n\t * task into the pending queue and go to sleep.\n\t */\n\t\t\n\tqueue.sops = sops;\n\tqueue.nsops = nsops;\n\tqueue.undo = un;\n\tqueue.pid = task_tgid_vnr(current);\n\tqueue.alter = alter;\n\n\tif (nsops == 1) {\n\t\tstruct sem *curr;\n\t\tcurr = &sma->sem_base[sops->sem_num];\n\n\t\tif (alter)\n\t\t\tlist_add_tail(&queue.list, &curr->sem_pending);\n\t\telse\n\t\t\tlist_add(&queue.list, &curr->sem_pending);\n\t} else {\n\t\tif (alter)\n\t\t\tlist_add_tail(&queue.list, &sma->sem_pending);\n\t\telse\n\t\t\tlist_add(&queue.list, &sma->sem_pending);\n\t\tsma->complex_count++;\n\t}\n\n\tqueue.status = -EINTR;\n\tqueue.sleeper = current;\n\nsleep_again:\n\tcurrent->state = TASK_INTERRUPTIBLE;\n\tsem_unlock(sma, locknum);\n\n\tif (timeout)\n\t\tjiffies_left = schedule_timeout(jiffies_left);\n\telse\n\t\tschedule();\n\n\terror = get_queue_result(&queue);\n\n\tif (error != -EINTR) {\n\t\t/* fast path: update_queue already obtained all requested\n\t\t * resources.\n\t\t * Perform a smp_mb(): User space could assume that semop()\n\t\t * is a memory barrier: Without the mb(), the cpu could\n\t\t * speculatively read in user space stale data that was\n\t\t * overwritten by the previous owner of the semaphore.\n\t\t */\n\t\tsmp_mb();\n\n\t\tgoto out_free;\n\t}\n\n\tsma = sem_obtain_lock(ns, semid, sops, nsops, &locknum);\n\n\t/*\n\t * Wait until it's guaranteed that no wakeup_sem_queue_do() is ongoing.\n\t */\n\terror = get_queue_result(&queue);\n\n\t/*\n\t * Array removed? If yes, leave without sem_unlock().\n\t */\n\tif (IS_ERR(sma)) {\n\t\tgoto out_free;\n\t}\n\n\n\t/*\n\t * If queue.status != -EINTR we are woken up by another process.\n\t * Leave without unlink_queue(), but with sem_unlock().\n\t */\n\n\tif (error != -EINTR) {\n\t\tgoto out_unlock_free;\n\t}\n\n\t/*\n\t * If an interrupt occurred we have to clean up the queue\n\t */\n\tif (timeout && jiffies_left == 0)\n\t\terror = -EAGAIN;\n\n\t/*\n\t * If the wakeup was spurious, just retry\n\t */\n\tif (error == -EINTR && !signal_pending(current))\n\t\tgoto sleep_again;\n\n\tunlink_queue(sma, &queue);\n\nout_unlock_free:\n\tsem_unlock(sma, locknum);\nout_wakeup:\n\twake_up_sem_queue_do(&tasks);\nout_free:\n\tif(sops != fast_sops)\n\t\tkfree(sops);\n\treturn error;\n}\n\nSYSCALL_DEFINE3(semop, int, semid, struct sembuf __user *, tsops,\n\t\tunsigned, nsops)\n{\n\treturn sys_semtimedop(semid, tsops, nsops, NULL);\n}\n\n/* If CLONE_SYSVSEM is set, establish sharing of SEM_UNDO state between\n * parent and child tasks.\n */\n\nint copy_semundo(unsigned long clone_flags, struct task_struct *tsk)\n{\n\tstruct sem_undo_list *undo_list;\n\tint error;\n\n\tif (clone_flags & CLONE_SYSVSEM) {\n\t\terror = get_undo_list(&undo_list);\n\t\tif (error)\n\t\t\treturn error;\n\t\tatomic_inc(&undo_list->refcnt);\n\t\ttsk->sysvsem.undo_list = undo_list;\n\t} else \n\t\ttsk->sysvsem.undo_list = NULL;\n\n\treturn 0;\n}\n\n/*\n * add semadj values to semaphores, free undo structures.\n * undo structures are not freed when semaphore arrays are destroyed\n * so some of them may be out of date.\n * IMPLEMENTATION NOTE: There is some confusion over whether the\n * set of adjustments that needs to be done should be done in an atomic\n * manner or not. That is, if we are attempting to decrement the semval\n * should we queue up and wait until we can do so legally?\n * The original implementation attempted to do this (queue and wait).\n * The current implementation does not do so. The POSIX standard\n * and SVID should be consulted to determine what behavior is mandated.\n */\nvoid exit_sem(struct task_struct *tsk)\n{\n\tstruct sem_undo_list *ulp;\n\n\tulp = tsk->sysvsem.undo_list;\n\tif (!ulp)\n\t\treturn;\n\ttsk->sysvsem.undo_list = NULL;\n\n\tif (!atomic_dec_and_test(&ulp->refcnt))\n\t\treturn;\n\n\tfor (;;) {\n\t\tstruct sem_array *sma;\n\t\tstruct sem_undo *un;\n\t\tstruct list_head tasks;\n\t\tint semid, i;\n\n\t\trcu_read_lock();\n\t\tun = list_entry_rcu(ulp->list_proc.next,\n\t\t\t\t    struct sem_undo, list_proc);\n\t\tif (&un->list_proc == &ulp->list_proc)\n\t\t\tsemid = -1;\n\t\t else\n\t\t\tsemid = un->semid;\n\n\t\tif (semid == -1) {\n\t\t\trcu_read_unlock();\n\t\t\tbreak;\n\t\t}\n\n\t\tsma = sem_obtain_object_check(tsk->nsproxy->ipc_ns, un->semid);\n\t\t/* exit_sem raced with IPC_RMID, nothing to do */\n\t\tif (IS_ERR(sma)) {\n\t\t\trcu_read_unlock();\n\t\t\tcontinue;\n\t\t}\n\n\t\tsem_lock(sma, NULL, -1);\n\t\tun = __lookup_undo(ulp, semid);\n\t\tif (un == NULL) {\n\t\t\t/* exit_sem raced with IPC_RMID+semget() that created\n\t\t\t * exactly the same semid. Nothing to do.\n\t\t\t */\n\t\t\tsem_unlock(sma, -1);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* remove un from the linked lists */\n\t\tassert_spin_locked(&sma->sem_perm.lock);\n\t\tlist_del(&un->list_id);\n\n\t\tspin_lock(&ulp->lock);\n\t\tlist_del_rcu(&un->list_proc);\n\t\tspin_unlock(&ulp->lock);\n\n\t\t/* perform adjustments registered in un */\n\t\tfor (i = 0; i < sma->sem_nsems; i++) {\n\t\t\tstruct sem * semaphore = &sma->sem_base[i];\n\t\t\tif (un->semadj[i]) {\n\t\t\t\tsemaphore->semval += un->semadj[i];\n\t\t\t\t/*\n\t\t\t\t * Range checks of the new semaphore value,\n\t\t\t\t * not defined by sus:\n\t\t\t\t * - Some unices ignore the undo entirely\n\t\t\t\t *   (e.g. HP UX 11i 11.22, Tru64 V5.1)\n\t\t\t\t * - some cap the value (e.g. FreeBSD caps\n\t\t\t\t *   at 0, but doesn't enforce SEMVMX)\n\t\t\t\t *\n\t\t\t\t * Linux caps the semaphore value, both at 0\n\t\t\t\t * and at SEMVMX.\n\t\t\t\t *\n\t\t\t\t * \tManfred <manfred@colorfullife.com>\n\t\t\t\t */\n\t\t\t\tif (semaphore->semval < 0)\n\t\t\t\t\tsemaphore->semval = 0;\n\t\t\t\tif (semaphore->semval > SEMVMX)\n\t\t\t\t\tsemaphore->semval = SEMVMX;\n\t\t\t\tsemaphore->sempid = task_tgid_vnr(current);\n\t\t\t}"
    },
    {
        "patch_model": "Delete",
        "change_type": "if-Condition",
        "line_old": 1539,
        "critical_vars": [
            "un->semid"
        ],
        "function": "SYSCALL_DEFINE4",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "False",
        "function_code": "\n\nSYSCALL_DEFINE4(semtimedop, int, semid, struct sembuf __user *, tsops,\n\t\tunsigned, nsops, const struct timespec __user *, timeout)\n{\n\tint error = -EINVAL;\n\tstruct sem_array *sma;\n\tstruct sembuf fast_sops[SEMOPM_FAST];\n\tstruct sembuf* sops = fast_sops, *sop;\n\tstruct sem_undo *un;\n\tint undos = 0, alter = 0, max;\n\tstruct sem_queue queue;\n\tunsigned long jiffies_left = 0;\n\tstruct ipc_namespace *ns;\n\tstruct list_head tasks;\n\n\tns = current->nsproxy->ipc_ns;\n\n\tif (nsops < 1 || semid < 0)\n\t\treturn -EINVAL;\n\tif (nsops > ns->sc_semopm)\n\t\treturn -E2BIG;\n\tif(nsops > SEMOPM_FAST) {\n\t\tsops = kmalloc(sizeof(*sops)*nsops,GFP_KERNEL);\n\t\tif(sops==NULL)\n\t\t\treturn -ENOMEM;\n\t}\n\tif (copy_from_user (sops, tsops, nsops * sizeof(*tsops))) {\n\t\terror=-EFAULT;\n\t\tgoto out_free;\n\t}\n\tif (timeout) {\n\t\tstruct timespec _timeout;\n\t\tif (copy_from_user(&_timeout, timeout, sizeof(*timeout))) {\n\t\t\terror = -EFAULT;\n\t\t\tgoto out_free;\n\t\t}\n\t\tif (_timeout.tv_sec < 0 || _timeout.tv_nsec < 0 ||\n\t\t\t_timeout.tv_nsec >= 1000000000L) {\n\t\t\terror = -EINVAL;\n\t\t\tgoto out_free;\n\t\t}\n\t\tjiffies_left = timespec_to_jiffies(&_timeout);\n\t}\n\tmax = 0;\n\tfor (sop = sops; sop < sops + nsops; sop++) {\n\t\tif (sop->sem_num >= max)\n\t\t\tmax = sop->sem_num;\n\t\tif (sop->sem_flg & SEM_UNDO)\n\t\t\tundos = 1;\n\t\tif (sop->sem_op != 0)\n\t\t\talter = 1;\n\t}\n\n\tif (undos) {\n\t\tun = find_alloc_undo(ns, semid);\n\t\tif (IS_ERR(un)) {\n\t\t\terror = PTR_ERR(un);\n\t\t\tgoto out_free;\n\t\t}\n\t} else\n\t\tun = NULL;\n\n\tINIT_LIST_HEAD(&tasks);\n\n\trcu_read_lock();\n\tsma = sem_obtain_object_check(ns, semid);\n\tif (IS_ERR(sma)) {\n\t\tif (un)\n\t\t\trcu_read_unlock();\n\t\terror = PTR_ERR(sma);\n\t\tgoto out_free;\n\t}\n\n\terror = -EFBIG;\n\tif (max >= sma->sem_nsems) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terror = -EACCES;\n\tif (ipcperms(ns, &sma->sem_perm, alter ? S_IWUGO : S_IRUGO)) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terror = security_sem_semop(sma, sops, nsops, alter);\n\tif (error) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\t/*\n\t * semid identifiers are not unique - find_alloc_undo may have\n\t * allocated an undo structure, it was invalidated by an RMID\n\t * and now a new array with received the same id. Check and fail.\n\t * This case can be detected checking un->semid. The existence of\n\t * \"un\" itself is guaranteed by rcu.\n\t */\n\terror = -EIDRM;\n\tipc_lock_object(&sma->sem_perm);\n\tif (un) {\n\t\tif (un->semid == -1) {\n\t\t\trcu_read_unlock();\n\t\t\tgoto out_unlock_free;\n\t\t} else {\n\t\t\t/*\n\t\t\t * rcu lock can be released, \"un\" cannot disappear:\n\t\t\t * - sem_lock is acquired, thus IPC_RMID is\n\t\t\t *   impossible.\n\t\t\t * - exit_sem is impossible, it always operates on\n\t\t\t *   current (or a dead task).\n\t\t\t */\n\n\t\t\trcu_read_unlock();\n\t\t}\n\t}\n\n\terror = try_atomic_semop (sma, sops, nsops, un, task_tgid_vnr(current));\n\tif (error <= 0) {\n\t\tif (alter && error == 0)\n\t\t\tdo_smart_update(sma, sops, nsops, 1, &tasks);\n\n\t\tgoto out_unlock_free;\n\t}\n\n\t/* We need to sleep on this operation, so we put the current\n\t * task into the pending queue and go to sleep.\n\t */\n\t\t\n\tqueue.sops = sops;\n\tqueue.nsops = nsops;\n\tqueue.undo = un;\n\tqueue.pid = task_tgid_vnr(current);\n\tqueue.alter = alter;\n\n\tif (nsops == 1) {\n\t\tstruct sem *curr;\n\t\tcurr = &sma->sem_base[sops->sem_num];\n\n\t\tif (alter)\n\t\t\tlist_add_tail(&queue.list, &curr->sem_pending);\n\t\telse\n\t\t\tlist_add(&queue.list, &curr->sem_pending);\n\t} else {\n\t\tif (alter)\n\t\t\tlist_add_tail(&queue.list, &sma->sem_pending);\n\t\telse\n\t\t\tlist_add(&queue.list, &sma->sem_pending);\n\t\tsma->complex_count++;\n\t}\n\n\tqueue.status = -EINTR;\n\tqueue.sleeper = current;\n\nsleep_again:\n\tcurrent->state = TASK_INTERRUPTIBLE;\n\tsem_unlock(sma);\n\n\tif (timeout)\n\t\tjiffies_left = schedule_timeout(jiffies_left);\n\telse\n\t\tschedule();\n\n\terror = get_queue_result(&queue);\n\n\tif (error != -EINTR) {\n\t\t/* fast path: update_queue already obtained all requested\n\t\t * resources.\n\t\t * Perform a smp_mb(): User space could assume that semop()\n\t\t * is a memory barrier: Without the mb(), the cpu could\n\t\t * speculatively read in user space stale data that was\n\t\t * overwritten by the previous owner of the semaphore.\n\t\t */\n\t\tsmp_mb();\n\n\t\tgoto out_free;\n\t}\n\n\tsma = sem_obtain_lock(ns, semid);\n\n\t/*\n\t * Wait until it's guaranteed that no wakeup_sem_queue_do() is ongoing.\n\t */\n\terror = get_queue_result(&queue);\n\n\t/*\n\t * Array removed? If yes, leave without sem_unlock().\n\t */\n\tif (IS_ERR(sma)) {\n\t\tgoto out_free;\n\t}\n\n\n\t/*\n\t * If queue.status != -EINTR we are woken up by another process.\n\t * Leave without unlink_queue(), but with sem_unlock().\n\t */\n\n\tif (error != -EINTR) {\n\t\tgoto out_unlock_free;\n\t}\n\n\t/*\n\t * If an interrupt occurred we have to clean up the queue\n\t */\n\tif (timeout && jiffies_left == 0)\n\t\terror = -EAGAIN;\n\n\t/*\n\t * If the wakeup was spurious, just retry\n\t */\n\tif (error == -EINTR && !signal_pending(current))\n\t\tgoto sleep_again;\n\n\tunlink_queue(sma, &queue);\n\nout_unlock_free:\n\tsem_unlock(sma);\nout_wakeup:\n\twake_up_sem_queue_do(&tasks);\nout_free:\n\tif(sops != fast_sops)\n\t\tkfree(sops);\n\treturn error;\n}\n\nSYSCALL_DEFINE3(semop, int, semid, struct sembuf __user *, tsops,\n\t\tunsigned, nsops)\n{\n\treturn sys_semtimedop(semid, tsops, nsops, NULL);\n}\n\n/* If CLONE_SYSVSEM is set, establish sharing of SEM_UNDO state between\n * parent and child tasks.\n */\n\nint copy_semundo(unsigned long clone_flags, struct task_struct *tsk)\n{\n\tstruct sem_undo_list *undo_list;\n\tint error;\n\n\tif (clone_flags & CLONE_SYSVSEM) {\n\t\terror = get_undo_list(&undo_list);\n\t\tif (error)\n\t\t\treturn error;\n\t\tatomic_inc(&undo_list->refcnt);\n\t\ttsk->sysvsem.undo_list = undo_list;\n\t} else \n\t\ttsk->sysvsem.undo_list = NULL;\n\n\treturn 0;\n}\n\n/*\n * add semadj values to semaphores, free undo structures.\n * undo structures are not freed when semaphore arrays are destroyed\n * so some of them may be out of date.\n * IMPLEMENTATION NOTE: There is some confusion over whether the\n * set of adjustments that needs to be done should be done in an atomic\n * manner or not. That is, if we are attempting to decrement the semval\n * should we queue up and wait until we can do so legally?\n * The original implementation attempted to do this (queue and wait).\n * The current implementation does not do so. The POSIX standard\n * and SVID should be consulted to determine what behavior is mandated.\n */\nvoid exit_sem(struct task_struct *tsk)\n{\n\tstruct sem_undo_list *ulp;\n\n\tulp = tsk->sysvsem.undo_list;\n\tif (!ulp)\n\t\treturn;\n\ttsk->sysvsem.undo_list = NULL;\n\n\tif (!atomic_dec_and_test(&ulp->refcnt))\n\t\treturn;\n\n\tfor (;;) {\n\t\tstruct sem_array *sma;\n\t\tstruct sem_undo *un;\n\t\tstruct list_head tasks;\n\t\tint semid;\n\t\tint i;\n\n\t\trcu_read_lock();\n\t\tun = list_entry_rcu(ulp->list_proc.next,\n\t\t\t\t    struct sem_undo, list_proc);\n\t\tif (&un->list_proc == &ulp->list_proc)\n\t\t\tsemid = -1;\n\t\t else\n\t\t\tsemid = un->semid;\n\t\trcu_read_unlock();\n\n\t\tif (semid == -1)\n\t\t\tbreak;\n\n\t\tsma = sem_lock_check(tsk->nsproxy->ipc_ns, un->semid);\n\n\t\t/* exit_sem raced with IPC_RMID, nothing to do */\n\t\tif (IS_ERR(sma))\n\t\t\tcontinue;\n\n\t\tun = __lookup_undo(ulp, semid);\n\t\tif (un == NULL) {\n\t\t\t/* exit_sem raced with IPC_RMID+semget() that created\n\t\t\t * exactly the same semid. Nothing to do.\n\t\t\t */\n\t\t\tsem_unlock(sma);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* remove un from the linked lists */\n\t\tassert_spin_locked(&sma->sem_perm.lock);\n\t\tlist_del(&un->list_id);\n\n\t\tspin_lock(&ulp->lock);\n\t\tlist_del_rcu(&un->list_proc);\n\t\tspin_unlock(&ulp->lock);\n\n\t\t/* perform adjustments registered in un */\n\t\tfor (i = 0; i < sma->sem_nsems; i++) {\n\t\t\tstruct sem * semaphore = &sma->sem_base[i];\n\t\t\tif (un->semadj[i]) {\n\t\t\t\tsemaphore->semval += un->semadj[i];\n\t\t\t\t/*\n\t\t\t\t * Range checks of the new semaphore value,\n\t\t\t\t * not defined by sus:\n\t\t\t\t * - Some unices ignore the undo entirely\n\t\t\t\t *   (e.g. HP UX 11i 11.22, Tru64 V5.1)\n\t\t\t\t * - some cap the value (e.g. FreeBSD caps\n\t\t\t\t *   at 0, but doesn't enforce SEMVMX)\n\t\t\t\t *\n\t\t\t\t * Linux caps the semaphore value, both at 0\n\t\t\t\t * and at SEMVMX.\n\t\t\t\t *\n\t\t\t\t * \tManfred <manfred@colorfullife.com>\n\t\t\t\t */\n\t\t\t\tif (semaphore->semval < 0)\n\t\t\t\t\tsemaphore->semval = 0;\n\t\t\t\tif (semaphore->semval > SEMVMX)\n\t\t\t\t\tsemaphore->semval = SEMVMX;\n\t\t\t\tsemaphore->sempid = task_tgid_vnr(current);\n\t\t\t}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 1663,
        "critical_vars": [
            "locknum"
        ],
        "function": "SYSCALL_DEFINE4",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "True",
        "function_code": "\n\nSYSCALL_DEFINE4(semtimedop, int, semid, struct sembuf __user *, tsops,\n\t\tunsigned, nsops, const struct timespec __user *, timeout)\n{\n\tint error = -EINVAL;\n\tstruct sem_array *sma;\n\tstruct sembuf fast_sops[SEMOPM_FAST];\n\tstruct sembuf* sops = fast_sops, *sop;\n\tstruct sem_undo *un;\n\tint undos = 0, alter = 0, max, locknum;\n\tstruct sem_queue queue;\n\tunsigned long jiffies_left = 0;\n\tstruct ipc_namespace *ns;\n\tstruct list_head tasks;\n\n\tns = current->nsproxy->ipc_ns;\n\n\tif (nsops < 1 || semid < 0)\n\t\treturn -EINVAL;\n\tif (nsops > ns->sc_semopm)\n\t\treturn -E2BIG;\n\tif(nsops > SEMOPM_FAST) {\n\t\tsops = kmalloc(sizeof(*sops)*nsops,GFP_KERNEL);\n\t\tif(sops==NULL)\n\t\t\treturn -ENOMEM;\n\t}\n\tif (copy_from_user (sops, tsops, nsops * sizeof(*tsops))) {\n\t\terror=-EFAULT;\n\t\tgoto out_free;\n\t}\n\tif (timeout) {\n\t\tstruct timespec _timeout;\n\t\tif (copy_from_user(&_timeout, timeout, sizeof(*timeout))) {\n\t\t\terror = -EFAULT;\n\t\t\tgoto out_free;\n\t\t}\n\t\tif (_timeout.tv_sec < 0 || _timeout.tv_nsec < 0 ||\n\t\t\t_timeout.tv_nsec >= 1000000000L) {\n\t\t\terror = -EINVAL;\n\t\t\tgoto out_free;\n\t\t}\n\t\tjiffies_left = timespec_to_jiffies(&_timeout);\n\t}\n\tmax = 0;\n\tfor (sop = sops; sop < sops + nsops; sop++) {\n\t\tif (sop->sem_num >= max)\n\t\t\tmax = sop->sem_num;\n\t\tif (sop->sem_flg & SEM_UNDO)\n\t\t\tundos = 1;\n\t\tif (sop->sem_op != 0)\n\t\t\talter = 1;\n\t}\n\n\tINIT_LIST_HEAD(&tasks);\n\n\tif (undos) {\n\t\t/* On success, find_alloc_undo takes the rcu_read_lock */\n\t\tun = find_alloc_undo(ns, semid);\n\t\tif (IS_ERR(un)) {\n\t\t\terror = PTR_ERR(un);\n\t\t\tgoto out_free;\n\t\t}\n\t} else {\n\t\tun = NULL;\n\t\trcu_read_lock();\n\t}\n\n\tsma = sem_obtain_object_check(ns, semid);\n\tif (IS_ERR(sma)) {\n\t\trcu_read_unlock();\n\t\terror = PTR_ERR(sma);\n\t\tgoto out_free;\n\t}\n\n\terror = -EFBIG;\n\tif (max >= sma->sem_nsems) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terror = -EACCES;\n\tif (ipcperms(ns, &sma->sem_perm, alter ? S_IWUGO : S_IRUGO)) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terror = security_sem_semop(sma, sops, nsops, alter);\n\tif (error) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\t/*\n\t * semid identifiers are not unique - find_alloc_undo may have\n\t * allocated an undo structure, it was invalidated by an RMID\n\t * and now a new array with received the same id. Check and fail.\n\t * This case can be detected checking un->semid. The existence of\n\t * \"un\" itself is guaranteed by rcu.\n\t */\n\terror = -EIDRM;\n\tlocknum = sem_lock(sma, sops, nsops);\n\tif (un && un->semid == -1)\n\t\tgoto out_unlock_free;\n\n\terror = try_atomic_semop (sma, sops, nsops, un, task_tgid_vnr(current));\n\tif (error <= 0) {\n\t\tif (alter && error == 0)\n\t\t\tdo_smart_update(sma, sops, nsops, 1, &tasks);\n\n\t\tgoto out_unlock_free;\n\t}\n\n\t/* We need to sleep on this operation, so we put the current\n\t * task into the pending queue and go to sleep.\n\t */\n\t\t\n\tqueue.sops = sops;\n\tqueue.nsops = nsops;\n\tqueue.undo = un;\n\tqueue.pid = task_tgid_vnr(current);\n\tqueue.alter = alter;\n\n\tif (nsops == 1) {\n\t\tstruct sem *curr;\n\t\tcurr = &sma->sem_base[sops->sem_num];\n\n\t\tif (alter)\n\t\t\tlist_add_tail(&queue.list, &curr->sem_pending);\n\t\telse\n\t\t\tlist_add(&queue.list, &curr->sem_pending);\n\t} else {\n\t\tif (alter)\n\t\t\tlist_add_tail(&queue.list, &sma->sem_pending);\n\t\telse\n\t\t\tlist_add(&queue.list, &sma->sem_pending);\n\t\tsma->complex_count++;\n\t}\n\n\tqueue.status = -EINTR;\n\tqueue.sleeper = current;\n\nsleep_again:\n\tcurrent->state = TASK_INTERRUPTIBLE;\n\tsem_unlock(sma, locknum);\n\n\tif (timeout)\n\t\tjiffies_left = schedule_timeout(jiffies_left);\n\telse\n\t\tschedule();\n\n\terror = get_queue_result(&queue);\n\n\tif (error != -EINTR) {\n\t\t/* fast path: update_queue already obtained all requested\n\t\t * resources.\n\t\t * Perform a smp_mb(): User space could assume that semop()\n\t\t * is a memory barrier: Without the mb(), the cpu could\n\t\t * speculatively read in user space stale data that was\n\t\t * overwritten by the previous owner of the semaphore.\n\t\t */\n\t\tsmp_mb();\n\n\t\tgoto out_free;\n\t}\n\n\tsma = sem_obtain_lock(ns, semid, sops, nsops, &locknum);\n\n\t/*\n\t * Wait until it's guaranteed that no wakeup_sem_queue_do() is ongoing.\n\t */\n\terror = get_queue_result(&queue);\n\n\t/*\n\t * Array removed? If yes, leave without sem_unlock().\n\t */\n\tif (IS_ERR(sma)) {\n\t\tgoto out_free;\n\t}\n\n\n\t/*\n\t * If queue.status != -EINTR we are woken up by another process.\n\t * Leave without unlink_queue(), but with sem_unlock().\n\t */\n\n\tif (error != -EINTR) {\n\t\tgoto out_unlock_free;\n\t}\n\n\t/*\n\t * If an interrupt occurred we have to clean up the queue\n\t */\n\tif (timeout && jiffies_left == 0)\n\t\terror = -EAGAIN;\n\n\t/*\n\t * If the wakeup was spurious, just retry\n\t */\n\tif (error == -EINTR && !signal_pending(current))\n\t\tgoto sleep_again;\n\n\tunlink_queue(sma, &queue);\n\nout_unlock_free:\n\tsem_unlock(sma, locknum);\nout_wakeup:\n\twake_up_sem_queue_do(&tasks);\nout_free:\n\tif(sops != fast_sops)\n\t\tkfree(sops);\n\treturn error;\n}\n\nSYSCALL_DEFINE3(semop, int, semid, struct sembuf __user *, tsops,\n\t\tunsigned, nsops)\n{\n\treturn sys_semtimedop(semid, tsops, nsops, NULL);\n}\n\n/* If CLONE_SYSVSEM is set, establish sharing of SEM_UNDO state between\n * parent and child tasks.\n */\n\nint copy_semundo(unsigned long clone_flags, struct task_struct *tsk)\n{\n\tstruct sem_undo_list *undo_list;\n\tint error;\n\n\tif (clone_flags & CLONE_SYSVSEM) {\n\t\terror = get_undo_list(&undo_list);\n\t\tif (error)\n\t\t\treturn error;\n\t\tatomic_inc(&undo_list->refcnt);\n\t\ttsk->sysvsem.undo_list = undo_list;\n\t} else \n\t\ttsk->sysvsem.undo_list = NULL;\n\n\treturn 0;\n}\n\n/*\n * add semadj values to semaphores, free undo structures.\n * undo structures are not freed when semaphore arrays are destroyed\n * so some of them may be out of date.\n * IMPLEMENTATION NOTE: There is some confusion over whether the\n * set of adjustments that needs to be done should be done in an atomic\n * manner or not. That is, if we are attempting to decrement the semval\n * should we queue up and wait until we can do so legally?\n * The original implementation attempted to do this (queue and wait).\n * The current implementation does not do so. The POSIX standard\n * and SVID should be consulted to determine what behavior is mandated.\n */\nvoid exit_sem(struct task_struct *tsk)\n{\n\tstruct sem_undo_list *ulp;\n\n\tulp = tsk->sysvsem.undo_list;\n\tif (!ulp)\n\t\treturn;\n\ttsk->sysvsem.undo_list = NULL;\n\n\tif (!atomic_dec_and_test(&ulp->refcnt))\n\t\treturn;\n\n\tfor (;;) {\n\t\tstruct sem_array *sma;\n\t\tstruct sem_undo *un;\n\t\tstruct list_head tasks;\n\t\tint semid, i;\n\n\t\trcu_read_lock();\n\t\tun = list_entry_rcu(ulp->list_proc.next,\n\t\t\t\t    struct sem_undo, list_proc);\n\t\tif (&un->list_proc == &ulp->list_proc)\n\t\t\tsemid = -1;\n\t\t else\n\t\t\tsemid = un->semid;\n\n\t\tif (semid == -1) {\n\t\t\trcu_read_unlock();\n\t\t\tbreak;\n\t\t}\n\n\t\tsma = sem_obtain_object_check(tsk->nsproxy->ipc_ns, un->semid);\n\t\t/* exit_sem raced with IPC_RMID, nothing to do */\n\t\tif (IS_ERR(sma)) {\n\t\t\trcu_read_unlock();\n\t\t\tcontinue;\n\t\t}\n\n\t\tsem_lock(sma, NULL, -1);\n\t\tun = __lookup_undo(ulp, semid);\n\t\tif (un == NULL) {\n\t\t\t/* exit_sem raced with IPC_RMID+semget() that created\n\t\t\t * exactly the same semid. Nothing to do.\n\t\t\t */\n\t\t\tsem_unlock(sma, -1);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* remove un from the linked lists */\n\t\tassert_spin_locked(&sma->sem_perm.lock);\n\t\tlist_del(&un->list_id);\n\n\t\tspin_lock(&ulp->lock);\n\t\tlist_del_rcu(&un->list_proc);\n\t\tspin_unlock(&ulp->lock);\n\n\t\t/* perform adjustments registered in un */\n\t\tfor (i = 0; i < sma->sem_nsems; i++) {\n\t\t\tstruct sem * semaphore = &sma->sem_base[i];\n\t\t\tif (un->semadj[i]) {\n\t\t\t\tsemaphore->semval += un->semadj[i];\n\t\t\t\t/*\n\t\t\t\t * Range checks of the new semaphore value,\n\t\t\t\t * not defined by sus:\n\t\t\t\t * - Some unices ignore the undo entirely\n\t\t\t\t *   (e.g. HP UX 11i 11.22, Tru64 V5.1)\n\t\t\t\t * - some cap the value (e.g. FreeBSD caps\n\t\t\t\t *   at 0, but doesn't enforce SEMVMX)\n\t\t\t\t *\n\t\t\t\t * Linux caps the semaphore value, both at 0\n\t\t\t\t * and at SEMVMX.\n\t\t\t\t *\n\t\t\t\t * \tManfred <manfred@colorfullife.com>\n\t\t\t\t */\n\t\t\t\tif (semaphore->semval < 0)\n\t\t\t\t\tsemaphore->semval = 0;\n\t\t\t\tif (semaphore->semval > SEMVMX)\n\t\t\t\t\tsemaphore->semval = SEMVMX;\n\t\t\t\tsemaphore->sempid = task_tgid_vnr(current);\n\t\t\t}"
    },
    {
        "patch_model": "Replace",
        "change_type": "Assignment",
        "line_old": 1616,
        "line_new": 1685,
        "critical_vars": [
            "sma"
        ],
        "function": "SYSCALL_DEFINE4",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "True",
        "function_code": "\n\nSYSCALL_DEFINE4(semtimedop, int, semid, struct sembuf __user *, tsops,\n\t\tunsigned, nsops, const struct timespec __user *, timeout)\n{\n\tint error = -EINVAL;\n\tstruct sem_array *sma;\n\tstruct sembuf fast_sops[SEMOPM_FAST];\n\tstruct sembuf* sops = fast_sops, *sop;\n\tstruct sem_undo *un;\n\tint undos = 0, alter = 0, max, locknum;\n\tstruct sem_queue queue;\n\tunsigned long jiffies_left = 0;\n\tstruct ipc_namespace *ns;\n\tstruct list_head tasks;\n\n\tns = current->nsproxy->ipc_ns;\n\n\tif (nsops < 1 || semid < 0)\n\t\treturn -EINVAL;\n\tif (nsops > ns->sc_semopm)\n\t\treturn -E2BIG;\n\tif(nsops > SEMOPM_FAST) {\n\t\tsops = kmalloc(sizeof(*sops)*nsops,GFP_KERNEL);\n\t\tif(sops==NULL)\n\t\t\treturn -ENOMEM;\n\t}\n\tif (copy_from_user (sops, tsops, nsops * sizeof(*tsops))) {\n\t\terror=-EFAULT;\n\t\tgoto out_free;\n\t}\n\tif (timeout) {\n\t\tstruct timespec _timeout;\n\t\tif (copy_from_user(&_timeout, timeout, sizeof(*timeout))) {\n\t\t\terror = -EFAULT;\n\t\t\tgoto out_free;\n\t\t}\n\t\tif (_timeout.tv_sec < 0 || _timeout.tv_nsec < 0 ||\n\t\t\t_timeout.tv_nsec >= 1000000000L) {\n\t\t\terror = -EINVAL;\n\t\t\tgoto out_free;\n\t\t}\n\t\tjiffies_left = timespec_to_jiffies(&_timeout);\n\t}\n\tmax = 0;\n\tfor (sop = sops; sop < sops + nsops; sop++) {\n\t\tif (sop->sem_num >= max)\n\t\t\tmax = sop->sem_num;\n\t\tif (sop->sem_flg & SEM_UNDO)\n\t\t\tundos = 1;\n\t\tif (sop->sem_op != 0)\n\t\t\talter = 1;\n\t}\n\n\tINIT_LIST_HEAD(&tasks);\n\n\tif (undos) {\n\t\t/* On success, find_alloc_undo takes the rcu_read_lock */\n\t\tun = find_alloc_undo(ns, semid);\n\t\tif (IS_ERR(un)) {\n\t\t\terror = PTR_ERR(un);\n\t\t\tgoto out_free;\n\t\t}\n\t} else {\n\t\tun = NULL;\n\t\trcu_read_lock();\n\t}\n\n\tsma = sem_obtain_object_check(ns, semid);\n\tif (IS_ERR(sma)) {\n\t\trcu_read_unlock();\n\t\terror = PTR_ERR(sma);\n\t\tgoto out_free;\n\t}\n\n\terror = -EFBIG;\n\tif (max >= sma->sem_nsems) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terror = -EACCES;\n\tif (ipcperms(ns, &sma->sem_perm, alter ? S_IWUGO : S_IRUGO)) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terror = security_sem_semop(sma, sops, nsops, alter);\n\tif (error) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\t/*\n\t * semid identifiers are not unique - find_alloc_undo may have\n\t * allocated an undo structure, it was invalidated by an RMID\n\t * and now a new array with received the same id. Check and fail.\n\t * This case can be detected checking un->semid. The existence of\n\t * \"un\" itself is guaranteed by rcu.\n\t */\n\terror = -EIDRM;\n\tlocknum = sem_lock(sma, sops, nsops);\n\tif (un && un->semid == -1)\n\t\tgoto out_unlock_free;\n\n\terror = try_atomic_semop (sma, sops, nsops, un, task_tgid_vnr(current));\n\tif (error <= 0) {\n\t\tif (alter && error == 0)\n\t\t\tdo_smart_update(sma, sops, nsops, 1, &tasks);\n\n\t\tgoto out_unlock_free;\n\t}\n\n\t/* We need to sleep on this operation, so we put the current\n\t * task into the pending queue and go to sleep.\n\t */\n\t\t\n\tqueue.sops = sops;\n\tqueue.nsops = nsops;\n\tqueue.undo = un;\n\tqueue.pid = task_tgid_vnr(current);\n\tqueue.alter = alter;\n\n\tif (nsops == 1) {\n\t\tstruct sem *curr;\n\t\tcurr = &sma->sem_base[sops->sem_num];\n\n\t\tif (alter)\n\t\t\tlist_add_tail(&queue.list, &curr->sem_pending);\n\t\telse\n\t\t\tlist_add(&queue.list, &curr->sem_pending);\n\t} else {\n\t\tif (alter)\n\t\t\tlist_add_tail(&queue.list, &sma->sem_pending);\n\t\telse\n\t\t\tlist_add(&queue.list, &sma->sem_pending);\n\t\tsma->complex_count++;\n\t}\n\n\tqueue.status = -EINTR;\n\tqueue.sleeper = current;\n\nsleep_again:\n\tcurrent->state = TASK_INTERRUPTIBLE;\n\tsem_unlock(sma, locknum);\n\n\tif (timeout)\n\t\tjiffies_left = schedule_timeout(jiffies_left);\n\telse\n\t\tschedule();\n\n\terror = get_queue_result(&queue);\n\n\tif (error != -EINTR) {\n\t\t/* fast path: update_queue already obtained all requested\n\t\t * resources.\n\t\t * Perform a smp_mb(): User space could assume that semop()\n\t\t * is a memory barrier: Without the mb(), the cpu could\n\t\t * speculatively read in user space stale data that was\n\t\t * overwritten by the previous owner of the semaphore.\n\t\t */\n\t\tsmp_mb();\n\n\t\tgoto out_free;\n\t}\n\n\tsma = sem_obtain_lock(ns, semid, sops, nsops, &locknum);\n\n\t/*\n\t * Wait until it's guaranteed that no wakeup_sem_queue_do() is ongoing.\n\t */\n\terror = get_queue_result(&queue);\n\n\t/*\n\t * Array removed? If yes, leave without sem_unlock().\n\t */\n\tif (IS_ERR(sma)) {\n\t\tgoto out_free;\n\t}\n\n\n\t/*\n\t * If queue.status != -EINTR we are woken up by another process.\n\t * Leave without unlink_queue(), but with sem_unlock().\n\t */\n\n\tif (error != -EINTR) {\n\t\tgoto out_unlock_free;\n\t}\n\n\t/*\n\t * If an interrupt occurred we have to clean up the queue\n\t */\n\tif (timeout && jiffies_left == 0)\n\t\terror = -EAGAIN;\n\n\t/*\n\t * If the wakeup was spurious, just retry\n\t */\n\tif (error == -EINTR && !signal_pending(current))\n\t\tgoto sleep_again;\n\n\tunlink_queue(sma, &queue);\n\nout_unlock_free:\n\tsem_unlock(sma, locknum);\nout_wakeup:\n\twake_up_sem_queue_do(&tasks);\nout_free:\n\tif(sops != fast_sops)\n\t\tkfree(sops);\n\treturn error;\n}\n\nSYSCALL_DEFINE3(semop, int, semid, struct sembuf __user *, tsops,\n\t\tunsigned, nsops)\n{\n\treturn sys_semtimedop(semid, tsops, nsops, NULL);\n}\n\n/* If CLONE_SYSVSEM is set, establish sharing of SEM_UNDO state between\n * parent and child tasks.\n */\n\nint copy_semundo(unsigned long clone_flags, struct task_struct *tsk)\n{\n\tstruct sem_undo_list *undo_list;\n\tint error;\n\n\tif (clone_flags & CLONE_SYSVSEM) {\n\t\terror = get_undo_list(&undo_list);\n\t\tif (error)\n\t\t\treturn error;\n\t\tatomic_inc(&undo_list->refcnt);\n\t\ttsk->sysvsem.undo_list = undo_list;\n\t} else \n\t\ttsk->sysvsem.undo_list = NULL;\n\n\treturn 0;\n}\n\n/*\n * add semadj values to semaphores, free undo structures.\n * undo structures are not freed when semaphore arrays are destroyed\n * so some of them may be out of date.\n * IMPLEMENTATION NOTE: There is some confusion over whether the\n * set of adjustments that needs to be done should be done in an atomic\n * manner or not. That is, if we are attempting to decrement the semval\n * should we queue up and wait until we can do so legally?\n * The original implementation attempted to do this (queue and wait).\n * The current implementation does not do so. The POSIX standard\n * and SVID should be consulted to determine what behavior is mandated.\n */\nvoid exit_sem(struct task_struct *tsk)\n{\n\tstruct sem_undo_list *ulp;\n\n\tulp = tsk->sysvsem.undo_list;\n\tif (!ulp)\n\t\treturn;\n\ttsk->sysvsem.undo_list = NULL;\n\n\tif (!atomic_dec_and_test(&ulp->refcnt))\n\t\treturn;\n\n\tfor (;;) {\n\t\tstruct sem_array *sma;\n\t\tstruct sem_undo *un;\n\t\tstruct list_head tasks;\n\t\tint semid, i;\n\n\t\trcu_read_lock();\n\t\tun = list_entry_rcu(ulp->list_proc.next,\n\t\t\t\t    struct sem_undo, list_proc);\n\t\tif (&un->list_proc == &ulp->list_proc)\n\t\t\tsemid = -1;\n\t\t else\n\t\t\tsemid = un->semid;\n\n\t\tif (semid == -1) {\n\t\t\trcu_read_unlock();\n\t\t\tbreak;\n\t\t}\n\n\t\tsma = sem_obtain_object_check(tsk->nsproxy->ipc_ns, un->semid);\n\t\t/* exit_sem raced with IPC_RMID, nothing to do */\n\t\tif (IS_ERR(sma)) {\n\t\t\trcu_read_unlock();\n\t\t\tcontinue;\n\t\t}\n\n\t\tsem_lock(sma, NULL, -1);\n\t\tun = __lookup_undo(ulp, semid);\n\t\tif (un == NULL) {\n\t\t\t/* exit_sem raced with IPC_RMID+semget() that created\n\t\t\t * exactly the same semid. Nothing to do.\n\t\t\t */\n\t\t\tsem_unlock(sma, -1);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* remove un from the linked lists */\n\t\tassert_spin_locked(&sma->sem_perm.lock);\n\t\tlist_del(&un->list_id);\n\n\t\tspin_lock(&ulp->lock);\n\t\tlist_del_rcu(&un->list_proc);\n\t\tspin_unlock(&ulp->lock);\n\n\t\t/* perform adjustments registered in un */\n\t\tfor (i = 0; i < sma->sem_nsems; i++) {\n\t\t\tstruct sem * semaphore = &sma->sem_base[i];\n\t\t\tif (un->semadj[i]) {\n\t\t\t\tsemaphore->semval += un->semadj[i];\n\t\t\t\t/*\n\t\t\t\t * Range checks of the new semaphore value,\n\t\t\t\t * not defined by sus:\n\t\t\t\t * - Some unices ignore the undo entirely\n\t\t\t\t *   (e.g. HP UX 11i 11.22, Tru64 V5.1)\n\t\t\t\t * - some cap the value (e.g. FreeBSD caps\n\t\t\t\t *   at 0, but doesn't enforce SEMVMX)\n\t\t\t\t *\n\t\t\t\t * Linux caps the semaphore value, both at 0\n\t\t\t\t * and at SEMVMX.\n\t\t\t\t *\n\t\t\t\t * \tManfred <manfred@colorfullife.com>\n\t\t\t\t */\n\t\t\t\tif (semaphore->semval < 0)\n\t\t\t\t\tsemaphore->semval = 0;\n\t\t\t\tif (semaphore->semval > SEMVMX)\n\t\t\t\t\tsemaphore->semval = SEMVMX;\n\t\t\t\tsemaphore->sempid = task_tgid_vnr(current);\n\t\t\t}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 1724,
        "critical_vars": [
            "locknum"
        ],
        "function": "SYSCALL_DEFINE4",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "True",
        "function_code": "\n\nSYSCALL_DEFINE4(semtimedop, int, semid, struct sembuf __user *, tsops,\n\t\tunsigned, nsops, const struct timespec __user *, timeout)\n{\n\tint error = -EINVAL;\n\tstruct sem_array *sma;\n\tstruct sembuf fast_sops[SEMOPM_FAST];\n\tstruct sembuf* sops = fast_sops, *sop;\n\tstruct sem_undo *un;\n\tint undos = 0, alter = 0, max, locknum;\n\tstruct sem_queue queue;\n\tunsigned long jiffies_left = 0;\n\tstruct ipc_namespace *ns;\n\tstruct list_head tasks;\n\n\tns = current->nsproxy->ipc_ns;\n\n\tif (nsops < 1 || semid < 0)\n\t\treturn -EINVAL;\n\tif (nsops > ns->sc_semopm)\n\t\treturn -E2BIG;\n\tif(nsops > SEMOPM_FAST) {\n\t\tsops = kmalloc(sizeof(*sops)*nsops,GFP_KERNEL);\n\t\tif(sops==NULL)\n\t\t\treturn -ENOMEM;\n\t}\n\tif (copy_from_user (sops, tsops, nsops * sizeof(*tsops))) {\n\t\terror=-EFAULT;\n\t\tgoto out_free;\n\t}\n\tif (timeout) {\n\t\tstruct timespec _timeout;\n\t\tif (copy_from_user(&_timeout, timeout, sizeof(*timeout))) {\n\t\t\terror = -EFAULT;\n\t\t\tgoto out_free;\n\t\t}\n\t\tif (_timeout.tv_sec < 0 || _timeout.tv_nsec < 0 ||\n\t\t\t_timeout.tv_nsec >= 1000000000L) {\n\t\t\terror = -EINVAL;\n\t\t\tgoto out_free;\n\t\t}\n\t\tjiffies_left = timespec_to_jiffies(&_timeout);\n\t}\n\tmax = 0;\n\tfor (sop = sops; sop < sops + nsops; sop++) {\n\t\tif (sop->sem_num >= max)\n\t\t\tmax = sop->sem_num;\n\t\tif (sop->sem_flg & SEM_UNDO)\n\t\t\tundos = 1;\n\t\tif (sop->sem_op != 0)\n\t\t\talter = 1;\n\t}\n\n\tINIT_LIST_HEAD(&tasks);\n\n\tif (undos) {\n\t\t/* On success, find_alloc_undo takes the rcu_read_lock */\n\t\tun = find_alloc_undo(ns, semid);\n\t\tif (IS_ERR(un)) {\n\t\t\terror = PTR_ERR(un);\n\t\t\tgoto out_free;\n\t\t}\n\t} else {\n\t\tun = NULL;\n\t\trcu_read_lock();\n\t}\n\n\tsma = sem_obtain_object_check(ns, semid);\n\tif (IS_ERR(sma)) {\n\t\trcu_read_unlock();\n\t\terror = PTR_ERR(sma);\n\t\tgoto out_free;\n\t}\n\n\terror = -EFBIG;\n\tif (max >= sma->sem_nsems) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terror = -EACCES;\n\tif (ipcperms(ns, &sma->sem_perm, alter ? S_IWUGO : S_IRUGO)) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terror = security_sem_semop(sma, sops, nsops, alter);\n\tif (error) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\t/*\n\t * semid identifiers are not unique - find_alloc_undo may have\n\t * allocated an undo structure, it was invalidated by an RMID\n\t * and now a new array with received the same id. Check and fail.\n\t * This case can be detected checking un->semid. The existence of\n\t * \"un\" itself is guaranteed by rcu.\n\t */\n\terror = -EIDRM;\n\tlocknum = sem_lock(sma, sops, nsops);\n\tif (un && un->semid == -1)\n\t\tgoto out_unlock_free;\n\n\terror = try_atomic_semop (sma, sops, nsops, un, task_tgid_vnr(current));\n\tif (error <= 0) {\n\t\tif (alter && error == 0)\n\t\t\tdo_smart_update(sma, sops, nsops, 1, &tasks);\n\n\t\tgoto out_unlock_free;\n\t}\n\n\t/* We need to sleep on this operation, so we put the current\n\t * task into the pending queue and go to sleep.\n\t */\n\t\t\n\tqueue.sops = sops;\n\tqueue.nsops = nsops;\n\tqueue.undo = un;\n\tqueue.pid = task_tgid_vnr(current);\n\tqueue.alter = alter;\n\n\tif (nsops == 1) {\n\t\tstruct sem *curr;\n\t\tcurr = &sma->sem_base[sops->sem_num];\n\n\t\tif (alter)\n\t\t\tlist_add_tail(&queue.list, &curr->sem_pending);\n\t\telse\n\t\t\tlist_add(&queue.list, &curr->sem_pending);\n\t} else {\n\t\tif (alter)\n\t\t\tlist_add_tail(&queue.list, &sma->sem_pending);\n\t\telse\n\t\t\tlist_add(&queue.list, &sma->sem_pending);\n\t\tsma->complex_count++;\n\t}\n\n\tqueue.status = -EINTR;\n\tqueue.sleeper = current;\n\nsleep_again:\n\tcurrent->state = TASK_INTERRUPTIBLE;\n\tsem_unlock(sma, locknum);\n\n\tif (timeout)\n\t\tjiffies_left = schedule_timeout(jiffies_left);\n\telse\n\t\tschedule();\n\n\terror = get_queue_result(&queue);\n\n\tif (error != -EINTR) {\n\t\t/* fast path: update_queue already obtained all requested\n\t\t * resources.\n\t\t * Perform a smp_mb(): User space could assume that semop()\n\t\t * is a memory barrier: Without the mb(), the cpu could\n\t\t * speculatively read in user space stale data that was\n\t\t * overwritten by the previous owner of the semaphore.\n\t\t */\n\t\tsmp_mb();\n\n\t\tgoto out_free;\n\t}\n\n\tsma = sem_obtain_lock(ns, semid, sops, nsops, &locknum);\n\n\t/*\n\t * Wait until it's guaranteed that no wakeup_sem_queue_do() is ongoing.\n\t */\n\terror = get_queue_result(&queue);\n\n\t/*\n\t * Array removed? If yes, leave without sem_unlock().\n\t */\n\tif (IS_ERR(sma)) {\n\t\tgoto out_free;\n\t}\n\n\n\t/*\n\t * If queue.status != -EINTR we are woken up by another process.\n\t * Leave without unlink_queue(), but with sem_unlock().\n\t */\n\n\tif (error != -EINTR) {\n\t\tgoto out_unlock_free;\n\t}\n\n\t/*\n\t * If an interrupt occurred we have to clean up the queue\n\t */\n\tif (timeout && jiffies_left == 0)\n\t\terror = -EAGAIN;\n\n\t/*\n\t * If the wakeup was spurious, just retry\n\t */\n\tif (error == -EINTR && !signal_pending(current))\n\t\tgoto sleep_again;\n\n\tunlink_queue(sma, &queue);\n\nout_unlock_free:\n\tsem_unlock(sma, locknum);\nout_wakeup:\n\twake_up_sem_queue_do(&tasks);\nout_free:\n\tif(sops != fast_sops)\n\t\tkfree(sops);\n\treturn error;\n}\n\nSYSCALL_DEFINE3(semop, int, semid, struct sembuf __user *, tsops,\n\t\tunsigned, nsops)\n{\n\treturn sys_semtimedop(semid, tsops, nsops, NULL);\n}\n\n/* If CLONE_SYSVSEM is set, establish sharing of SEM_UNDO state between\n * parent and child tasks.\n */\n\nint copy_semundo(unsigned long clone_flags, struct task_struct *tsk)\n{\n\tstruct sem_undo_list *undo_list;\n\tint error;\n\n\tif (clone_flags & CLONE_SYSVSEM) {\n\t\terror = get_undo_list(&undo_list);\n\t\tif (error)\n\t\t\treturn error;\n\t\tatomic_inc(&undo_list->refcnt);\n\t\ttsk->sysvsem.undo_list = undo_list;\n\t} else \n\t\ttsk->sysvsem.undo_list = NULL;\n\n\treturn 0;\n}\n\n/*\n * add semadj values to semaphores, free undo structures.\n * undo structures are not freed when semaphore arrays are destroyed\n * so some of them may be out of date.\n * IMPLEMENTATION NOTE: There is some confusion over whether the\n * set of adjustments that needs to be done should be done in an atomic\n * manner or not. That is, if we are attempting to decrement the semval\n * should we queue up and wait until we can do so legally?\n * The original implementation attempted to do this (queue and wait).\n * The current implementation does not do so. The POSIX standard\n * and SVID should be consulted to determine what behavior is mandated.\n */\nvoid exit_sem(struct task_struct *tsk)\n{\n\tstruct sem_undo_list *ulp;\n\n\tulp = tsk->sysvsem.undo_list;\n\tif (!ulp)\n\t\treturn;\n\ttsk->sysvsem.undo_list = NULL;\n\n\tif (!atomic_dec_and_test(&ulp->refcnt))\n\t\treturn;\n\n\tfor (;;) {\n\t\tstruct sem_array *sma;\n\t\tstruct sem_undo *un;\n\t\tstruct list_head tasks;\n\t\tint semid, i;\n\n\t\trcu_read_lock();\n\t\tun = list_entry_rcu(ulp->list_proc.next,\n\t\t\t\t    struct sem_undo, list_proc);\n\t\tif (&un->list_proc == &ulp->list_proc)\n\t\t\tsemid = -1;\n\t\t else\n\t\t\tsemid = un->semid;\n\n\t\tif (semid == -1) {\n\t\t\trcu_read_unlock();\n\t\t\tbreak;\n\t\t}\n\n\t\tsma = sem_obtain_object_check(tsk->nsproxy->ipc_ns, un->semid);\n\t\t/* exit_sem raced with IPC_RMID, nothing to do */\n\t\tif (IS_ERR(sma)) {\n\t\t\trcu_read_unlock();\n\t\t\tcontinue;\n\t\t}\n\n\t\tsem_lock(sma, NULL, -1);\n\t\tun = __lookup_undo(ulp, semid);\n\t\tif (un == NULL) {\n\t\t\t/* exit_sem raced with IPC_RMID+semget() that created\n\t\t\t * exactly the same semid. Nothing to do.\n\t\t\t */\n\t\t\tsem_unlock(sma, -1);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* remove un from the linked lists */\n\t\tassert_spin_locked(&sma->sem_perm.lock);\n\t\tlist_del(&un->list_id);\n\n\t\tspin_lock(&ulp->lock);\n\t\tlist_del_rcu(&un->list_proc);\n\t\tspin_unlock(&ulp->lock);\n\n\t\t/* perform adjustments registered in un */\n\t\tfor (i = 0; i < sma->sem_nsems; i++) {\n\t\t\tstruct sem * semaphore = &sma->sem_base[i];\n\t\t\tif (un->semadj[i]) {\n\t\t\t\tsemaphore->semval += un->semadj[i];\n\t\t\t\t/*\n\t\t\t\t * Range checks of the new semaphore value,\n\t\t\t\t * not defined by sus:\n\t\t\t\t * - Some unices ignore the undo entirely\n\t\t\t\t *   (e.g. HP UX 11i 11.22, Tru64 V5.1)\n\t\t\t\t * - some cap the value (e.g. FreeBSD caps\n\t\t\t\t *   at 0, but doesn't enforce SEMVMX)\n\t\t\t\t *\n\t\t\t\t * Linux caps the semaphore value, both at 0\n\t\t\t\t * and at SEMVMX.\n\t\t\t\t *\n\t\t\t\t * \tManfred <manfred@colorfullife.com>\n\t\t\t\t */\n\t\t\t\tif (semaphore->semval < 0)\n\t\t\t\t\tsemaphore->semval = 0;\n\t\t\t\tif (semaphore->semval > SEMVMX)\n\t\t\t\t\tsemaphore->semval = SEMVMX;\n\t\t\t\tsemaphore->sempid = task_tgid_vnr(current);\n\t\t\t}"
    },
    {
        "patch_model": "Add",
        "change_type": "Var-Declaration",
        "line_new": 1788,
        "critical_vars": [
            "i"
        ],
        "function": "exit_sem",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "True",
        "function_code": "void exit_sem(struct task_struct *tsk)\n{\n\tstruct sem_undo_list *ulp;\n\n\tulp = tsk->sysvsem.undo_list;\n\tif (!ulp)\n\t\treturn;\n\ttsk->sysvsem.undo_list = NULL;\n\n\tif (!atomic_dec_and_test(&ulp->refcnt))\n\t\treturn;\n\n\tfor (;;) {\n\t\tstruct sem_array *sma;\n\t\tstruct sem_undo *un;\n\t\tstruct list_head tasks;\n\t\tint semid, i;\n\n\t\trcu_read_lock();\n\t\tun = list_entry_rcu(ulp->list_proc.next,\n\t\t\t\t    struct sem_undo, list_proc);\n\t\tif (&un->list_proc == &ulp->list_proc)\n\t\t\tsemid = -1;\n\t\t else\n\t\t\tsemid = un->semid;\n\n\t\tif (semid == -1) {\n\t\t\trcu_read_unlock();\n\t\t\tbreak;\n\t\t}\n\n\t\tsma = sem_obtain_object_check(tsk->nsproxy->ipc_ns, un->semid);\n\t\t/* exit_sem raced with IPC_RMID, nothing to do */\n\t\tif (IS_ERR(sma)) {\n\t\t\trcu_read_unlock();\n\t\t\tcontinue;\n\t\t}\n\n\t\tsem_lock(sma, NULL, -1);\n\t\tun = __lookup_undo(ulp, semid);\n\t\tif (un == NULL) {\n\t\t\t/* exit_sem raced with IPC_RMID+semget() that created\n\t\t\t * exactly the same semid. Nothing to do.\n\t\t\t */\n\t\t\tsem_unlock(sma, -1);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* remove un from the linked lists */\n\t\tassert_spin_locked(&sma->sem_perm.lock);\n\t\tlist_del(&un->list_id);\n\n\t\tspin_lock(&ulp->lock);\n\t\tlist_del_rcu(&un->list_proc);\n\t\tspin_unlock(&ulp->lock);\n\n\t\t/* perform adjustments registered in un */\n\t\tfor (i = 0; i < sma->sem_nsems; i++) {\n\t\t\tstruct sem * semaphore = &sma->sem_base[i];\n\t\t\tif (un->semadj[i]) {\n\t\t\t\tsemaphore->semval += un->semadj[i];\n\t\t\t\t/*\n\t\t\t\t * Range checks of the new semaphore value,\n\t\t\t\t * not defined by sus:\n\t\t\t\t * - Some unices ignore the undo entirely\n\t\t\t\t *   (e.g. HP UX 11i 11.22, Tru64 V5.1)\n\t\t\t\t * - some cap the value (e.g. FreeBSD caps\n\t\t\t\t *   at 0, but doesn't enforce SEMVMX)\n\t\t\t\t *\n\t\t\t\t * Linux caps the semaphore value, both at 0\n\t\t\t\t * and at SEMVMX.\n\t\t\t\t *\n\t\t\t\t * \tManfred <manfred@colorfullife.com>\n\t\t\t\t */\n\t\t\t\tif (semaphore->semval < 0)\n\t\t\t\t\tsemaphore->semval = 0;\n\t\t\t\tif (semaphore->semval > SEMVMX)\n\t\t\t\t\tsemaphore->semval = SEMVMX;\n\t\t\t\tsemaphore->sempid = task_tgid_vnr(current);\n\t\t\t}\n\t\t}\n\t\t/* maybe some queued-up processes were waiting for this */\n... (function end not found)"
    },
    {
        "patch_model": "Delete",
        "change_type": "Var-Declaration",
        "line_old": 1720,
        "critical_vars": [
            "i"
        ],
        "function": "exit_sem",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "False",
        "function_code": "void exit_sem(struct task_struct *tsk)\n{\n\tstruct sem_undo_list *ulp;\n\n\tulp = tsk->sysvsem.undo_list;\n\tif (!ulp)\n\t\treturn;\n\ttsk->sysvsem.undo_list = NULL;\n\n\tif (!atomic_dec_and_test(&ulp->refcnt))\n\t\treturn;\n\n\tfor (;;) {\n\t\tstruct sem_array *sma;\n\t\tstruct sem_undo *un;\n\t\tstruct list_head tasks;\n\t\tint semid;\n\t\tint i;\n\n\t\trcu_read_lock();\n\t\tun = list_entry_rcu(ulp->list_proc.next,\n\t\t\t\t    struct sem_undo, list_proc);\n\t\tif (&un->list_proc == &ulp->list_proc)\n\t\t\tsemid = -1;\n\t\t else\n\t\t\tsemid = un->semid;\n\t\trcu_read_unlock();\n\n\t\tif (semid == -1)\n\t\t\tbreak;\n\n\t\tsma = sem_lock_check(tsk->nsproxy->ipc_ns, un->semid);\n\n\t\t/* exit_sem raced with IPC_RMID, nothing to do */\n\t\tif (IS_ERR(sma))\n\t\t\tcontinue;\n\n\t\tun = __lookup_undo(ulp, semid);\n\t\tif (un == NULL) {\n\t\t\t/* exit_sem raced with IPC_RMID+semget() that created\n\t\t\t * exactly the same semid. Nothing to do.\n\t\t\t */\n\t\t\tsem_unlock(sma);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* remove un from the linked lists */\n\t\tassert_spin_locked(&sma->sem_perm.lock);\n\t\tlist_del(&un->list_id);\n\n\t\tspin_lock(&ulp->lock);\n\t\tlist_del_rcu(&un->list_proc);\n\t\tspin_unlock(&ulp->lock);\n\n\t\t/* perform adjustments registered in un */\n\t\tfor (i = 0; i < sma->sem_nsems; i++) {\n\t\t\tstruct sem * semaphore = &sma->sem_base[i];\n\t\t\tif (un->semadj[i]) {\n\t\t\t\tsemaphore->semval += un->semadj[i];\n\t\t\t\t/*\n\t\t\t\t * Range checks of the new semaphore value,\n\t\t\t\t * not defined by sus:\n\t\t\t\t * - Some unices ignore the undo entirely\n\t\t\t\t *   (e.g. HP UX 11i 11.22, Tru64 V5.1)\n\t\t\t\t * - some cap the value (e.g. FreeBSD caps\n\t\t\t\t *   at 0, but doesn't enforce SEMVMX)\n\t\t\t\t *\n\t\t\t\t * Linux caps the semaphore value, both at 0\n\t\t\t\t * and at SEMVMX.\n\t\t\t\t *\n\t\t\t\t * \tManfred <manfred@colorfullife.com>\n\t\t\t\t */\n\t\t\t\tif (semaphore->semval < 0)\n\t\t\t\t\tsemaphore->semval = 0;\n\t\t\t\tif (semaphore->semval > SEMVMX)\n\t\t\t\t\tsemaphore->semval = SEMVMX;\n\t\t\t\tsemaphore->sempid = task_tgid_vnr(current);\n\t\t\t}\n\t\t}\n\t\t/* maybe some queued-up processes were waiting for this */\n\t\tINIT_LIST_HEAD(&tasks);\n\t\tdo_smart_update(sma, NULL, 0, 1, &tasks\n... (function end not found)"
    },
    {
        "patch_model": "Replace",
        "change_type": "Assignment",
        "line_old": 1734,
        "line_new": 1803,
        "critical_vars": [
            "sma"
        ],
        "function": "exit_sem",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "True",
        "function_code": "void exit_sem(struct task_struct *tsk)\n{\n\tstruct sem_undo_list *ulp;\n\n\tulp = tsk->sysvsem.undo_list;\n\tif (!ulp)\n\t\treturn;\n\ttsk->sysvsem.undo_list = NULL;\n\n\tif (!atomic_dec_and_test(&ulp->refcnt))\n\t\treturn;\n\n\tfor (;;) {\n\t\tstruct sem_array *sma;\n\t\tstruct sem_undo *un;\n\t\tstruct list_head tasks;\n\t\tint semid, i;\n\n\t\trcu_read_lock();\n\t\tun = list_entry_rcu(ulp->list_proc.next,\n\t\t\t\t    struct sem_undo, list_proc);\n\t\tif (&un->list_proc == &ulp->list_proc)\n\t\t\tsemid = -1;\n\t\t else\n\t\t\tsemid = un->semid;\n\n\t\tif (semid == -1) {\n\t\t\trcu_read_unlock();\n\t\t\tbreak;\n\t\t}\n\n\t\tsma = sem_obtain_object_check(tsk->nsproxy->ipc_ns, un->semid);\n\t\t/* exit_sem raced with IPC_RMID, nothing to do */\n\t\tif (IS_ERR(sma)) {\n\t\t\trcu_read_unlock();\n\t\t\tcontinue;\n\t\t}\n\n\t\tsem_lock(sma, NULL, -1);\n\t\tun = __lookup_undo(ulp, semid);\n\t\tif (un == NULL) {\n\t\t\t/* exit_sem raced with IPC_RMID+semget() that created\n\t\t\t * exactly the same semid. Nothing to do.\n\t\t\t */\n\t\t\tsem_unlock(sma, -1);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* remove un from the linked lists */\n\t\tassert_spin_locked(&sma->sem_perm.lock);\n\t\tlist_del(&un->list_id);\n\n\t\tspin_lock(&ulp->lock);\n\t\tlist_del_rcu(&un->list_proc);\n\t\tspin_unlock(&ulp->lock);\n\n\t\t/* perform adjustments registered in un */\n\t\tfor (i = 0; i < sma->sem_nsems; i++) {\n\t\t\tstruct sem * semaphore = &sma->sem_base[i];\n\t\t\tif (un->semadj[i]) {\n\t\t\t\tsemaphore->semval += un->semadj[i];\n\t\t\t\t/*\n\t\t\t\t * Range checks of the new semaphore value,\n\t\t\t\t * not defined by sus:\n\t\t\t\t * - Some unices ignore the undo entirely\n\t\t\t\t *   (e.g. HP UX 11i 11.22, Tru64 V5.1)\n\t\t\t\t * - some cap the value (e.g. FreeBSD caps\n\t\t\t\t *   at 0, but doesn't enforce SEMVMX)\n\t\t\t\t *\n\t\t\t\t * Linux caps the semaphore value, both at 0\n\t\t\t\t * and at SEMVMX.\n\t\t\t\t *\n\t\t\t\t * \tManfred <manfred@colorfullife.com>\n\t\t\t\t */\n\t\t\t\tif (semaphore->semval < 0)\n\t\t\t\t\tsemaphore->semval = 0;\n\t\t\t\tif (semaphore->semval > SEMVMX)\n\t\t\t\t\tsemaphore->semval = SEMVMX;\n\t\t\t\tsemaphore->sempid = task_tgid_vnr(current);\n\t\t\t}\n\t\t}\n\t\t/* maybe some queued-up processes were waiting for this */\n... (function end not found)"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 1810,
        "critical_vars": [
            "sma"
        ],
        "function": "exit_sem",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "True",
        "function_code": "void exit_sem(struct task_struct *tsk)\n{\n\tstruct sem_undo_list *ulp;\n\n\tulp = tsk->sysvsem.undo_list;\n\tif (!ulp)\n\t\treturn;\n\ttsk->sysvsem.undo_list = NULL;\n\n\tif (!atomic_dec_and_test(&ulp->refcnt))\n\t\treturn;\n\n\tfor (;;) {\n\t\tstruct sem_array *sma;\n\t\tstruct sem_undo *un;\n\t\tstruct list_head tasks;\n\t\tint semid, i;\n\n\t\trcu_read_lock();\n\t\tun = list_entry_rcu(ulp->list_proc.next,\n\t\t\t\t    struct sem_undo, list_proc);\n\t\tif (&un->list_proc == &ulp->list_proc)\n\t\t\tsemid = -1;\n\t\t else\n\t\t\tsemid = un->semid;\n\n\t\tif (semid == -1) {\n\t\t\trcu_read_unlock();\n\t\t\tbreak;\n\t\t}\n\n\t\tsma = sem_obtain_object_check(tsk->nsproxy->ipc_ns, un->semid);\n\t\t/* exit_sem raced with IPC_RMID, nothing to do */\n\t\tif (IS_ERR(sma)) {\n\t\t\trcu_read_unlock();\n\t\t\tcontinue;\n\t\t}\n\n\t\tsem_lock(sma, NULL, -1);\n\t\tun = __lookup_undo(ulp, semid);\n\t\tif (un == NULL) {\n\t\t\t/* exit_sem raced with IPC_RMID+semget() that created\n\t\t\t * exactly the same semid. Nothing to do.\n\t\t\t */\n\t\t\tsem_unlock(sma, -1);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* remove un from the linked lists */\n\t\tassert_spin_locked(&sma->sem_perm.lock);\n\t\tlist_del(&un->list_id);\n\n\t\tspin_lock(&ulp->lock);\n\t\tlist_del_rcu(&un->list_proc);\n\t\tspin_unlock(&ulp->lock);\n\n\t\t/* perform adjustments registered in un */\n\t\tfor (i = 0; i < sma->sem_nsems; i++) {\n\t\t\tstruct sem * semaphore = &sma->sem_base[i];\n\t\t\tif (un->semadj[i]) {\n\t\t\t\tsemaphore->semval += un->semadj[i];\n\t\t\t\t/*\n\t\t\t\t * Range checks of the new semaphore value,\n\t\t\t\t * not defined by sus:\n\t\t\t\t * - Some unices ignore the undo entirely\n\t\t\t\t *   (e.g. HP UX 11i 11.22, Tru64 V5.1)\n\t\t\t\t * - some cap the value (e.g. FreeBSD caps\n\t\t\t\t *   at 0, but doesn't enforce SEMVMX)\n\t\t\t\t *\n\t\t\t\t * Linux caps the semaphore value, both at 0\n\t\t\t\t * and at SEMVMX.\n\t\t\t\t *\n\t\t\t\t * \tManfred <manfred@colorfullife.com>\n\t\t\t\t */\n\t\t\t\tif (semaphore->semval < 0)\n\t\t\t\t\tsemaphore->semval = 0;\n\t\t\t\tif (semaphore->semval > SEMVMX)\n\t\t\t\t\tsemaphore->semval = SEMVMX;\n\t\t\t\tsemaphore->sempid = task_tgid_vnr(current);\n\t\t\t}\n\t\t}\n\t\t/* maybe some queued-up processes were waiting for this */\n... (function end not found)"
    },
    {
        "patch_model": "Replace",
        "change_type": "Fun-Call",
        "line_old": 1745,
        "line_new": 1816,
        "critical_vars": [
            "sma"
        ],
        "function": "exit_sem",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "True",
        "function_code": "void exit_sem(struct task_struct *tsk)\n{\n\tstruct sem_undo_list *ulp;\n\n\tulp = tsk->sysvsem.undo_list;\n\tif (!ulp)\n\t\treturn;\n\ttsk->sysvsem.undo_list = NULL;\n\n\tif (!atomic_dec_and_test(&ulp->refcnt))\n\t\treturn;\n\n\tfor (;;) {\n\t\tstruct sem_array *sma;\n\t\tstruct sem_undo *un;\n\t\tstruct list_head tasks;\n\t\tint semid, i;\n\n\t\trcu_read_lock();\n\t\tun = list_entry_rcu(ulp->list_proc.next,\n\t\t\t\t    struct sem_undo, list_proc);\n\t\tif (&un->list_proc == &ulp->list_proc)\n\t\t\tsemid = -1;\n\t\t else\n\t\t\tsemid = un->semid;\n\n\t\tif (semid == -1) {\n\t\t\trcu_read_unlock();\n\t\t\tbreak;\n\t\t}\n\n\t\tsma = sem_obtain_object_check(tsk->nsproxy->ipc_ns, un->semid);\n\t\t/* exit_sem raced with IPC_RMID, nothing to do */\n\t\tif (IS_ERR(sma)) {\n\t\t\trcu_read_unlock();\n\t\t\tcontinue;\n\t\t}\n\n\t\tsem_lock(sma, NULL, -1);\n\t\tun = __lookup_undo(ulp, semid);\n\t\tif (un == NULL) {\n\t\t\t/* exit_sem raced with IPC_RMID+semget() that created\n\t\t\t * exactly the same semid. Nothing to do.\n\t\t\t */\n\t\t\tsem_unlock(sma, -1);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* remove un from the linked lists */\n\t\tassert_spin_locked(&sma->sem_perm.lock);\n\t\tlist_del(&un->list_id);\n\n\t\tspin_lock(&ulp->lock);\n\t\tlist_del_rcu(&un->list_proc);\n\t\tspin_unlock(&ulp->lock);\n\n\t\t/* perform adjustments registered in un */\n\t\tfor (i = 0; i < sma->sem_nsems; i++) {\n\t\t\tstruct sem * semaphore = &sma->sem_base[i];\n\t\t\tif (un->semadj[i]) {\n\t\t\t\tsemaphore->semval += un->semadj[i];\n\t\t\t\t/*\n\t\t\t\t * Range checks of the new semaphore value,\n\t\t\t\t * not defined by sus:\n\t\t\t\t * - Some unices ignore the undo entirely\n\t\t\t\t *   (e.g. HP UX 11i 11.22, Tru64 V5.1)\n\t\t\t\t * - some cap the value (e.g. FreeBSD caps\n\t\t\t\t *   at 0, but doesn't enforce SEMVMX)\n\t\t\t\t *\n\t\t\t\t * Linux caps the semaphore value, both at 0\n\t\t\t\t * and at SEMVMX.\n\t\t\t\t *\n\t\t\t\t * \tManfred <manfred@colorfullife.com>\n\t\t\t\t */\n\t\t\t\tif (semaphore->semval < 0)\n\t\t\t\t\tsemaphore->semval = 0;\n\t\t\t\tif (semaphore->semval > SEMVMX)\n\t\t\t\t\tsemaphore->semval = SEMVMX;\n\t\t\t\tsemaphore->sempid = task_tgid_vnr(current);\n\t\t\t}\n\t\t}\n\t\t/* maybe some queued-up processes were waiting for this */\n... (function end not found)"
    },
    {
        "patch_model": "Replace",
        "change_type": "Fun-Call",
        "line_old": 1785,
        "line_new": 1856,
        "critical_vars": [
            "sma"
        ],
        "function": "exit_sem",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_sem.c.diff",
        "label": "True",
        "function_code": "void exit_sem(struct task_struct *tsk)\n{\n\tstruct sem_undo_list *ulp;\n\n\tulp = tsk->sysvsem.undo_list;\n\tif (!ulp)\n\t\treturn;\n\ttsk->sysvsem.undo_list = NULL;\n\n\tif (!atomic_dec_and_test(&ulp->refcnt))\n\t\treturn;\n\n\tfor (;;) {\n\t\tstruct sem_array *sma;\n\t\tstruct sem_undo *un;\n\t\tstruct list_head tasks;\n\t\tint semid, i;\n\n\t\trcu_read_lock();\n\t\tun = list_entry_rcu(ulp->list_proc.next,\n\t\t\t\t    struct sem_undo, list_proc);\n\t\tif (&un->list_proc == &ulp->list_proc)\n\t\t\tsemid = -1;\n\t\t else\n\t\t\tsemid = un->semid;\n\n\t\tif (semid == -1) {\n\t\t\trcu_read_unlock();\n\t\t\tbreak;\n\t\t}\n\n\t\tsma = sem_obtain_object_check(tsk->nsproxy->ipc_ns, un->semid);\n\t\t/* exit_sem raced with IPC_RMID, nothing to do */\n\t\tif (IS_ERR(sma)) {\n\t\t\trcu_read_unlock();\n\t\t\tcontinue;\n\t\t}\n\n\t\tsem_lock(sma, NULL, -1);\n\t\tun = __lookup_undo(ulp, semid);\n\t\tif (un == NULL) {\n\t\t\t/* exit_sem raced with IPC_RMID+semget() that created\n\t\t\t * exactly the same semid. Nothing to do.\n\t\t\t */\n\t\t\tsem_unlock(sma, -1);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* remove un from the linked lists */\n\t\tassert_spin_locked(&sma->sem_perm.lock);\n\t\tlist_del(&un->list_id);\n\n\t\tspin_lock(&ulp->lock);\n\t\tlist_del_rcu(&un->list_proc);\n\t\tspin_unlock(&ulp->lock);\n\n\t\t/* perform adjustments registered in un */\n\t\tfor (i = 0; i < sma->sem_nsems; i++) {\n\t\t\tstruct sem * semaphore = &sma->sem_base[i];\n\t\t\tif (un->semadj[i]) {\n\t\t\t\tsemaphore->semval += un->semadj[i];\n\t\t\t\t/*\n\t\t\t\t * Range checks of the new semaphore value,\n\t\t\t\t * not defined by sus:\n\t\t\t\t * - Some unices ignore the undo entirely\n\t\t\t\t *   (e.g. HP UX 11i 11.22, Tru64 V5.1)\n\t\t\t\t * - some cap the value (e.g. FreeBSD caps\n\t\t\t\t *   at 0, but doesn't enforce SEMVMX)\n\t\t\t\t *\n\t\t\t\t * Linux caps the semaphore value, both at 0\n\t\t\t\t * and at SEMVMX.\n\t\t\t\t *\n\t\t\t\t * \tManfred <manfred@colorfullife.com>\n\t\t\t\t */\n\t\t\t\tif (semaphore->semval < 0)\n\t\t\t\t\tsemaphore->semval = 0;\n\t\t\t\tif (semaphore->semval > SEMVMX)\n\t\t\t\t\tsemaphore->semval = SEMVMX;\n\t\t\t\tsemaphore->sempid = task_tgid_vnr(current);\n\t\t\t}\n\t\t}\n\t\t/* maybe some queued-up processes were waiting for this */\n... (function end not found)"
    },
    {
        "patch_model": "Replace",
        "change_type": "Var-Declaration",
        "line_old": 481,
        "line_new": 481,
        "critical_vars": [
            "refcount"
        ],
        "function": "ipc_free",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_util.c.diff",
        "label": "True",
        "function_code": "\nvoid ipc_free(void* ptr, int size)\n{\n\tif(size > PAGE_SIZE)\n\t\tvfree(ptr);\n\telse\n\t\tkfree(ptr);\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Var-Declaration",
        "line_old": 525,
        "critical_vars": [
            "out"
        ],
        "function": "rcu_use_vmalloc",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_util.c.diff",
        "label": "False",
        "function_code": "\nstatic inline int rcu_use_vmalloc(int size)\n{\n\t/* Too big for a single page? */\n\tif (HDRLEN_KMALLOC + size > PAGE_SIZE)\n\t\treturn 1;\n\treturn 0;\n}"
    },
    {
        "patch_model": "Replace",
        "change_type": "if-Condition",
        "line_old": 532,
        "line_new": 531,
        "critical_vars": [
            "out"
        ],
        "function": "rcu_use_vmalloc",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_util.c.diff",
        "label": "True",
        "function_code": "\nstatic inline int rcu_use_vmalloc(int size)\n{\n\t/* Too big for a single page? */\n\tif (HDRLEN_KMALLOC + size > PAGE_SIZE)\n\t\treturn 1;\n\treturn 0;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 533,
        "critical_vars": [
            "out"
        ],
        "function": "rcu_use_vmalloc",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_util.c.diff",
        "label": "False",
        "function_code": "\nstatic inline int rcu_use_vmalloc(int size)\n{\n\t/* Too big for a single page? */\n\tif (HDRLEN_KMALLOC + size > PAGE_SIZE)\n\t\treturn 1;\n\treturn 0;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 534,
        "critical_vars": [
            "ipc_rcu_hdr",
            "data",
            "out"
        ],
        "function": "rcu_use_vmalloc",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_util.c.diff",
        "label": "False",
        "function_code": "\nstatic inline int rcu_use_vmalloc(int size)\n{\n\t/* Too big for a single page? */\n\tif (HDRLEN_KMALLOC + size > PAGE_SIZE)\n\t\treturn 1;\n\treturn 0;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 534,
        "critical_vars": [
            "out"
        ],
        "function": "rcu_use_vmalloc",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_util.c.diff",
        "label": "True",
        "function_code": "\nstatic inline int rcu_use_vmalloc(int size)\n{\n\t/* Too big for a single page? */\n\tif (HDRLEN_KMALLOC + size > PAGE_SIZE)\n\t\treturn 1;\n\treturn 0;\n}"
    },
    {
        "patch_model": "Replace",
        "change_type": "Assignment",
        "line_old": 535,
        "line_new": 535,
        "critical_vars": [
            "ipc_rcu_hdr",
            "data",
            "out"
        ],
        "function": "rcu_use_vmalloc",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_util.c.diff",
        "label": "True",
        "function_code": "\nstatic inline int rcu_use_vmalloc(int size)\n{\n\t/* Too big for a single page? */\n\tif (HDRLEN_KMALLOC + size > PAGE_SIZE)\n\t\treturn 1;\n\treturn 0;\n}"
    },
    {
        "patch_model": "Replace",
        "change_type": "if-Condition",
        "line_old": 539,
        "line_new": 538,
        "critical_vars": [
            "out"
        ],
        "function": "rcu_use_vmalloc",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_util.c.diff",
        "label": "True",
        "function_code": "\nstatic inline int rcu_use_vmalloc(int size)\n{\n\t/* Too big for a single page? */\n\tif (HDRLEN_KMALLOC + size > PAGE_SIZE)\n\t\treturn 1;\n\treturn 0;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 540,
        "critical_vars": [
            "out"
        ],
        "function": "rcu_use_vmalloc",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_util.c.diff",
        "label": "False",
        "function_code": "\nstatic inline int rcu_use_vmalloc(int size)\n{\n\t/* Too big for a single page? */\n\tif (HDRLEN_KMALLOC + size > PAGE_SIZE)\n\t\treturn 1;\n\treturn 0;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 541,
        "critical_vars": [
            "ipc_rcu_hdr",
            "data",
            "out"
        ],
        "function": "rcu_use_vmalloc",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_util.c.diff",
        "label": "False",
        "function_code": "\nstatic inline int rcu_use_vmalloc(int size)\n{\n\t/* Too big for a single page? */\n\tif (HDRLEN_KMALLOC + size > PAGE_SIZE)\n\t\treturn 1;\n\treturn 0;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 541,
        "critical_vars": [
            "out"
        ],
        "function": "rcu_use_vmalloc",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_util.c.diff",
        "label": "True",
        "function_code": "\nstatic inline int rcu_use_vmalloc(int size)\n{\n\t/* Too big for a single page? */\n\tif (HDRLEN_KMALLOC + size > PAGE_SIZE)\n\t\treturn 1;\n\treturn 0;\n}"
    },
    {
        "patch_model": "Replace",
        "change_type": "Assignment",
        "line_old": 542,
        "line_new": 542,
        "critical_vars": [
            "ipc_rcu_hdr",
            "data",
            "out"
        ],
        "function": "rcu_use_vmalloc",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_util.c.diff",
        "label": "True",
        "function_code": "\nstatic inline int rcu_use_vmalloc(int size)\n{\n\t/* Too big for a single page? */\n\tif (HDRLEN_KMALLOC + size > PAGE_SIZE)\n\t\treturn 1;\n\treturn 0;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 546,
        "critical_vars": [
            "ipc_rcu_hdr",
            "out"
        ],
        "function": "rcu_use_vmalloc",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_util.c.diff",
        "label": "True",
        "function_code": "\nstatic inline int rcu_use_vmalloc(int size)\n{\n\t/* Too big for a single page? */\n\tif (HDRLEN_KMALLOC + size > PAGE_SIZE)\n\t\treturn 1;\n\treturn 0;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Fun-Call",
        "line_old": 549,
        "critical_vars": [
            "ptr"
        ],
        "function": "rcu_use_vmalloc",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_util.c.diff",
        "label": "False",
        "function_code": "\nstatic inline int rcu_use_vmalloc(int size)\n{\n\t/* Too big for a single page? */\n\tif (HDRLEN_KMALLOC + size > PAGE_SIZE)\n\t\treturn 1;\n\treturn 0;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Head",
        "line_new": 551,
        "critical_vars": [
            "*ptr"
        ],
        "function": "rcu_use_vmalloc",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_util.c.diff",
        "label": "True",
        "function_code": "\nstatic inline int rcu_use_vmalloc(int size)\n{\n\t/* Too big for a single page? */\n\tif (HDRLEN_KMALLOC + size > PAGE_SIZE)\n\t\treturn 1;\n\treturn 0;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 551,
        "critical_vars": [
            "container_of(ptr,structipc_rcu_hdr,data)->refcount"
        ],
        "function": "rcu_use_vmalloc",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_util.c.diff",
        "label": "False",
        "function_code": "\nstatic inline int rcu_use_vmalloc(int size)\n{\n\t/* Too big for a single page? */\n\tif (HDRLEN_KMALLOC + size > PAGE_SIZE)\n\t\treturn 1;\n\treturn 0;\n}"
    },
    {
        "patch_model": "Replace",
        "change_type": "if-Condition",
        "line_old": 581,
        "line_new": 583,
        "critical_vars": [
            "ipc_rcu_hdr",
            "ptr",
            "data"
        ],
        "function": "ipc_schedule_free",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_util.c.diff",
        "label": "True",
        "function_code": "static void ipc_schedule_free(struct rcu_head *head)\n{\n\tstruct ipc_rcu_grace *grace;\n\tstruct ipc_rcu_sched *sched;\n\n\tgrace = container_of(head, struct ipc_rcu_grace, rcu);\n\tsched = container_of(&(grace->data[0]), struct ipc_rcu_sched,\n\t\t\t\tdata[0]);\n\n\tINIT_WORK(&sched->work, ipc_do_vfree);\n\tschedule_work(&sched->work);\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Fun-Call",
        "line_old": 690,
        "critical_vars": [
            "msq"
        ],
        "function": "do_msgsnd",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_msg.c.diff",
        "label": "False",
        "function_code": "\nlong do_msgsnd(int msqid, long mtype, void __user *mtext,\n\t\tsize_t msgsz, int msgflg)\n{\n\tstruct msg_queue *msq;\n\tstruct msg_msg *msg;\n\tint err;\n\tstruct ipc_namespace *ns;\n\n\tns = current->nsproxy->ipc_ns;\n\n\tif (msgsz > ns->msg_ctlmax || (long) msgsz < 0 || msqid < 0)\n\t\treturn -EINVAL;\n\tif (mtype < 1)\n\t\treturn -EINVAL;\n\n\tmsg = load_msg(mtext, msgsz);\n\tif (IS_ERR(msg))\n\t\treturn PTR_ERR(msg);\n\n\tmsg->m_type = mtype;\n\tmsg->m_ts = msgsz;\n\n\tmsq = msg_lock_check(ns, msqid);\n\tif (IS_ERR(msq)) {\n\t\terr = PTR_ERR(msq);\n\t\tgoto out_free;\n\t}\n\n\tfor (;;) {\n\t\tstruct msg_sender s;\n\n\t\terr = -EACCES;\n\t\tif (ipcperms(ns, &msq->q_perm, S_IWUGO))\n\t\t\tgoto out_unlock_free;\n\n\t\terr = security_msg_queue_msgsnd(msq, msg, msgflg);\n\t\tif (err)\n\t\t\tgoto out_unlock_free;\n\n\t\tif (msgsz + msq->q_cbytes <= msq->q_qbytes &&\n\t\t\t\t1 + msq->q_qnum <= msq->q_qbytes) {\n\t\t\tbreak;\n\t\t}\n\n\t\t/* queue full, wait: */\n\t\tif (msgflg & IPC_NOWAIT) {\n\t\t\terr = -EAGAIN;\n\t\t\tgoto out_unlock_free;\n\t\t}\n\t\tss_add(msq, &s);\n\t\tipc_rcu_getref(msq);\n\t\tmsg_unlock(msq);\n\t\tschedule();\n\n\t\tipc_lock_by_ptr(&msq->q_perm);\n\t\tipc_rcu_putref(msq);\n\t\tif (msq->q_perm.deleted) {\n\t\t\terr = -EIDRM;\n\t\t\tgoto out_unlock_free;\n\t\t}\n\t\tss_del(&s);\n\n\t\tif (signal_pending(current)) {\n\t\t\terr = -ERESTARTNOHAND;\n\t\t\tgoto out_unlock_free;\n\t\t}\n\t}\n\n\tmsq->q_lspid = task_tgid_vnr(current);\n\tmsq->q_stime = get_seconds();\n\n\tif (!pipelined_send(msq, msg)) {\n\t\t/* no one is waiting for this message, enqueue it */\n\t\tlist_add_tail(&msg->m_list, &msq->q_messages);\n\t\tmsq->q_cbytes += msgsz;\n\t\tmsq->q_qnum++;\n\t\tatomic_add(msgsz, &ns->msg_bytes);\n\t\tatomic_inc(&ns->msg_hdrs);\n\t}\n\n\terr = 0;\n\tmsg = NULL;\n\nout_unlock_free:\n\tmsg_unlock(msq);\nout_free:\n\tif (msg != NULL)\n\t\tfree_msg(msg);\n\treturn err;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 691,
        "critical_vars": [
            "msq"
        ],
        "function": "do_msgsnd",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_msg.c.diff",
        "label": "True",
        "function_code": "\nlong do_msgsnd(int msqid, long mtype, void __user *mtext,\n\t\tsize_t msgsz, int msgflg)\n{\n\tstruct msg_queue *msq;\n\tstruct msg_msg *msg;\n\tint err;\n\tstruct ipc_namespace *ns;\n\n\tns = current->nsproxy->ipc_ns;\n\n\tif (msgsz > ns->msg_ctlmax || (long) msgsz < 0 || msqid < 0)\n\t\treturn -EINVAL;\n\tif (mtype < 1)\n\t\treturn -EINVAL;\n\n\tmsg = load_msg(mtext, msgsz);\n\tif (IS_ERR(msg))\n\t\treturn PTR_ERR(msg);\n\n\tmsg->m_type = mtype;\n\tmsg->m_ts = msgsz;\n\n\tmsq = msg_lock_check(ns, msqid);\n\tif (IS_ERR(msq)) {\n\t\terr = PTR_ERR(msq);\n\t\tgoto out_free;\n\t}\n\n\tfor (;;) {\n\t\tstruct msg_sender s;\n\n\t\terr = -EACCES;\n\t\tif (ipcperms(ns, &msq->q_perm, S_IWUGO))\n\t\t\tgoto out_unlock_free;\n\n\t\terr = security_msg_queue_msgsnd(msq, msg, msgflg);\n\t\tif (err)\n\t\t\tgoto out_unlock_free;\n\n\t\tif (msgsz + msq->q_cbytes <= msq->q_qbytes &&\n\t\t\t\t1 + msq->q_qnum <= msq->q_qbytes) {\n\t\t\tbreak;\n\t\t}\n\n\t\t/* queue full, wait: */\n\t\tif (msgflg & IPC_NOWAIT) {\n\t\t\terr = -EAGAIN;\n\t\t\tgoto out_unlock_free;\n\t\t}\n\t\tss_add(msq, &s);\n\n\t\tif (!ipc_rcu_getref(msq)) {\n\t\t\terr = -EIDRM;\n\t\t\tgoto out_unlock_free;\n\t\t}\n\n\t\tmsg_unlock(msq);\n\t\tschedule();\n\n\t\tipc_lock_by_ptr(&msq->q_perm);\n\t\tipc_rcu_putref(msq);\n\t\tif (msq->q_perm.deleted) {\n\t\t\terr = -EIDRM;\n\t\t\tgoto out_unlock_free;\n\t\t}\n\t\tss_del(&s);\n\n\t\tif (signal_pending(current)) {\n\t\t\terr = -ERESTARTNOHAND;\n\t\t\tgoto out_unlock_free;\n\t\t}\n\t}\n\n\tmsq->q_lspid = task_tgid_vnr(current);\n\tmsq->q_stime = get_seconds();\n\n\tif (!pipelined_send(msq, msg)) {\n\t\t/* no one is waiting for this message, enqueue it */\n\t\tlist_add_tail(&msg->m_list, &msq->q_messages);\n\t\tmsq->q_cbytes += msgsz;\n\t\tmsq->q_qnum++;\n\t\tatomic_add(msgsz, &ns->msg_bytes);\n\t\tatomic_inc(&ns->msg_hdrs);\n\t}\n\n\terr = 0;\n\tmsg = NULL;\n\nout_unlock_free:\n\tmsg_unlock(msq);\nout_free:\n\tif (msg != NULL)\n\t\tfree_msg(msg);\n\treturn err;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 692,
        "critical_vars": [
            "err"
        ],
        "function": "do_msgsnd",
        "filename": "linux/CVE-2013-4483/CVE-2013-4483_CWE-189_6062a8dc0517bce23e3c2f7d2fea5e22411269a3_msg.c.diff",
        "label": "True",
        "function_code": "\nlong do_msgsnd(int msqid, long mtype, void __user *mtext,\n\t\tsize_t msgsz, int msgflg)\n{\n\tstruct msg_queue *msq;\n\tstruct msg_msg *msg;\n\tint err;\n\tstruct ipc_namespace *ns;\n\n\tns = current->nsproxy->ipc_ns;\n\n\tif (msgsz > ns->msg_ctlmax || (long) msgsz < 0 || msqid < 0)\n\t\treturn -EINVAL;\n\tif (mtype < 1)\n\t\treturn -EINVAL;\n\n\tmsg = load_msg(mtext, msgsz);\n\tif (IS_ERR(msg))\n\t\treturn PTR_ERR(msg);\n\n\tmsg->m_type = mtype;\n\tmsg->m_ts = msgsz;\n\n\tmsq = msg_lock_check(ns, msqid);\n\tif (IS_ERR(msq)) {\n\t\terr = PTR_ERR(msq);\n\t\tgoto out_free;\n\t}\n\n\tfor (;;) {\n\t\tstruct msg_sender s;\n\n\t\terr = -EACCES;\n\t\tif (ipcperms(ns, &msq->q_perm, S_IWUGO))\n\t\t\tgoto out_unlock_free;\n\n\t\terr = security_msg_queue_msgsnd(msq, msg, msgflg);\n\t\tif (err)\n\t\t\tgoto out_unlock_free;\n\n\t\tif (msgsz + msq->q_cbytes <= msq->q_qbytes &&\n\t\t\t\t1 + msq->q_qnum <= msq->q_qbytes) {\n\t\t\tbreak;\n\t\t}\n\n\t\t/* queue full, wait: */\n\t\tif (msgflg & IPC_NOWAIT) {\n\t\t\terr = -EAGAIN;\n\t\t\tgoto out_unlock_free;\n\t\t}\n\t\tss_add(msq, &s);\n\n\t\tif (!ipc_rcu_getref(msq)) {\n\t\t\terr = -EIDRM;\n\t\t\tgoto out_unlock_free;\n\t\t}\n\n\t\tmsg_unlock(msq);\n\t\tschedule();\n\n\t\tipc_lock_by_ptr(&msq->q_perm);\n\t\tipc_rcu_putref(msq);\n\t\tif (msq->q_perm.deleted) {\n\t\t\terr = -EIDRM;\n\t\t\tgoto out_unlock_free;\n\t\t}\n\t\tss_del(&s);\n\n\t\tif (signal_pending(current)) {\n\t\t\terr = -ERESTARTNOHAND;\n\t\t\tgoto out_unlock_free;\n\t\t}\n\t}\n\n\tmsq->q_lspid = task_tgid_vnr(current);\n\tmsq->q_stime = get_seconds();\n\n\tif (!pipelined_send(msq, msg)) {\n\t\t/* no one is waiting for this message, enqueue it */\n\t\tlist_add_tail(&msg->m_list, &msq->q_messages);\n\t\tmsq->q_cbytes += msgsz;\n\t\tmsq->q_qnum++;\n\t\tatomic_add(msgsz, &ns->msg_bytes);\n\t\tatomic_inc(&ns->msg_hdrs);\n\t}\n\n\terr = 0;\n\tmsg = NULL;\n\nout_unlock_free:\n\tmsg_unlock(msq);\nout_free:\n\tif (msg != NULL)\n\t\tfree_msg(msg);\n\treturn err;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 1272,
        "critical_vars": [
            "val",
            "long"
        ],
        "function": "perf_instruction_pointer",
        "filename": "linux/CVE-2011-4611/CVE-2011-4611_CWE-189_0837e3242c73566fc1c0196b4ec61779c25ffc93_perf_event.c.diff",
        "label": "True",
        "function_code": "unsigned long perf_instruction_pointer(struct pt_regs *regs)\n{\n\tunsigned long ip;\n\n\tif (TRAP(regs) != 0xf00)\n\t\treturn regs->nip;\t/* not a PMU interrupt */\n\n\tip = mfspr(SPRN_SIAR) + perf_ip_adjust(regs);\n\treturn ip;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 1341,
        "critical_vars": [
            "val"
        ],
        "function": "perf_event_interrupt",
        "filename": "linux/CVE-2011-4611/CVE-2011-4611_CWE-189_0837e3242c73566fc1c0196b4ec61779c25ffc93_perf_event.c.diff",
        "label": "True",
        "function_code": "\nstatic void perf_event_interrupt(struct pt_regs *regs);\n\nvoid perf_event_print_debug(void)\n{\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 327,
        "critical_vars": [
            "_copy_from_pages"
        ],
        "function": "_copy_from_pages",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_xdr.c.diff",
        "label": "True",
        "function_code": "void\n_copy_from_pages(char *p, struct page **pages, size_t pgbase, size_t len)\n{\n\tstruct page **pgfrom;\n\tchar *vfrom;\n\tsize_t copy;\n\n\tpgfrom = pages + (pgbase >> PAGE_CACHE_SHIFT);\n\tpgbase &= ~PAGE_CACHE_MASK;\n\n\tdo {\n\t\tcopy = PAGE_CACHE_SIZE - pgbase;\n\t\tif (copy > len)\n\t\t\tcopy = len;\n\n\t\tvfrom = kmap_atomic(*pgfrom, KM_USER0);\n\t\tmemcpy(p, vfrom + pgbase, copy);\n\t\tkunmap_atomic(vfrom, KM_USER0);\n\n\t\tpgbase += copy;\n\t\tif (pgbase == PAGE_CACHE_SIZE) {\n\t\t\tpgbase = 0;\n\t\t\tpgfrom++;\n\t\t}\n\t\tp += copy;\n\n\t} while ((len -= copy) != 0);\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Fun-Call",
        "line_old": 3429,
        "critical_vars": [
            "buflen",
            "pgbase",
            "buf",
            "page",
            "pages"
        ],
        "function": "nfs4_server_supports_acls",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "False",
        "function_code": "\nstatic inline int nfs4_server_supports_acls(struct nfs_server *server)\n{\n\treturn (server->caps & NFS_CAP_ACLS)\n\t\t&& (server->acl_bitmask & ACL4_SUPPORT_ALLOW_ACL)\n\t\t&& (server->acl_bitmask & ACL4_SUPPORT_DENY_ACL);\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 3432,
        "critical_vars": [
            "*p"
        ],
        "function": "nfs4_server_supports_acls",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "False",
        "function_code": "\nstatic inline int nfs4_server_supports_acls(struct nfs_server *server)\n{\n\treturn (server->caps & NFS_CAP_ACLS)\n\t\t&& (server->acl_bitmask & ACL4_SUPPORT_ALLOW_ACL)\n\t\t&& (server->acl_bitmask & ACL4_SUPPORT_DENY_ACL);\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 3434,
        "critical_vars": [
            "*pgbase"
        ],
        "function": "nfs4_server_supports_acls",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "False",
        "function_code": "\nstatic inline int nfs4_server_supports_acls(struct nfs_server *server)\n{\n\treturn (server->caps & NFS_CAP_ACLS)\n\t\t&& (server->acl_bitmask & ACL4_SUPPORT_ALLOW_ACL)\n\t\t&& (server->acl_bitmask & ACL4_SUPPORT_DENY_ACL);\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 3435,
        "critical_vars": [
            "p"
        ],
        "function": "nfs4_server_supports_acls",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "False",
        "function_code": "\nstatic inline int nfs4_server_supports_acls(struct nfs_server *server)\n{\n\treturn (server->caps & NFS_CAP_ACLS)\n\t\t&& (server->acl_bitmask & ACL4_SUPPORT_ALLOW_ACL)\n\t\t&& (server->acl_bitmask & ACL4_SUPPORT_DENY_ACL);\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "while-Condition",
        "line_old": 3436,
        "critical_vars": [
            "p"
        ],
        "function": "nfs4_server_supports_acls",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "False",
        "function_code": "\nstatic inline int nfs4_server_supports_acls(struct nfs_server *server)\n{\n\treturn (server->caps & NFS_CAP_ACLS)\n\t\t&& (server->acl_bitmask & ACL4_SUPPORT_ALLOW_ACL)\n\t\t&& (server->acl_bitmask & ACL4_SUPPORT_DENY_ACL);\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 3438,
        "critical_vars": [
            "p"
        ],
        "function": "nfs4_server_supports_acls",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "False",
        "function_code": "\nstatic inline int nfs4_server_supports_acls(struct nfs_server *server)\n{\n\treturn (server->caps & NFS_CAP_ACLS)\n\t\t&& (server->acl_bitmask & ACL4_SUPPORT_ALLOW_ACL)\n\t\t&& (server->acl_bitmask & ACL4_SUPPORT_DENY_ACL);\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Var-Declaration",
        "line_old": 3540,
        "critical_vars": [
            "*pages"
        ],
        "function": "nfs4_write_cached_acl",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "False",
        "function_code": "\nstatic void nfs4_write_cached_acl(struct inode *inode, const char *buf, size_t acl_len)\n{\n\tstruct nfs4_cached_acl *acl;\n\n\tif (buf && acl_len <= PAGE_SIZE) {\n\t\tacl = kmalloc(sizeof(*acl) + acl_len, GFP_KERNEL);\n\t\tif (acl == NULL)\n\t\t\tgoto out;\n\t\tacl->cached = 1;\n\t\tmemcpy(acl->data, buf, acl_len);\n\t} else {\n\t\tacl = kmalloc(sizeof(*acl), GFP_KERNEL);\n\t\tif (acl == NULL)\n\t\t\tgoto out;\n\t\tacl->cached = 0;\n\t}\n\tacl->len = acl_len;\nout:\n\tnfs4_set_cached_acl(inode, acl);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 3537,
        "critical_vars": [
            "structpage*pages[NFS4ACL_MAXPAGES]"
        ],
        "function": "nfs4_write_cached_acl",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "True",
        "function_code": "\nstatic void nfs4_write_cached_acl(struct inode *inode, const char *buf, size_t acl_len)\n{\n\tstruct nfs4_cached_acl *acl;\n\n\tif (buf && acl_len <= PAGE_SIZE) {\n\t\tacl = kmalloc(sizeof(*acl) + acl_len, GFP_KERNEL);\n\t\tif (acl == NULL)\n\t\t\tgoto out;\n\t\tacl->cached = 1;\n\t\tmemcpy(acl->data, buf, acl_len);\n\t} else {\n\t\tacl = kmalloc(sizeof(*acl), GFP_KERNEL);\n\t\tif (acl == NULL)\n\t\t\tgoto out;\n\t\tacl->cached = 0;\n\t}\n\tacl->len = acl_len;\nout:\n\tnfs4_set_cached_acl(inode, acl);\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 3555,
        "critical_vars": [
            "*localpage"
        ],
        "function": "__nfs4_get_acl_uncached",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "False",
        "function_code": "\nstatic ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)\n{\n\tstruct page *pages[NFS4ACL_MAXPAGES];\n\tstruct nfs_getaclargs args = {\n\t\t.fh = NFS_FH(inode),\n\t\t.acl_pages = pages,\n\t\t.acl_len = buflen,\n\t};\n\tstruct nfs_getaclres res = {\n\t\t.acl_len = buflen,\n\t};\n\tvoid *resp_buf;\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &nfs4_procedures[NFSPROC4_CLNT_GETACL],\n\t\t.rpc_argp = &args,\n\t\t.rpc_resp = &res,\n\t};\n\tstruct page *localpage = NULL;\n\tint ret;\n\n\tif (buflen < PAGE_SIZE) {\n\t\t/* As long as we're doing a round trip to the server anyway,\n\t\t * let's be prepared for a page of acl data. */\n\t\tlocalpage = alloc_page(GFP_KERNEL);\n\t\tresp_buf = page_address(localpage);\n\t\tif (localpage == NULL)\n\t\t\treturn -ENOMEM;\n\t\targs.acl_pages[0] = localpage;\n\t\targs.acl_pgbase = 0;\n\t\targs.acl_len = PAGE_SIZE;\n\t} else {\n\t\tresp_buf = buf;\n\t\tbuf_to_pages(buf, buflen, args.acl_pages, &args.acl_pgbase);\n\t}\n\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode), &msg, &args.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\tgoto out_free;\n\tif (res.acl_len > args.acl_len)\n\t\tnfs4_write_cached_acl(inode, NULL, res.acl_len);\n\telse\n\t\tnfs4_write_cached_acl(inode, resp_buf, res.acl_len);\n\tif (buf) {\n\t\tret = -ERANGE;\n\t\tif (res.acl_len > buflen)\n\t\t\tgoto out_free;\n\t\tif (localpage)\n\t\t\tmemcpy(buf, resp_buf, res.acl_len);\n\t}\n\tret = res.acl_len;\nout_free:\n\tif (localpage)\n\t\t__free_page(localpage);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 3552,
        "critical_vars": [
            "ret"
        ],
        "function": "__nfs4_get_acl_uncached",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "True",
        "function_code": "static ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)\n{\n\tstruct page *pages[NFS4ACL_MAXPAGES] = {NULL, };\n\tstruct nfs_getaclargs args = {\n\t\t.fh = NFS_FH(inode),\n\t\t.acl_pages = pages,\n\t\t.acl_len = buflen,\n\t};\n\tstruct nfs_getaclres res = {\n\t\t.acl_len = buflen,\n\t};\n\tvoid *resp_buf;\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &nfs4_procedures[NFSPROC4_CLNT_GETACL],\n\t\t.rpc_argp = &args,\n\t\t.rpc_resp = &res,\n\t};\n\tint ret = -ENOMEM, npages, i, acl_len = 0;\n\n\tnpages = (buflen + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t/* As long as we're doing a round trip to the server anyway,\n\t * let's be prepared for a page of acl data. */\n\tif (npages == 0)\n\t\tnpages = 1;\n\n\tfor (i = 0; i < npages; i++) {\n\t\tpages[i] = alloc_page(GFP_KERNEL);\n\t\tif (!pages[i])\n\t\t\tgoto out_free;\n\t}\n\tif (npages > 1) {\n\t\t/* for decoding across pages */\n\t\targs.acl_scratch = alloc_page(GFP_KERNEL);\n\t\tif (!args.acl_scratch)\n\t\t\tgoto out_free;\n\t}\n\targs.acl_len = npages * PAGE_SIZE;\n\targs.acl_pgbase = 0;\n\t/* Let decode_getfacl know not to fail if the ACL data is larger than\n\t * the page we send as a guess */\n\tif (buf == NULL)\n\t\tres.acl_flags |= NFS4_ACL_LEN_REQUEST;\n\tresp_buf = page_address(pages[0]);\n\n\tdprintk(\"%s  buf %p buflen %ld npages %d args.acl_len %ld\\n\",\n\t\t__func__, buf, buflen, npages, args.acl_len);\n\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode),\n\t\t\t     &msg, &args.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\tgoto out_free;\n\n\tacl_len = res.acl_len - res.acl_data_offset;\n\tif (acl_len > args.acl_len)\n\t\tnfs4_write_cached_acl(inode, NULL, acl_len);\n\telse\n\t\tnfs4_write_cached_acl(inode, resp_buf + res.acl_data_offset,\n\t\t\t\t      acl_len);\n\tif (buf) {\n\t\tret = -ERANGE;\n\t\tif (acl_len > buflen)\n\t\t\tgoto out_free;\n\t\t_copy_from_pages(buf, pages, res.acl_data_offset,\n\t\t\t\tres.acl_len);\n\t}\n\tret = acl_len;\nout_free:\n\tfor (i = 0; i < npages; i++)\n\t\tif (pages[i])\n\t\t\t__free_page(pages[i]);\n\tif (args.acl_scratch)\n\t\t__free_page(args.acl_scratch);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Var-Declaration",
        "line_old": 3556,
        "critical_vars": [
            "ret"
        ],
        "function": "__nfs4_get_acl_uncached",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "False",
        "function_code": "\nstatic ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)\n{\n\tstruct page *pages[NFS4ACL_MAXPAGES];\n\tstruct nfs_getaclargs args = {\n\t\t.fh = NFS_FH(inode),\n\t\t.acl_pages = pages,\n\t\t.acl_len = buflen,\n\t};\n\tstruct nfs_getaclres res = {\n\t\t.acl_len = buflen,\n\t};\n\tvoid *resp_buf;\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &nfs4_procedures[NFSPROC4_CLNT_GETACL],\n\t\t.rpc_argp = &args,\n\t\t.rpc_resp = &res,\n\t};\n\tstruct page *localpage = NULL;\n\tint ret;\n\n\tif (buflen < PAGE_SIZE) {\n\t\t/* As long as we're doing a round trip to the server anyway,\n\t\t * let's be prepared for a page of acl data. */\n\t\tlocalpage = alloc_page(GFP_KERNEL);\n\t\tresp_buf = page_address(localpage);\n\t\tif (localpage == NULL)\n\t\t\treturn -ENOMEM;\n\t\targs.acl_pages[0] = localpage;\n\t\targs.acl_pgbase = 0;\n\t\targs.acl_len = PAGE_SIZE;\n\t} else {\n\t\tresp_buf = buf;\n\t\tbuf_to_pages(buf, buflen, args.acl_pages, &args.acl_pgbase);\n\t}\n\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode), &msg, &args.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\tgoto out_free;\n\tif (res.acl_len > args.acl_len)\n\t\tnfs4_write_cached_acl(inode, NULL, res.acl_len);\n\telse\n\t\tnfs4_write_cached_acl(inode, resp_buf, res.acl_len);\n\tif (buf) {\n\t\tret = -ERANGE;\n\t\tif (res.acl_len > buflen)\n\t\t\tgoto out_free;\n\t\tif (localpage)\n\t\t\tmemcpy(buf, resp_buf, res.acl_len);\n\t}\n\tret = res.acl_len;\nout_free:\n\tif (localpage)\n\t\t__free_page(localpage);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "if-Condition",
        "line_old": 3558,
        "critical_vars": [
            "buflen"
        ],
        "function": "__nfs4_get_acl_uncached",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "False",
        "function_code": "\nstatic ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)\n{\n\tstruct page *pages[NFS4ACL_MAXPAGES];\n\tstruct nfs_getaclargs args = {\n\t\t.fh = NFS_FH(inode),\n\t\t.acl_pages = pages,\n\t\t.acl_len = buflen,\n\t};\n\tstruct nfs_getaclres res = {\n\t\t.acl_len = buflen,\n\t};\n\tvoid *resp_buf;\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &nfs4_procedures[NFSPROC4_CLNT_GETACL],\n\t\t.rpc_argp = &args,\n\t\t.rpc_resp = &res,\n\t};\n\tstruct page *localpage = NULL;\n\tint ret;\n\n\tif (buflen < PAGE_SIZE) {\n\t\t/* As long as we're doing a round trip to the server anyway,\n\t\t * let's be prepared for a page of acl data. */\n\t\tlocalpage = alloc_page(GFP_KERNEL);\n\t\tresp_buf = page_address(localpage);\n\t\tif (localpage == NULL)\n\t\t\treturn -ENOMEM;\n\t\targs.acl_pages[0] = localpage;\n\t\targs.acl_pgbase = 0;\n\t\targs.acl_len = PAGE_SIZE;\n\t} else {\n\t\tresp_buf = buf;\n\t\tbuf_to_pages(buf, buflen, args.acl_pages, &args.acl_pgbase);\n\t}\n\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode), &msg, &args.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\tgoto out_free;\n\tif (res.acl_len > args.acl_len)\n\t\tnfs4_write_cached_acl(inode, NULL, res.acl_len);\n\telse\n\t\tnfs4_write_cached_acl(inode, resp_buf, res.acl_len);\n\tif (buf) {\n\t\tret = -ERANGE;\n\t\tif (res.acl_len > buflen)\n\t\t\tgoto out_free;\n\t\tif (localpage)\n\t\t\tmemcpy(buf, resp_buf, res.acl_len);\n\t}\n\tret = res.acl_len;\nout_free:\n\tif (localpage)\n\t\t__free_page(localpage);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 3554,
        "critical_vars": [
            "npages"
        ],
        "function": "__nfs4_get_acl_uncached",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "True",
        "function_code": "static ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)\n{\n\tstruct page *pages[NFS4ACL_MAXPAGES] = {NULL, };\n\tstruct nfs_getaclargs args = {\n\t\t.fh = NFS_FH(inode),\n\t\t.acl_pages = pages,\n\t\t.acl_len = buflen,\n\t};\n\tstruct nfs_getaclres res = {\n\t\t.acl_len = buflen,\n\t};\n\tvoid *resp_buf;\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &nfs4_procedures[NFSPROC4_CLNT_GETACL],\n\t\t.rpc_argp = &args,\n\t\t.rpc_resp = &res,\n\t};\n\tint ret = -ENOMEM, npages, i, acl_len = 0;\n\n\tnpages = (buflen + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t/* As long as we're doing a round trip to the server anyway,\n\t * let's be prepared for a page of acl data. */\n\tif (npages == 0)\n\t\tnpages = 1;\n\n\tfor (i = 0; i < npages; i++) {\n\t\tpages[i] = alloc_page(GFP_KERNEL);\n\t\tif (!pages[i])\n\t\t\tgoto out_free;\n\t}\n\tif (npages > 1) {\n\t\t/* for decoding across pages */\n\t\targs.acl_scratch = alloc_page(GFP_KERNEL);\n\t\tif (!args.acl_scratch)\n\t\t\tgoto out_free;\n\t}\n\targs.acl_len = npages * PAGE_SIZE;\n\targs.acl_pgbase = 0;\n\t/* Let decode_getfacl know not to fail if the ACL data is larger than\n\t * the page we send as a guess */\n\tif (buf == NULL)\n\t\tres.acl_flags |= NFS4_ACL_LEN_REQUEST;\n\tresp_buf = page_address(pages[0]);\n\n\tdprintk(\"%s  buf %p buflen %ld npages %d args.acl_len %ld\\n\",\n\t\t__func__, buf, buflen, npages, args.acl_len);\n\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode),\n\t\t\t     &msg, &args.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\tgoto out_free;\n\n\tacl_len = res.acl_len - res.acl_data_offset;\n\tif (acl_len > args.acl_len)\n\t\tnfs4_write_cached_acl(inode, NULL, acl_len);\n\telse\n\t\tnfs4_write_cached_acl(inode, resp_buf + res.acl_data_offset,\n\t\t\t\t      acl_len);\n\tif (buf) {\n\t\tret = -ERANGE;\n\t\tif (acl_len > buflen)\n\t\t\tgoto out_free;\n\t\t_copy_from_pages(buf, pages, res.acl_data_offset,\n\t\t\t\tres.acl_len);\n\t}\n\tret = acl_len;\nout_free:\n\tfor (i = 0; i < npages; i++)\n\t\tif (pages[i])\n\t\t\t__free_page(pages[i]);\n\tif (args.acl_scratch)\n\t\t__free_page(args.acl_scratch);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 3561,
        "critical_vars": [
            "localpage"
        ],
        "function": "__nfs4_get_acl_uncached",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "False",
        "function_code": "\nstatic ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)\n{\n\tstruct page *pages[NFS4ACL_MAXPAGES];\n\tstruct nfs_getaclargs args = {\n\t\t.fh = NFS_FH(inode),\n\t\t.acl_pages = pages,\n\t\t.acl_len = buflen,\n\t};\n\tstruct nfs_getaclres res = {\n\t\t.acl_len = buflen,\n\t};\n\tvoid *resp_buf;\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &nfs4_procedures[NFSPROC4_CLNT_GETACL],\n\t\t.rpc_argp = &args,\n\t\t.rpc_resp = &res,\n\t};\n\tstruct page *localpage = NULL;\n\tint ret;\n\n\tif (buflen < PAGE_SIZE) {\n\t\t/* As long as we're doing a round trip to the server anyway,\n\t\t * let's be prepared for a page of acl data. */\n\t\tlocalpage = alloc_page(GFP_KERNEL);\n\t\tresp_buf = page_address(localpage);\n\t\tif (localpage == NULL)\n\t\t\treturn -ENOMEM;\n\t\targs.acl_pages[0] = localpage;\n\t\targs.acl_pgbase = 0;\n\t\targs.acl_len = PAGE_SIZE;\n\t} else {\n\t\tresp_buf = buf;\n\t\tbuf_to_pages(buf, buflen, args.acl_pages, &args.acl_pgbase);\n\t}\n\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode), &msg, &args.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\tgoto out_free;\n\tif (res.acl_len > args.acl_len)\n\t\tnfs4_write_cached_acl(inode, NULL, res.acl_len);\n\telse\n\t\tnfs4_write_cached_acl(inode, resp_buf, res.acl_len);\n\tif (buf) {\n\t\tret = -ERANGE;\n\t\tif (res.acl_len > buflen)\n\t\t\tgoto out_free;\n\t\tif (localpage)\n\t\t\tmemcpy(buf, resp_buf, res.acl_len);\n\t}\n\tret = res.acl_len;\nout_free:\n\tif (localpage)\n\t\t__free_page(localpage);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 3557,
        "critical_vars": [
            "npages"
        ],
        "function": "__nfs4_get_acl_uncached",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "True",
        "function_code": "static ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)\n{\n\tstruct page *pages[NFS4ACL_MAXPAGES] = {NULL, };\n\tstruct nfs_getaclargs args = {\n\t\t.fh = NFS_FH(inode),\n\t\t.acl_pages = pages,\n\t\t.acl_len = buflen,\n\t};\n\tstruct nfs_getaclres res = {\n\t\t.acl_len = buflen,\n\t};\n\tvoid *resp_buf;\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &nfs4_procedures[NFSPROC4_CLNT_GETACL],\n\t\t.rpc_argp = &args,\n\t\t.rpc_resp = &res,\n\t};\n\tint ret = -ENOMEM, npages, i, acl_len = 0;\n\n\tnpages = (buflen + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t/* As long as we're doing a round trip to the server anyway,\n\t * let's be prepared for a page of acl data. */\n\tif (npages == 0)\n\t\tnpages = 1;\n\n\tfor (i = 0; i < npages; i++) {\n\t\tpages[i] = alloc_page(GFP_KERNEL);\n\t\tif (!pages[i])\n\t\t\tgoto out_free;\n\t}\n\tif (npages > 1) {\n\t\t/* for decoding across pages */\n\t\targs.acl_scratch = alloc_page(GFP_KERNEL);\n\t\tif (!args.acl_scratch)\n\t\t\tgoto out_free;\n\t}\n\targs.acl_len = npages * PAGE_SIZE;\n\targs.acl_pgbase = 0;\n\t/* Let decode_getfacl know not to fail if the ACL data is larger than\n\t * the page we send as a guess */\n\tif (buf == NULL)\n\t\tres.acl_flags |= NFS4_ACL_LEN_REQUEST;\n\tresp_buf = page_address(pages[0]);\n\n\tdprintk(\"%s  buf %p buflen %ld npages %d args.acl_len %ld\\n\",\n\t\t__func__, buf, buflen, npages, args.acl_len);\n\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode),\n\t\t\t     &msg, &args.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\tgoto out_free;\n\n\tacl_len = res.acl_len - res.acl_data_offset;\n\tif (acl_len > args.acl_len)\n\t\tnfs4_write_cached_acl(inode, NULL, acl_len);\n\telse\n\t\tnfs4_write_cached_acl(inode, resp_buf + res.acl_data_offset,\n\t\t\t\t      acl_len);\n\tif (buf) {\n\t\tret = -ERANGE;\n\t\tif (acl_len > buflen)\n\t\t\tgoto out_free;\n\t\t_copy_from_pages(buf, pages, res.acl_data_offset,\n\t\t\t\tres.acl_len);\n\t}\n\tret = acl_len;\nout_free:\n\tfor (i = 0; i < npages; i++)\n\t\tif (pages[i])\n\t\t\t__free_page(pages[i]);\n\tif (args.acl_scratch)\n\t\t__free_page(args.acl_scratch);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 3562,
        "critical_vars": [
            "resp_buf"
        ],
        "function": "__nfs4_get_acl_uncached",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "False",
        "function_code": "\nstatic ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)\n{\n\tstruct page *pages[NFS4ACL_MAXPAGES];\n\tstruct nfs_getaclargs args = {\n\t\t.fh = NFS_FH(inode),\n\t\t.acl_pages = pages,\n\t\t.acl_len = buflen,\n\t};\n\tstruct nfs_getaclres res = {\n\t\t.acl_len = buflen,\n\t};\n\tvoid *resp_buf;\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &nfs4_procedures[NFSPROC4_CLNT_GETACL],\n\t\t.rpc_argp = &args,\n\t\t.rpc_resp = &res,\n\t};\n\tstruct page *localpage = NULL;\n\tint ret;\n\n\tif (buflen < PAGE_SIZE) {\n\t\t/* As long as we're doing a round trip to the server anyway,\n\t\t * let's be prepared for a page of acl data. */\n\t\tlocalpage = alloc_page(GFP_KERNEL);\n\t\tresp_buf = page_address(localpage);\n\t\tif (localpage == NULL)\n\t\t\treturn -ENOMEM;\n\t\targs.acl_pages[0] = localpage;\n\t\targs.acl_pgbase = 0;\n\t\targs.acl_len = PAGE_SIZE;\n\t} else {\n\t\tresp_buf = buf;\n\t\tbuf_to_pages(buf, buflen, args.acl_pages, &args.acl_pgbase);\n\t}\n\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode), &msg, &args.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\tgoto out_free;\n\tif (res.acl_len > args.acl_len)\n\t\tnfs4_write_cached_acl(inode, NULL, res.acl_len);\n\telse\n\t\tnfs4_write_cached_acl(inode, resp_buf, res.acl_len);\n\tif (buf) {\n\t\tret = -ERANGE;\n\t\tif (res.acl_len > buflen)\n\t\t\tgoto out_free;\n\t\tif (localpage)\n\t\t\tmemcpy(buf, resp_buf, res.acl_len);\n\t}\n\tret = res.acl_len;\nout_free:\n\tif (localpage)\n\t\t__free_page(localpage);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 3558,
        "critical_vars": [
            "npages"
        ],
        "function": "__nfs4_get_acl_uncached",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "True",
        "function_code": "static ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)\n{\n\tstruct page *pages[NFS4ACL_MAXPAGES] = {NULL, };\n\tstruct nfs_getaclargs args = {\n\t\t.fh = NFS_FH(inode),\n\t\t.acl_pages = pages,\n\t\t.acl_len = buflen,\n\t};\n\tstruct nfs_getaclres res = {\n\t\t.acl_len = buflen,\n\t};\n\tvoid *resp_buf;\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &nfs4_procedures[NFSPROC4_CLNT_GETACL],\n\t\t.rpc_argp = &args,\n\t\t.rpc_resp = &res,\n\t};\n\tint ret = -ENOMEM, npages, i, acl_len = 0;\n\n\tnpages = (buflen + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t/* As long as we're doing a round trip to the server anyway,\n\t * let's be prepared for a page of acl data. */\n\tif (npages == 0)\n\t\tnpages = 1;\n\n\tfor (i = 0; i < npages; i++) {\n\t\tpages[i] = alloc_page(GFP_KERNEL);\n\t\tif (!pages[i])\n\t\t\tgoto out_free;\n\t}\n\tif (npages > 1) {\n\t\t/* for decoding across pages */\n\t\targs.acl_scratch = alloc_page(GFP_KERNEL);\n\t\tif (!args.acl_scratch)\n\t\t\tgoto out_free;\n\t}\n\targs.acl_len = npages * PAGE_SIZE;\n\targs.acl_pgbase = 0;\n\t/* Let decode_getfacl know not to fail if the ACL data is larger than\n\t * the page we send as a guess */\n\tif (buf == NULL)\n\t\tres.acl_flags |= NFS4_ACL_LEN_REQUEST;\n\tresp_buf = page_address(pages[0]);\n\n\tdprintk(\"%s  buf %p buflen %ld npages %d args.acl_len %ld\\n\",\n\t\t__func__, buf, buflen, npages, args.acl_len);\n\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode),\n\t\t\t     &msg, &args.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\tgoto out_free;\n\n\tacl_len = res.acl_len - res.acl_data_offset;\n\tif (acl_len > args.acl_len)\n\t\tnfs4_write_cached_acl(inode, NULL, acl_len);\n\telse\n\t\tnfs4_write_cached_acl(inode, resp_buf + res.acl_data_offset,\n\t\t\t\t      acl_len);\n\tif (buf) {\n\t\tret = -ERANGE;\n\t\tif (acl_len > buflen)\n\t\t\tgoto out_free;\n\t\t_copy_from_pages(buf, pages, res.acl_data_offset,\n\t\t\t\tres.acl_len);\n\t}\n\tret = acl_len;\nout_free:\n\tfor (i = 0; i < npages; i++)\n\t\tif (pages[i])\n\t\t\t__free_page(pages[i]);\n\tif (args.acl_scratch)\n\t\t__free_page(args.acl_scratch);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "if-Condition",
        "line_old": 3563,
        "critical_vars": [
            "localpage"
        ],
        "function": "__nfs4_get_acl_uncached",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "False",
        "function_code": "\nstatic ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)\n{\n\tstruct page *pages[NFS4ACL_MAXPAGES];\n\tstruct nfs_getaclargs args = {\n\t\t.fh = NFS_FH(inode),\n\t\t.acl_pages = pages,\n\t\t.acl_len = buflen,\n\t};\n\tstruct nfs_getaclres res = {\n\t\t.acl_len = buflen,\n\t};\n\tvoid *resp_buf;\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &nfs4_procedures[NFSPROC4_CLNT_GETACL],\n\t\t.rpc_argp = &args,\n\t\t.rpc_resp = &res,\n\t};\n\tstruct page *localpage = NULL;\n\tint ret;\n\n\tif (buflen < PAGE_SIZE) {\n\t\t/* As long as we're doing a round trip to the server anyway,\n\t\t * let's be prepared for a page of acl data. */\n\t\tlocalpage = alloc_page(GFP_KERNEL);\n\t\tresp_buf = page_address(localpage);\n\t\tif (localpage == NULL)\n\t\t\treturn -ENOMEM;\n\t\targs.acl_pages[0] = localpage;\n\t\targs.acl_pgbase = 0;\n\t\targs.acl_len = PAGE_SIZE;\n\t} else {\n\t\tresp_buf = buf;\n\t\tbuf_to_pages(buf, buflen, args.acl_pages, &args.acl_pgbase);\n\t}\n\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode), &msg, &args.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\tgoto out_free;\n\tif (res.acl_len > args.acl_len)\n\t\tnfs4_write_cached_acl(inode, NULL, res.acl_len);\n\telse\n\t\tnfs4_write_cached_acl(inode, resp_buf, res.acl_len);\n\tif (buf) {\n\t\tret = -ERANGE;\n\t\tif (res.acl_len > buflen)\n\t\t\tgoto out_free;\n\t\tif (localpage)\n\t\t\tmemcpy(buf, resp_buf, res.acl_len);\n\t}\n\tret = res.acl_len;\nout_free:\n\tif (localpage)\n\t\t__free_page(localpage);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "for-Condition",
        "line_new": 3560,
        "critical_vars": [
            "i"
        ],
        "function": "__nfs4_get_acl_uncached",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "True",
        "function_code": "static ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)\n{\n\tstruct page *pages[NFS4ACL_MAXPAGES] = {NULL, };\n\tstruct nfs_getaclargs args = {\n\t\t.fh = NFS_FH(inode),\n\t\t.acl_pages = pages,\n\t\t.acl_len = buflen,\n\t};\n\tstruct nfs_getaclres res = {\n\t\t.acl_len = buflen,\n\t};\n\tvoid *resp_buf;\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &nfs4_procedures[NFSPROC4_CLNT_GETACL],\n\t\t.rpc_argp = &args,\n\t\t.rpc_resp = &res,\n\t};\n\tint ret = -ENOMEM, npages, i, acl_len = 0;\n\n\tnpages = (buflen + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t/* As long as we're doing a round trip to the server anyway,\n\t * let's be prepared for a page of acl data. */\n\tif (npages == 0)\n\t\tnpages = 1;\n\n\tfor (i = 0; i < npages; i++) {\n\t\tpages[i] = alloc_page(GFP_KERNEL);\n\t\tif (!pages[i])\n\t\t\tgoto out_free;\n\t}\n\tif (npages > 1) {\n\t\t/* for decoding across pages */\n\t\targs.acl_scratch = alloc_page(GFP_KERNEL);\n\t\tif (!args.acl_scratch)\n\t\t\tgoto out_free;\n\t}\n\targs.acl_len = npages * PAGE_SIZE;\n\targs.acl_pgbase = 0;\n\t/* Let decode_getfacl know not to fail if the ACL data is larger than\n\t * the page we send as a guess */\n\tif (buf == NULL)\n\t\tres.acl_flags |= NFS4_ACL_LEN_REQUEST;\n\tresp_buf = page_address(pages[0]);\n\n\tdprintk(\"%s  buf %p buflen %ld npages %d args.acl_len %ld\\n\",\n\t\t__func__, buf, buflen, npages, args.acl_len);\n\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode),\n\t\t\t     &msg, &args.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\tgoto out_free;\n\n\tacl_len = res.acl_len - res.acl_data_offset;\n\tif (acl_len > args.acl_len)\n\t\tnfs4_write_cached_acl(inode, NULL, acl_len);\n\telse\n\t\tnfs4_write_cached_acl(inode, resp_buf + res.acl_data_offset,\n\t\t\t\t      acl_len);\n\tif (buf) {\n\t\tret = -ERANGE;\n\t\tif (acl_len > buflen)\n\t\t\tgoto out_free;\n\t\t_copy_from_pages(buf, pages, res.acl_data_offset,\n\t\t\t\tres.acl_len);\n\t}\n\tret = acl_len;\nout_free:\n\tfor (i = 0; i < npages; i++)\n\t\tif (pages[i])\n\t\t\t__free_page(pages[i]);\n\tif (args.acl_scratch)\n\t\t__free_page(args.acl_scratch);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 3561,
        "critical_vars": [
            "pages[i]"
        ],
        "function": "__nfs4_get_acl_uncached",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "True",
        "function_code": "static ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)\n{\n\tstruct page *pages[NFS4ACL_MAXPAGES] = {NULL, };\n\tstruct nfs_getaclargs args = {\n\t\t.fh = NFS_FH(inode),\n\t\t.acl_pages = pages,\n\t\t.acl_len = buflen,\n\t};\n\tstruct nfs_getaclres res = {\n\t\t.acl_len = buflen,\n\t};\n\tvoid *resp_buf;\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &nfs4_procedures[NFSPROC4_CLNT_GETACL],\n\t\t.rpc_argp = &args,\n\t\t.rpc_resp = &res,\n\t};\n\tint ret = -ENOMEM, npages, i, acl_len = 0;\n\n\tnpages = (buflen + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t/* As long as we're doing a round trip to the server anyway,\n\t * let's be prepared for a page of acl data. */\n\tif (npages == 0)\n\t\tnpages = 1;\n\n\tfor (i = 0; i < npages; i++) {\n\t\tpages[i] = alloc_page(GFP_KERNEL);\n\t\tif (!pages[i])\n\t\t\tgoto out_free;\n\t}\n\tif (npages > 1) {\n\t\t/* for decoding across pages */\n\t\targs.acl_scratch = alloc_page(GFP_KERNEL);\n\t\tif (!args.acl_scratch)\n\t\t\tgoto out_free;\n\t}\n\targs.acl_len = npages * PAGE_SIZE;\n\targs.acl_pgbase = 0;\n\t/* Let decode_getfacl know not to fail if the ACL data is larger than\n\t * the page we send as a guess */\n\tif (buf == NULL)\n\t\tres.acl_flags |= NFS4_ACL_LEN_REQUEST;\n\tresp_buf = page_address(pages[0]);\n\n\tdprintk(\"%s  buf %p buflen %ld npages %d args.acl_len %ld\\n\",\n\t\t__func__, buf, buflen, npages, args.acl_len);\n\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode),\n\t\t\t     &msg, &args.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\tgoto out_free;\n\n\tacl_len = res.acl_len - res.acl_data_offset;\n\tif (acl_len > args.acl_len)\n\t\tnfs4_write_cached_acl(inode, NULL, acl_len);\n\telse\n\t\tnfs4_write_cached_acl(inode, resp_buf + res.acl_data_offset,\n\t\t\t\t      acl_len);\n\tif (buf) {\n\t\tret = -ERANGE;\n\t\tif (acl_len > buflen)\n\t\t\tgoto out_free;\n\t\t_copy_from_pages(buf, pages, res.acl_data_offset,\n\t\t\t\tres.acl_len);\n\t}\n\tret = acl_len;\nout_free:\n\tfor (i = 0; i < npages; i++)\n\t\tif (pages[i])\n\t\t\t__free_page(pages[i]);\n\tif (args.acl_scratch)\n\t\t__free_page(args.acl_scratch);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 3565,
        "critical_vars": [
            "args.acl_pages[0]"
        ],
        "function": "__nfs4_get_acl_uncached",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "False",
        "function_code": "\nstatic ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)\n{\n\tstruct page *pages[NFS4ACL_MAXPAGES];\n\tstruct nfs_getaclargs args = {\n\t\t.fh = NFS_FH(inode),\n\t\t.acl_pages = pages,\n\t\t.acl_len = buflen,\n\t};\n\tstruct nfs_getaclres res = {\n\t\t.acl_len = buflen,\n\t};\n\tvoid *resp_buf;\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &nfs4_procedures[NFSPROC4_CLNT_GETACL],\n\t\t.rpc_argp = &args,\n\t\t.rpc_resp = &res,\n\t};\n\tstruct page *localpage = NULL;\n\tint ret;\n\n\tif (buflen < PAGE_SIZE) {\n\t\t/* As long as we're doing a round trip to the server anyway,\n\t\t * let's be prepared for a page of acl data. */\n\t\tlocalpage = alloc_page(GFP_KERNEL);\n\t\tresp_buf = page_address(localpage);\n\t\tif (localpage == NULL)\n\t\t\treturn -ENOMEM;\n\t\targs.acl_pages[0] = localpage;\n\t\targs.acl_pgbase = 0;\n\t\targs.acl_len = PAGE_SIZE;\n\t} else {\n\t\tresp_buf = buf;\n\t\tbuf_to_pages(buf, buflen, args.acl_pages, &args.acl_pgbase);\n\t}\n\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode), &msg, &args.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\tgoto out_free;\n\tif (res.acl_len > args.acl_len)\n\t\tnfs4_write_cached_acl(inode, NULL, res.acl_len);\n\telse\n\t\tnfs4_write_cached_acl(inode, resp_buf, res.acl_len);\n\tif (buf) {\n\t\tret = -ERANGE;\n\t\tif (res.acl_len > buflen)\n\t\t\tgoto out_free;\n\t\tif (localpage)\n\t\t\tmemcpy(buf, resp_buf, res.acl_len);\n\t}\n\tret = res.acl_len;\nout_free:\n\tif (localpage)\n\t\t__free_page(localpage);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 3562,
        "critical_vars": [
            "pages[i]"
        ],
        "function": "__nfs4_get_acl_uncached",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "True",
        "function_code": "static ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)\n{\n\tstruct page *pages[NFS4ACL_MAXPAGES] = {NULL, };\n\tstruct nfs_getaclargs args = {\n\t\t.fh = NFS_FH(inode),\n\t\t.acl_pages = pages,\n\t\t.acl_len = buflen,\n\t};\n\tstruct nfs_getaclres res = {\n\t\t.acl_len = buflen,\n\t};\n\tvoid *resp_buf;\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &nfs4_procedures[NFSPROC4_CLNT_GETACL],\n\t\t.rpc_argp = &args,\n\t\t.rpc_resp = &res,\n\t};\n\tint ret = -ENOMEM, npages, i, acl_len = 0;\n\n\tnpages = (buflen + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t/* As long as we're doing a round trip to the server anyway,\n\t * let's be prepared for a page of acl data. */\n\tif (npages == 0)\n\t\tnpages = 1;\n\n\tfor (i = 0; i < npages; i++) {\n\t\tpages[i] = alloc_page(GFP_KERNEL);\n\t\tif (!pages[i])\n\t\t\tgoto out_free;\n\t}\n\tif (npages > 1) {\n\t\t/* for decoding across pages */\n\t\targs.acl_scratch = alloc_page(GFP_KERNEL);\n\t\tif (!args.acl_scratch)\n\t\t\tgoto out_free;\n\t}\n\targs.acl_len = npages * PAGE_SIZE;\n\targs.acl_pgbase = 0;\n\t/* Let decode_getfacl know not to fail if the ACL data is larger than\n\t * the page we send as a guess */\n\tif (buf == NULL)\n\t\tres.acl_flags |= NFS4_ACL_LEN_REQUEST;\n\tresp_buf = page_address(pages[0]);\n\n\tdprintk(\"%s  buf %p buflen %ld npages %d args.acl_len %ld\\n\",\n\t\t__func__, buf, buflen, npages, args.acl_len);\n\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode),\n\t\t\t     &msg, &args.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\tgoto out_free;\n\n\tacl_len = res.acl_len - res.acl_data_offset;\n\tif (acl_len > args.acl_len)\n\t\tnfs4_write_cached_acl(inode, NULL, acl_len);\n\telse\n\t\tnfs4_write_cached_acl(inode, resp_buf + res.acl_data_offset,\n\t\t\t\t      acl_len);\n\tif (buf) {\n\t\tret = -ERANGE;\n\t\tif (acl_len > buflen)\n\t\t\tgoto out_free;\n\t\t_copy_from_pages(buf, pages, res.acl_data_offset,\n\t\t\t\tres.acl_len);\n\t}\n\tret = acl_len;\nout_free:\n\tfor (i = 0; i < npages; i++)\n\t\tif (pages[i])\n\t\t\t__free_page(pages[i]);\n\tif (args.acl_scratch)\n\t\t__free_page(args.acl_scratch);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 3566,
        "critical_vars": [
            "args.acl_pgbase"
        ],
        "function": "__nfs4_get_acl_uncached",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "False",
        "function_code": "\nstatic ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)\n{\n\tstruct page *pages[NFS4ACL_MAXPAGES];\n\tstruct nfs_getaclargs args = {\n\t\t.fh = NFS_FH(inode),\n\t\t.acl_pages = pages,\n\t\t.acl_len = buflen,\n\t};\n\tstruct nfs_getaclres res = {\n\t\t.acl_len = buflen,\n\t};\n\tvoid *resp_buf;\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &nfs4_procedures[NFSPROC4_CLNT_GETACL],\n\t\t.rpc_argp = &args,\n\t\t.rpc_resp = &res,\n\t};\n\tstruct page *localpage = NULL;\n\tint ret;\n\n\tif (buflen < PAGE_SIZE) {\n\t\t/* As long as we're doing a round trip to the server anyway,\n\t\t * let's be prepared for a page of acl data. */\n\t\tlocalpage = alloc_page(GFP_KERNEL);\n\t\tresp_buf = page_address(localpage);\n\t\tif (localpage == NULL)\n\t\t\treturn -ENOMEM;\n\t\targs.acl_pages[0] = localpage;\n\t\targs.acl_pgbase = 0;\n\t\targs.acl_len = PAGE_SIZE;\n\t} else {\n\t\tresp_buf = buf;\n\t\tbuf_to_pages(buf, buflen, args.acl_pages, &args.acl_pgbase);\n\t}\n\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode), &msg, &args.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\tgoto out_free;\n\tif (res.acl_len > args.acl_len)\n\t\tnfs4_write_cached_acl(inode, NULL, res.acl_len);\n\telse\n\t\tnfs4_write_cached_acl(inode, resp_buf, res.acl_len);\n\tif (buf) {\n\t\tret = -ERANGE;\n\t\tif (res.acl_len > buflen)\n\t\t\tgoto out_free;\n\t\tif (localpage)\n\t\t\tmemcpy(buf, resp_buf, res.acl_len);\n\t}\n\tret = res.acl_len;\nout_free:\n\tif (localpage)\n\t\t__free_page(localpage);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 3567,
        "critical_vars": [
            "args.acl_len"
        ],
        "function": "__nfs4_get_acl_uncached",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "False",
        "function_code": "\nstatic ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)\n{\n\tstruct page *pages[NFS4ACL_MAXPAGES];\n\tstruct nfs_getaclargs args = {\n\t\t.fh = NFS_FH(inode),\n\t\t.acl_pages = pages,\n\t\t.acl_len = buflen,\n\t};\n\tstruct nfs_getaclres res = {\n\t\t.acl_len = buflen,\n\t};\n\tvoid *resp_buf;\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &nfs4_procedures[NFSPROC4_CLNT_GETACL],\n\t\t.rpc_argp = &args,\n\t\t.rpc_resp = &res,\n\t};\n\tstruct page *localpage = NULL;\n\tint ret;\n\n\tif (buflen < PAGE_SIZE) {\n\t\t/* As long as we're doing a round trip to the server anyway,\n\t\t * let's be prepared for a page of acl data. */\n\t\tlocalpage = alloc_page(GFP_KERNEL);\n\t\tresp_buf = page_address(localpage);\n\t\tif (localpage == NULL)\n\t\t\treturn -ENOMEM;\n\t\targs.acl_pages[0] = localpage;\n\t\targs.acl_pgbase = 0;\n\t\targs.acl_len = PAGE_SIZE;\n\t} else {\n\t\tresp_buf = buf;\n\t\tbuf_to_pages(buf, buflen, args.acl_pages, &args.acl_pgbase);\n\t}\n\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode), &msg, &args.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\tgoto out_free;\n\tif (res.acl_len > args.acl_len)\n\t\tnfs4_write_cached_acl(inode, NULL, res.acl_len);\n\telse\n\t\tnfs4_write_cached_acl(inode, resp_buf, res.acl_len);\n\tif (buf) {\n\t\tret = -ERANGE;\n\t\tif (res.acl_len > buflen)\n\t\t\tgoto out_free;\n\t\tif (localpage)\n\t\t\tmemcpy(buf, resp_buf, res.acl_len);\n\t}\n\tret = res.acl_len;\nout_free:\n\tif (localpage)\n\t\t__free_page(localpage);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 3565,
        "critical_vars": [
            "npages"
        ],
        "function": "__nfs4_get_acl_uncached",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "True",
        "function_code": "static ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)\n{\n\tstruct page *pages[NFS4ACL_MAXPAGES] = {NULL, };\n\tstruct nfs_getaclargs args = {\n\t\t.fh = NFS_FH(inode),\n\t\t.acl_pages = pages,\n\t\t.acl_len = buflen,\n\t};\n\tstruct nfs_getaclres res = {\n\t\t.acl_len = buflen,\n\t};\n\tvoid *resp_buf;\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &nfs4_procedures[NFSPROC4_CLNT_GETACL],\n\t\t.rpc_argp = &args,\n\t\t.rpc_resp = &res,\n\t};\n\tint ret = -ENOMEM, npages, i, acl_len = 0;\n\n\tnpages = (buflen + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t/* As long as we're doing a round trip to the server anyway,\n\t * let's be prepared for a page of acl data. */\n\tif (npages == 0)\n\t\tnpages = 1;\n\n\tfor (i = 0; i < npages; i++) {\n\t\tpages[i] = alloc_page(GFP_KERNEL);\n\t\tif (!pages[i])\n\t\t\tgoto out_free;\n\t}\n\tif (npages > 1) {\n\t\t/* for decoding across pages */\n\t\targs.acl_scratch = alloc_page(GFP_KERNEL);\n\t\tif (!args.acl_scratch)\n\t\t\tgoto out_free;\n\t}\n\targs.acl_len = npages * PAGE_SIZE;\n\targs.acl_pgbase = 0;\n\t/* Let decode_getfacl know not to fail if the ACL data is larger than\n\t * the page we send as a guess */\n\tif (buf == NULL)\n\t\tres.acl_flags |= NFS4_ACL_LEN_REQUEST;\n\tresp_buf = page_address(pages[0]);\n\n\tdprintk(\"%s  buf %p buflen %ld npages %d args.acl_len %ld\\n\",\n\t\t__func__, buf, buflen, npages, args.acl_len);\n\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode),\n\t\t\t     &msg, &args.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\tgoto out_free;\n\n\tacl_len = res.acl_len - res.acl_data_offset;\n\tif (acl_len > args.acl_len)\n\t\tnfs4_write_cached_acl(inode, NULL, acl_len);\n\telse\n\t\tnfs4_write_cached_acl(inode, resp_buf + res.acl_data_offset,\n\t\t\t\t      acl_len);\n\tif (buf) {\n\t\tret = -ERANGE;\n\t\tif (acl_len > buflen)\n\t\t\tgoto out_free;\n\t\t_copy_from_pages(buf, pages, res.acl_data_offset,\n\t\t\t\tres.acl_len);\n\t}\n\tret = acl_len;\nout_free:\n\tfor (i = 0; i < npages; i++)\n\t\tif (pages[i])\n\t\t\t__free_page(pages[i]);\n\tif (args.acl_scratch)\n\t\t__free_page(args.acl_scratch);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 3567,
        "critical_vars": [
            "args.acl_scratch"
        ],
        "function": "__nfs4_get_acl_uncached",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "True",
        "function_code": "static ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)\n{\n\tstruct page *pages[NFS4ACL_MAXPAGES] = {NULL, };\n\tstruct nfs_getaclargs args = {\n\t\t.fh = NFS_FH(inode),\n\t\t.acl_pages = pages,\n\t\t.acl_len = buflen,\n\t};\n\tstruct nfs_getaclres res = {\n\t\t.acl_len = buflen,\n\t};\n\tvoid *resp_buf;\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &nfs4_procedures[NFSPROC4_CLNT_GETACL],\n\t\t.rpc_argp = &args,\n\t\t.rpc_resp = &res,\n\t};\n\tint ret = -ENOMEM, npages, i, acl_len = 0;\n\n\tnpages = (buflen + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t/* As long as we're doing a round trip to the server anyway,\n\t * let's be prepared for a page of acl data. */\n\tif (npages == 0)\n\t\tnpages = 1;\n\n\tfor (i = 0; i < npages; i++) {\n\t\tpages[i] = alloc_page(GFP_KERNEL);\n\t\tif (!pages[i])\n\t\t\tgoto out_free;\n\t}\n\tif (npages > 1) {\n\t\t/* for decoding across pages */\n\t\targs.acl_scratch = alloc_page(GFP_KERNEL);\n\t\tif (!args.acl_scratch)\n\t\t\tgoto out_free;\n\t}\n\targs.acl_len = npages * PAGE_SIZE;\n\targs.acl_pgbase = 0;\n\t/* Let decode_getfacl know not to fail if the ACL data is larger than\n\t * the page we send as a guess */\n\tif (buf == NULL)\n\t\tres.acl_flags |= NFS4_ACL_LEN_REQUEST;\n\tresp_buf = page_address(pages[0]);\n\n\tdprintk(\"%s  buf %p buflen %ld npages %d args.acl_len %ld\\n\",\n\t\t__func__, buf, buflen, npages, args.acl_len);\n\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode),\n\t\t\t     &msg, &args.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\tgoto out_free;\n\n\tacl_len = res.acl_len - res.acl_data_offset;\n\tif (acl_len > args.acl_len)\n\t\tnfs4_write_cached_acl(inode, NULL, acl_len);\n\telse\n\t\tnfs4_write_cached_acl(inode, resp_buf + res.acl_data_offset,\n\t\t\t\t      acl_len);\n\tif (buf) {\n\t\tret = -ERANGE;\n\t\tif (acl_len > buflen)\n\t\t\tgoto out_free;\n\t\t_copy_from_pages(buf, pages, res.acl_data_offset,\n\t\t\t\tres.acl_len);\n\t}\n\tret = acl_len;\nout_free:\n\tfor (i = 0; i < npages; i++)\n\t\tif (pages[i])\n\t\t\t__free_page(pages[i]);\n\tif (args.acl_scratch)\n\t\t__free_page(args.acl_scratch);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 3569,
        "critical_vars": [
            "resp_buf"
        ],
        "function": "__nfs4_get_acl_uncached",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "False",
        "function_code": "\nstatic ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)\n{\n\tstruct page *pages[NFS4ACL_MAXPAGES];\n\tstruct nfs_getaclargs args = {\n\t\t.fh = NFS_FH(inode),\n\t\t.acl_pages = pages,\n\t\t.acl_len = buflen,\n\t};\n\tstruct nfs_getaclres res = {\n\t\t.acl_len = buflen,\n\t};\n\tvoid *resp_buf;\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &nfs4_procedures[NFSPROC4_CLNT_GETACL],\n\t\t.rpc_argp = &args,\n\t\t.rpc_resp = &res,\n\t};\n\tstruct page *localpage = NULL;\n\tint ret;\n\n\tif (buflen < PAGE_SIZE) {\n\t\t/* As long as we're doing a round trip to the server anyway,\n\t\t * let's be prepared for a page of acl data. */\n\t\tlocalpage = alloc_page(GFP_KERNEL);\n\t\tresp_buf = page_address(localpage);\n\t\tif (localpage == NULL)\n\t\t\treturn -ENOMEM;\n\t\targs.acl_pages[0] = localpage;\n\t\targs.acl_pgbase = 0;\n\t\targs.acl_len = PAGE_SIZE;\n\t} else {\n\t\tresp_buf = buf;\n\t\tbuf_to_pages(buf, buflen, args.acl_pages, &args.acl_pgbase);\n\t}\n\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode), &msg, &args.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\tgoto out_free;\n\tif (res.acl_len > args.acl_len)\n\t\tnfs4_write_cached_acl(inode, NULL, res.acl_len);\n\telse\n\t\tnfs4_write_cached_acl(inode, resp_buf, res.acl_len);\n\tif (buf) {\n\t\tret = -ERANGE;\n\t\tif (res.acl_len > buflen)\n\t\t\tgoto out_free;\n\t\tif (localpage)\n\t\t\tmemcpy(buf, resp_buf, res.acl_len);\n\t}\n\tret = res.acl_len;\nout_free:\n\tif (localpage)\n\t\t__free_page(localpage);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 3568,
        "critical_vars": [
            "args.acl_scratch"
        ],
        "function": "__nfs4_get_acl_uncached",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "True",
        "function_code": "static ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)\n{\n\tstruct page *pages[NFS4ACL_MAXPAGES] = {NULL, };\n\tstruct nfs_getaclargs args = {\n\t\t.fh = NFS_FH(inode),\n\t\t.acl_pages = pages,\n\t\t.acl_len = buflen,\n\t};\n\tstruct nfs_getaclres res = {\n\t\t.acl_len = buflen,\n\t};\n\tvoid *resp_buf;\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &nfs4_procedures[NFSPROC4_CLNT_GETACL],\n\t\t.rpc_argp = &args,\n\t\t.rpc_resp = &res,\n\t};\n\tint ret = -ENOMEM, npages, i, acl_len = 0;\n\n\tnpages = (buflen + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t/* As long as we're doing a round trip to the server anyway,\n\t * let's be prepared for a page of acl data. */\n\tif (npages == 0)\n\t\tnpages = 1;\n\n\tfor (i = 0; i < npages; i++) {\n\t\tpages[i] = alloc_page(GFP_KERNEL);\n\t\tif (!pages[i])\n\t\t\tgoto out_free;\n\t}\n\tif (npages > 1) {\n\t\t/* for decoding across pages */\n\t\targs.acl_scratch = alloc_page(GFP_KERNEL);\n\t\tif (!args.acl_scratch)\n\t\t\tgoto out_free;\n\t}\n\targs.acl_len = npages * PAGE_SIZE;\n\targs.acl_pgbase = 0;\n\t/* Let decode_getfacl know not to fail if the ACL data is larger than\n\t * the page we send as a guess */\n\tif (buf == NULL)\n\t\tres.acl_flags |= NFS4_ACL_LEN_REQUEST;\n\tresp_buf = page_address(pages[0]);\n\n\tdprintk(\"%s  buf %p buflen %ld npages %d args.acl_len %ld\\n\",\n\t\t__func__, buf, buflen, npages, args.acl_len);\n\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode),\n\t\t\t     &msg, &args.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\tgoto out_free;\n\n\tacl_len = res.acl_len - res.acl_data_offset;\n\tif (acl_len > args.acl_len)\n\t\tnfs4_write_cached_acl(inode, NULL, acl_len);\n\telse\n\t\tnfs4_write_cached_acl(inode, resp_buf + res.acl_data_offset,\n\t\t\t\t      acl_len);\n\tif (buf) {\n\t\tret = -ERANGE;\n\t\tif (acl_len > buflen)\n\t\t\tgoto out_free;\n\t\t_copy_from_pages(buf, pages, res.acl_data_offset,\n\t\t\t\tres.acl_len);\n\t}\n\tret = acl_len;\nout_free:\n\tfor (i = 0; i < npages; i++)\n\t\tif (pages[i])\n\t\t\t__free_page(pages[i]);\n\tif (args.acl_scratch)\n\t\t__free_page(args.acl_scratch);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Fun-Call",
        "line_old": 3570,
        "critical_vars": [
            "buf",
            "&args.acl_pgbase",
            "args.acl_pages",
            "buflen"
        ],
        "function": "__nfs4_get_acl_uncached",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "False",
        "function_code": "\nstatic ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)\n{\n\tstruct page *pages[NFS4ACL_MAXPAGES];\n\tstruct nfs_getaclargs args = {\n\t\t.fh = NFS_FH(inode),\n\t\t.acl_pages = pages,\n\t\t.acl_len = buflen,\n\t};\n\tstruct nfs_getaclres res = {\n\t\t.acl_len = buflen,\n\t};\n\tvoid *resp_buf;\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &nfs4_procedures[NFSPROC4_CLNT_GETACL],\n\t\t.rpc_argp = &args,\n\t\t.rpc_resp = &res,\n\t};\n\tstruct page *localpage = NULL;\n\tint ret;\n\n\tif (buflen < PAGE_SIZE) {\n\t\t/* As long as we're doing a round trip to the server anyway,\n\t\t * let's be prepared for a page of acl data. */\n\t\tlocalpage = alloc_page(GFP_KERNEL);\n\t\tresp_buf = page_address(localpage);\n\t\tif (localpage == NULL)\n\t\t\treturn -ENOMEM;\n\t\targs.acl_pages[0] = localpage;\n\t\targs.acl_pgbase = 0;\n\t\targs.acl_len = PAGE_SIZE;\n\t} else {\n\t\tresp_buf = buf;\n\t\tbuf_to_pages(buf, buflen, args.acl_pages, &args.acl_pgbase);\n\t}\n\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode), &msg, &args.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\tgoto out_free;\n\tif (res.acl_len > args.acl_len)\n\t\tnfs4_write_cached_acl(inode, NULL, res.acl_len);\n\telse\n\t\tnfs4_write_cached_acl(inode, resp_buf, res.acl_len);\n\tif (buf) {\n\t\tret = -ERANGE;\n\t\tif (res.acl_len > buflen)\n\t\t\tgoto out_free;\n\t\tif (localpage)\n\t\t\tmemcpy(buf, resp_buf, res.acl_len);\n\t}\n\tret = res.acl_len;\nout_free:\n\tif (localpage)\n\t\t__free_page(localpage);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 3572,
        "critical_vars": [
            "ret"
        ],
        "function": "__nfs4_get_acl_uncached",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "False",
        "function_code": "\nstatic ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)\n{\n\tstruct page *pages[NFS4ACL_MAXPAGES];\n\tstruct nfs_getaclargs args = {\n\t\t.fh = NFS_FH(inode),\n\t\t.acl_pages = pages,\n\t\t.acl_len = buflen,\n\t};\n\tstruct nfs_getaclres res = {\n\t\t.acl_len = buflen,\n\t};\n\tvoid *resp_buf;\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &nfs4_procedures[NFSPROC4_CLNT_GETACL],\n\t\t.rpc_argp = &args,\n\t\t.rpc_resp = &res,\n\t};\n\tstruct page *localpage = NULL;\n\tint ret;\n\n\tif (buflen < PAGE_SIZE) {\n\t\t/* As long as we're doing a round trip to the server anyway,\n\t\t * let's be prepared for a page of acl data. */\n\t\tlocalpage = alloc_page(GFP_KERNEL);\n\t\tresp_buf = page_address(localpage);\n\t\tif (localpage == NULL)\n\t\t\treturn -ENOMEM;\n\t\targs.acl_pages[0] = localpage;\n\t\targs.acl_pgbase = 0;\n\t\targs.acl_len = PAGE_SIZE;\n\t} else {\n\t\tresp_buf = buf;\n\t\tbuf_to_pages(buf, buflen, args.acl_pages, &args.acl_pgbase);\n\t}\n\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode), &msg, &args.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\tgoto out_free;\n\tif (res.acl_len > args.acl_len)\n\t\tnfs4_write_cached_acl(inode, NULL, res.acl_len);\n\telse\n\t\tnfs4_write_cached_acl(inode, resp_buf, res.acl_len);\n\tif (buf) {\n\t\tret = -ERANGE;\n\t\tif (res.acl_len > buflen)\n\t\t\tgoto out_free;\n\t\tif (localpage)\n\t\t\tmemcpy(buf, resp_buf, res.acl_len);\n\t}\n\tret = res.acl_len;\nout_free:\n\tif (localpage)\n\t\t__free_page(localpage);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 3571,
        "critical_vars": [
            "args.acl_len"
        ],
        "function": "__nfs4_get_acl_uncached",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "True",
        "function_code": "static ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)\n{\n\tstruct page *pages[NFS4ACL_MAXPAGES] = {NULL, };\n\tstruct nfs_getaclargs args = {\n\t\t.fh = NFS_FH(inode),\n\t\t.acl_pages = pages,\n\t\t.acl_len = buflen,\n\t};\n\tstruct nfs_getaclres res = {\n\t\t.acl_len = buflen,\n\t};\n\tvoid *resp_buf;\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &nfs4_procedures[NFSPROC4_CLNT_GETACL],\n\t\t.rpc_argp = &args,\n\t\t.rpc_resp = &res,\n\t};\n\tint ret = -ENOMEM, npages, i, acl_len = 0;\n\n\tnpages = (buflen + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t/* As long as we're doing a round trip to the server anyway,\n\t * let's be prepared for a page of acl data. */\n\tif (npages == 0)\n\t\tnpages = 1;\n\n\tfor (i = 0; i < npages; i++) {\n\t\tpages[i] = alloc_page(GFP_KERNEL);\n\t\tif (!pages[i])\n\t\t\tgoto out_free;\n\t}\n\tif (npages > 1) {\n\t\t/* for decoding across pages */\n\t\targs.acl_scratch = alloc_page(GFP_KERNEL);\n\t\tif (!args.acl_scratch)\n\t\t\tgoto out_free;\n\t}\n\targs.acl_len = npages * PAGE_SIZE;\n\targs.acl_pgbase = 0;\n\t/* Let decode_getfacl know not to fail if the ACL data is larger than\n\t * the page we send as a guess */\n\tif (buf == NULL)\n\t\tres.acl_flags |= NFS4_ACL_LEN_REQUEST;\n\tresp_buf = page_address(pages[0]);\n\n\tdprintk(\"%s  buf %p buflen %ld npages %d args.acl_len %ld\\n\",\n\t\t__func__, buf, buflen, npages, args.acl_len);\n\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode),\n\t\t\t     &msg, &args.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\tgoto out_free;\n\n\tacl_len = res.acl_len - res.acl_data_offset;\n\tif (acl_len > args.acl_len)\n\t\tnfs4_write_cached_acl(inode, NULL, acl_len);\n\telse\n\t\tnfs4_write_cached_acl(inode, resp_buf + res.acl_data_offset,\n\t\t\t\t      acl_len);\n\tif (buf) {\n\t\tret = -ERANGE;\n\t\tif (acl_len > buflen)\n\t\t\tgoto out_free;\n\t\t_copy_from_pages(buf, pages, res.acl_data_offset,\n\t\t\t\tres.acl_len);\n\t}\n\tret = acl_len;\nout_free:\n\tfor (i = 0; i < npages; i++)\n\t\tif (pages[i])\n\t\t\t__free_page(pages[i]);\n\tif (args.acl_scratch)\n\t\t__free_page(args.acl_scratch);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 3572,
        "critical_vars": [
            "args.acl_pgbase"
        ],
        "function": "__nfs4_get_acl_uncached",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "True",
        "function_code": "static ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)\n{\n\tstruct page *pages[NFS4ACL_MAXPAGES] = {NULL, };\n\tstruct nfs_getaclargs args = {\n\t\t.fh = NFS_FH(inode),\n\t\t.acl_pages = pages,\n\t\t.acl_len = buflen,\n\t};\n\tstruct nfs_getaclres res = {\n\t\t.acl_len = buflen,\n\t};\n\tvoid *resp_buf;\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &nfs4_procedures[NFSPROC4_CLNT_GETACL],\n\t\t.rpc_argp = &args,\n\t\t.rpc_resp = &res,\n\t};\n\tint ret = -ENOMEM, npages, i, acl_len = 0;\n\n\tnpages = (buflen + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t/* As long as we're doing a round trip to the server anyway,\n\t * let's be prepared for a page of acl data. */\n\tif (npages == 0)\n\t\tnpages = 1;\n\n\tfor (i = 0; i < npages; i++) {\n\t\tpages[i] = alloc_page(GFP_KERNEL);\n\t\tif (!pages[i])\n\t\t\tgoto out_free;\n\t}\n\tif (npages > 1) {\n\t\t/* for decoding across pages */\n\t\targs.acl_scratch = alloc_page(GFP_KERNEL);\n\t\tif (!args.acl_scratch)\n\t\t\tgoto out_free;\n\t}\n\targs.acl_len = npages * PAGE_SIZE;\n\targs.acl_pgbase = 0;\n\t/* Let decode_getfacl know not to fail if the ACL data is larger than\n\t * the page we send as a guess */\n\tif (buf == NULL)\n\t\tres.acl_flags |= NFS4_ACL_LEN_REQUEST;\n\tresp_buf = page_address(pages[0]);\n\n\tdprintk(\"%s  buf %p buflen %ld npages %d args.acl_len %ld\\n\",\n\t\t__func__, buf, buflen, npages, args.acl_len);\n\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode),\n\t\t\t     &msg, &args.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\tgoto out_free;\n\n\tacl_len = res.acl_len - res.acl_data_offset;\n\tif (acl_len > args.acl_len)\n\t\tnfs4_write_cached_acl(inode, NULL, acl_len);\n\telse\n\t\tnfs4_write_cached_acl(inode, resp_buf + res.acl_data_offset,\n\t\t\t\t      acl_len);\n\tif (buf) {\n\t\tret = -ERANGE;\n\t\tif (acl_len > buflen)\n\t\t\tgoto out_free;\n\t\t_copy_from_pages(buf, pages, res.acl_data_offset,\n\t\t\t\tres.acl_len);\n\t}\n\tret = acl_len;\nout_free:\n\tfor (i = 0; i < npages; i++)\n\t\tif (pages[i])\n\t\t\t__free_page(pages[i]);\n\tif (args.acl_scratch)\n\t\t__free_page(args.acl_scratch);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 3575,
        "critical_vars": [
            "buf"
        ],
        "function": "__nfs4_get_acl_uncached",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "True",
        "function_code": "static ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)\n{\n\tstruct page *pages[NFS4ACL_MAXPAGES] = {NULL, };\n\tstruct nfs_getaclargs args = {\n\t\t.fh = NFS_FH(inode),\n\t\t.acl_pages = pages,\n\t\t.acl_len = buflen,\n\t};\n\tstruct nfs_getaclres res = {\n\t\t.acl_len = buflen,\n\t};\n\tvoid *resp_buf;\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &nfs4_procedures[NFSPROC4_CLNT_GETACL],\n\t\t.rpc_argp = &args,\n\t\t.rpc_resp = &res,\n\t};\n\tint ret = -ENOMEM, npages, i, acl_len = 0;\n\n\tnpages = (buflen + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t/* As long as we're doing a round trip to the server anyway,\n\t * let's be prepared for a page of acl data. */\n\tif (npages == 0)\n\t\tnpages = 1;\n\n\tfor (i = 0; i < npages; i++) {\n\t\tpages[i] = alloc_page(GFP_KERNEL);\n\t\tif (!pages[i])\n\t\t\tgoto out_free;\n\t}\n\tif (npages > 1) {\n\t\t/* for decoding across pages */\n\t\targs.acl_scratch = alloc_page(GFP_KERNEL);\n\t\tif (!args.acl_scratch)\n\t\t\tgoto out_free;\n\t}\n\targs.acl_len = npages * PAGE_SIZE;\n\targs.acl_pgbase = 0;\n\t/* Let decode_getfacl know not to fail if the ACL data is larger than\n\t * the page we send as a guess */\n\tif (buf == NULL)\n\t\tres.acl_flags |= NFS4_ACL_LEN_REQUEST;\n\tresp_buf = page_address(pages[0]);\n\n\tdprintk(\"%s  buf %p buflen %ld npages %d args.acl_len %ld\\n\",\n\t\t__func__, buf, buflen, npages, args.acl_len);\n\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode),\n\t\t\t     &msg, &args.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\tgoto out_free;\n\n\tacl_len = res.acl_len - res.acl_data_offset;\n\tif (acl_len > args.acl_len)\n\t\tnfs4_write_cached_acl(inode, NULL, acl_len);\n\telse\n\t\tnfs4_write_cached_acl(inode, resp_buf + res.acl_data_offset,\n\t\t\t\t      acl_len);\n\tif (buf) {\n\t\tret = -ERANGE;\n\t\tif (acl_len > buflen)\n\t\t\tgoto out_free;\n\t\t_copy_from_pages(buf, pages, res.acl_data_offset,\n\t\t\t\tres.acl_len);\n\t}\n\tret = acl_len;\nout_free:\n\tfor (i = 0; i < npages; i++)\n\t\tif (pages[i])\n\t\t\t__free_page(pages[i]);\n\tif (args.acl_scratch)\n\t\t__free_page(args.acl_scratch);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 3576,
        "critical_vars": [
            "res.acl_flags"
        ],
        "function": "__nfs4_get_acl_uncached",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "True",
        "function_code": "static ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)\n{\n\tstruct page *pages[NFS4ACL_MAXPAGES] = {NULL, };\n\tstruct nfs_getaclargs args = {\n\t\t.fh = NFS_FH(inode),\n\t\t.acl_pages = pages,\n\t\t.acl_len = buflen,\n\t};\n\tstruct nfs_getaclres res = {\n\t\t.acl_len = buflen,\n\t};\n\tvoid *resp_buf;\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &nfs4_procedures[NFSPROC4_CLNT_GETACL],\n\t\t.rpc_argp = &args,\n\t\t.rpc_resp = &res,\n\t};\n\tint ret = -ENOMEM, npages, i, acl_len = 0;\n\n\tnpages = (buflen + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t/* As long as we're doing a round trip to the server anyway,\n\t * let's be prepared for a page of acl data. */\n\tif (npages == 0)\n\t\tnpages = 1;\n\n\tfor (i = 0; i < npages; i++) {\n\t\tpages[i] = alloc_page(GFP_KERNEL);\n\t\tif (!pages[i])\n\t\t\tgoto out_free;\n\t}\n\tif (npages > 1) {\n\t\t/* for decoding across pages */\n\t\targs.acl_scratch = alloc_page(GFP_KERNEL);\n\t\tif (!args.acl_scratch)\n\t\t\tgoto out_free;\n\t}\n\targs.acl_len = npages * PAGE_SIZE;\n\targs.acl_pgbase = 0;\n\t/* Let decode_getfacl know not to fail if the ACL data is larger than\n\t * the page we send as a guess */\n\tif (buf == NULL)\n\t\tres.acl_flags |= NFS4_ACL_LEN_REQUEST;\n\tresp_buf = page_address(pages[0]);\n\n\tdprintk(\"%s  buf %p buflen %ld npages %d args.acl_len %ld\\n\",\n\t\t__func__, buf, buflen, npages, args.acl_len);\n\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode),\n\t\t\t     &msg, &args.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\tgoto out_free;\n\n\tacl_len = res.acl_len - res.acl_data_offset;\n\tif (acl_len > args.acl_len)\n\t\tnfs4_write_cached_acl(inode, NULL, acl_len);\n\telse\n\t\tnfs4_write_cached_acl(inode, resp_buf + res.acl_data_offset,\n\t\t\t\t      acl_len);\n\tif (buf) {\n\t\tret = -ERANGE;\n\t\tif (acl_len > buflen)\n\t\t\tgoto out_free;\n\t\t_copy_from_pages(buf, pages, res.acl_data_offset,\n\t\t\t\tres.acl_len);\n\t}\n\tret = acl_len;\nout_free:\n\tfor (i = 0; i < npages; i++)\n\t\tif (pages[i])\n\t\t\t__free_page(pages[i]);\n\tif (args.acl_scratch)\n\t\t__free_page(args.acl_scratch);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 3577,
        "critical_vars": [
            "resp_buf"
        ],
        "function": "__nfs4_get_acl_uncached",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "True",
        "function_code": "static ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)\n{\n\tstruct page *pages[NFS4ACL_MAXPAGES] = {NULL, };\n\tstruct nfs_getaclargs args = {\n\t\t.fh = NFS_FH(inode),\n\t\t.acl_pages = pages,\n\t\t.acl_len = buflen,\n\t};\n\tstruct nfs_getaclres res = {\n\t\t.acl_len = buflen,\n\t};\n\tvoid *resp_buf;\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &nfs4_procedures[NFSPROC4_CLNT_GETACL],\n\t\t.rpc_argp = &args,\n\t\t.rpc_resp = &res,\n\t};\n\tint ret = -ENOMEM, npages, i, acl_len = 0;\n\n\tnpages = (buflen + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t/* As long as we're doing a round trip to the server anyway,\n\t * let's be prepared for a page of acl data. */\n\tif (npages == 0)\n\t\tnpages = 1;\n\n\tfor (i = 0; i < npages; i++) {\n\t\tpages[i] = alloc_page(GFP_KERNEL);\n\t\tif (!pages[i])\n\t\t\tgoto out_free;\n\t}\n\tif (npages > 1) {\n\t\t/* for decoding across pages */\n\t\targs.acl_scratch = alloc_page(GFP_KERNEL);\n\t\tif (!args.acl_scratch)\n\t\t\tgoto out_free;\n\t}\n\targs.acl_len = npages * PAGE_SIZE;\n\targs.acl_pgbase = 0;\n\t/* Let decode_getfacl know not to fail if the ACL data is larger than\n\t * the page we send as a guess */\n\tif (buf == NULL)\n\t\tres.acl_flags |= NFS4_ACL_LEN_REQUEST;\n\tresp_buf = page_address(pages[0]);\n\n\tdprintk(\"%s  buf %p buflen %ld npages %d args.acl_len %ld\\n\",\n\t\t__func__, buf, buflen, npages, args.acl_len);\n\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode),\n\t\t\t     &msg, &args.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\tgoto out_free;\n\n\tacl_len = res.acl_len - res.acl_data_offset;\n\tif (acl_len > args.acl_len)\n\t\tnfs4_write_cached_acl(inode, NULL, acl_len);\n\telse\n\t\tnfs4_write_cached_acl(inode, resp_buf + res.acl_data_offset,\n\t\t\t\t      acl_len);\n\tif (buf) {\n\t\tret = -ERANGE;\n\t\tif (acl_len > buflen)\n\t\t\tgoto out_free;\n\t\t_copy_from_pages(buf, pages, res.acl_data_offset,\n\t\t\t\tres.acl_len);\n\t}\n\tret = acl_len;\nout_free:\n\tfor (i = 0; i < npages; i++)\n\t\tif (pages[i])\n\t\t\t__free_page(pages[i]);\n\tif (args.acl_scratch)\n\t\t__free_page(args.acl_scratch);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 3579,
        "critical_vars": [
            "args.acl_len",
            "buflen",
            "npages",
            "buf",
            "__func__"
        ],
        "function": "__nfs4_get_acl_uncached",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "True",
        "function_code": "static ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)\n{\n\tstruct page *pages[NFS4ACL_MAXPAGES] = {NULL, };\n\tstruct nfs_getaclargs args = {\n\t\t.fh = NFS_FH(inode),\n\t\t.acl_pages = pages,\n\t\t.acl_len = buflen,\n\t};\n\tstruct nfs_getaclres res = {\n\t\t.acl_len = buflen,\n\t};\n\tvoid *resp_buf;\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &nfs4_procedures[NFSPROC4_CLNT_GETACL],\n\t\t.rpc_argp = &args,\n\t\t.rpc_resp = &res,\n\t};\n\tint ret = -ENOMEM, npages, i, acl_len = 0;\n\n\tnpages = (buflen + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t/* As long as we're doing a round trip to the server anyway,\n\t * let's be prepared for a page of acl data. */\n\tif (npages == 0)\n\t\tnpages = 1;\n\n\tfor (i = 0; i < npages; i++) {\n\t\tpages[i] = alloc_page(GFP_KERNEL);\n\t\tif (!pages[i])\n\t\t\tgoto out_free;\n\t}\n\tif (npages > 1) {\n\t\t/* for decoding across pages */\n\t\targs.acl_scratch = alloc_page(GFP_KERNEL);\n\t\tif (!args.acl_scratch)\n\t\t\tgoto out_free;\n\t}\n\targs.acl_len = npages * PAGE_SIZE;\n\targs.acl_pgbase = 0;\n\t/* Let decode_getfacl know not to fail if the ACL data is larger than\n\t * the page we send as a guess */\n\tif (buf == NULL)\n\t\tres.acl_flags |= NFS4_ACL_LEN_REQUEST;\n\tresp_buf = page_address(pages[0]);\n\n\tdprintk(\"%s  buf %p buflen %ld npages %d args.acl_len %ld\\n\",\n\t\t__func__, buf, buflen, npages, args.acl_len);\n\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode),\n\t\t\t     &msg, &args.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\tgoto out_free;\n\n\tacl_len = res.acl_len - res.acl_data_offset;\n\tif (acl_len > args.acl_len)\n\t\tnfs4_write_cached_acl(inode, NULL, acl_len);\n\telse\n\t\tnfs4_write_cached_acl(inode, resp_buf + res.acl_data_offset,\n\t\t\t\t      acl_len);\n\tif (buf) {\n\t\tret = -ERANGE;\n\t\tif (acl_len > buflen)\n\t\t\tgoto out_free;\n\t\t_copy_from_pages(buf, pages, res.acl_data_offset,\n\t\t\t\tres.acl_len);\n\t}\n\tret = acl_len;\nout_free:\n\tfor (i = 0; i < npages; i++)\n\t\tif (pages[i])\n\t\t\t__free_page(pages[i]);\n\tif (args.acl_scratch)\n\t\t__free_page(args.acl_scratch);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 3581,
        "critical_vars": [
            "ret"
        ],
        "function": "__nfs4_get_acl_uncached",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "True",
        "function_code": "static ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)\n{\n\tstruct page *pages[NFS4ACL_MAXPAGES] = {NULL, };\n\tstruct nfs_getaclargs args = {\n\t\t.fh = NFS_FH(inode),\n\t\t.acl_pages = pages,\n\t\t.acl_len = buflen,\n\t};\n\tstruct nfs_getaclres res = {\n\t\t.acl_len = buflen,\n\t};\n\tvoid *resp_buf;\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &nfs4_procedures[NFSPROC4_CLNT_GETACL],\n\t\t.rpc_argp = &args,\n\t\t.rpc_resp = &res,\n\t};\n\tint ret = -ENOMEM, npages, i, acl_len = 0;\n\n\tnpages = (buflen + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t/* As long as we're doing a round trip to the server anyway,\n\t * let's be prepared for a page of acl data. */\n\tif (npages == 0)\n\t\tnpages = 1;\n\n\tfor (i = 0; i < npages; i++) {\n\t\tpages[i] = alloc_page(GFP_KERNEL);\n\t\tif (!pages[i])\n\t\t\tgoto out_free;\n\t}\n\tif (npages > 1) {\n\t\t/* for decoding across pages */\n\t\targs.acl_scratch = alloc_page(GFP_KERNEL);\n\t\tif (!args.acl_scratch)\n\t\t\tgoto out_free;\n\t}\n\targs.acl_len = npages * PAGE_SIZE;\n\targs.acl_pgbase = 0;\n\t/* Let decode_getfacl know not to fail if the ACL data is larger than\n\t * the page we send as a guess */\n\tif (buf == NULL)\n\t\tres.acl_flags |= NFS4_ACL_LEN_REQUEST;\n\tresp_buf = page_address(pages[0]);\n\n\tdprintk(\"%s  buf %p buflen %ld npages %d args.acl_len %ld\\n\",\n\t\t__func__, buf, buflen, npages, args.acl_len);\n\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode),\n\t\t\t     &msg, &args.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\tgoto out_free;\n\n\tacl_len = res.acl_len - res.acl_data_offset;\n\tif (acl_len > args.acl_len)\n\t\tnfs4_write_cached_acl(inode, NULL, acl_len);\n\telse\n\t\tnfs4_write_cached_acl(inode, resp_buf + res.acl_data_offset,\n\t\t\t\t      acl_len);\n\tif (buf) {\n\t\tret = -ERANGE;\n\t\tif (acl_len > buflen)\n\t\t\tgoto out_free;\n\t\t_copy_from_pages(buf, pages, res.acl_data_offset,\n\t\t\t\tres.acl_len);\n\t}\n\tret = acl_len;\nout_free:\n\tfor (i = 0; i < npages; i++)\n\t\tif (pages[i])\n\t\t\t__free_page(pages[i]);\n\tif (args.acl_scratch)\n\t\t__free_page(args.acl_scratch);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "if-Condition",
        "line_old": 3575,
        "critical_vars": [
            "res.acl_len"
        ],
        "function": "__nfs4_get_acl_uncached",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "False",
        "function_code": "\nstatic ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)\n{\n\tstruct page *pages[NFS4ACL_MAXPAGES];\n\tstruct nfs_getaclargs args = {\n\t\t.fh = NFS_FH(inode),\n\t\t.acl_pages = pages,\n\t\t.acl_len = buflen,\n\t};\n\tstruct nfs_getaclres res = {\n\t\t.acl_len = buflen,\n\t};\n\tvoid *resp_buf;\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &nfs4_procedures[NFSPROC4_CLNT_GETACL],\n\t\t.rpc_argp = &args,\n\t\t.rpc_resp = &res,\n\t};\n\tstruct page *localpage = NULL;\n\tint ret;\n\n\tif (buflen < PAGE_SIZE) {\n\t\t/* As long as we're doing a round trip to the server anyway,\n\t\t * let's be prepared for a page of acl data. */\n\t\tlocalpage = alloc_page(GFP_KERNEL);\n\t\tresp_buf = page_address(localpage);\n\t\tif (localpage == NULL)\n\t\t\treturn -ENOMEM;\n\t\targs.acl_pages[0] = localpage;\n\t\targs.acl_pgbase = 0;\n\t\targs.acl_len = PAGE_SIZE;\n\t} else {\n\t\tresp_buf = buf;\n\t\tbuf_to_pages(buf, buflen, args.acl_pages, &args.acl_pgbase);\n\t}\n\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode), &msg, &args.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\tgoto out_free;\n\tif (res.acl_len > args.acl_len)\n\t\tnfs4_write_cached_acl(inode, NULL, res.acl_len);\n\telse\n\t\tnfs4_write_cached_acl(inode, resp_buf, res.acl_len);\n\tif (buf) {\n\t\tret = -ERANGE;\n\t\tif (res.acl_len > buflen)\n\t\t\tgoto out_free;\n\t\tif (localpage)\n\t\t\tmemcpy(buf, resp_buf, res.acl_len);\n\t}\n\tret = res.acl_len;\nout_free:\n\tif (localpage)\n\t\t__free_page(localpage);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 3586,
        "critical_vars": [
            "acl_len"
        ],
        "function": "__nfs4_get_acl_uncached",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "True",
        "function_code": "static ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)\n{\n\tstruct page *pages[NFS4ACL_MAXPAGES] = {NULL, };\n\tstruct nfs_getaclargs args = {\n\t\t.fh = NFS_FH(inode),\n\t\t.acl_pages = pages,\n\t\t.acl_len = buflen,\n\t};\n\tstruct nfs_getaclres res = {\n\t\t.acl_len = buflen,\n\t};\n\tvoid *resp_buf;\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &nfs4_procedures[NFSPROC4_CLNT_GETACL],\n\t\t.rpc_argp = &args,\n\t\t.rpc_resp = &res,\n\t};\n\tint ret = -ENOMEM, npages, i, acl_len = 0;\n\n\tnpages = (buflen + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t/* As long as we're doing a round trip to the server anyway,\n\t * let's be prepared for a page of acl data. */\n\tif (npages == 0)\n\t\tnpages = 1;\n\n\tfor (i = 0; i < npages; i++) {\n\t\tpages[i] = alloc_page(GFP_KERNEL);\n\t\tif (!pages[i])\n\t\t\tgoto out_free;\n\t}\n\tif (npages > 1) {\n\t\t/* for decoding across pages */\n\t\targs.acl_scratch = alloc_page(GFP_KERNEL);\n\t\tif (!args.acl_scratch)\n\t\t\tgoto out_free;\n\t}\n\targs.acl_len = npages * PAGE_SIZE;\n\targs.acl_pgbase = 0;\n\t/* Let decode_getfacl know not to fail if the ACL data is larger than\n\t * the page we send as a guess */\n\tif (buf == NULL)\n\t\tres.acl_flags |= NFS4_ACL_LEN_REQUEST;\n\tresp_buf = page_address(pages[0]);\n\n\tdprintk(\"%s  buf %p buflen %ld npages %d args.acl_len %ld\\n\",\n\t\t__func__, buf, buflen, npages, args.acl_len);\n\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode),\n\t\t\t     &msg, &args.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\tgoto out_free;\n\n\tacl_len = res.acl_len - res.acl_data_offset;\n\tif (acl_len > args.acl_len)\n\t\tnfs4_write_cached_acl(inode, NULL, acl_len);\n\telse\n\t\tnfs4_write_cached_acl(inode, resp_buf + res.acl_data_offset,\n\t\t\t\t      acl_len);\n\tif (buf) {\n\t\tret = -ERANGE;\n\t\tif (acl_len > buflen)\n\t\t\tgoto out_free;\n\t\t_copy_from_pages(buf, pages, res.acl_data_offset,\n\t\t\t\tres.acl_len);\n\t}\n\tret = acl_len;\nout_free:\n\tfor (i = 0; i < npages; i++)\n\t\tif (pages[i])\n\t\t\t__free_page(pages[i]);\n\tif (args.acl_scratch)\n\t\t__free_page(args.acl_scratch);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Fun-Call",
        "line_old": 3576,
        "critical_vars": [
            "inode",
            "res.acl_len"
        ],
        "function": "__nfs4_get_acl_uncached",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "False",
        "function_code": "\nstatic ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)\n{\n\tstruct page *pages[NFS4ACL_MAXPAGES];\n\tstruct nfs_getaclargs args = {\n\t\t.fh = NFS_FH(inode),\n\t\t.acl_pages = pages,\n\t\t.acl_len = buflen,\n\t};\n\tstruct nfs_getaclres res = {\n\t\t.acl_len = buflen,\n\t};\n\tvoid *resp_buf;\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &nfs4_procedures[NFSPROC4_CLNT_GETACL],\n\t\t.rpc_argp = &args,\n\t\t.rpc_resp = &res,\n\t};\n\tstruct page *localpage = NULL;\n\tint ret;\n\n\tif (buflen < PAGE_SIZE) {\n\t\t/* As long as we're doing a round trip to the server anyway,\n\t\t * let's be prepared for a page of acl data. */\n\t\tlocalpage = alloc_page(GFP_KERNEL);\n\t\tresp_buf = page_address(localpage);\n\t\tif (localpage == NULL)\n\t\t\treturn -ENOMEM;\n\t\targs.acl_pages[0] = localpage;\n\t\targs.acl_pgbase = 0;\n\t\targs.acl_len = PAGE_SIZE;\n\t} else {\n\t\tresp_buf = buf;\n\t\tbuf_to_pages(buf, buflen, args.acl_pages, &args.acl_pgbase);\n\t}\n\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode), &msg, &args.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\tgoto out_free;\n\tif (res.acl_len > args.acl_len)\n\t\tnfs4_write_cached_acl(inode, NULL, res.acl_len);\n\telse\n\t\tnfs4_write_cached_acl(inode, resp_buf, res.acl_len);\n\tif (buf) {\n\t\tret = -ERANGE;\n\t\tif (res.acl_len > buflen)\n\t\t\tgoto out_free;\n\t\tif (localpage)\n\t\t\tmemcpy(buf, resp_buf, res.acl_len);\n\t}\n\tret = res.acl_len;\nout_free:\n\tif (localpage)\n\t\t__free_page(localpage);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 3587,
        "critical_vars": [
            "acl_len"
        ],
        "function": "__nfs4_get_acl_uncached",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "True",
        "function_code": "static ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)\n{\n\tstruct page *pages[NFS4ACL_MAXPAGES] = {NULL, };\n\tstruct nfs_getaclargs args = {\n\t\t.fh = NFS_FH(inode),\n\t\t.acl_pages = pages,\n\t\t.acl_len = buflen,\n\t};\n\tstruct nfs_getaclres res = {\n\t\t.acl_len = buflen,\n\t};\n\tvoid *resp_buf;\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &nfs4_procedures[NFSPROC4_CLNT_GETACL],\n\t\t.rpc_argp = &args,\n\t\t.rpc_resp = &res,\n\t};\n\tint ret = -ENOMEM, npages, i, acl_len = 0;\n\n\tnpages = (buflen + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t/* As long as we're doing a round trip to the server anyway,\n\t * let's be prepared for a page of acl data. */\n\tif (npages == 0)\n\t\tnpages = 1;\n\n\tfor (i = 0; i < npages; i++) {\n\t\tpages[i] = alloc_page(GFP_KERNEL);\n\t\tif (!pages[i])\n\t\t\tgoto out_free;\n\t}\n\tif (npages > 1) {\n\t\t/* for decoding across pages */\n\t\targs.acl_scratch = alloc_page(GFP_KERNEL);\n\t\tif (!args.acl_scratch)\n\t\t\tgoto out_free;\n\t}\n\targs.acl_len = npages * PAGE_SIZE;\n\targs.acl_pgbase = 0;\n\t/* Let decode_getfacl know not to fail if the ACL data is larger than\n\t * the page we send as a guess */\n\tif (buf == NULL)\n\t\tres.acl_flags |= NFS4_ACL_LEN_REQUEST;\n\tresp_buf = page_address(pages[0]);\n\n\tdprintk(\"%s  buf %p buflen %ld npages %d args.acl_len %ld\\n\",\n\t\t__func__, buf, buflen, npages, args.acl_len);\n\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode),\n\t\t\t     &msg, &args.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\tgoto out_free;\n\n\tacl_len = res.acl_len - res.acl_data_offset;\n\tif (acl_len > args.acl_len)\n\t\tnfs4_write_cached_acl(inode, NULL, acl_len);\n\telse\n\t\tnfs4_write_cached_acl(inode, resp_buf + res.acl_data_offset,\n\t\t\t\t      acl_len);\n\tif (buf) {\n\t\tret = -ERANGE;\n\t\tif (acl_len > buflen)\n\t\t\tgoto out_free;\n\t\t_copy_from_pages(buf, pages, res.acl_data_offset,\n\t\t\t\tres.acl_len);\n\t}\n\tret = acl_len;\nout_free:\n\tfor (i = 0; i < npages; i++)\n\t\tif (pages[i])\n\t\t\t__free_page(pages[i]);\n\tif (args.acl_scratch)\n\t\t__free_page(args.acl_scratch);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 3588,
        "critical_vars": [
            "acl_len",
            "inode"
        ],
        "function": "__nfs4_get_acl_uncached",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "True",
        "function_code": "static ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)\n{\n\tstruct page *pages[NFS4ACL_MAXPAGES] = {NULL, };\n\tstruct nfs_getaclargs args = {\n\t\t.fh = NFS_FH(inode),\n\t\t.acl_pages = pages,\n\t\t.acl_len = buflen,\n\t};\n\tstruct nfs_getaclres res = {\n\t\t.acl_len = buflen,\n\t};\n\tvoid *resp_buf;\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &nfs4_procedures[NFSPROC4_CLNT_GETACL],\n\t\t.rpc_argp = &args,\n\t\t.rpc_resp = &res,\n\t};\n\tint ret = -ENOMEM, npages, i, acl_len = 0;\n\n\tnpages = (buflen + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t/* As long as we're doing a round trip to the server anyway,\n\t * let's be prepared for a page of acl data. */\n\tif (npages == 0)\n\t\tnpages = 1;\n\n\tfor (i = 0; i < npages; i++) {\n\t\tpages[i] = alloc_page(GFP_KERNEL);\n\t\tif (!pages[i])\n\t\t\tgoto out_free;\n\t}\n\tif (npages > 1) {\n\t\t/* for decoding across pages */\n\t\targs.acl_scratch = alloc_page(GFP_KERNEL);\n\t\tif (!args.acl_scratch)\n\t\t\tgoto out_free;\n\t}\n\targs.acl_len = npages * PAGE_SIZE;\n\targs.acl_pgbase = 0;\n\t/* Let decode_getfacl know not to fail if the ACL data is larger than\n\t * the page we send as a guess */\n\tif (buf == NULL)\n\t\tres.acl_flags |= NFS4_ACL_LEN_REQUEST;\n\tresp_buf = page_address(pages[0]);\n\n\tdprintk(\"%s  buf %p buflen %ld npages %d args.acl_len %ld\\n\",\n\t\t__func__, buf, buflen, npages, args.acl_len);\n\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode),\n\t\t\t     &msg, &args.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\tgoto out_free;\n\n\tacl_len = res.acl_len - res.acl_data_offset;\n\tif (acl_len > args.acl_len)\n\t\tnfs4_write_cached_acl(inode, NULL, acl_len);\n\telse\n\t\tnfs4_write_cached_acl(inode, resp_buf + res.acl_data_offset,\n\t\t\t\t      acl_len);\n\tif (buf) {\n\t\tret = -ERANGE;\n\t\tif (acl_len > buflen)\n\t\t\tgoto out_free;\n\t\t_copy_from_pages(buf, pages, res.acl_data_offset,\n\t\t\t\tres.acl_len);\n\t}\n\tret = acl_len;\nout_free:\n\tfor (i = 0; i < npages; i++)\n\t\tif (pages[i])\n\t\t\t__free_page(pages[i]);\n\tif (args.acl_scratch)\n\t\t__free_page(args.acl_scratch);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Replace",
        "change_type": "Fun-Call",
        "line_old": 3578,
        "line_new": 3590,
        "critical_vars": [
            "acl_len",
            "res.acl_data_offset",
            "res.acl_len"
        ],
        "function": "__nfs4_get_acl_uncached",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "True",
        "function_code": "static ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)\n{\n\tstruct page *pages[NFS4ACL_MAXPAGES] = {NULL, };\n\tstruct nfs_getaclargs args = {\n\t\t.fh = NFS_FH(inode),\n\t\t.acl_pages = pages,\n\t\t.acl_len = buflen,\n\t};\n\tstruct nfs_getaclres res = {\n\t\t.acl_len = buflen,\n\t};\n\tvoid *resp_buf;\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &nfs4_procedures[NFSPROC4_CLNT_GETACL],\n\t\t.rpc_argp = &args,\n\t\t.rpc_resp = &res,\n\t};\n\tint ret = -ENOMEM, npages, i, acl_len = 0;\n\n\tnpages = (buflen + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t/* As long as we're doing a round trip to the server anyway,\n\t * let's be prepared for a page of acl data. */\n\tif (npages == 0)\n\t\tnpages = 1;\n\n\tfor (i = 0; i < npages; i++) {\n\t\tpages[i] = alloc_page(GFP_KERNEL);\n\t\tif (!pages[i])\n\t\t\tgoto out_free;\n\t}\n\tif (npages > 1) {\n\t\t/* for decoding across pages */\n\t\targs.acl_scratch = alloc_page(GFP_KERNEL);\n\t\tif (!args.acl_scratch)\n\t\t\tgoto out_free;\n\t}\n\targs.acl_len = npages * PAGE_SIZE;\n\targs.acl_pgbase = 0;\n\t/* Let decode_getfacl know not to fail if the ACL data is larger than\n\t * the page we send as a guess */\n\tif (buf == NULL)\n\t\tres.acl_flags |= NFS4_ACL_LEN_REQUEST;\n\tresp_buf = page_address(pages[0]);\n\n\tdprintk(\"%s  buf %p buflen %ld npages %d args.acl_len %ld\\n\",\n\t\t__func__, buf, buflen, npages, args.acl_len);\n\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode),\n\t\t\t     &msg, &args.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\tgoto out_free;\n\n\tacl_len = res.acl_len - res.acl_data_offset;\n\tif (acl_len > args.acl_len)\n\t\tnfs4_write_cached_acl(inode, NULL, acl_len);\n\telse\n\t\tnfs4_write_cached_acl(inode, resp_buf + res.acl_data_offset,\n\t\t\t\t      acl_len);\n\tif (buf) {\n\t\tret = -ERANGE;\n\t\tif (acl_len > buflen)\n\t\t\tgoto out_free;\n\t\t_copy_from_pages(buf, pages, res.acl_data_offset,\n\t\t\t\tres.acl_len);\n\t}\n\tret = acl_len;\nout_free:\n\tfor (i = 0; i < npages; i++)\n\t\tif (pages[i])\n\t\t\t__free_page(pages[i]);\n\tif (args.acl_scratch)\n\t\t__free_page(args.acl_scratch);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "if-Condition",
        "line_old": 3581,
        "critical_vars": [
            "res.acl_len"
        ],
        "function": "__nfs4_get_acl_uncached",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "False",
        "function_code": "\nstatic ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)\n{\n\tstruct page *pages[NFS4ACL_MAXPAGES];\n\tstruct nfs_getaclargs args = {\n\t\t.fh = NFS_FH(inode),\n\t\t.acl_pages = pages,\n\t\t.acl_len = buflen,\n\t};\n\tstruct nfs_getaclres res = {\n\t\t.acl_len = buflen,\n\t};\n\tvoid *resp_buf;\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &nfs4_procedures[NFSPROC4_CLNT_GETACL],\n\t\t.rpc_argp = &args,\n\t\t.rpc_resp = &res,\n\t};\n\tstruct page *localpage = NULL;\n\tint ret;\n\n\tif (buflen < PAGE_SIZE) {\n\t\t/* As long as we're doing a round trip to the server anyway,\n\t\t * let's be prepared for a page of acl data. */\n\t\tlocalpage = alloc_page(GFP_KERNEL);\n\t\tresp_buf = page_address(localpage);\n\t\tif (localpage == NULL)\n\t\t\treturn -ENOMEM;\n\t\targs.acl_pages[0] = localpage;\n\t\targs.acl_pgbase = 0;\n\t\targs.acl_len = PAGE_SIZE;\n\t} else {\n\t\tresp_buf = buf;\n\t\tbuf_to_pages(buf, buflen, args.acl_pages, &args.acl_pgbase);\n\t}\n\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode), &msg, &args.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\tgoto out_free;\n\tif (res.acl_len > args.acl_len)\n\t\tnfs4_write_cached_acl(inode, NULL, res.acl_len);\n\telse\n\t\tnfs4_write_cached_acl(inode, resp_buf, res.acl_len);\n\tif (buf) {\n\t\tret = -ERANGE;\n\t\tif (res.acl_len > buflen)\n\t\t\tgoto out_free;\n\t\tif (localpage)\n\t\t\tmemcpy(buf, resp_buf, res.acl_len);\n\t}\n\tret = res.acl_len;\nout_free:\n\tif (localpage)\n\t\t__free_page(localpage);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 3594,
        "critical_vars": [
            "acl_len"
        ],
        "function": "__nfs4_get_acl_uncached",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "True",
        "function_code": "static ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)\n{\n\tstruct page *pages[NFS4ACL_MAXPAGES] = {NULL, };\n\tstruct nfs_getaclargs args = {\n\t\t.fh = NFS_FH(inode),\n\t\t.acl_pages = pages,\n\t\t.acl_len = buflen,\n\t};\n\tstruct nfs_getaclres res = {\n\t\t.acl_len = buflen,\n\t};\n\tvoid *resp_buf;\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &nfs4_procedures[NFSPROC4_CLNT_GETACL],\n\t\t.rpc_argp = &args,\n\t\t.rpc_resp = &res,\n\t};\n\tint ret = -ENOMEM, npages, i, acl_len = 0;\n\n\tnpages = (buflen + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t/* As long as we're doing a round trip to the server anyway,\n\t * let's be prepared for a page of acl data. */\n\tif (npages == 0)\n\t\tnpages = 1;\n\n\tfor (i = 0; i < npages; i++) {\n\t\tpages[i] = alloc_page(GFP_KERNEL);\n\t\tif (!pages[i])\n\t\t\tgoto out_free;\n\t}\n\tif (npages > 1) {\n\t\t/* for decoding across pages */\n\t\targs.acl_scratch = alloc_page(GFP_KERNEL);\n\t\tif (!args.acl_scratch)\n\t\t\tgoto out_free;\n\t}\n\targs.acl_len = npages * PAGE_SIZE;\n\targs.acl_pgbase = 0;\n\t/* Let decode_getfacl know not to fail if the ACL data is larger than\n\t * the page we send as a guess */\n\tif (buf == NULL)\n\t\tres.acl_flags |= NFS4_ACL_LEN_REQUEST;\n\tresp_buf = page_address(pages[0]);\n\n\tdprintk(\"%s  buf %p buflen %ld npages %d args.acl_len %ld\\n\",\n\t\t__func__, buf, buflen, npages, args.acl_len);\n\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode),\n\t\t\t     &msg, &args.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\tgoto out_free;\n\n\tacl_len = res.acl_len - res.acl_data_offset;\n\tif (acl_len > args.acl_len)\n\t\tnfs4_write_cached_acl(inode, NULL, acl_len);\n\telse\n\t\tnfs4_write_cached_acl(inode, resp_buf + res.acl_data_offset,\n\t\t\t\t      acl_len);\n\tif (buf) {\n\t\tret = -ERANGE;\n\t\tif (acl_len > buflen)\n\t\t\tgoto out_free;\n\t\t_copy_from_pages(buf, pages, res.acl_data_offset,\n\t\t\t\tres.acl_len);\n\t}\n\tret = acl_len;\nout_free:\n\tfor (i = 0; i < npages; i++)\n\t\tif (pages[i])\n\t\t\t__free_page(pages[i]);\n\tif (args.acl_scratch)\n\t\t__free_page(args.acl_scratch);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "if-Condition",
        "line_old": 3583,
        "critical_vars": [
            "localpage"
        ],
        "function": "__nfs4_get_acl_uncached",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "False",
        "function_code": "\nstatic ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)\n{\n\tstruct page *pages[NFS4ACL_MAXPAGES];\n\tstruct nfs_getaclargs args = {\n\t\t.fh = NFS_FH(inode),\n\t\t.acl_pages = pages,\n\t\t.acl_len = buflen,\n\t};\n\tstruct nfs_getaclres res = {\n\t\t.acl_len = buflen,\n\t};\n\tvoid *resp_buf;\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &nfs4_procedures[NFSPROC4_CLNT_GETACL],\n\t\t.rpc_argp = &args,\n\t\t.rpc_resp = &res,\n\t};\n\tstruct page *localpage = NULL;\n\tint ret;\n\n\tif (buflen < PAGE_SIZE) {\n\t\t/* As long as we're doing a round trip to the server anyway,\n\t\t * let's be prepared for a page of acl data. */\n\t\tlocalpage = alloc_page(GFP_KERNEL);\n\t\tresp_buf = page_address(localpage);\n\t\tif (localpage == NULL)\n\t\t\treturn -ENOMEM;\n\t\targs.acl_pages[0] = localpage;\n\t\targs.acl_pgbase = 0;\n\t\targs.acl_len = PAGE_SIZE;\n\t} else {\n\t\tresp_buf = buf;\n\t\tbuf_to_pages(buf, buflen, args.acl_pages, &args.acl_pgbase);\n\t}\n\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode), &msg, &args.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\tgoto out_free;\n\tif (res.acl_len > args.acl_len)\n\t\tnfs4_write_cached_acl(inode, NULL, res.acl_len);\n\telse\n\t\tnfs4_write_cached_acl(inode, resp_buf, res.acl_len);\n\tif (buf) {\n\t\tret = -ERANGE;\n\t\tif (res.acl_len > buflen)\n\t\t\tgoto out_free;\n\t\tif (localpage)\n\t\t\tmemcpy(buf, resp_buf, res.acl_len);\n\t}\n\tret = res.acl_len;\nout_free:\n\tif (localpage)\n\t\t__free_page(localpage);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 3596,
        "critical_vars": [
            "pages",
            "buf",
            "res.acl_data_offset",
            "res.acl_len"
        ],
        "function": "__nfs4_get_acl_uncached",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "True",
        "function_code": "static ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)\n{\n\tstruct page *pages[NFS4ACL_MAXPAGES] = {NULL, };\n\tstruct nfs_getaclargs args = {\n\t\t.fh = NFS_FH(inode),\n\t\t.acl_pages = pages,\n\t\t.acl_len = buflen,\n\t};\n\tstruct nfs_getaclres res = {\n\t\t.acl_len = buflen,\n\t};\n\tvoid *resp_buf;\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &nfs4_procedures[NFSPROC4_CLNT_GETACL],\n\t\t.rpc_argp = &args,\n\t\t.rpc_resp = &res,\n\t};\n\tint ret = -ENOMEM, npages, i, acl_len = 0;\n\n\tnpages = (buflen + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t/* As long as we're doing a round trip to the server anyway,\n\t * let's be prepared for a page of acl data. */\n\tif (npages == 0)\n\t\tnpages = 1;\n\n\tfor (i = 0; i < npages; i++) {\n\t\tpages[i] = alloc_page(GFP_KERNEL);\n\t\tif (!pages[i])\n\t\t\tgoto out_free;\n\t}\n\tif (npages > 1) {\n\t\t/* for decoding across pages */\n\t\targs.acl_scratch = alloc_page(GFP_KERNEL);\n\t\tif (!args.acl_scratch)\n\t\t\tgoto out_free;\n\t}\n\targs.acl_len = npages * PAGE_SIZE;\n\targs.acl_pgbase = 0;\n\t/* Let decode_getfacl know not to fail if the ACL data is larger than\n\t * the page we send as a guess */\n\tif (buf == NULL)\n\t\tres.acl_flags |= NFS4_ACL_LEN_REQUEST;\n\tresp_buf = page_address(pages[0]);\n\n\tdprintk(\"%s  buf %p buflen %ld npages %d args.acl_len %ld\\n\",\n\t\t__func__, buf, buflen, npages, args.acl_len);\n\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode),\n\t\t\t     &msg, &args.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\tgoto out_free;\n\n\tacl_len = res.acl_len - res.acl_data_offset;\n\tif (acl_len > args.acl_len)\n\t\tnfs4_write_cached_acl(inode, NULL, acl_len);\n\telse\n\t\tnfs4_write_cached_acl(inode, resp_buf + res.acl_data_offset,\n\t\t\t\t      acl_len);\n\tif (buf) {\n\t\tret = -ERANGE;\n\t\tif (acl_len > buflen)\n\t\t\tgoto out_free;\n\t\t_copy_from_pages(buf, pages, res.acl_data_offset,\n\t\t\t\tres.acl_len);\n\t}\n\tret = acl_len;\nout_free:\n\tfor (i = 0; i < npages; i++)\n\t\tif (pages[i])\n\t\t\t__free_page(pages[i]);\n\tif (args.acl_scratch)\n\t\t__free_page(args.acl_scratch);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Fun-Call",
        "line_old": 3584,
        "critical_vars": [
            "buf",
            "resp_buf",
            "res.acl_len"
        ],
        "function": "__nfs4_get_acl_uncached",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "False",
        "function_code": "\nstatic ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)\n{\n\tstruct page *pages[NFS4ACL_MAXPAGES];\n\tstruct nfs_getaclargs args = {\n\t\t.fh = NFS_FH(inode),\n\t\t.acl_pages = pages,\n\t\t.acl_len = buflen,\n\t};\n\tstruct nfs_getaclres res = {\n\t\t.acl_len = buflen,\n\t};\n\tvoid *resp_buf;\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &nfs4_procedures[NFSPROC4_CLNT_GETACL],\n\t\t.rpc_argp = &args,\n\t\t.rpc_resp = &res,\n\t};\n\tstruct page *localpage = NULL;\n\tint ret;\n\n\tif (buflen < PAGE_SIZE) {\n\t\t/* As long as we're doing a round trip to the server anyway,\n\t\t * let's be prepared for a page of acl data. */\n\t\tlocalpage = alloc_page(GFP_KERNEL);\n\t\tresp_buf = page_address(localpage);\n\t\tif (localpage == NULL)\n\t\t\treturn -ENOMEM;\n\t\targs.acl_pages[0] = localpage;\n\t\targs.acl_pgbase = 0;\n\t\targs.acl_len = PAGE_SIZE;\n\t} else {\n\t\tresp_buf = buf;\n\t\tbuf_to_pages(buf, buflen, args.acl_pages, &args.acl_pgbase);\n\t}\n\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode), &msg, &args.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\tgoto out_free;\n\tif (res.acl_len > args.acl_len)\n\t\tnfs4_write_cached_acl(inode, NULL, res.acl_len);\n\telse\n\t\tnfs4_write_cached_acl(inode, resp_buf, res.acl_len);\n\tif (buf) {\n\t\tret = -ERANGE;\n\t\tif (res.acl_len > buflen)\n\t\t\tgoto out_free;\n\t\tif (localpage)\n\t\t\tmemcpy(buf, resp_buf, res.acl_len);\n\t}\n\tret = res.acl_len;\nout_free:\n\tif (localpage)\n\t\t__free_page(localpage);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Replace",
        "change_type": "Assignment",
        "line_old": 3586,
        "line_new": 3599,
        "critical_vars": [
            "ret"
        ],
        "function": "__nfs4_get_acl_uncached",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "True",
        "function_code": "static ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)\n{\n\tstruct page *pages[NFS4ACL_MAXPAGES] = {NULL, };\n\tstruct nfs_getaclargs args = {\n\t\t.fh = NFS_FH(inode),\n\t\t.acl_pages = pages,\n\t\t.acl_len = buflen,\n\t};\n\tstruct nfs_getaclres res = {\n\t\t.acl_len = buflen,\n\t};\n\tvoid *resp_buf;\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &nfs4_procedures[NFSPROC4_CLNT_GETACL],\n\t\t.rpc_argp = &args,\n\t\t.rpc_resp = &res,\n\t};\n\tint ret = -ENOMEM, npages, i, acl_len = 0;\n\n\tnpages = (buflen + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t/* As long as we're doing a round trip to the server anyway,\n\t * let's be prepared for a page of acl data. */\n\tif (npages == 0)\n\t\tnpages = 1;\n\n\tfor (i = 0; i < npages; i++) {\n\t\tpages[i] = alloc_page(GFP_KERNEL);\n\t\tif (!pages[i])\n\t\t\tgoto out_free;\n\t}\n\tif (npages > 1) {\n\t\t/* for decoding across pages */\n\t\targs.acl_scratch = alloc_page(GFP_KERNEL);\n\t\tif (!args.acl_scratch)\n\t\t\tgoto out_free;\n\t}\n\targs.acl_len = npages * PAGE_SIZE;\n\targs.acl_pgbase = 0;\n\t/* Let decode_getfacl know not to fail if the ACL data is larger than\n\t * the page we send as a guess */\n\tif (buf == NULL)\n\t\tres.acl_flags |= NFS4_ACL_LEN_REQUEST;\n\tresp_buf = page_address(pages[0]);\n\n\tdprintk(\"%s  buf %p buflen %ld npages %d args.acl_len %ld\\n\",\n\t\t__func__, buf, buflen, npages, args.acl_len);\n\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode),\n\t\t\t     &msg, &args.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\tgoto out_free;\n\n\tacl_len = res.acl_len - res.acl_data_offset;\n\tif (acl_len > args.acl_len)\n\t\tnfs4_write_cached_acl(inode, NULL, acl_len);\n\telse\n\t\tnfs4_write_cached_acl(inode, resp_buf + res.acl_data_offset,\n\t\t\t\t      acl_len);\n\tif (buf) {\n\t\tret = -ERANGE;\n\t\tif (acl_len > buflen)\n\t\t\tgoto out_free;\n\t\t_copy_from_pages(buf, pages, res.acl_data_offset,\n\t\t\t\tres.acl_len);\n\t}\n\tret = acl_len;\nout_free:\n\tfor (i = 0; i < npages; i++)\n\t\tif (pages[i])\n\t\t\t__free_page(pages[i]);\n\tif (args.acl_scratch)\n\t\t__free_page(args.acl_scratch);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "if-Condition",
        "line_old": 3588,
        "critical_vars": [
            "localpage"
        ],
        "function": "__nfs4_get_acl_uncached",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "False",
        "function_code": "\nstatic ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)\n{\n\tstruct page *pages[NFS4ACL_MAXPAGES];\n\tstruct nfs_getaclargs args = {\n\t\t.fh = NFS_FH(inode),\n\t\t.acl_pages = pages,\n\t\t.acl_len = buflen,\n\t};\n\tstruct nfs_getaclres res = {\n\t\t.acl_len = buflen,\n\t};\n\tvoid *resp_buf;\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &nfs4_procedures[NFSPROC4_CLNT_GETACL],\n\t\t.rpc_argp = &args,\n\t\t.rpc_resp = &res,\n\t};\n\tstruct page *localpage = NULL;\n\tint ret;\n\n\tif (buflen < PAGE_SIZE) {\n\t\t/* As long as we're doing a round trip to the server anyway,\n\t\t * let's be prepared for a page of acl data. */\n\t\tlocalpage = alloc_page(GFP_KERNEL);\n\t\tresp_buf = page_address(localpage);\n\t\tif (localpage == NULL)\n\t\t\treturn -ENOMEM;\n\t\targs.acl_pages[0] = localpage;\n\t\targs.acl_pgbase = 0;\n\t\targs.acl_len = PAGE_SIZE;\n\t} else {\n\t\tresp_buf = buf;\n\t\tbuf_to_pages(buf, buflen, args.acl_pages, &args.acl_pgbase);\n\t}\n\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode), &msg, &args.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\tgoto out_free;\n\tif (res.acl_len > args.acl_len)\n\t\tnfs4_write_cached_acl(inode, NULL, res.acl_len);\n\telse\n\t\tnfs4_write_cached_acl(inode, resp_buf, res.acl_len);\n\tif (buf) {\n\t\tret = -ERANGE;\n\t\tif (res.acl_len > buflen)\n\t\t\tgoto out_free;\n\t\tif (localpage)\n\t\t\tmemcpy(buf, resp_buf, res.acl_len);\n\t}\n\tret = res.acl_len;\nout_free:\n\tif (localpage)\n\t\t__free_page(localpage);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "for-Condition",
        "line_new": 3601,
        "critical_vars": [
            "i"
        ],
        "function": "__nfs4_get_acl_uncached",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "True",
        "function_code": "static ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)\n{\n\tstruct page *pages[NFS4ACL_MAXPAGES] = {NULL, };\n\tstruct nfs_getaclargs args = {\n\t\t.fh = NFS_FH(inode),\n\t\t.acl_pages = pages,\n\t\t.acl_len = buflen,\n\t};\n\tstruct nfs_getaclres res = {\n\t\t.acl_len = buflen,\n\t};\n\tvoid *resp_buf;\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &nfs4_procedures[NFSPROC4_CLNT_GETACL],\n\t\t.rpc_argp = &args,\n\t\t.rpc_resp = &res,\n\t};\n\tint ret = -ENOMEM, npages, i, acl_len = 0;\n\n\tnpages = (buflen + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t/* As long as we're doing a round trip to the server anyway,\n\t * let's be prepared for a page of acl data. */\n\tif (npages == 0)\n\t\tnpages = 1;\n\n\tfor (i = 0; i < npages; i++) {\n\t\tpages[i] = alloc_page(GFP_KERNEL);\n\t\tif (!pages[i])\n\t\t\tgoto out_free;\n\t}\n\tif (npages > 1) {\n\t\t/* for decoding across pages */\n\t\targs.acl_scratch = alloc_page(GFP_KERNEL);\n\t\tif (!args.acl_scratch)\n\t\t\tgoto out_free;\n\t}\n\targs.acl_len = npages * PAGE_SIZE;\n\targs.acl_pgbase = 0;\n\t/* Let decode_getfacl know not to fail if the ACL data is larger than\n\t * the page we send as a guess */\n\tif (buf == NULL)\n\t\tres.acl_flags |= NFS4_ACL_LEN_REQUEST;\n\tresp_buf = page_address(pages[0]);\n\n\tdprintk(\"%s  buf %p buflen %ld npages %d args.acl_len %ld\\n\",\n\t\t__func__, buf, buflen, npages, args.acl_len);\n\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode),\n\t\t\t     &msg, &args.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\tgoto out_free;\n\n\tacl_len = res.acl_len - res.acl_data_offset;\n\tif (acl_len > args.acl_len)\n\t\tnfs4_write_cached_acl(inode, NULL, acl_len);\n\telse\n\t\tnfs4_write_cached_acl(inode, resp_buf + res.acl_data_offset,\n\t\t\t\t      acl_len);\n\tif (buf) {\n\t\tret = -ERANGE;\n\t\tif (acl_len > buflen)\n\t\t\tgoto out_free;\n\t\t_copy_from_pages(buf, pages, res.acl_data_offset,\n\t\t\t\tres.acl_len);\n\t}\n\tret = acl_len;\nout_free:\n\tfor (i = 0; i < npages; i++)\n\t\tif (pages[i])\n\t\t\t__free_page(pages[i]);\n\tif (args.acl_scratch)\n\t\t__free_page(args.acl_scratch);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Fun-Call",
        "line_old": 3589,
        "critical_vars": [
            "localpage"
        ],
        "function": "__nfs4_get_acl_uncached",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "False",
        "function_code": "\nstatic ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)\n{\n\tstruct page *pages[NFS4ACL_MAXPAGES];\n\tstruct nfs_getaclargs args = {\n\t\t.fh = NFS_FH(inode),\n\t\t.acl_pages = pages,\n\t\t.acl_len = buflen,\n\t};\n\tstruct nfs_getaclres res = {\n\t\t.acl_len = buflen,\n\t};\n\tvoid *resp_buf;\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &nfs4_procedures[NFSPROC4_CLNT_GETACL],\n\t\t.rpc_argp = &args,\n\t\t.rpc_resp = &res,\n\t};\n\tstruct page *localpage = NULL;\n\tint ret;\n\n\tif (buflen < PAGE_SIZE) {\n\t\t/* As long as we're doing a round trip to the server anyway,\n\t\t * let's be prepared for a page of acl data. */\n\t\tlocalpage = alloc_page(GFP_KERNEL);\n\t\tresp_buf = page_address(localpage);\n\t\tif (localpage == NULL)\n\t\t\treturn -ENOMEM;\n\t\targs.acl_pages[0] = localpage;\n\t\targs.acl_pgbase = 0;\n\t\targs.acl_len = PAGE_SIZE;\n\t} else {\n\t\tresp_buf = buf;\n\t\tbuf_to_pages(buf, buflen, args.acl_pages, &args.acl_pgbase);\n\t}\n\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode), &msg, &args.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\tgoto out_free;\n\tif (res.acl_len > args.acl_len)\n\t\tnfs4_write_cached_acl(inode, NULL, res.acl_len);\n\telse\n\t\tnfs4_write_cached_acl(inode, resp_buf, res.acl_len);\n\tif (buf) {\n\t\tret = -ERANGE;\n\t\tif (res.acl_len > buflen)\n\t\t\tgoto out_free;\n\t\tif (localpage)\n\t\t\tmemcpy(buf, resp_buf, res.acl_len);\n\t}\n\tret = res.acl_len;\nout_free:\n\tif (localpage)\n\t\t__free_page(localpage);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 3602,
        "critical_vars": [
            "pages[i]"
        ],
        "function": "__nfs4_get_acl_uncached",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "True",
        "function_code": "static ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)\n{\n\tstruct page *pages[NFS4ACL_MAXPAGES] = {NULL, };\n\tstruct nfs_getaclargs args = {\n\t\t.fh = NFS_FH(inode),\n\t\t.acl_pages = pages,\n\t\t.acl_len = buflen,\n\t};\n\tstruct nfs_getaclres res = {\n\t\t.acl_len = buflen,\n\t};\n\tvoid *resp_buf;\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &nfs4_procedures[NFSPROC4_CLNT_GETACL],\n\t\t.rpc_argp = &args,\n\t\t.rpc_resp = &res,\n\t};\n\tint ret = -ENOMEM, npages, i, acl_len = 0;\n\n\tnpages = (buflen + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t/* As long as we're doing a round trip to the server anyway,\n\t * let's be prepared for a page of acl data. */\n\tif (npages == 0)\n\t\tnpages = 1;\n\n\tfor (i = 0; i < npages; i++) {\n\t\tpages[i] = alloc_page(GFP_KERNEL);\n\t\tif (!pages[i])\n\t\t\tgoto out_free;\n\t}\n\tif (npages > 1) {\n\t\t/* for decoding across pages */\n\t\targs.acl_scratch = alloc_page(GFP_KERNEL);\n\t\tif (!args.acl_scratch)\n\t\t\tgoto out_free;\n\t}\n\targs.acl_len = npages * PAGE_SIZE;\n\targs.acl_pgbase = 0;\n\t/* Let decode_getfacl know not to fail if the ACL data is larger than\n\t * the page we send as a guess */\n\tif (buf == NULL)\n\t\tres.acl_flags |= NFS4_ACL_LEN_REQUEST;\n\tresp_buf = page_address(pages[0]);\n\n\tdprintk(\"%s  buf %p buflen %ld npages %d args.acl_len %ld\\n\",\n\t\t__func__, buf, buflen, npages, args.acl_len);\n\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode),\n\t\t\t     &msg, &args.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\tgoto out_free;\n\n\tacl_len = res.acl_len - res.acl_data_offset;\n\tif (acl_len > args.acl_len)\n\t\tnfs4_write_cached_acl(inode, NULL, acl_len);\n\telse\n\t\tnfs4_write_cached_acl(inode, resp_buf + res.acl_data_offset,\n\t\t\t\t      acl_len);\n\tif (buf) {\n\t\tret = -ERANGE;\n\t\tif (acl_len > buflen)\n\t\t\tgoto out_free;\n\t\t_copy_from_pages(buf, pages, res.acl_data_offset,\n\t\t\t\tres.acl_len);\n\t}\n\tret = acl_len;\nout_free:\n\tfor (i = 0; i < npages; i++)\n\t\tif (pages[i])\n\t\t\t__free_page(pages[i]);\n\tif (args.acl_scratch)\n\t\t__free_page(args.acl_scratch);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 3603,
        "critical_vars": [
            "pages[i]"
        ],
        "function": "__nfs4_get_acl_uncached",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "True",
        "function_code": "static ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)\n{\n\tstruct page *pages[NFS4ACL_MAXPAGES] = {NULL, };\n\tstruct nfs_getaclargs args = {\n\t\t.fh = NFS_FH(inode),\n\t\t.acl_pages = pages,\n\t\t.acl_len = buflen,\n\t};\n\tstruct nfs_getaclres res = {\n\t\t.acl_len = buflen,\n\t};\n\tvoid *resp_buf;\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &nfs4_procedures[NFSPROC4_CLNT_GETACL],\n\t\t.rpc_argp = &args,\n\t\t.rpc_resp = &res,\n\t};\n\tint ret = -ENOMEM, npages, i, acl_len = 0;\n\n\tnpages = (buflen + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t/* As long as we're doing a round trip to the server anyway,\n\t * let's be prepared for a page of acl data. */\n\tif (npages == 0)\n\t\tnpages = 1;\n\n\tfor (i = 0; i < npages; i++) {\n\t\tpages[i] = alloc_page(GFP_KERNEL);\n\t\tif (!pages[i])\n\t\t\tgoto out_free;\n\t}\n\tif (npages > 1) {\n\t\t/* for decoding across pages */\n\t\targs.acl_scratch = alloc_page(GFP_KERNEL);\n\t\tif (!args.acl_scratch)\n\t\t\tgoto out_free;\n\t}\n\targs.acl_len = npages * PAGE_SIZE;\n\targs.acl_pgbase = 0;\n\t/* Let decode_getfacl know not to fail if the ACL data is larger than\n\t * the page we send as a guess */\n\tif (buf == NULL)\n\t\tres.acl_flags |= NFS4_ACL_LEN_REQUEST;\n\tresp_buf = page_address(pages[0]);\n\n\tdprintk(\"%s  buf %p buflen %ld npages %d args.acl_len %ld\\n\",\n\t\t__func__, buf, buflen, npages, args.acl_len);\n\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode),\n\t\t\t     &msg, &args.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\tgoto out_free;\n\n\tacl_len = res.acl_len - res.acl_data_offset;\n\tif (acl_len > args.acl_len)\n\t\tnfs4_write_cached_acl(inode, NULL, acl_len);\n\telse\n\t\tnfs4_write_cached_acl(inode, resp_buf + res.acl_data_offset,\n\t\t\t\t      acl_len);\n\tif (buf) {\n\t\tret = -ERANGE;\n\t\tif (acl_len > buflen)\n\t\t\tgoto out_free;\n\t\t_copy_from_pages(buf, pages, res.acl_data_offset,\n\t\t\t\tres.acl_len);\n\t}\n\tret = acl_len;\nout_free:\n\tfor (i = 0; i < npages; i++)\n\t\tif (pages[i])\n\t\t\t__free_page(pages[i]);\n\tif (args.acl_scratch)\n\t\t__free_page(args.acl_scratch);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 3604,
        "critical_vars": [
            "args.acl_scratch"
        ],
        "function": "__nfs4_get_acl_uncached",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "True",
        "function_code": "static ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)\n{\n\tstruct page *pages[NFS4ACL_MAXPAGES] = {NULL, };\n\tstruct nfs_getaclargs args = {\n\t\t.fh = NFS_FH(inode),\n\t\t.acl_pages = pages,\n\t\t.acl_len = buflen,\n\t};\n\tstruct nfs_getaclres res = {\n\t\t.acl_len = buflen,\n\t};\n\tvoid *resp_buf;\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &nfs4_procedures[NFSPROC4_CLNT_GETACL],\n\t\t.rpc_argp = &args,\n\t\t.rpc_resp = &res,\n\t};\n\tint ret = -ENOMEM, npages, i, acl_len = 0;\n\n\tnpages = (buflen + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t/* As long as we're doing a round trip to the server anyway,\n\t * let's be prepared for a page of acl data. */\n\tif (npages == 0)\n\t\tnpages = 1;\n\n\tfor (i = 0; i < npages; i++) {\n\t\tpages[i] = alloc_page(GFP_KERNEL);\n\t\tif (!pages[i])\n\t\t\tgoto out_free;\n\t}\n\tif (npages > 1) {\n\t\t/* for decoding across pages */\n\t\targs.acl_scratch = alloc_page(GFP_KERNEL);\n\t\tif (!args.acl_scratch)\n\t\t\tgoto out_free;\n\t}\n\targs.acl_len = npages * PAGE_SIZE;\n\targs.acl_pgbase = 0;\n\t/* Let decode_getfacl know not to fail if the ACL data is larger than\n\t * the page we send as a guess */\n\tif (buf == NULL)\n\t\tres.acl_flags |= NFS4_ACL_LEN_REQUEST;\n\tresp_buf = page_address(pages[0]);\n\n\tdprintk(\"%s  buf %p buflen %ld npages %d args.acl_len %ld\\n\",\n\t\t__func__, buf, buflen, npages, args.acl_len);\n\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode),\n\t\t\t     &msg, &args.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\tgoto out_free;\n\n\tacl_len = res.acl_len - res.acl_data_offset;\n\tif (acl_len > args.acl_len)\n\t\tnfs4_write_cached_acl(inode, NULL, acl_len);\n\telse\n\t\tnfs4_write_cached_acl(inode, resp_buf + res.acl_data_offset,\n\t\t\t\t      acl_len);\n\tif (buf) {\n\t\tret = -ERANGE;\n\t\tif (acl_len > buflen)\n\t\t\tgoto out_free;\n\t\t_copy_from_pages(buf, pages, res.acl_data_offset,\n\t\t\t\tres.acl_len);\n\t}\n\tret = acl_len;\nout_free:\n\tfor (i = 0; i < npages; i++)\n\t\tif (pages[i])\n\t\t\t__free_page(pages[i]);\n\tif (args.acl_scratch)\n\t\t__free_page(args.acl_scratch);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 3605,
        "critical_vars": [
            "args.acl_scratch"
        ],
        "function": "__nfs4_get_acl_uncached",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4proc.c.diff",
        "label": "True",
        "function_code": "static ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)\n{\n\tstruct page *pages[NFS4ACL_MAXPAGES] = {NULL, };\n\tstruct nfs_getaclargs args = {\n\t\t.fh = NFS_FH(inode),\n\t\t.acl_pages = pages,\n\t\t.acl_len = buflen,\n\t};\n\tstruct nfs_getaclres res = {\n\t\t.acl_len = buflen,\n\t};\n\tvoid *resp_buf;\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &nfs4_procedures[NFSPROC4_CLNT_GETACL],\n\t\t.rpc_argp = &args,\n\t\t.rpc_resp = &res,\n\t};\n\tint ret = -ENOMEM, npages, i, acl_len = 0;\n\n\tnpages = (buflen + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t/* As long as we're doing a round trip to the server anyway,\n\t * let's be prepared for a page of acl data. */\n\tif (npages == 0)\n\t\tnpages = 1;\n\n\tfor (i = 0; i < npages; i++) {\n\t\tpages[i] = alloc_page(GFP_KERNEL);\n\t\tif (!pages[i])\n\t\t\tgoto out_free;\n\t}\n\tif (npages > 1) {\n\t\t/* for decoding across pages */\n\t\targs.acl_scratch = alloc_page(GFP_KERNEL);\n\t\tif (!args.acl_scratch)\n\t\t\tgoto out_free;\n\t}\n\targs.acl_len = npages * PAGE_SIZE;\n\targs.acl_pgbase = 0;\n\t/* Let decode_getfacl know not to fail if the ACL data is larger than\n\t * the page we send as a guess */\n\tif (buf == NULL)\n\t\tres.acl_flags |= NFS4_ACL_LEN_REQUEST;\n\tresp_buf = page_address(pages[0]);\n\n\tdprintk(\"%s  buf %p buflen %ld npages %d args.acl_len %ld\\n\",\n\t\t__func__, buf, buflen, npages, args.acl_len);\n\tret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode),\n\t\t\t     &msg, &args.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\tgoto out_free;\n\n\tacl_len = res.acl_len - res.acl_data_offset;\n\tif (acl_len > args.acl_len)\n\t\tnfs4_write_cached_acl(inode, NULL, acl_len);\n\telse\n\t\tnfs4_write_cached_acl(inode, resp_buf + res.acl_data_offset,\n\t\t\t\t      acl_len);\n\tif (buf) {\n\t\tret = -ERANGE;\n\t\tif (acl_len > buflen)\n\t\t\tgoto out_free;\n\t\t_copy_from_pages(buf, pages, res.acl_data_offset,\n\t\t\t\tres.acl_len);\n\t}\n\tret = acl_len;\nout_free:\n\tfor (i = 0; i < npages; i++)\n\t\tif (pages[i])\n\t\t\t__free_page(pages[i]);\n\tif (args.acl_scratch)\n\t\t__free_page(args.acl_scratch);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Replace",
        "change_type": "Assignment",
        "line_old": 2520,
        "line_new": 2520,
        "critical_vars": [
            "replen"
        ],
        "function": "nfs4_xdr_enc_getacl",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4xdr.c.diff",
        "label": "True",
        "function_code": "static void nfs4_xdr_enc_getacl(struct rpc_rqst *req, struct xdr_stream *xdr,\n\t\t\t\tstruct nfs_getaclargs *args)\n{\n\tstruct compound_hdr hdr = {\n\t\t.minorversion = nfs4_xdr_minorversion(&args->seq_args),\n\t};\n\tuint32_t replen;\n\n\tencode_compound_hdr(xdr, req, &hdr);\n\tencode_sequence(xdr, &args->seq_args, &hdr);\n\tencode_putfh(xdr, args->fh, &hdr);\n\treplen = hdr.replen + op_decode_hdr_maxsz + 1;\n\tencode_getattr_two(xdr, FATTR4_WORD0_ACL, 0, &hdr);\n\n\txdr_inline_pages(&req->rq_rcv_buf, replen << 2,\n\t\targs->acl_pages, args->acl_pgbase, args->acl_len);\n\txdr_set_scratch_buffer(xdr, page_address(args->acl_scratch), PAGE_SIZE);\n\n\tencode_nops(&hdr);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 2525,
        "critical_vars": [
            "args->acl_scratch",
            "xdr"
        ],
        "function": "nfs4_xdr_enc_getacl",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4xdr.c.diff",
        "label": "True",
        "function_code": "static void nfs4_xdr_enc_getacl(struct rpc_rqst *req, struct xdr_stream *xdr,\n\t\t\t\tstruct nfs_getaclargs *args)\n{\n\tstruct compound_hdr hdr = {\n\t\t.minorversion = nfs4_xdr_minorversion(&args->seq_args),\n\t};\n\tuint32_t replen;\n\n\tencode_compound_hdr(xdr, req, &hdr);\n\tencode_sequence(xdr, &args->seq_args, &hdr);\n\tencode_putfh(xdr, args->fh, &hdr);\n\treplen = hdr.replen + op_decode_hdr_maxsz + 1;\n\tencode_getattr_two(xdr, FATTR4_WORD0_ACL, 0, &hdr);\n\n\txdr_inline_pages(&req->rq_rcv_buf, replen << 2,\n\t\targs->acl_pages, args->acl_pgbase, args->acl_len);\n\txdr_set_scratch_buffer(xdr, page_address(args->acl_scratch), PAGE_SIZE);\n\n\tencode_nops(&hdr);\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Var-Declaration",
        "line_old": 4960,
        "critical_vars": [
            "*acl_len"
        ],
        "function": "decode_restorefh",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4xdr.c.diff",
        "label": "False",
        "function_code": "\nstatic int\ndecode_restorefh(struct xdr_stream *xdr)\n{\n\treturn decode_op_hdr(xdr, OP_RESTOREFH);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Var-Declaration",
        "line_new": 4962,
        "critical_vars": [
            "*res"
        ],
        "function": "decode_restorefh",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4xdr.c.diff",
        "label": "True",
        "function_code": "\nstatic int\ndecode_restorefh(struct xdr_stream *xdr)\n{\n\treturn decode_op_hdr(xdr, OP_RESTOREFH);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Var-Declaration",
        "line_new": 4964,
        "critical_vars": [
            "*bm_p"
        ],
        "function": "decode_restorefh",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4xdr.c.diff",
        "label": "True",
        "function_code": "\nstatic int\ndecode_restorefh(struct xdr_stream *xdr)\n{\n\treturn decode_op_hdr(xdr, OP_RESTOREFH);\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 4968,
        "critical_vars": [
            "*acl_len"
        ],
        "function": "decode_restorefh",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4xdr.c.diff",
        "label": "False",
        "function_code": "\nstatic int\ndecode_restorefh(struct xdr_stream *xdr)\n{\n\treturn decode_op_hdr(xdr, OP_RESTOREFH);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 4970,
        "critical_vars": [
            "res->acl_len"
        ],
        "function": "decode_restorefh",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4xdr.c.diff",
        "label": "True",
        "function_code": "\nstatic int\ndecode_restorefh(struct xdr_stream *xdr)\n{\n\treturn decode_op_hdr(xdr, OP_RESTOREFH);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 4973,
        "critical_vars": [
            "bm_p"
        ],
        "function": "decode_restorefh",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4xdr.c.diff",
        "label": "True",
        "function_code": "\nstatic int\ndecode_restorefh(struct xdr_stream *xdr)\n{\n\treturn decode_op_hdr(xdr, OP_RESTOREFH);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 4988,
        "critical_vars": [
            "xdr->p"
        ],
        "function": "decode_getacl",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4xdr.c.diff",
        "label": "True",
        "function_code": "\nstatic int decode_getacl(struct xdr_stream *xdr, struct rpc_rqst *req,\n\t\t\t struct nfs_getaclres *res)\n{\n\t__be32 *savep, *bm_p;\n\tuint32_t attrlen,\n\t\t bitmap[3] = {0};\n\tstruct kvec *iov = req->rq_rcv_buf.head;\n\tint status;\n\n\tres->acl_len = 0;\n\tif ((status = decode_op_hdr(xdr, OP_GETATTR)) != 0)\n\t\tgoto out;\n\tbm_p = xdr->p;\n\tif ((status = decode_attr_bitmap(xdr, bitmap)) != 0)\n\t\tgoto out;\n\tif ((status = decode_attr_length(xdr, &attrlen, &savep)) != 0)\n\t\tgoto out;\n\n\tif (unlikely(bitmap[0] & (FATTR4_WORD0_ACL - 1U)))\n\t\treturn -EIO;\n\tif (likely(bitmap[0] & FATTR4_WORD0_ACL)) {\n\t\tsize_t hdrlen;\n\t\tu32 recvd;\n\n\t\t/* The bitmap (xdr len + bitmaps) and the attr xdr len words\n\t\t * are stored with the acl data to handle the problem of\n\t\t * variable length bitmaps.*/\n\t\txdr->p = bm_p;\n\t\tres->acl_data_offset = be32_to_cpup(bm_p) + 2;\n\t\tres->acl_data_offset <<= 2;\n\n\t\t/* We ignore &savep and don't do consistency checks on\n\t\t * the attr length.  Let userspace figure it out.... */\n\t\thdrlen = (u8 *)xdr->p - (u8 *)iov->iov_base;\n\t\tattrlen += res->acl_data_offset;\n\t\trecvd = req->rq_rcv_buf.len - hdrlen;\n\t\tif (attrlen > recvd) {\n\t\t\tif (res->acl_flags & NFS4_ACL_LEN_REQUEST) {\n\t\t\t\t/* getxattr interface called with a NULL buf */\n\t\t\t\tres->acl_len = attrlen;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tdprintk(\"NFS: acl reply: attrlen %u > recvd %u\\n\",\n\t\t\t\t\tattrlen, recvd);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\txdr_read_pages(xdr, attrlen);\n\t\tres->acl_len = attrlen;\n\t} else\n\t\tstatus = -EOPNOTSUPP;\n\nout:\n\treturn status;\n}\n\nstatic int\ndecode_savefh(struct xdr_stream *xdr)\n{\n\treturn decode_op_hdr(xdr, OP_SAVEFH);\n}\n\nstatic int decode_setattr(struct xdr_stream *xdr)\n{\n\t__be32 *p;\n\tuint32_t bmlen;\n\tint status;\n\n\tstatus = decode_op_hdr(xdr, OP_SETATTR);\n\tif (status)\n\t\treturn status;\n\tp = xdr_inline_decode(xdr, 4);\n\tif (unlikely(!p))\n\t\tgoto out_overflow;\n\tbmlen = be32_to_cpup(p);\n\tp = xdr_inline_decode(xdr, bmlen << 2);\n\tif (likely(p))\n\t\treturn 0;\nout_overflow:\n\tprint_overflow_msg(__func__, xdr);\n\treturn -EIO;\n}\n\nstatic int decode_setcl\n... (function end not found)"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 4989,
        "critical_vars": [
            "res->acl_data_offset"
        ],
        "function": "decode_getacl",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4xdr.c.diff",
        "label": "True",
        "function_code": "\nstatic int decode_getacl(struct xdr_stream *xdr, struct rpc_rqst *req,\n\t\t\t struct nfs_getaclres *res)\n{\n\t__be32 *savep, *bm_p;\n\tuint32_t attrlen,\n\t\t bitmap[3] = {0};\n\tstruct kvec *iov = req->rq_rcv_buf.head;\n\tint status;\n\n\tres->acl_len = 0;\n\tif ((status = decode_op_hdr(xdr, OP_GETATTR)) != 0)\n\t\tgoto out;\n\tbm_p = xdr->p;\n\tif ((status = decode_attr_bitmap(xdr, bitmap)) != 0)\n\t\tgoto out;\n\tif ((status = decode_attr_length(xdr, &attrlen, &savep)) != 0)\n\t\tgoto out;\n\n\tif (unlikely(bitmap[0] & (FATTR4_WORD0_ACL - 1U)))\n\t\treturn -EIO;\n\tif (likely(bitmap[0] & FATTR4_WORD0_ACL)) {\n\t\tsize_t hdrlen;\n\t\tu32 recvd;\n\n\t\t/* The bitmap (xdr len + bitmaps) and the attr xdr len words\n\t\t * are stored with the acl data to handle the problem of\n\t\t * variable length bitmaps.*/\n\t\txdr->p = bm_p;\n\t\tres->acl_data_offset = be32_to_cpup(bm_p) + 2;\n\t\tres->acl_data_offset <<= 2;\n\n\t\t/* We ignore &savep and don't do consistency checks on\n\t\t * the attr length.  Let userspace figure it out.... */\n\t\thdrlen = (u8 *)xdr->p - (u8 *)iov->iov_base;\n\t\tattrlen += res->acl_data_offset;\n\t\trecvd = req->rq_rcv_buf.len - hdrlen;\n\t\tif (attrlen > recvd) {\n\t\t\tif (res->acl_flags & NFS4_ACL_LEN_REQUEST) {\n\t\t\t\t/* getxattr interface called with a NULL buf */\n\t\t\t\tres->acl_len = attrlen;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tdprintk(\"NFS: acl reply: attrlen %u > recvd %u\\n\",\n\t\t\t\t\tattrlen, recvd);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\txdr_read_pages(xdr, attrlen);\n\t\tres->acl_len = attrlen;\n\t} else\n\t\tstatus = -EOPNOTSUPP;\n\nout:\n\treturn status;\n}\n\nstatic int\ndecode_savefh(struct xdr_stream *xdr)\n{\n\treturn decode_op_hdr(xdr, OP_SAVEFH);\n}\n\nstatic int decode_setattr(struct xdr_stream *xdr)\n{\n\t__be32 *p;\n\tuint32_t bmlen;\n\tint status;\n\n\tstatus = decode_op_hdr(xdr, OP_SETATTR);\n\tif (status)\n\t\treturn status;\n\tp = xdr_inline_decode(xdr, 4);\n\tif (unlikely(!p))\n\t\tgoto out_overflow;\n\tbmlen = be32_to_cpup(p);\n\tp = xdr_inline_decode(xdr, bmlen << 2);\n\tif (likely(p))\n\t\treturn 0;\nout_overflow:\n\tprint_overflow_msg(__func__, xdr);\n\treturn -EIO;\n}\n\nstatic int decode_setcl\n... (function end not found)"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 4990,
        "critical_vars": [
            "acl_data_offset"
        ],
        "function": "decode_getacl",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4xdr.c.diff",
        "label": "True",
        "function_code": "\nstatic int decode_getacl(struct xdr_stream *xdr, struct rpc_rqst *req,\n\t\t\t struct nfs_getaclres *res)\n{\n\t__be32 *savep, *bm_p;\n\tuint32_t attrlen,\n\t\t bitmap[3] = {0};\n\tstruct kvec *iov = req->rq_rcv_buf.head;\n\tint status;\n\n\tres->acl_len = 0;\n\tif ((status = decode_op_hdr(xdr, OP_GETATTR)) != 0)\n\t\tgoto out;\n\tbm_p = xdr->p;\n\tif ((status = decode_attr_bitmap(xdr, bitmap)) != 0)\n\t\tgoto out;\n\tif ((status = decode_attr_length(xdr, &attrlen, &savep)) != 0)\n\t\tgoto out;\n\n\tif (unlikely(bitmap[0] & (FATTR4_WORD0_ACL - 1U)))\n\t\treturn -EIO;\n\tif (likely(bitmap[0] & FATTR4_WORD0_ACL)) {\n\t\tsize_t hdrlen;\n\t\tu32 recvd;\n\n\t\t/* The bitmap (xdr len + bitmaps) and the attr xdr len words\n\t\t * are stored with the acl data to handle the problem of\n\t\t * variable length bitmaps.*/\n\t\txdr->p = bm_p;\n\t\tres->acl_data_offset = be32_to_cpup(bm_p) + 2;\n\t\tres->acl_data_offset <<= 2;\n\n\t\t/* We ignore &savep and don't do consistency checks on\n\t\t * the attr length.  Let userspace figure it out.... */\n\t\thdrlen = (u8 *)xdr->p - (u8 *)iov->iov_base;\n\t\tattrlen += res->acl_data_offset;\n\t\trecvd = req->rq_rcv_buf.len - hdrlen;\n\t\tif (attrlen > recvd) {\n\t\t\tif (res->acl_flags & NFS4_ACL_LEN_REQUEST) {\n\t\t\t\t/* getxattr interface called with a NULL buf */\n\t\t\t\tres->acl_len = attrlen;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tdprintk(\"NFS: acl reply: attrlen %u > recvd %u\\n\",\n\t\t\t\t\tattrlen, recvd);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\txdr_read_pages(xdr, attrlen);\n\t\tres->acl_len = attrlen;\n\t} else\n\t\tstatus = -EOPNOTSUPP;\n\nout:\n\treturn status;\n}\n\nstatic int\ndecode_savefh(struct xdr_stream *xdr)\n{\n\treturn decode_op_hdr(xdr, OP_SAVEFH);\n}\n\nstatic int decode_setattr(struct xdr_stream *xdr)\n{\n\t__be32 *p;\n\tuint32_t bmlen;\n\tint status;\n\n\tstatus = decode_op_hdr(xdr, OP_SETATTR);\n\tif (status)\n\t\treturn status;\n\tp = xdr_inline_decode(xdr, 4);\n\tif (unlikely(!p))\n\t\tgoto out_overflow;\n\tbmlen = be32_to_cpup(p);\n\tp = xdr_inline_decode(xdr, bmlen << 2);\n\tif (likely(p))\n\t\treturn 0;\nout_overflow:\n\tprint_overflow_msg(__func__, xdr);\n\treturn -EIO;\n}\n\nstatic int decode_setcl\n... (function end not found)"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 4995,
        "critical_vars": [
            "attrlen"
        ],
        "function": "decode_getacl",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4xdr.c.diff",
        "label": "True",
        "function_code": "\nstatic int decode_getacl(struct xdr_stream *xdr, struct rpc_rqst *req,\n\t\t\t struct nfs_getaclres *res)\n{\n\t__be32 *savep, *bm_p;\n\tuint32_t attrlen,\n\t\t bitmap[3] = {0};\n\tstruct kvec *iov = req->rq_rcv_buf.head;\n\tint status;\n\n\tres->acl_len = 0;\n\tif ((status = decode_op_hdr(xdr, OP_GETATTR)) != 0)\n\t\tgoto out;\n\tbm_p = xdr->p;\n\tif ((status = decode_attr_bitmap(xdr, bitmap)) != 0)\n\t\tgoto out;\n\tif ((status = decode_attr_length(xdr, &attrlen, &savep)) != 0)\n\t\tgoto out;\n\n\tif (unlikely(bitmap[0] & (FATTR4_WORD0_ACL - 1U)))\n\t\treturn -EIO;\n\tif (likely(bitmap[0] & FATTR4_WORD0_ACL)) {\n\t\tsize_t hdrlen;\n\t\tu32 recvd;\n\n\t\t/* The bitmap (xdr len + bitmaps) and the attr xdr len words\n\t\t * are stored with the acl data to handle the problem of\n\t\t * variable length bitmaps.*/\n\t\txdr->p = bm_p;\n\t\tres->acl_data_offset = be32_to_cpup(bm_p) + 2;\n\t\tres->acl_data_offset <<= 2;\n\n\t\t/* We ignore &savep and don't do consistency checks on\n\t\t * the attr length.  Let userspace figure it out.... */\n\t\thdrlen = (u8 *)xdr->p - (u8 *)iov->iov_base;\n\t\tattrlen += res->acl_data_offset;\n\t\trecvd = req->rq_rcv_buf.len - hdrlen;\n\t\tif (attrlen > recvd) {\n\t\t\tif (res->acl_flags & NFS4_ACL_LEN_REQUEST) {\n\t\t\t\t/* getxattr interface called with a NULL buf */\n\t\t\t\tres->acl_len = attrlen;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tdprintk(\"NFS: acl reply: attrlen %u > recvd %u\\n\",\n\t\t\t\t\tattrlen, recvd);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\txdr_read_pages(xdr, attrlen);\n\t\tres->acl_len = attrlen;\n\t} else\n\t\tstatus = -EOPNOTSUPP;\n\nout:\n\treturn status;\n}\n\nstatic int\ndecode_savefh(struct xdr_stream *xdr)\n{\n\treturn decode_op_hdr(xdr, OP_SAVEFH);\n}\n\nstatic int decode_setattr(struct xdr_stream *xdr)\n{\n\t__be32 *p;\n\tuint32_t bmlen;\n\tint status;\n\n\tstatus = decode_op_hdr(xdr, OP_SETATTR);\n\tif (status)\n\t\treturn status;\n\tp = xdr_inline_decode(xdr, 4);\n\tif (unlikely(!p))\n\t\tgoto out_overflow;\n\tbmlen = be32_to_cpup(p);\n\tp = xdr_inline_decode(xdr, bmlen << 2);\n\tif (likely(p))\n\t\treturn 0;\nout_overflow:\n\tprint_overflow_msg(__func__, xdr);\n\treturn -EIO;\n}\n\nstatic int decode_setcl\n... (function end not found)"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 4998,
        "critical_vars": [
            "res->acl_flags"
        ],
        "function": "decode_getacl",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4xdr.c.diff",
        "label": "True",
        "function_code": "\nstatic int decode_getacl(struct xdr_stream *xdr, struct rpc_rqst *req,\n\t\t\t struct nfs_getaclres *res)\n{\n\t__be32 *savep, *bm_p;\n\tuint32_t attrlen,\n\t\t bitmap[3] = {0};\n\tstruct kvec *iov = req->rq_rcv_buf.head;\n\tint status;\n\n\tres->acl_len = 0;\n\tif ((status = decode_op_hdr(xdr, OP_GETATTR)) != 0)\n\t\tgoto out;\n\tbm_p = xdr->p;\n\tif ((status = decode_attr_bitmap(xdr, bitmap)) != 0)\n\t\tgoto out;\n\tif ((status = decode_attr_length(xdr, &attrlen, &savep)) != 0)\n\t\tgoto out;\n\n\tif (unlikely(bitmap[0] & (FATTR4_WORD0_ACL - 1U)))\n\t\treturn -EIO;\n\tif (likely(bitmap[0] & FATTR4_WORD0_ACL)) {\n\t\tsize_t hdrlen;\n\t\tu32 recvd;\n\n\t\t/* The bitmap (xdr len + bitmaps) and the attr xdr len words\n\t\t * are stored with the acl data to handle the problem of\n\t\t * variable length bitmaps.*/\n\t\txdr->p = bm_p;\n\t\tres->acl_data_offset = be32_to_cpup(bm_p) + 2;\n\t\tres->acl_data_offset <<= 2;\n\n\t\t/* We ignore &savep and don't do consistency checks on\n\t\t * the attr length.  Let userspace figure it out.... */\n\t\thdrlen = (u8 *)xdr->p - (u8 *)iov->iov_base;\n\t\tattrlen += res->acl_data_offset;\n\t\trecvd = req->rq_rcv_buf.len - hdrlen;\n\t\tif (attrlen > recvd) {\n\t\t\tif (res->acl_flags & NFS4_ACL_LEN_REQUEST) {\n\t\t\t\t/* getxattr interface called with a NULL buf */\n\t\t\t\tres->acl_len = attrlen;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tdprintk(\"NFS: acl reply: attrlen %u > recvd %u\\n\",\n\t\t\t\t\tattrlen, recvd);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\txdr_read_pages(xdr, attrlen);\n\t\tres->acl_len = attrlen;\n\t} else\n\t\tstatus = -EOPNOTSUPP;\n\nout:\n\treturn status;\n}\n\nstatic int\ndecode_savefh(struct xdr_stream *xdr)\n{\n\treturn decode_op_hdr(xdr, OP_SAVEFH);\n}\n\nstatic int decode_setattr(struct xdr_stream *xdr)\n{\n\t__be32 *p;\n\tuint32_t bmlen;\n\tint status;\n\n\tstatus = decode_op_hdr(xdr, OP_SETATTR);\n\tif (status)\n\t\treturn status;\n\tp = xdr_inline_decode(xdr, 4);\n\tif (unlikely(!p))\n\t\tgoto out_overflow;\n\tbmlen = be32_to_cpup(p);\n\tp = xdr_inline_decode(xdr, bmlen << 2);\n\tif (likely(p))\n\t\treturn 0;\nout_overflow:\n\tprint_overflow_msg(__func__, xdr);\n\treturn -EIO;\n}\n\nstatic int decode_setcl\n... (function end not found)"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 5000,
        "critical_vars": [
            "res->acl_len"
        ],
        "function": "decode_getacl",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4xdr.c.diff",
        "label": "True",
        "function_code": "\nstatic int decode_getacl(struct xdr_stream *xdr, struct rpc_rqst *req,\n\t\t\t struct nfs_getaclres *res)\n{\n\t__be32 *savep, *bm_p;\n\tuint32_t attrlen,\n\t\t bitmap[3] = {0};\n\tstruct kvec *iov = req->rq_rcv_buf.head;\n\tint status;\n\n\tres->acl_len = 0;\n\tif ((status = decode_op_hdr(xdr, OP_GETATTR)) != 0)\n\t\tgoto out;\n\tbm_p = xdr->p;\n\tif ((status = decode_attr_bitmap(xdr, bitmap)) != 0)\n\t\tgoto out;\n\tif ((status = decode_attr_length(xdr, &attrlen, &savep)) != 0)\n\t\tgoto out;\n\n\tif (unlikely(bitmap[0] & (FATTR4_WORD0_ACL - 1U)))\n\t\treturn -EIO;\n\tif (likely(bitmap[0] & FATTR4_WORD0_ACL)) {\n\t\tsize_t hdrlen;\n\t\tu32 recvd;\n\n\t\t/* The bitmap (xdr len + bitmaps) and the attr xdr len words\n\t\t * are stored with the acl data to handle the problem of\n\t\t * variable length bitmaps.*/\n\t\txdr->p = bm_p;\n\t\tres->acl_data_offset = be32_to_cpup(bm_p) + 2;\n\t\tres->acl_data_offset <<= 2;\n\n\t\t/* We ignore &savep and don't do consistency checks on\n\t\t * the attr length.  Let userspace figure it out.... */\n\t\thdrlen = (u8 *)xdr->p - (u8 *)iov->iov_base;\n\t\tattrlen += res->acl_data_offset;\n\t\trecvd = req->rq_rcv_buf.len - hdrlen;\n\t\tif (attrlen > recvd) {\n\t\t\tif (res->acl_flags & NFS4_ACL_LEN_REQUEST) {\n\t\t\t\t/* getxattr interface called with a NULL buf */\n\t\t\t\tres->acl_len = attrlen;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tdprintk(\"NFS: acl reply: attrlen %u > recvd %u\\n\",\n\t\t\t\t\tattrlen, recvd);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\txdr_read_pages(xdr, attrlen);\n\t\tres->acl_len = attrlen;\n\t} else\n\t\tstatus = -EOPNOTSUPP;\n\nout:\n\treturn status;\n}\n\nstatic int\ndecode_savefh(struct xdr_stream *xdr)\n{\n\treturn decode_op_hdr(xdr, OP_SAVEFH);\n}\n\nstatic int decode_setattr(struct xdr_stream *xdr)\n{\n\t__be32 *p;\n\tuint32_t bmlen;\n\tint status;\n\n\tstatus = decode_op_hdr(xdr, OP_SETATTR);\n\tif (status)\n\t\treturn status;\n\tp = xdr_inline_decode(xdr, 4);\n\tif (unlikely(!p))\n\t\tgoto out_overflow;\n\tbmlen = be32_to_cpup(p);\n\tp = xdr_inline_decode(xdr, bmlen << 2);\n\tif (likely(p))\n\t\treturn 0;\nout_overflow:\n\tprint_overflow_msg(__func__, xdr);\n\treturn -EIO;\n}\n\nstatic int decode_setcl\n... (function end not found)"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 4993,
        "critical_vars": [
            "*acl_len"
        ],
        "function": "decode_getacl",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4xdr.c.diff",
        "label": "False",
        "function_code": "\nstatic int decode_getacl(struct xdr_stream *xdr, struct rpc_rqst *req,\n\t\tsize_t *acl_len)\n{\n\t__be32 *savep;\n\tuint32_t attrlen,\n\t\t bitmap[3] = {0};\n\tstruct kvec *iov = req->rq_rcv_buf.head;\n\tint status;\n\n\t*acl_len = 0;\n\tif ((status = decode_op_hdr(xdr, OP_GETATTR)) != 0)\n\t\tgoto out;\n\tif ((status = decode_attr_bitmap(xdr, bitmap)) != 0)\n\t\tgoto out;\n\tif ((status = decode_attr_length(xdr, &attrlen, &savep)) != 0)\n\t\tgoto out;\n\n\tif (unlikely(bitmap[0] & (FATTR4_WORD0_ACL - 1U)))\n\t\treturn -EIO;\n\tif (likely(bitmap[0] & FATTR4_WORD0_ACL)) {\n\t\tsize_t hdrlen;\n\t\tu32 recvd;\n\n\t\t/* We ignore &savep and don't do consistency checks on\n\t\t * the attr length.  Let userspace figure it out.... */\n\t\thdrlen = (u8 *)xdr->p - (u8 *)iov->iov_base;\n\t\trecvd = req->rq_rcv_buf.len - hdrlen;\n\t\tif (attrlen > recvd) {\n\t\t\tdprintk(\"NFS: server cheating in getattr\"\n\t\t\t\t\t\" acl reply: attrlen %u > recvd %u\\n\",\n\t\t\t\t\tattrlen, recvd);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\txdr_read_pages(xdr, attrlen);\n\t\t*acl_len = attrlen;\n\t} else\n\t\tstatus = -EOPNOTSUPP;\n\nout:\n\treturn status;\n}\n\nstatic int\ndecode_savefh(struct xdr_stream *xdr)\n{\n\treturn decode_op_hdr(xdr, OP_SAVEFH);\n}\n\nstatic int decode_setattr(struct xdr_stream *xdr)\n{\n\t__be32 *p;\n\tuint32_t bmlen;\n\tint status;\n\n\tstatus = decode_op_hdr(xdr, OP_SETATTR);\n\tif (status)\n\t\treturn status;\n\tp = xdr_inline_decode(xdr, 4);\n\tif (unlikely(!p))\n\t\tgoto out_overflow;\n\tbmlen = be32_to_cpup(p);\n\tp = xdr_inline_decode(xdr, bmlen << 2);\n\tif (likely(p))\n\t\treturn 0;\nout_overflow:\n\tprint_overflow_msg(__func__, xdr);\n\treturn -EIO;\n}\n\nstatic int decode_setclientid(struct xdr_stream *xdr, struct nfs4_setclientid_res *res)\n{\n\t__be32 *p;\n\tuint32_t opnum;\n\tint32_t nfserr;\n\n\tp = xdr_inline_decode(xdr, 8);\n\tif (unlikely(!p))\n\t\tgoto out_overflow;\n\topnum = be32_to_cpup(p++);\n\tif (opnum != OP_SETCLIENTID) {\n\t\tdprintk(\"nfs: decode_setclientid: Server returned operation\"\n\t\t\t\" %d\\n\", opnum);\n\t\treturn -EIO;\n\t}\n\tnfserr = be32_to_cpup(p);\n\tif (nfserr == NFS_OK) {\n\t\tp = xdr_inline_decode(xdr, 8 + NFS4_VERI\n... (function end not found)"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 5008,
        "critical_vars": [
            "res->acl_len"
        ],
        "function": "decode_getacl",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4xdr.c.diff",
        "label": "True",
        "function_code": "\nstatic int decode_getacl(struct xdr_stream *xdr, struct rpc_rqst *req,\n\t\t\t struct nfs_getaclres *res)\n{\n\t__be32 *savep, *bm_p;\n\tuint32_t attrlen,\n\t\t bitmap[3] = {0};\n\tstruct kvec *iov = req->rq_rcv_buf.head;\n\tint status;\n\n\tres->acl_len = 0;\n\tif ((status = decode_op_hdr(xdr, OP_GETATTR)) != 0)\n\t\tgoto out;\n\tbm_p = xdr->p;\n\tif ((status = decode_attr_bitmap(xdr, bitmap)) != 0)\n\t\tgoto out;\n\tif ((status = decode_attr_length(xdr, &attrlen, &savep)) != 0)\n\t\tgoto out;\n\n\tif (unlikely(bitmap[0] & (FATTR4_WORD0_ACL - 1U)))\n\t\treturn -EIO;\n\tif (likely(bitmap[0] & FATTR4_WORD0_ACL)) {\n\t\tsize_t hdrlen;\n\t\tu32 recvd;\n\n\t\t/* The bitmap (xdr len + bitmaps) and the attr xdr len words\n\t\t * are stored with the acl data to handle the problem of\n\t\t * variable length bitmaps.*/\n\t\txdr->p = bm_p;\n\t\tres->acl_data_offset = be32_to_cpup(bm_p) + 2;\n\t\tres->acl_data_offset <<= 2;\n\n\t\t/* We ignore &savep and don't do consistency checks on\n\t\t * the attr length.  Let userspace figure it out.... */\n\t\thdrlen = (u8 *)xdr->p - (u8 *)iov->iov_base;\n\t\tattrlen += res->acl_data_offset;\n\t\trecvd = req->rq_rcv_buf.len - hdrlen;\n\t\tif (attrlen > recvd) {\n\t\t\tif (res->acl_flags & NFS4_ACL_LEN_REQUEST) {\n\t\t\t\t/* getxattr interface called with a NULL buf */\n\t\t\t\tres->acl_len = attrlen;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tdprintk(\"NFS: acl reply: attrlen %u > recvd %u\\n\",\n\t\t\t\t\tattrlen, recvd);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\txdr_read_pages(xdr, attrlen);\n\t\tres->acl_len = attrlen;\n\t} else\n\t\tstatus = -EOPNOTSUPP;\n\nout:\n\treturn status;\n}\n\nstatic int\ndecode_savefh(struct xdr_stream *xdr)\n{\n\treturn decode_op_hdr(xdr, OP_SAVEFH);\n}\n\nstatic int decode_setattr(struct xdr_stream *xdr)\n{\n\t__be32 *p;\n\tuint32_t bmlen;\n\tint status;\n\n\tstatus = decode_op_hdr(xdr, OP_SETATTR);\n\tif (status)\n\t\treturn status;\n\tp = xdr_inline_decode(xdr, 4);\n\tif (unlikely(!p))\n\t\tgoto out_overflow;\n\tbmlen = be32_to_cpup(p);\n\tp = xdr_inline_decode(xdr, bmlen << 2);\n\tif (likely(p))\n\t\treturn 0;\nout_overflow:\n\tprint_overflow_msg(__func__, xdr);\n\treturn -EIO;\n}\n\nstatic int decode_setcl\n... (function end not found)"
    },
    {
        "patch_model": "Replace",
        "change_type": "Assignment",
        "line_old": 6031,
        "line_new": 6046,
        "critical_vars": [
            "status"
        ],
        "function": "nfs4_xdr_dec_getacl",
        "filename": "linux/CVE-2011-4131/CVE-2011-4131_CWE-189_bf118a342f10dafe44b14451a1392c3254629a1f_nfs4xdr.c.diff",
        "label": "True",
        "function_code": "static int\nnfs4_xdr_dec_getacl(struct rpc_rqst *rqstp, struct xdr_stream *xdr,\n\t\t    struct nfs_getaclres *res)\n{\n\tstruct compound_hdr hdr;\n\tint status;\n\n\tstatus = decode_compound_hdr(xdr, &hdr);\n\tif (status)\n\t\tgoto out;\n\tstatus = decode_sequence(xdr, &res->seq_res, rqstp);\n\tif (status)\n\t\tgoto out;\n\tstatus = decode_putfh(xdr);\n\tif (status)\n\t\tgoto out;\n\tstatus = decode_getacl(xdr, rqstp, res);\n\nout:\n\treturn status;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Var-Declaration",
        "line_old": 823,
        "critical_vars": [
            "offs"
        ],
        "function": "opl3_hw_control",
        "filename": "linux/CVE-2011-1476/CVE-2011-1476_CWE-189_b769f49463711205d57286e64cf535ed4daf59e9_opl3.c.diff",
        "label": "False",
        "function_code": "\nstatic void opl3_hw_control(int dev, unsigned char *event)\n{\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 833,
        "critical_vars": [
            "&ins",
            "ins",
            "addr"
        ],
        "function": "opl3_load_patch",
        "filename": "linux/CVE-2011-1476/CVE-2011-1476_CWE-189_b769f49463711205d57286e64cf535ed4daf59e9_opl3.c.diff",
        "label": "True",
        "function_code": "\nstatic int opl3_load_patch(int dev, int format, const char __user *addr,\n\t\tint count, int pmgr_flag)\n{\n\tstruct sbi_instrument ins;\n\n\tif (count <sizeof(ins))\n\t{\n\t\tprintk(KERN_WARNING \"FM Error: Patch record too short\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (copy_from_user(&ins, addr, sizeof(ins)))\n\t\treturn -EFAULT;\n\n\tif (ins.channel < 0 || ins.channel >= SBFM_MAXINSTR)\n\t{\n\t\tprintk(KERN_WARNING \"FM Error: Invalid instrument number %d\\n\", ins.channel);\n\t\treturn -EINVAL;\n\t}\n\tins.key = format;\n\n\treturn store_instr(ins.channel, &ins);\n}"
    },
    {
        "patch_model": "Replace",
        "change_type": "Assignment",
        "line_old": 244,
        "line_new": 244,
        "critical_vars": [
            "err"
        ],
        "function": "sequencer_write",
        "filename": "linux/CVE-2011-1476/CVE-2011-1476_CWE-189_b769f49463711205d57286e64cf535ed4daf59e9_sequencer.c.diff",
        "label": "True",
        "function_code": "\nint sequencer_write(int dev, struct file *file, const char __user *buf, int count)\n{\n\tunsigned char event_rec[EV_SZ], ev_code;\n\tint p = 0, c, ev_size;\n\tint mode = translate_mode(file);\n\n\tdev = dev >> 4;\n\n\tDEB(printk(\"sequencer_write(dev=%d, count=%d)\\n\", dev, count));\n\n\tif (mode == OPEN_READ)\n\t\treturn -EIO;\n\n\tc = count;\n\n\twhile (c >= 4)\n\t{\n\t\tif (copy_from_user((char *) event_rec, &(buf)[p], 4))\n\t\t\tgoto out;\n\t\tev_code = event_rec[0];\n\n\t\tif (ev_code == SEQ_FULLSIZE)\n\t\t{\n\t\t\tint err, fmt;\n\n\t\t\tdev = *(unsigned short *) &event_rec[2];\n\t\t\tif (dev < 0 || dev >= max_synthdev || synth_devs[dev] == NULL)\n\t\t\t\treturn -ENXIO;\n\n\t\t\tif (!(synth_open_mask & (1 << dev)))\n\t\t\t\treturn -ENXIO;\n\n\t\t\tfmt = (*(short *) &event_rec[0]) & 0xffff;\n\t\t\terr = synth_devs[dev]->load_patch(dev, fmt, buf + p, c, 0);\n\t\t\tif (err < 0)\n\t\t\t\treturn err;\n\n\t\t\treturn err;\n\t\t}\n\t\tif (ev_code >= 128)\n\t\t{\n\t\t\tif (seq_mode == SEQ_2 && ev_code == SEQ_EXTENDED)\n\t\t\t{\n\t\t\t\tprintk(KERN_WARNING \"Sequencer: Invalid level 2 event %x\\n\", ev_code);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tev_size = 8;\n\n\t\t\tif (c < ev_size)\n\t\t\t{\n\t\t\t\tif (!seq_playing)\n\t\t\t\t\tseq_startplay();\n\t\t\t\treturn count - c;\n\t\t\t}\n\t\t\tif (copy_from_user((char *)&event_rec[4],\n\t\t\t\t\t   &(buf)[p + 4], 4))\n\t\t\t\tgoto out;\n\n\t\t}\n\t\telse\n\t\t{\n\t\t\tif (seq_mode == SEQ_2)\n\t\t\t{\n\t\t\t\tprintk(KERN_WARNING \"Sequencer: 4 byte event in level 2 mode\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tev_size = 4;\n\n\t\t\tif (event_rec[0] != SEQ_MIDIPUTC)\n\t\t\t\tobsolete_api_used = 1;\n\t\t}\n\n\t\tif (event_rec[0] == SEQ_MIDIPUTC)\n\t\t{\n\t\t\tif (!midi_opened[event_rec[2]])\n\t\t\t{\n\t\t\t\tint err, mode;\n\t\t\t\tint dev = event_rec[2];\n\n\t\t\t\tif (dev >= max_mididev || midi_devs[dev]==NULL)\n\t\t\t\t{\n\t\t\t\t\t/*printk(\"Sequencer Error: Nonexistent MIDI device %d\\n\", dev);*/\n\t\t\t\t\treturn -ENXIO;\n\t\t\t\t}\n\t\t\t\tmode = translate_mode(file);\n\n\t\t\t\tif ((err = midi_devs[dev]->open(dev, mode,\n\t\t\t\t\t\t\t\tsequencer_midi_input, sequencer_midi_output)) < 0)\n\t\t\t\t{\n\t\t\t\t\tseq_reset();\n\t\t\t\t\tprintk(KERN_WARNING \"Sequencer Error: Unable to open Midi #%d\\n\", dev);\n\t\t\t\t\treturn err;\n\t\t\t\t}\n\t\t\t\tmidi_opened[dev] = 1;\n\t\t\t}\n\t\t}\n\t\tif (!seq_queue(event_rec, (file->f_flags & (O_NONBLOCK) ? 1 : 0)))\n\t\t{\n\t\t\tint processed = count - c;\n\n\t\t\tif (!seq_playing)\n\t\t\t\tseq_startplay();\n\n\t\t\tif (!processed && (file->f_flags & O_NONBLOCK))\n\t\t\t\treturn -EAGAIN;\n\t\t\telse\n\t\t\t\treturn processed;\n\t\t}\n\t\tp += ev_size;\n\t\tc -= ev_size;\n\t}\n\n\tif (!seq_playing)\n\t\tseq_startplay();\nout:\n\treturn count;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Var-Declaration",
        "line_old": 479,
        "critical_vars": [
            "offs"
        ],
        "function": "EXPORT_SYMBOL",
        "filename": "linux/CVE-2011-1476/CVE-2011-1476_CWE-189_b769f49463711205d57286e64cf535ed4daf59e9_midi_synth.c.diff",
        "label": "False",
        "function_code": "Function not found"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 508,
        "critical_vars": [
            "addr",
            "hdr_size",
            "&sysex"
        ],
        "function": "midi_synth_load_patch",
        "filename": "linux/CVE-2011-1476/CVE-2011-1476_CWE-189_b769f49463711205d57286e64cf535ed4daf59e9_midi_synth.c.diff",
        "label": "True",
        "function_code": "\nint\nmidi_synth_load_patch(int dev, int format, const char __user *addr,\n\t\t      int count, int pmgr_flag)\n{\n\tint             orig_dev = synth_devs[dev]->midi_dev;\n\n\tstruct sysex_info sysex;\n\tint             i;\n\tunsigned long   left, src_offs, eox_seen = 0;\n\tint             first_byte = 1;\n\tint             hdr_size = (unsigned long) &sysex.data[0] - (unsigned long) &sysex;\n\n\tleave_sysex(dev);\n\n\tif (!prefix_cmd(orig_dev, 0xf0))\n\t\treturn 0;\n\n\t/* Invalid patch format */\n\tif (format != SYSEX_PATCH)\n\t\t  return -EINVAL;\n\n\t/* Patch header too short */\n\tif (count < hdr_size)\n\t\treturn -EINVAL;\n\n\tcount -= hdr_size;\n\n\t/*\n\t * Copy the header from user space\n\t */\n\n\tif (copy_from_user(&sysex, addr, hdr_size))\n\t\treturn -EFAULT;\n\n\t/* Sysex record too short */\n\tif ((unsigned)count < (unsigned)sysex.len)\n\t\tsysex.len = count;\n\n\tleft = sysex.len;\n\tsrc_offs = 0;\n\n\tfor (i = 0; i < left && !signal_pending(current); i++)\n\t{\n\t\tunsigned char   data;\n\n\t\tif (get_user(data,\n\t\t    (unsigned char __user *)(addr + hdr_size + i)))\n\t\t\treturn -EFAULT;\n\n\t\teox_seen = (i > 0 && data & 0x80);\t/* End of sysex */\n\n\t\tif (eox_seen && data != 0xf7)\n\t\t\tdata = 0xf7;\n\n\t\tif (i == 0)\n\t\t{\n\t\t\tif (data != 0xf0)\n\t\t\t{\n\t\t\t\tprintk(KERN_WARNING \"midi_synth: Sysex start missing\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\t\twhile (!midi_devs[orig_dev]->outputc(orig_dev, (unsigned char) (data & 0xff)) &&\n\t\t\t!signal_pending(current))\n\t\t\tschedule();\n\n\t\tif (!first_byte && data & 0x80)\n\t\t\treturn 0;\n\t\tfirst_byte = 0;\n\t}\n\n\tif (!eox_seen)\n\t\tmidi_outc(orig_dev, 0xf7);\n\treturn 0;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "if-Condition",
        "line_old": 514,
        "critical_vars": [
            "count"
        ],
        "function": "midi_synth_load_patch",
        "filename": "linux/CVE-2011-1476/CVE-2011-1476_CWE-189_b769f49463711205d57286e64cf535ed4daf59e9_midi_synth.c.diff",
        "label": "False",
        "function_code": "\nint\nmidi_synth_load_patch(int dev, int format, const char __user *addr,\n\t\t      int offs, int count, int pmgr_flag)\n{\n\tint             orig_dev = synth_devs[dev]->midi_dev;\n\n\tstruct sysex_info sysex;\n\tint             i;\n\tunsigned long   left, src_offs, eox_seen = 0;\n\tint             first_byte = 1;\n\tint             hdr_size = (unsigned long) &sysex.data[0] - (unsigned long) &sysex;\n\n\tleave_sysex(dev);\n\n\tif (!prefix_cmd(orig_dev, 0xf0))\n\t\treturn 0;\n\n\tif (format != SYSEX_PATCH)\n\t{\n/*\t\t  printk(\"MIDI Error: Invalid patch format (key) 0x%x\\n\", format);*/\n\t\t  return -EINVAL;\n\t}\n\tif (count < hdr_size)\n\t{\n/*\t\tprintk(\"MIDI Error: Patch header too short\\n\");*/\n\t\treturn -EINVAL;\n\t}\n\tcount -= hdr_size;\n\n\t/*\n\t * Copy the header from user space but ignore the first bytes which have\n\t * been transferred already.\n\t */\n\n\tif(copy_from_user(&((char *) &sysex)[offs], &(addr)[offs], hdr_size - offs))\n\t\treturn -EFAULT;\n \n \tif (count < sysex.len)\n\t{\n/*\t\tprintk(KERN_WARNING \"MIDI Warning: Sysex record too short (%d<%d)\\n\", count, (int) sysex.len);*/\n\t\tsysex.len = count;\n\t}\n  \tleft = sysex.len;\n  \tsrc_offs = 0;\n\n\tfor (i = 0; i < left && !signal_pending(current); i++)\n\t{\n\t\tunsigned char   data;\n\n\t\tif (get_user(data,\n\t\t    (unsigned char __user *)(addr + hdr_size + i)))\n\t\t\treturn -EFAULT;\n\n\t\teox_seen = (i > 0 && data & 0x80);\t/* End of sysex */\n\n\t\tif (eox_seen && data != 0xf7)\n\t\t\tdata = 0xf7;\n\n\t\tif (i == 0)\n\t\t{\n\t\t\tif (data != 0xf0)\n\t\t\t{\n\t\t\t\tprintk(KERN_WARNING \"midi_synth: Sysex start missing\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\t\twhile (!midi_devs[orig_dev]->outputc(orig_dev, (unsigned char) (data & 0xff)) &&\n\t\t\t!signal_pending(current))\n\t\t\tschedule();\n\n\t\tif (!first_byte && data & 0x80)\n\t\t\treturn 0;\n\t\tfirst_byte = 0;\n\t}\n\n\tif (!eox_seen)\n\t\tmidi_outc(orig_dev, 0xf7);\n\treturn 0;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 64,
        "critical_vars": [
            "pfn_t",
            "npages",
            "kvm",
            "pfn",
            "long"
        ],
        "function": "kvm_pin_pages",
        "filename": "linux/CVE-2014-3601/CVE-2014-3601_CWE-189_350b8bdd689cd2ab2c67c8a86a0be86cfa0751a7_iommu.c.diff",
        "label": "True",
        "function_code": "\nstatic pfn_t kvm_pin_pages(struct kvm_memory_slot *slot, gfn_t gfn,\n\t\t\t   unsigned long size)\n{\n\tgfn_t end_gfn;\n\tpfn_t pfn;\n\n\tpfn     = gfn_to_pfn_memslot(slot, gfn);\n\tend_gfn = gfn + (size >> PAGE_SHIFT);\n\tgfn    += 1;\n\n\tif (is_error_noslot_pfn(pfn))\n\t\treturn pfn;\n\n\twhile (gfn < end_gfn)\n\t\tgfn_to_pfn_memslot(slot, gfn++);\n\n\treturn pfn;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Var-Declaration",
        "line_new": 66,
        "critical_vars": [
            "i"
        ],
        "function": "kvm_pin_pages",
        "filename": "linux/CVE-2014-3601/CVE-2014-3601_CWE-189_350b8bdd689cd2ab2c67c8a86a0be86cfa0751a7_iommu.c.diff",
        "label": "True",
        "function_code": "\nstatic pfn_t kvm_pin_pages(struct kvm_memory_slot *slot, gfn_t gfn,\n\t\t\t   unsigned long size)\n{\n\tgfn_t end_gfn;\n\tpfn_t pfn;\n\n\tpfn     = gfn_to_pfn_memslot(slot, gfn);\n\tend_gfn = gfn + (size >> PAGE_SHIFT);\n\tgfn    += 1;\n\n\tif (is_error_noslot_pfn(pfn))\n\t\treturn pfn;\n\n\twhile (gfn < end_gfn)\n\t\tgfn_to_pfn_memslot(slot, gfn++);\n\n\treturn pfn;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "for-Condition",
        "line_new": 68,
        "critical_vars": [
            "i"
        ],
        "function": "kvm_pin_pages",
        "filename": "linux/CVE-2014-3601/CVE-2014-3601_CWE-189_350b8bdd689cd2ab2c67c8a86a0be86cfa0751a7_iommu.c.diff",
        "label": "True",
        "function_code": "\nstatic pfn_t kvm_pin_pages(struct kvm_memory_slot *slot, gfn_t gfn,\n\t\t\t   unsigned long size)\n{\n\tgfn_t end_gfn;\n\tpfn_t pfn;\n\n\tpfn     = gfn_to_pfn_memslot(slot, gfn);\n\tend_gfn = gfn + (size >> PAGE_SHIFT);\n\tgfn    += 1;\n\n\tif (is_error_noslot_pfn(pfn))\n\t\treturn pfn;\n\n\twhile (gfn < end_gfn)\n\t\tgfn_to_pfn_memslot(slot, gfn++);\n\n\treturn pfn;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 69,
        "critical_vars": [
            "i",
            "pfn"
        ],
        "function": "kvm_pin_pages",
        "filename": "linux/CVE-2014-3601/CVE-2014-3601_CWE-189_350b8bdd689cd2ab2c67c8a86a0be86cfa0751a7_iommu.c.diff",
        "label": "True",
        "function_code": "\nstatic pfn_t kvm_pin_pages(struct kvm_memory_slot *slot, gfn_t gfn,\n\t\t\t   unsigned long size)\n{\n\tgfn_t end_gfn;\n\tpfn_t pfn;\n\n\tpfn     = gfn_to_pfn_memslot(slot, gfn);\n\tend_gfn = gfn + (size >> PAGE_SHIFT);\n\tgfn    += 1;\n\n\tif (is_error_noslot_pfn(pfn))\n\t\treturn pfn;\n\n\twhile (gfn < end_gfn)\n\t\tgfn_to_pfn_memslot(slot, gfn++);\n\n\treturn pfn;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 134,
        "critical_vars": [
            "page_size",
            "pfn",
            "kvm"
        ],
        "function": "kvm_iommu_map_pages",
        "filename": "linux/CVE-2014-3601/CVE-2014-3601_CWE-189_350b8bdd689cd2ab2c67c8a86a0be86cfa0751a7_iommu.c.diff",
        "label": "True",
        "function_code": "\nint kvm_iommu_map_pages(struct kvm *kvm, struct kvm_memory_slot *slot)\n{\n\tgfn_t gfn, end_gfn;\n\tpfn_t pfn;\n\tint r = 0;\n\tstruct iommu_domain *domain = kvm->arch.iommu_domain;\n\tint flags;\n\n\t/* check if iommu exists and in use */\n\tif (!domain)\n\t\treturn 0;\n\n\tgfn     = slot->base_gfn;\n\tend_gfn = gfn + slot->npages;\n\n\tflags = IOMMU_READ;\n\tif (!(slot->flags & KVM_MEM_READONLY))\n\t\tflags |= IOMMU_WRITE;\n\tif (!kvm->arch.iommu_noncoherent)\n\t\tflags |= IOMMU_CACHE;\n\n\n\twhile (gfn < end_gfn) {\n\t\tunsigned long page_size;\n\n\t\t/* Check if already mapped */\n\t\tif (iommu_iova_to_phys(domain, gfn_to_gpa(gfn))) {\n\t\t\tgfn += 1;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Get the page size we could use to map */\n\t\tpage_size = kvm_host_page_size(kvm, gfn);\n\n\t\t/* Make sure the page_size does not exceed the memslot */\n\t\twhile ((gfn + (page_size >> PAGE_SHIFT)) > end_gfn)\n\t\t\tpage_size >>= 1;\n\n\t\t/* Make sure gfn is aligned to the page size we want to map */\n\t\twhile ((gfn << PAGE_SHIFT) & (page_size - 1))\n\t\t\tpage_size >>= 1;\n\n\t\t/* Make sure hva is aligned to the page size we want to map */\n\t\twhile (__gfn_to_hva_memslot(slot, gfn) & (page_size - 1))\n\t\t\tpage_size >>= 1;\n\n\t\t/*\n\t\t * Pin all pages we are about to map in memory. This is\n\t\t * important because we unmap and unpin in 4kb steps later.\n\t\t */\n\t\tpfn = kvm_pin_pages(slot, gfn, page_size);\n\t\tif (is_error_noslot_pfn(pfn)) {\n\t\t\tgfn += 1;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Map into IO address space */\n\t\tr = iommu_map(domain, gfn_to_gpa(gfn), pfn_to_hpa(pfn),\n\t\t\t      page_size, flags);\n\t\tif (r) {\n\t\t\tprintk(KERN_ERR \"kvm_iommu_map_address:\"\n\t\t\t       \"iommu failed to map pfn=%llx\\n\", pfn);\n\t\t\tkvm_unpin_pages(kvm, pfn, page_size);\n\t\t\tgoto unmap_pages;\n\t\t}\n\n\t\tgfn += page_size >> PAGE_SHIFT;\n\n\n\t}\n\n\treturn 0;\n\nunmap_pages:\n\tkvm_iommu_put_pages(kvm, slot->base_gfn, gfn - slot->base_gfn);\n\treturn r;\n}"
    },
    {
        "patch_model": "Replace",
        "change_type": "Fun-Call",
        "line_old": 137,
        "line_new": 146,
        "critical_vars": [
            "gfn",
            "slot->base_gfn",
            "kvm"
        ],
        "function": "kvm_iommu_map_pages",
        "filename": "linux/CVE-2014-3601/CVE-2014-3601_CWE-189_350b8bdd689cd2ab2c67c8a86a0be86cfa0751a7_iommu.c.diff",
        "label": "True",
        "function_code": "\nint kvm_iommu_map_pages(struct kvm *kvm, struct kvm_memory_slot *slot)\n{\n\tgfn_t gfn, end_gfn;\n\tpfn_t pfn;\n\tint r = 0;\n\tstruct iommu_domain *domain = kvm->arch.iommu_domain;\n\tint flags;\n\n\t/* check if iommu exists and in use */\n\tif (!domain)\n\t\treturn 0;\n\n\tgfn     = slot->base_gfn;\n\tend_gfn = gfn + slot->npages;\n\n\tflags = IOMMU_READ;\n\tif (!(slot->flags & KVM_MEM_READONLY))\n\t\tflags |= IOMMU_WRITE;\n\tif (!kvm->arch.iommu_noncoherent)\n\t\tflags |= IOMMU_CACHE;\n\n\n\twhile (gfn < end_gfn) {\n\t\tunsigned long page_size;\n\n\t\t/* Check if already mapped */\n\t\tif (iommu_iova_to_phys(domain, gfn_to_gpa(gfn))) {\n\t\t\tgfn += 1;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Get the page size we could use to map */\n\t\tpage_size = kvm_host_page_size(kvm, gfn);\n\n\t\t/* Make sure the page_size does not exceed the memslot */\n\t\twhile ((gfn + (page_size >> PAGE_SHIFT)) > end_gfn)\n\t\t\tpage_size >>= 1;\n\n\t\t/* Make sure gfn is aligned to the page size we want to map */\n\t\twhile ((gfn << PAGE_SHIFT) & (page_size - 1))\n\t\t\tpage_size >>= 1;\n\n\t\t/* Make sure hva is aligned to the page size we want to map */\n\t\twhile (__gfn_to_hva_memslot(slot, gfn) & (page_size - 1))\n\t\t\tpage_size >>= 1;\n\n\t\t/*\n\t\t * Pin all pages we are about to map in memory. This is\n\t\t * important because we unmap and unpin in 4kb steps later.\n\t\t */\n\t\tpfn = kvm_pin_pages(slot, gfn, page_size);\n\t\tif (is_error_noslot_pfn(pfn)) {\n\t\t\tgfn += 1;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Map into IO address space */\n\t\tr = iommu_map(domain, gfn_to_gpa(gfn), pfn_to_hpa(pfn),\n\t\t\t      page_size, flags);\n\t\tif (r) {\n\t\t\tprintk(KERN_ERR \"kvm_iommu_map_address:\"\n\t\t\t       \"iommu failed to map pfn=%llx\\n\", pfn);\n\t\t\tkvm_unpin_pages(kvm, pfn, page_size);\n\t\t\tgoto unmap_pages;\n\t\t}\n\n\t\tgfn += page_size >> PAGE_SHIFT;\n\n\n\t}\n\n\treturn 0;\n\nunmap_pages:\n\tkvm_iommu_put_pages(kvm, slot->base_gfn, gfn - slot->base_gfn);\n\treturn r;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Fun-Call",
        "line_old": 269,
        "critical_vars": [
            "pfn_t",
            "npages",
            "kvm",
            "pfn",
            "long"
        ],
        "function": "kvm_iommu_map_guest",
        "filename": "linux/CVE-2014-3601/CVE-2014-3601_CWE-189_350b8bdd689cd2ab2c67c8a86a0be86cfa0751a7_iommu.c.diff",
        "label": "False",
        "function_code": "\nint kvm_iommu_map_guest(struct kvm *kvm)\n{\n\tint r;\n\n\tif (!iommu_present(&pci_bus_type)) {\n\t\tprintk(KERN_ERR \"%s: iommu not found\\n\", __func__);\n\t\treturn -ENODEV;\n\t}\n\n\tmutex_lock(&kvm->slots_lock);\n\n\tkvm->arch.iommu_domain = iommu_domain_alloc(&pci_bus_type);\n\tif (!kvm->arch.iommu_domain) {\n\t\tr = -ENOMEM;\n\t\tgoto out_unlock;\n\t}\n\n\tif (!allow_unsafe_assigned_interrupts &&\n\t    !iommu_domain_has_cap(kvm->arch.iommu_domain,\n\t\t\t\t  IOMMU_CAP_INTR_REMAP)) {\n\t\tprintk(KERN_WARNING \"%s: No interrupt remapping support,\"\n\t\t       \" disallowing device assignment.\"\n\t\t       \" Re-enble with \\\"allow_unsafe_assigned_interrupts=1\\\"\"\n\t\t       \" module option.\\n\", __func__);\n\t\tiommu_domain_free(kvm->arch.iommu_domain);\n\t\tkvm->arch.iommu_domain = NULL;\n\t\tr = -EPERM;\n\t\tgoto out_unlock;\n\t}\n\n\tr = kvm_iommu_map_memslots(kvm);\n\tif (r)\n\t\tkvm_iommu_unmap_memslots(kvm);\n\nout_unlock:\n\tmutex_unlock(&kvm->slots_lock);\n\treturn r;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Var-Declaration",
        "line_old": 271,
        "critical_vars": [
            "i"
        ],
        "function": "kvm_iommu_map_guest",
        "filename": "linux/CVE-2014-3601/CVE-2014-3601_CWE-189_350b8bdd689cd2ab2c67c8a86a0be86cfa0751a7_iommu.c.diff",
        "label": "False",
        "function_code": "\nint kvm_iommu_map_guest(struct kvm *kvm)\n{\n\tint r;\n\n\tif (!iommu_present(&pci_bus_type)) {\n\t\tprintk(KERN_ERR \"%s: iommu not found\\n\", __func__);\n\t\treturn -ENODEV;\n\t}\n\n\tmutex_lock(&kvm->slots_lock);\n\n\tkvm->arch.iommu_domain = iommu_domain_alloc(&pci_bus_type);\n\tif (!kvm->arch.iommu_domain) {\n\t\tr = -ENOMEM;\n\t\tgoto out_unlock;\n\t}\n\n\tif (!allow_unsafe_assigned_interrupts &&\n\t    !iommu_domain_has_cap(kvm->arch.iommu_domain,\n\t\t\t\t  IOMMU_CAP_INTR_REMAP)) {\n\t\tprintk(KERN_WARNING \"%s: No interrupt remapping support,\"\n\t\t       \" disallowing device assignment.\"\n\t\t       \" Re-enble with \\\"allow_unsafe_assigned_interrupts=1\\\"\"\n\t\t       \" module option.\\n\", __func__);\n\t\tiommu_domain_free(kvm->arch.iommu_domain);\n\t\tkvm->arch.iommu_domain = NULL;\n\t\tr = -EPERM;\n\t\tgoto out_unlock;\n\t}\n\n\tr = kvm_iommu_map_memslots(kvm);\n\tif (r)\n\t\tkvm_iommu_unmap_memslots(kvm);\n\nout_unlock:\n\tmutex_unlock(&kvm->slots_lock);\n\treturn r;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "for-Condition",
        "line_old": 273,
        "critical_vars": [
            "i"
        ],
        "function": "kvm_iommu_map_guest",
        "filename": "linux/CVE-2014-3601/CVE-2014-3601_CWE-189_350b8bdd689cd2ab2c67c8a86a0be86cfa0751a7_iommu.c.diff",
        "label": "False",
        "function_code": "\nint kvm_iommu_map_guest(struct kvm *kvm)\n{\n\tint r;\n\n\tif (!iommu_present(&pci_bus_type)) {\n\t\tprintk(KERN_ERR \"%s: iommu not found\\n\", __func__);\n\t\treturn -ENODEV;\n\t}\n\n\tmutex_lock(&kvm->slots_lock);\n\n\tkvm->arch.iommu_domain = iommu_domain_alloc(&pci_bus_type);\n\tif (!kvm->arch.iommu_domain) {\n\t\tr = -ENOMEM;\n\t\tgoto out_unlock;\n\t}\n\n\tif (!allow_unsafe_assigned_interrupts &&\n\t    !iommu_domain_has_cap(kvm->arch.iommu_domain,\n\t\t\t\t  IOMMU_CAP_INTR_REMAP)) {\n\t\tprintk(KERN_WARNING \"%s: No interrupt remapping support,\"\n\t\t       \" disallowing device assignment.\"\n\t\t       \" Re-enble with \\\"allow_unsafe_assigned_interrupts=1\\\"\"\n\t\t       \" module option.\\n\", __func__);\n\t\tiommu_domain_free(kvm->arch.iommu_domain);\n\t\tkvm->arch.iommu_domain = NULL;\n\t\tr = -EPERM;\n\t\tgoto out_unlock;\n\t}\n\n\tr = kvm_iommu_map_memslots(kvm);\n\tif (r)\n\t\tkvm_iommu_unmap_memslots(kvm);\n\nout_unlock:\n\tmutex_unlock(&kvm->slots_lock);\n\treturn r;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Fun-Call",
        "line_old": 274,
        "critical_vars": [
            "i",
            "pfn"
        ],
        "function": "kvm_iommu_map_guest",
        "filename": "linux/CVE-2014-3601/CVE-2014-3601_CWE-189_350b8bdd689cd2ab2c67c8a86a0be86cfa0751a7_iommu.c.diff",
        "label": "False",
        "function_code": "\nint kvm_iommu_map_guest(struct kvm *kvm)\n{\n\tint r;\n\n\tif (!iommu_present(&pci_bus_type)) {\n\t\tprintk(KERN_ERR \"%s: iommu not found\\n\", __func__);\n\t\treturn -ENODEV;\n\t}\n\n\tmutex_lock(&kvm->slots_lock);\n\n\tkvm->arch.iommu_domain = iommu_domain_alloc(&pci_bus_type);\n\tif (!kvm->arch.iommu_domain) {\n\t\tr = -ENOMEM;\n\t\tgoto out_unlock;\n\t}\n\n\tif (!allow_unsafe_assigned_interrupts &&\n\t    !iommu_domain_has_cap(kvm->arch.iommu_domain,\n\t\t\t\t  IOMMU_CAP_INTR_REMAP)) {\n\t\tprintk(KERN_WARNING \"%s: No interrupt remapping support,\"\n\t\t       \" disallowing device assignment.\"\n\t\t       \" Re-enble with \\\"allow_unsafe_assigned_interrupts=1\\\"\"\n\t\t       \" module option.\\n\", __func__);\n\t\tiommu_domain_free(kvm->arch.iommu_domain);\n\t\tkvm->arch.iommu_domain = NULL;\n\t\tr = -EPERM;\n\t\tgoto out_unlock;\n\t}\n\n\tr = kvm_iommu_map_memslots(kvm);\n\tif (r)\n\t\tkvm_iommu_unmap_memslots(kvm);\n\nout_unlock:\n\tmutex_unlock(&kvm->slots_lock);\n\treturn r;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Var-Declaration",
        "line_new": 645,
        "critical_vars": [
            "*mem"
        ],
        "function": "uio_mmap_physical",
        "filename": "linux/CVE-2013-4511/CVE-2013-4511_CWE-189_7314e613d5ff9f0934f7a0f74ed7973b903315d1_uio.c.diff",
        "label": "True",
        "function_code": "\nstatic int uio_mmap_physical(struct vm_area_struct *vma)\n{\n\tstruct uio_device *idev = vma->vm_private_data;\n\tint mi = uio_find_mem_index(vma);\n\tstruct uio_mem *mem;\n\tif (mi < 0)\n\t\treturn -EINVAL;\n\tmem = idev->info->mem + mi;\n\n\tif (vma->vm_end - vma->vm_start > mem->size)\n\t\treturn -EINVAL;\n\n\tvma->vm_ops = &uio_physical_vm_ops;\n\tvma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);\n\n\t/*\n\t * We cannot use the vm_iomap_memory() helper here,\n\t * because vma->vm_pgoff is the map index we looked\n\t * up above in uio_find_mem_index(), rather than an\n\t * actual page offset into the mmap.\n\t *\n\t * So we just do the physical mmap without a page\n\t * offset.\n\t */\n\treturn remap_pfn_range(vma,\n\t\t\t       vma->vm_start,\n\t\t\t       mem->addr >> PAGE_SHIFT,\n\t\t\t       vma->vm_end - vma->vm_start,\n\t\t\t       vma->vm_page_prot);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 648,
        "critical_vars": [
            "mem"
        ],
        "function": "uio_mmap_physical",
        "filename": "linux/CVE-2013-4511/CVE-2013-4511_CWE-189_7314e613d5ff9f0934f7a0f74ed7973b903315d1_uio.c.diff",
        "label": "True",
        "function_code": "\nstatic int uio_mmap_physical(struct vm_area_struct *vma)\n{\n\tstruct uio_device *idev = vma->vm_private_data;\n\tint mi = uio_find_mem_index(vma);\n\tstruct uio_mem *mem;\n\tif (mi < 0)\n\t\treturn -EINVAL;\n\tmem = idev->info->mem + mi;\n\n\tif (vma->vm_end - vma->vm_start > mem->size)\n\t\treturn -EINVAL;\n\n\tvma->vm_ops = &uio_physical_vm_ops;\n\tvma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);\n\n\t/*\n\t * We cannot use the vm_iomap_memory() helper here,\n\t * because vma->vm_pgoff is the map index we looked\n\t * up above in uio_find_mem_index(), rather than an\n\t * actual page offset into the mmap.\n\t *\n\t * So we just do the physical mmap without a page\n\t * offset.\n\t */\n\treturn remap_pfn_range(vma,\n\t\t\t       vma->vm_start,\n\t\t\t       mem->addr >> PAGE_SHIFT,\n\t\t\t       vma->vm_end - vma->vm_start,\n\t\t\t       vma->vm_page_prot);\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 648,
        "critical_vars": [
            "vma->vm_ops"
        ],
        "function": "uio_mmap_physical",
        "filename": "linux/CVE-2013-4511/CVE-2013-4511_CWE-189_7314e613d5ff9f0934f7a0f74ed7973b903315d1_uio.c.diff",
        "label": "False",
        "function_code": "\nstatic int uio_mmap_physical(struct vm_area_struct *vma)\n{\n\tstruct uio_device *idev = vma->vm_private_data;\n\tint mi = uio_find_mem_index(vma);\n\tif (mi < 0)\n\t\treturn -EINVAL;\n\n\tvma->vm_ops = &uio_physical_vm_ops;\n\n\tvma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);\n\n\treturn remap_pfn_range(vma,\n\t\t\t       vma->vm_start,\n\t\t\t       idev->info->mem[mi].addr >> PAGE_SHIFT,\n\t\t\t       vma->vm_end - vma->vm_start,\n\t\t\t       vma->vm_page_prot);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 650,
        "critical_vars": [
            "vma->vm_end",
            "vma->vm_start"
        ],
        "function": "uio_mmap_physical",
        "filename": "linux/CVE-2013-4511/CVE-2013-4511_CWE-189_7314e613d5ff9f0934f7a0f74ed7973b903315d1_uio.c.diff",
        "label": "True",
        "function_code": "\nstatic int uio_mmap_physical(struct vm_area_struct *vma)\n{\n\tstruct uio_device *idev = vma->vm_private_data;\n\tint mi = uio_find_mem_index(vma);\n\tstruct uio_mem *mem;\n\tif (mi < 0)\n\t\treturn -EINVAL;\n\tmem = idev->info->mem + mi;\n\n\tif (vma->vm_end - vma->vm_start > mem->size)\n\t\treturn -EINVAL;\n\n\tvma->vm_ops = &uio_physical_vm_ops;\n\tvma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);\n\n\t/*\n\t * We cannot use the vm_iomap_memory() helper here,\n\t * because vma->vm_pgoff is the map index we looked\n\t * up above in uio_find_mem_index(), rather than an\n\t * actual page offset into the mmap.\n\t *\n\t * So we just do the physical mmap without a page\n\t * offset.\n\t */\n\treturn remap_pfn_range(vma,\n\t\t\t       vma->vm_start,\n\t\t\t       mem->addr >> PAGE_SHIFT,\n\t\t\t       vma->vm_end - vma->vm_start,\n\t\t\t       vma->vm_page_prot);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 653,
        "critical_vars": [
            "vma->vm_ops"
        ],
        "function": "uio_mmap_physical",
        "filename": "linux/CVE-2013-4511/CVE-2013-4511_CWE-189_7314e613d5ff9f0934f7a0f74ed7973b903315d1_uio.c.diff",
        "label": "True",
        "function_code": "\nstatic int uio_mmap_physical(struct vm_area_struct *vma)\n{\n\tstruct uio_device *idev = vma->vm_private_data;\n\tint mi = uio_find_mem_index(vma);\n\tstruct uio_mem *mem;\n\tif (mi < 0)\n\t\treturn -EINVAL;\n\tmem = idev->info->mem + mi;\n\n\tif (vma->vm_end - vma->vm_start > mem->size)\n\t\treturn -EINVAL;\n\n\tvma->vm_ops = &uio_physical_vm_ops;\n\tvma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);\n\n\t/*\n\t * We cannot use the vm_iomap_memory() helper here,\n\t * because vma->vm_pgoff is the map index we looked\n\t * up above in uio_find_mem_index(), rather than an\n\t * actual page offset into the mmap.\n\t *\n\t * So we just do the physical mmap without a page\n\t * offset.\n\t */\n\treturn remap_pfn_range(vma,\n\t\t\t       vma->vm_start,\n\t\t\t       mem->addr >> PAGE_SHIFT,\n\t\t\t       vma->vm_end - vma->vm_start,\n\t\t\t       vma->vm_page_prot);\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Var-Declaration",
        "line_old": 1238,
        "critical_vars": [
            "len"
        ],
        "function": "au1200fb_fb_blank",
        "filename": "linux/CVE-2013-4511/CVE-2013-4511_CWE-189_7314e613d5ff9f0934f7a0f74ed7973b903315d1_au1200fb.c.diff",
        "label": "False",
        "function_code": "static int au1200fb_fb_blank(int blank_mode, struct fb_info *fbi)\n{\n\tstruct au1200fb_device *fbdev = fbi->par;\n\n\t/* Short-circuit screen blanking */\n\tif (noblanking)\n\t\treturn 0;\n\n\tswitch (blank_mode) {\n\n\tcase FB_BLANK_UNBLANK:\n\tcase FB_BLANK_NORMAL:\n\t\t/* printk(\"turn on panel\\n\"); */\n\t\tau1200_setpanel(panel, fbdev->pd);\n\t\tbreak;\n\tcase FB_BLANK_VSYNC_SUSPEND:\n\tcase FB_BLANK_HSYNC_SUSPEND:\n\tcase FB_BLANK_POWERDOWN:\n\t\t/* printk(\"turn off panel\\n\"); */\n\t\tau1200_setpanel(NULL, fbdev->pd);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\n\t}\n\n\t/* FB_BLANK_NORMAL is a soft blank */\n\treturn (blank_mode == FB_BLANK_NORMAL) ? -EINVAL : 0;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 1239,
        "critical_vars": [
            "start"
        ],
        "function": "au1200fb_fb_blank",
        "filename": "linux/CVE-2013-4511/CVE-2013-4511_CWE-189_7314e613d5ff9f0934f7a0f74ed7973b903315d1_au1200fb.c.diff",
        "label": "False",
        "function_code": "static int au1200fb_fb_blank(int blank_mode, struct fb_info *fbi)\n{\n\tstruct au1200fb_device *fbdev = fbi->par;\n\n\t/* Short-circuit screen blanking */\n\tif (noblanking)\n\t\treturn 0;\n\n\tswitch (blank_mode) {\n\n\tcase FB_BLANK_UNBLANK:\n\tcase FB_BLANK_NORMAL:\n\t\t/* printk(\"turn on panel\\n\"); */\n\t\tau1200_setpanel(panel, fbdev->pd);\n\t\tbreak;\n\tcase FB_BLANK_VSYNC_SUSPEND:\n\tcase FB_BLANK_HSYNC_SUSPEND:\n\tcase FB_BLANK_POWERDOWN:\n\t\t/* printk(\"turn off panel\\n\"); */\n\t\tau1200_setpanel(NULL, fbdev->pd);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\n\t}\n\n\t/* FB_BLANK_NORMAL is a soft blank */\n\treturn (blank_mode == FB_BLANK_NORMAL) ? -EINVAL : 0;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "if-Condition",
        "line_old": 1242,
        "critical_vars": [
            "vma->vm_pgoff"
        ],
        "function": "au1200fb_fb_blank",
        "filename": "linux/CVE-2013-4511/CVE-2013-4511_CWE-189_7314e613d5ff9f0934f7a0f74ed7973b903315d1_au1200fb.c.diff",
        "label": "False",
        "function_code": "static int au1200fb_fb_blank(int blank_mode, struct fb_info *fbi)\n{\n\tstruct au1200fb_device *fbdev = fbi->par;\n\n\t/* Short-circuit screen blanking */\n\tif (noblanking)\n\t\treturn 0;\n\n\tswitch (blank_mode) {\n\n\tcase FB_BLANK_UNBLANK:\n\tcase FB_BLANK_NORMAL:\n\t\t/* printk(\"turn on panel\\n\"); */\n\t\tau1200_setpanel(panel, fbdev->pd);\n\t\tbreak;\n\tcase FB_BLANK_VSYNC_SUSPEND:\n\tcase FB_BLANK_HSYNC_SUSPEND:\n\tcase FB_BLANK_POWERDOWN:\n\t\t/* printk(\"turn off panel\\n\"); */\n\t\tau1200_setpanel(NULL, fbdev->pd);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\n\t}\n\n\t/* FB_BLANK_NORMAL is a soft blank */\n\treturn (blank_mode == FB_BLANK_NORMAL) ? -EINVAL : 0;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 1246,
        "critical_vars": [
            "start"
        ],
        "function": "au1200fb_fb_blank",
        "filename": "linux/CVE-2013-4511/CVE-2013-4511_CWE-189_7314e613d5ff9f0934f7a0f74ed7973b903315d1_au1200fb.c.diff",
        "label": "False",
        "function_code": "static int au1200fb_fb_blank(int blank_mode, struct fb_info *fbi)\n{\n\tstruct au1200fb_device *fbdev = fbi->par;\n\n\t/* Short-circuit screen blanking */\n\tif (noblanking)\n\t\treturn 0;\n\n\tswitch (blank_mode) {\n\n\tcase FB_BLANK_UNBLANK:\n\tcase FB_BLANK_NORMAL:\n\t\t/* printk(\"turn on panel\\n\"); */\n\t\tau1200_setpanel(panel, fbdev->pd);\n\t\tbreak;\n\tcase FB_BLANK_VSYNC_SUSPEND:\n\tcase FB_BLANK_HSYNC_SUSPEND:\n\tcase FB_BLANK_POWERDOWN:\n\t\t/* printk(\"turn off panel\\n\"); */\n\t\tau1200_setpanel(NULL, fbdev->pd);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\n\t}\n\n\t/* FB_BLANK_NORMAL is a soft blank */\n\treturn (blank_mode == FB_BLANK_NORMAL) ? -EINVAL : 0;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 1247,
        "critical_vars": [
            "len"
        ],
        "function": "au1200fb_fb_blank",
        "filename": "linux/CVE-2013-4511/CVE-2013-4511_CWE-189_7314e613d5ff9f0934f7a0f74ed7973b903315d1_au1200fb.c.diff",
        "label": "False",
        "function_code": "static int au1200fb_fb_blank(int blank_mode, struct fb_info *fbi)\n{\n\tstruct au1200fb_device *fbdev = fbi->par;\n\n\t/* Short-circuit screen blanking */\n\tif (noblanking)\n\t\treturn 0;\n\n\tswitch (blank_mode) {\n\n\tcase FB_BLANK_UNBLANK:\n\tcase FB_BLANK_NORMAL:\n\t\t/* printk(\"turn on panel\\n\"); */\n\t\tau1200_setpanel(panel, fbdev->pd);\n\t\tbreak;\n\tcase FB_BLANK_VSYNC_SUSPEND:\n\tcase FB_BLANK_HSYNC_SUSPEND:\n\tcase FB_BLANK_POWERDOWN:\n\t\t/* printk(\"turn off panel\\n\"); */\n\t\tau1200_setpanel(NULL, fbdev->pd);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\n\t}\n\n\t/* FB_BLANK_NORMAL is a soft blank */\n\treturn (blank_mode == FB_BLANK_NORMAL) ? -EINVAL : 0;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 1249,
        "critical_vars": [
            "off"
        ],
        "function": "au1200fb_fb_blank",
        "filename": "linux/CVE-2013-4511/CVE-2013-4511_CWE-189_7314e613d5ff9f0934f7a0f74ed7973b903315d1_au1200fb.c.diff",
        "label": "False",
        "function_code": "static int au1200fb_fb_blank(int blank_mode, struct fb_info *fbi)\n{\n\tstruct au1200fb_device *fbdev = fbi->par;\n\n\t/* Short-circuit screen blanking */\n\tif (noblanking)\n\t\treturn 0;\n\n\tswitch (blank_mode) {\n\n\tcase FB_BLANK_UNBLANK:\n\tcase FB_BLANK_NORMAL:\n\t\t/* printk(\"turn on panel\\n\"); */\n\t\tau1200_setpanel(panel, fbdev->pd);\n\t\tbreak;\n\tcase FB_BLANK_VSYNC_SUSPEND:\n\tcase FB_BLANK_HSYNC_SUSPEND:\n\tcase FB_BLANK_POWERDOWN:\n\t\t/* printk(\"turn off panel\\n\"); */\n\t\tau1200_setpanel(NULL, fbdev->pd);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\n\t}\n\n\t/* FB_BLANK_NORMAL is a soft blank */\n\treturn (blank_mode == FB_BLANK_NORMAL) ? -EINVAL : 0;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "if-Condition",
        "line_old": 1251,
        "critical_vars": [
            "vma->vm_end",
            "vma->vm_start",
            "off"
        ],
        "function": "au1200fb_fb_blank",
        "filename": "linux/CVE-2013-4511/CVE-2013-4511_CWE-189_7314e613d5ff9f0934f7a0f74ed7973b903315d1_au1200fb.c.diff",
        "label": "False",
        "function_code": "static int au1200fb_fb_blank(int blank_mode, struct fb_info *fbi)\n{\n\tstruct au1200fb_device *fbdev = fbi->par;\n\n\t/* Short-circuit screen blanking */\n\tif (noblanking)\n\t\treturn 0;\n\n\tswitch (blank_mode) {\n\n\tcase FB_BLANK_UNBLANK:\n\tcase FB_BLANK_NORMAL:\n\t\t/* printk(\"turn on panel\\n\"); */\n\t\tau1200_setpanel(panel, fbdev->pd);\n\t\tbreak;\n\tcase FB_BLANK_VSYNC_SUSPEND:\n\tcase FB_BLANK_HSYNC_SUSPEND:\n\tcase FB_BLANK_POWERDOWN:\n\t\t/* printk(\"turn off panel\\n\"); */\n\t\tau1200_setpanel(NULL, fbdev->pd);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\n\t}\n\n\t/* FB_BLANK_NORMAL is a soft blank */\n\treturn (blank_mode == FB_BLANK_NORMAL) ? -EINVAL : 0;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 1255,
        "critical_vars": [
            "off"
        ],
        "function": "au1200fb_fb_blank",
        "filename": "linux/CVE-2013-4511/CVE-2013-4511_CWE-189_7314e613d5ff9f0934f7a0f74ed7973b903315d1_au1200fb.c.diff",
        "label": "False",
        "function_code": "static int au1200fb_fb_blank(int blank_mode, struct fb_info *fbi)\n{\n\tstruct au1200fb_device *fbdev = fbi->par;\n\n\t/* Short-circuit screen blanking */\n\tif (noblanking)\n\t\treturn 0;\n\n\tswitch (blank_mode) {\n\n\tcase FB_BLANK_UNBLANK:\n\tcase FB_BLANK_NORMAL:\n\t\t/* printk(\"turn on panel\\n\"); */\n\t\tau1200_setpanel(panel, fbdev->pd);\n\t\tbreak;\n\tcase FB_BLANK_VSYNC_SUSPEND:\n\tcase FB_BLANK_HSYNC_SUSPEND:\n\tcase FB_BLANK_POWERDOWN:\n\t\t/* printk(\"turn off panel\\n\"); */\n\t\tau1200_setpanel(NULL, fbdev->pd);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\n\t}\n\n\t/* FB_BLANK_NORMAL is a soft blank */\n\treturn (blank_mode == FB_BLANK_NORMAL) ? -EINVAL : 0;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 1256,
        "critical_vars": [
            "vma->vm_pgoff"
        ],
        "function": "au1200fb_fb_blank",
        "filename": "linux/CVE-2013-4511/CVE-2013-4511_CWE-189_7314e613d5ff9f0934f7a0f74ed7973b903315d1_au1200fb.c.diff",
        "label": "False",
        "function_code": "static int au1200fb_fb_blank(int blank_mode, struct fb_info *fbi)\n{\n\tstruct au1200fb_device *fbdev = fbi->par;\n\n\t/* Short-circuit screen blanking */\n\tif (noblanking)\n\t\treturn 0;\n\n\tswitch (blank_mode) {\n\n\tcase FB_BLANK_UNBLANK:\n\tcase FB_BLANK_NORMAL:\n\t\t/* printk(\"turn on panel\\n\"); */\n\t\tau1200_setpanel(panel, fbdev->pd);\n\t\tbreak;\n\tcase FB_BLANK_VSYNC_SUSPEND:\n\tcase FB_BLANK_HSYNC_SUSPEND:\n\tcase FB_BLANK_POWERDOWN:\n\t\t/* printk(\"turn off panel\\n\"); */\n\t\tau1200_setpanel(NULL, fbdev->pd);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\n\t}\n\n\t/* FB_BLANK_NORMAL is a soft blank */\n\treturn (blank_mode == FB_BLANK_NORMAL) ? -EINVAL : 0;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Var-Declaration",
        "line_old": 364,
        "critical_vars": [
            "len"
        ],
        "function": "au1100fb_fb_rotate",
        "filename": "linux/CVE-2013-4511/CVE-2013-4511_CWE-189_7314e613d5ff9f0934f7a0f74ed7973b903315d1_au1100fb.c.diff",
        "label": "False",
        "function_code": "void au1100fb_fb_rotate(struct fb_info *fbi, int angle)\n{\n\tstruct au1100fb_device *fbdev = to_au1100fb_device(fbi);\n\n\tprint_dbg(\"fb_rotate %p %d\", fbi, angle);\n\n\tif (fbdev && (angle > 0) && !(angle % 90)) {\n\n\t\tfbdev->regs->lcd_control &= ~LCD_CONTROL_GO;\n\n\t\tfbdev->regs->lcd_control &= ~(LCD_CONTROL_SM_MASK);\n\t\tfbdev->regs->lcd_control |= ((angle/90) << LCD_CONTROL_SM_BIT);\n\n\t\tfbdev->regs->lcd_control |= LCD_CONTROL_GO;\n\t}\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 365,
        "critical_vars": [
            "start"
        ],
        "function": "au1100fb_fb_rotate",
        "filename": "linux/CVE-2013-4511/CVE-2013-4511_CWE-189_7314e613d5ff9f0934f7a0f74ed7973b903315d1_au1100fb.c.diff",
        "label": "False",
        "function_code": "void au1100fb_fb_rotate(struct fb_info *fbi, int angle)\n{\n\tstruct au1100fb_device *fbdev = to_au1100fb_device(fbi);\n\n\tprint_dbg(\"fb_rotate %p %d\", fbi, angle);\n\n\tif (fbdev && (angle > 0) && !(angle % 90)) {\n\n\t\tfbdev->regs->lcd_control &= ~LCD_CONTROL_GO;\n\n\t\tfbdev->regs->lcd_control &= ~(LCD_CONTROL_SM_MASK);\n\t\tfbdev->regs->lcd_control |= ((angle/90) << LCD_CONTROL_SM_BIT);\n\n\t\tfbdev->regs->lcd_control |= LCD_CONTROL_GO;\n\t}\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "if-Condition",
        "line_old": 369,
        "critical_vars": [
            "vma->vm_pgoff"
        ],
        "function": "au1100fb_fb_rotate",
        "filename": "linux/CVE-2013-4511/CVE-2013-4511_CWE-189_7314e613d5ff9f0934f7a0f74ed7973b903315d1_au1100fb.c.diff",
        "label": "False",
        "function_code": "void au1100fb_fb_rotate(struct fb_info *fbi, int angle)\n{\n\tstruct au1100fb_device *fbdev = to_au1100fb_device(fbi);\n\n\tprint_dbg(\"fb_rotate %p %d\", fbi, angle);\n\n\tif (fbdev && (angle > 0) && !(angle % 90)) {\n\n\t\tfbdev->regs->lcd_control &= ~LCD_CONTROL_GO;\n\n\t\tfbdev->regs->lcd_control &= ~(LCD_CONTROL_SM_MASK);\n\t\tfbdev->regs->lcd_control |= ((angle/90) << LCD_CONTROL_SM_BIT);\n\n\t\tfbdev->regs->lcd_control |= LCD_CONTROL_GO;\n\t}\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 373,
        "critical_vars": [
            "start"
        ],
        "function": "au1100fb_fb_rotate",
        "filename": "linux/CVE-2013-4511/CVE-2013-4511_CWE-189_7314e613d5ff9f0934f7a0f74ed7973b903315d1_au1100fb.c.diff",
        "label": "False",
        "function_code": "void au1100fb_fb_rotate(struct fb_info *fbi, int angle)\n{\n\tstruct au1100fb_device *fbdev = to_au1100fb_device(fbi);\n\n\tprint_dbg(\"fb_rotate %p %d\", fbi, angle);\n\n\tif (fbdev && (angle > 0) && !(angle % 90)) {\n\n\t\tfbdev->regs->lcd_control &= ~LCD_CONTROL_GO;\n\n\t\tfbdev->regs->lcd_control &= ~(LCD_CONTROL_SM_MASK);\n\t\tfbdev->regs->lcd_control |= ((angle/90) << LCD_CONTROL_SM_BIT);\n\n\t\tfbdev->regs->lcd_control |= LCD_CONTROL_GO;\n\t}\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 374,
        "critical_vars": [
            "len"
        ],
        "function": "au1100fb_fb_rotate",
        "filename": "linux/CVE-2013-4511/CVE-2013-4511_CWE-189_7314e613d5ff9f0934f7a0f74ed7973b903315d1_au1100fb.c.diff",
        "label": "False",
        "function_code": "void au1100fb_fb_rotate(struct fb_info *fbi, int angle)\n{\n\tstruct au1100fb_device *fbdev = to_au1100fb_device(fbi);\n\n\tprint_dbg(\"fb_rotate %p %d\", fbi, angle);\n\n\tif (fbdev && (angle > 0) && !(angle % 90)) {\n\n\t\tfbdev->regs->lcd_control &= ~LCD_CONTROL_GO;\n\n\t\tfbdev->regs->lcd_control &= ~(LCD_CONTROL_SM_MASK);\n\t\tfbdev->regs->lcd_control |= ((angle/90) << LCD_CONTROL_SM_BIT);\n\n\t\tfbdev->regs->lcd_control |= LCD_CONTROL_GO;\n\t}\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 376,
        "critical_vars": [
            "off"
        ],
        "function": "au1100fb_fb_rotate",
        "filename": "linux/CVE-2013-4511/CVE-2013-4511_CWE-189_7314e613d5ff9f0934f7a0f74ed7973b903315d1_au1100fb.c.diff",
        "label": "False",
        "function_code": "void au1100fb_fb_rotate(struct fb_info *fbi, int angle)\n{\n\tstruct au1100fb_device *fbdev = to_au1100fb_device(fbi);\n\n\tprint_dbg(\"fb_rotate %p %d\", fbi, angle);\n\n\tif (fbdev && (angle > 0) && !(angle % 90)) {\n\n\t\tfbdev->regs->lcd_control &= ~LCD_CONTROL_GO;\n\n\t\tfbdev->regs->lcd_control &= ~(LCD_CONTROL_SM_MASK);\n\t\tfbdev->regs->lcd_control |= ((angle/90) << LCD_CONTROL_SM_BIT);\n\n\t\tfbdev->regs->lcd_control |= LCD_CONTROL_GO;\n\t}\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "if-Condition",
        "line_old": 378,
        "critical_vars": [
            "vma->vm_end",
            "vma->vm_start",
            "off"
        ],
        "function": "au1100fb_fb_rotate",
        "filename": "linux/CVE-2013-4511/CVE-2013-4511_CWE-189_7314e613d5ff9f0934f7a0f74ed7973b903315d1_au1100fb.c.diff",
        "label": "False",
        "function_code": "void au1100fb_fb_rotate(struct fb_info *fbi, int angle)\n{\n\tstruct au1100fb_device *fbdev = to_au1100fb_device(fbi);\n\n\tprint_dbg(\"fb_rotate %p %d\", fbi, angle);\n\n\tif (fbdev && (angle > 0) && !(angle % 90)) {\n\n\t\tfbdev->regs->lcd_control &= ~LCD_CONTROL_GO;\n\n\t\tfbdev->regs->lcd_control &= ~(LCD_CONTROL_SM_MASK);\n\t\tfbdev->regs->lcd_control |= ((angle/90) << LCD_CONTROL_SM_BIT);\n\n\t\tfbdev->regs->lcd_control |= LCD_CONTROL_GO;\n\t}\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 382,
        "critical_vars": [
            "off"
        ],
        "function": "au1100fb_fb_rotate",
        "filename": "linux/CVE-2013-4511/CVE-2013-4511_CWE-189_7314e613d5ff9f0934f7a0f74ed7973b903315d1_au1100fb.c.diff",
        "label": "False",
        "function_code": "void au1100fb_fb_rotate(struct fb_info *fbi, int angle)\n{\n\tstruct au1100fb_device *fbdev = to_au1100fb_device(fbi);\n\n\tprint_dbg(\"fb_rotate %p %d\", fbi, angle);\n\n\tif (fbdev && (angle > 0) && !(angle % 90)) {\n\n\t\tfbdev->regs->lcd_control &= ~LCD_CONTROL_GO;\n\n\t\tfbdev->regs->lcd_control &= ~(LCD_CONTROL_SM_MASK);\n\t\tfbdev->regs->lcd_control |= ((angle/90) << LCD_CONTROL_SM_BIT);\n\n\t\tfbdev->regs->lcd_control |= LCD_CONTROL_GO;\n\t}\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 383,
        "critical_vars": [
            "vma->vm_pgoff"
        ],
        "function": "au1100fb_fb_rotate",
        "filename": "linux/CVE-2013-4511/CVE-2013-4511_CWE-189_7314e613d5ff9f0934f7a0f74ed7973b903315d1_au1100fb.c.diff",
        "label": "False",
        "function_code": "void au1100fb_fb_rotate(struct fb_info *fbi, int angle)\n{\n\tstruct au1100fb_device *fbdev = to_au1100fb_device(fbi);\n\n\tprint_dbg(\"fb_rotate %p %d\", fbi, angle);\n\n\tif (fbdev && (angle > 0) && !(angle % 90)) {\n\n\t\tfbdev->regs->lcd_control &= ~LCD_CONTROL_GO;\n\n\t\tfbdev->regs->lcd_control &= ~(LCD_CONTROL_SM_MASK);\n\t\tfbdev->regs->lcd_control |= ((angle/90) << LCD_CONTROL_SM_BIT);\n\n\t\tfbdev->regs->lcd_control |= LCD_CONTROL_GO;\n\t}\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "if-Condition",
        "line_old": 388,
        "critical_vars": [
            "vma",
            "vma->vm_start",
            "off"
        ],
        "function": "au1100fb_fb_rotate",
        "filename": "linux/CVE-2013-4511/CVE-2013-4511_CWE-189_7314e613d5ff9f0934f7a0f74ed7973b903315d1_au1100fb.c.diff",
        "label": "False",
        "function_code": "void au1100fb_fb_rotate(struct fb_info *fbi, int angle)\n{\n\tstruct au1100fb_device *fbdev = to_au1100fb_device(fbi);\n\n\tprint_dbg(\"fb_rotate %p %d\", fbi, angle);\n\n\tif (fbdev && (angle > 0) && !(angle % 90)) {\n\n\t\tfbdev->regs->lcd_control &= ~LCD_CONTROL_GO;\n\n\t\tfbdev->regs->lcd_control &= ~(LCD_CONTROL_SM_MASK);\n\t\tfbdev->regs->lcd_control |= ((angle/90) << LCD_CONTROL_SM_BIT);\n\n\t\tfbdev->regs->lcd_control |= LCD_CONTROL_GO;\n\t}\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 316,
        "critical_vars": [
            "t"
        ],
        "function": "tcp_illinois_info",
        "filename": "linux/CVE-2012-4565/CVE-2012-4565_CWE-189_8f363b77ee4fbf7c3bbcf5ec2c5ca482d396d664_tcp_illinois.c.diff",
        "label": "False",
        "function_code": "static void tcp_illinois_info(struct sock *sk, u32 ext,\n\t\t\t      struct sk_buff *skb)\n{\n\tconst struct illinois *ca = inet_csk_ca(sk);\n\n\tif (ext & (1 << (INET_DIAG_VEGASINFO - 1))) {\n\t\tstruct tcpvegas_info info = {\n\t\t\t.tcpv_enabled = 1,\n\t\t\t.tcpv_rttcnt = ca->cnt_rtt,\n\t\t\t.tcpv_minrtt = ca->base_rtt,\n\t\t};\n\t\tu64 t = ca->sum_rtt;\n\n\t\tdo_div(t, ca->cnt_rtt);\n\t\tinfo.tcpv_rtt = t;\n\n\t\tnla_put(skb, INET_DIAG_VEGASINFO, sizeof(info), &info);\n\t}\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Fun-Call",
        "line_old": 318,
        "critical_vars": [
            "t",
            "ca->cnt_rtt"
        ],
        "function": "tcp_illinois_info",
        "filename": "linux/CVE-2012-4565/CVE-2012-4565_CWE-189_8f363b77ee4fbf7c3bbcf5ec2c5ca482d396d664_tcp_illinois.c.diff",
        "label": "False",
        "function_code": "static void tcp_illinois_info(struct sock *sk, u32 ext,\n\t\t\t      struct sk_buff *skb)\n{\n\tconst struct illinois *ca = inet_csk_ca(sk);\n\n\tif (ext & (1 << (INET_DIAG_VEGASINFO - 1))) {\n\t\tstruct tcpvegas_info info = {\n\t\t\t.tcpv_enabled = 1,\n\t\t\t.tcpv_rttcnt = ca->cnt_rtt,\n\t\t\t.tcpv_minrtt = ca->base_rtt,\n\t\t};\n\t\tu64 t = ca->sum_rtt;\n\n\t\tdo_div(t, ca->cnt_rtt);\n\t\tinfo.tcpv_rtt = t;\n\n\t\tnla_put(skb, INET_DIAG_VEGASINFO, sizeof(info), &info);\n\t}\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 317,
        "critical_vars": [
            "info.tcpv_rttcnt"
        ],
        "function": "tcp_illinois_info",
        "filename": "linux/CVE-2012-4565/CVE-2012-4565_CWE-189_8f363b77ee4fbf7c3bbcf5ec2c5ca482d396d664_tcp_illinois.c.diff",
        "label": "True",
        "function_code": "static void tcp_illinois_info(struct sock *sk, u32 ext,\n\t\t\t      struct sk_buff *skb)\n{\n\tconst struct illinois *ca = inet_csk_ca(sk);\n\n\tif (ext & (1 << (INET_DIAG_VEGASINFO - 1))) {\n\t\tstruct tcpvegas_info info = {\n\t\t\t.tcpv_enabled = 1,\n\t\t\t.tcpv_rttcnt = ca->cnt_rtt,\n\t\t\t.tcpv_minrtt = ca->base_rtt,\n\t\t};\n\n\t\tif (info.tcpv_rttcnt > 0) {\n\t\t\tu64 t = ca->sum_rtt;\n\n\t\t\tdo_div(t, info.tcpv_rttcnt);\n\t\t\tinfo.tcpv_rtt = t;\n\t\t}\n\t\tnla_put(skb, INET_DIAG_VEGASINFO, sizeof(info), &info);\n\t}\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 319,
        "critical_vars": [
            "info.tcpv_rtt"
        ],
        "function": "tcp_illinois_info",
        "filename": "linux/CVE-2012-4565/CVE-2012-4565_CWE-189_8f363b77ee4fbf7c3bbcf5ec2c5ca482d396d664_tcp_illinois.c.diff",
        "label": "False",
        "function_code": "static void tcp_illinois_info(struct sock *sk, u32 ext,\n\t\t\t      struct sk_buff *skb)\n{\n\tconst struct illinois *ca = inet_csk_ca(sk);\n\n\tif (ext & (1 << (INET_DIAG_VEGASINFO - 1))) {\n\t\tstruct tcpvegas_info info = {\n\t\t\t.tcpv_enabled = 1,\n\t\t\t.tcpv_rttcnt = ca->cnt_rtt,\n\t\t\t.tcpv_minrtt = ca->base_rtt,\n\t\t};\n\t\tu64 t = ca->sum_rtt;\n\n\t\tdo_div(t, ca->cnt_rtt);\n\t\tinfo.tcpv_rtt = t;\n\n\t\tnla_put(skb, INET_DIAG_VEGASINFO, sizeof(info), &info);\n\t}\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 318,
        "critical_vars": [
            "t"
        ],
        "function": "tcp_illinois_info",
        "filename": "linux/CVE-2012-4565/CVE-2012-4565_CWE-189_8f363b77ee4fbf7c3bbcf5ec2c5ca482d396d664_tcp_illinois.c.diff",
        "label": "True",
        "function_code": "static void tcp_illinois_info(struct sock *sk, u32 ext,\n\t\t\t      struct sk_buff *skb)\n{\n\tconst struct illinois *ca = inet_csk_ca(sk);\n\n\tif (ext & (1 << (INET_DIAG_VEGASINFO - 1))) {\n\t\tstruct tcpvegas_info info = {\n\t\t\t.tcpv_enabled = 1,\n\t\t\t.tcpv_rttcnt = ca->cnt_rtt,\n\t\t\t.tcpv_minrtt = ca->base_rtt,\n\t\t};\n\n\t\tif (info.tcpv_rttcnt > 0) {\n\t\t\tu64 t = ca->sum_rtt;\n\n\t\t\tdo_div(t, info.tcpv_rttcnt);\n\t\t\tinfo.tcpv_rtt = t;\n\t\t}\n\t\tnla_put(skb, INET_DIAG_VEGASINFO, sizeof(info), &info);\n\t}\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 320,
        "critical_vars": [
            "info.tcpv_rttcnt",
            "t"
        ],
        "function": "tcp_illinois_info",
        "filename": "linux/CVE-2012-4565/CVE-2012-4565_CWE-189_8f363b77ee4fbf7c3bbcf5ec2c5ca482d396d664_tcp_illinois.c.diff",
        "label": "True",
        "function_code": "static void tcp_illinois_info(struct sock *sk, u32 ext,\n\t\t\t      struct sk_buff *skb)\n{\n\tconst struct illinois *ca = inet_csk_ca(sk);\n\n\tif (ext & (1 << (INET_DIAG_VEGASINFO - 1))) {\n\t\tstruct tcpvegas_info info = {\n\t\t\t.tcpv_enabled = 1,\n\t\t\t.tcpv_rttcnt = ca->cnt_rtt,\n\t\t\t.tcpv_minrtt = ca->base_rtt,\n\t\t};\n\n\t\tif (info.tcpv_rttcnt > 0) {\n\t\t\tu64 t = ca->sum_rtt;\n\n\t\t\tdo_div(t, info.tcpv_rttcnt);\n\t\t\tinfo.tcpv_rtt = t;\n\t\t}\n\t\tnla_put(skb, INET_DIAG_VEGASINFO, sizeof(info), &info);\n\t}\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 321,
        "critical_vars": [
            "info.tcpv_rtt"
        ],
        "function": "tcp_illinois_info",
        "filename": "linux/CVE-2012-4565/CVE-2012-4565_CWE-189_8f363b77ee4fbf7c3bbcf5ec2c5ca482d396d664_tcp_illinois.c.diff",
        "label": "True",
        "function_code": "static void tcp_illinois_info(struct sock *sk, u32 ext,\n\t\t\t      struct sk_buff *skb)\n{\n\tconst struct illinois *ca = inet_csk_ca(sk);\n\n\tif (ext & (1 << (INET_DIAG_VEGASINFO - 1))) {\n\t\tstruct tcpvegas_info info = {\n\t\t\t.tcpv_enabled = 1,\n\t\t\t.tcpv_rttcnt = ca->cnt_rtt,\n\t\t\t.tcpv_minrtt = ca->base_rtt,\n\t\t};\n\n\t\tif (info.tcpv_rttcnt > 0) {\n\t\t\tu64 t = ca->sum_rtt;\n\n\t\t\tdo_div(t, info.tcpv_rttcnt);\n\t\t\tinfo.tcpv_rtt = t;\n\t\t}\n\t\tnla_put(skb, INET_DIAG_VEGASINFO, sizeof(info), &info);\n\t}\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 1407,
        "critical_vars": [
            "args->buffer_count"
        ],
        "function": "i915_gem_execbuffer2",
        "filename": "linux/CVE-2012-2383/CVE-2012-2383_CWE-189_ed8cd3b2cd61004cab85380c52b1817aca1ca49b_i915_gem_execbuffer.c.diff",
        "label": "True",
        "function_code": "\nint\ni915_gem_execbuffer2(struct drm_device *dev, void *data,\n\t\t     struct drm_file *file)\n{\n\tstruct drm_i915_gem_execbuffer2 *args = data;\n\tstruct drm_i915_gem_exec_object2 *exec2_list = NULL;\n\tint ret;\n\n\tif (args->buffer_count < 1 ||\n\t    args->buffer_count > UINT_MAX / sizeof(*exec2_list)) {\n\t\tDRM_DEBUG(\"execbuf2 with %d buffers\\n\", args->buffer_count);\n\t\treturn -EINVAL;\n\t}\n\n\texec2_list = kmalloc(sizeof(*exec2_list)*args->buffer_count,\n\t\t\t     GFP_KERNEL | __GFP_NOWARN | __GFP_NORETRY);\n\tif (exec2_list == NULL)\n\t\texec2_list = drm_malloc_ab(sizeof(*exec2_list),\n\t\t\t\t\t   args->buffer_count);\n\tif (exec2_list == NULL) {\n\t\tDRM_DEBUG(\"Failed to allocate exec list for %d buffers\\n\",\n\t\t\t  args->buffer_count);\n\t\treturn -ENOMEM;\n\t}\n\tret = copy_from_user(exec2_list,\n\t\t\t     (struct drm_i915_relocation_entry __user *)\n\t\t\t     (uintptr_t) args->buffers_ptr,\n\t\t\t     sizeof(*exec2_list) * args->buffer_count);\n\tif (ret != 0) {\n\t\tDRM_DEBUG(\"copy %d exec entries failed %d\\n\",\n\t\t\t  args->buffer_count, ret);\n\t\tdrm_free_large(exec2_list);\n\t\treturn -EFAULT;\n\t}\n\n\tret = i915_gem_do_execbuffer(dev, data, file, args, exec2_list);\n\tif (!ret) {\n\t\t/* Copy the new buffer offsets back to the user's exec list. */\n\t\tret = copy_to_user((struct drm_i915_relocation_entry __user *)\n\t\t\t\t   (uintptr_t) args->buffers_ptr,\n\t\t\t\t   exec2_list,\n\t\t\t\t   sizeof(*exec2_list) * args->buffer_count);\n\t\tif (ret) {\n\t\t\tret = -EFAULT;\n\t\t\tDRM_DEBUG(\"failed to copy %d exec entries \"\n\t\t\t\t  \"back to user (%d)\\n\",\n\t\t\t\t  args->buffer_count, ret);\n\t\t}\n\t}\n\n\tdrm_free_large(exec2_list);\n\treturn ret;\n}\n\n... (function end not found)"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 51,
        "critical_vars": [
            "tp->tv_sec"
        ],
        "function": "sample_to_timespec",
        "filename": "linux/CVE-2011-3209/CVE-2011-3209_CWE-189_f8bd2258e2d520dff28c855658bd24bdafb5102d_posix-cpu-timers.c.diff",
        "label": "False",
        "function_code": "\nstatic void sample_to_timespec(const clockid_t which_clock,\n\t\t\t       union cpu_time_count cpu,\n\t\t\t       struct timespec *tp)\n{\n\tif (CPUCLOCK_WHICH(which_clock) == CPUCLOCK_SCHED) {\n\t\ttp->tv_sec = div_long_long_rem(cpu.sched,\n\t\t\t\t\t       NSEC_PER_SEC, &tp->tv_nsec);\n\t} else {\n\t\tcputime_to_timespec(cpu.cpu, tp);\n\t}\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 52,
        "critical_vars": [
            "*tp"
        ],
        "function": "sample_to_timespec",
        "filename": "linux/CVE-2011-3209/CVE-2011-3209_CWE-189_f8bd2258e2d520dff28c855658bd24bdafb5102d_posix-cpu-timers.c.diff",
        "label": "True",
        "function_code": "\nstatic void sample_to_timespec(const clockid_t which_clock,\n\t\t\t       union cpu_time_count cpu,\n\t\t\t       struct timespec *tp)\n{\n\tif (CPUCLOCK_WHICH(which_clock) == CPUCLOCK_SCHED)\n\t\t*tp = ns_to_timespec(cpu.sched);\n\telse\n\t\tcputime_to_timespec(cpu.cpu, tp);\n}"
    },
    {
        "patch_model": "Replace",
        "change_type": "Var-Declaration",
        "line_old": 105,
        "line_new": 106,
        "critical_vars": [
            "rem"
        ],
        "function": "jiffies_to_compat_timeval",
        "filename": "linux/CVE-2011-3209/CVE-2011-3209_CWE-189_f8bd2258e2d520dff28c855658bd24bdafb5102d_binfmt_elfn32.c.diff",
        "label": "True",
        "function_code": "static __inline__ void\njiffies_to_compat_timeval(unsigned long jiffies, struct compat_timeval *value)\n{\n\t/*\n\t * Convert jiffies to nanoseconds and separate with\n\t * one divide.\n\t */\n\tu64 nsec = (u64)jiffies * TICK_NSEC;\n\tu32 rem;\n\tvalue->tv_sec = div_u64_rem(nsec, NSEC_PER_SEC, &rem);\n\tvalue->tv_usec = rem / NSEC_PER_USEC;\n}"
    },
    {
        "patch_model": "Replace",
        "change_type": "Assignment",
        "line_old": 106,
        "line_new": 107,
        "critical_vars": [
            "value->tv_sec"
        ],
        "function": "jiffies_to_compat_timeval",
        "filename": "linux/CVE-2011-3209/CVE-2011-3209_CWE-189_f8bd2258e2d520dff28c855658bd24bdafb5102d_binfmt_elfn32.c.diff",
        "label": "True",
        "function_code": "static __inline__ void\njiffies_to_compat_timeval(unsigned long jiffies, struct compat_timeval *value)\n{\n\t/*\n\t * Convert jiffies to nanoseconds and separate with\n\t * one divide.\n\t */\n\tu64 nsec = (u64)jiffies * TICK_NSEC;\n\tu32 rem;\n\tvalue->tv_sec = div_u64_rem(nsec, NSEC_PER_SEC, &rem);\n\tvalue->tv_usec = rem / NSEC_PER_USEC;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Var-Declaration",
        "line_new": 395,
        "critical_vars": [
            "rem"
        ],
        "function": "EXPORT_SYMBOL",
        "filename": "linux/CVE-2011-3209/CVE-2011-3209_CWE-189_f8bd2258e2d520dff28c855658bd24bdafb5102d_time.c.diff",
        "label": "True",
        "function_code": "Function not found"
    },
    {
        "patch_model": "Replace",
        "change_type": "Assignment",
        "line_old": 399,
        "line_new": 400,
        "critical_vars": [
            "ts.tv_sec"
        ],
        "function": "EXPORT_SYMBOL",
        "filename": "linux/CVE-2011-3209/CVE-2011-3209_CWE-189_f8bd2258e2d520dff28c855658bd24bdafb5102d_time.c.diff",
        "label": "True",
        "function_code": "Function not found"
    },
    {
        "patch_model": "Delete",
        "change_type": "if-Condition",
        "line_old": 400,
        "critical_vars": [
            "nsec"
        ],
        "function": "EXPORT_SYMBOL",
        "filename": "linux/CVE-2011-3209/CVE-2011-3209_CWE-189_f8bd2258e2d520dff28c855658bd24bdafb5102d_time.c.diff",
        "label": "False",
        "function_code": "Function not found"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 401,
        "critical_vars": [
            "rem"
        ],
        "function": "EXPORT_SYMBOL",
        "filename": "linux/CVE-2011-3209/CVE-2011-3209_CWE-189_f8bd2258e2d520dff28c855658bd24bdafb5102d_time.c.diff",
        "label": "True",
        "function_code": "Function not found"
    },
    {
        "patch_model": "Delete",
        "change_type": "Fun-Call",
        "line_old": 401,
        "critical_vars": [
            "ts.tv_sec",
            "ts.tv_nsec",
            "&ts"
        ],
        "function": "EXPORT_SYMBOL",
        "filename": "linux/CVE-2011-3209/CVE-2011-3209_CWE-189_f8bd2258e2d520dff28c855658bd24bdafb5102d_time.c.diff",
        "label": "False",
        "function_code": "Function not found"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 402,
        "critical_vars": [
            "ts.tv_sec"
        ],
        "function": "EXPORT_SYMBOL",
        "filename": "linux/CVE-2011-3209/CVE-2011-3209_CWE-189_f8bd2258e2d520dff28c855658bd24bdafb5102d_time.c.diff",
        "label": "True",
        "function_code": "Function not found"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 403,
        "critical_vars": [
            "rem"
        ],
        "function": "EXPORT_SYMBOL",
        "filename": "linux/CVE-2011-3209/CVE-2011-3209_CWE-189_f8bd2258e2d520dff28c855658bd24bdafb5102d_time.c.diff",
        "label": "True",
        "function_code": "Function not found"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 405,
        "critical_vars": [
            "ts.tv_nsec"
        ],
        "function": "EXPORT_SYMBOL",
        "filename": "linux/CVE-2011-3209/CVE-2011-3209_CWE-189_f8bd2258e2d520dff28c855658bd24bdafb5102d_time.c.diff",
        "label": "True",
        "function_code": "Function not found"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 531,
        "critical_vars": [
            "nsec"
        ],
        "function": "jiffies_to_timespec",
        "filename": "linux/CVE-2011-3209/CVE-2011-3209_CWE-189_f8bd2258e2d520dff28c855658bd24bdafb5102d_time.c.diff",
        "label": "False",
        "function_code": "\nvoid\njiffies_to_timespec(const unsigned long jiffies, struct timespec *value)\n{\n\t/*\n\t * Convert jiffies to nanoseconds and separate with\n\t * one divide.\n\t */\n\tu64 nsec = (u64)jiffies * TICK_NSEC;\n\tvalue->tv_sec = div_long_long_rem(nsec, NSEC_PER_SEC, &value->tv_nsec);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Var-Declaration",
        "line_new": 535,
        "critical_vars": [
            "rem"
        ],
        "function": "jiffies_to_timespec",
        "filename": "linux/CVE-2011-3209/CVE-2011-3209_CWE-189_f8bd2258e2d520dff28c855658bd24bdafb5102d_time.c.diff",
        "label": "True",
        "function_code": "\nvoid\njiffies_to_timespec(const unsigned long jiffies, struct timespec *value)\n{\n\t/*\n\t * Convert jiffies to nanoseconds and separate with\n\t * one divide.\n\t */\n\tu32 rem;\n\tvalue->tv_sec = div_u64_rem((u64)jiffies * TICK_NSEC,\n\t\t\t\t    NSEC_PER_SEC, &rem);\n\tvalue->tv_nsec = rem;\n}"
    },
    {
        "patch_model": "Replace",
        "change_type": "Assignment",
        "line_old": 532,
        "line_new": 536,
        "critical_vars": [
            "value->tv_sec"
        ],
        "function": "jiffies_to_timespec",
        "filename": "linux/CVE-2011-3209/CVE-2011-3209_CWE-189_f8bd2258e2d520dff28c855658bd24bdafb5102d_time.c.diff",
        "label": "True",
        "function_code": "\nvoid\njiffies_to_timespec(const unsigned long jiffies, struct timespec *value)\n{\n\t/*\n\t * Convert jiffies to nanoseconds and separate with\n\t * one divide.\n\t */\n\tu32 rem;\n\tvalue->tv_sec = div_u64_rem((u64)jiffies * TICK_NSEC,\n\t\t\t\t    NSEC_PER_SEC, &rem);\n\tvalue->tv_nsec = rem;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 538,
        "critical_vars": [
            "value->tv_nsec"
        ],
        "function": "jiffies_to_timespec",
        "filename": "linux/CVE-2011-3209/CVE-2011-3209_CWE-189_f8bd2258e2d520dff28c855658bd24bdafb5102d_time.c.diff",
        "label": "True",
        "function_code": "\nvoid\njiffies_to_timespec(const unsigned long jiffies, struct timespec *value)\n{\n\t/*\n\t * Convert jiffies to nanoseconds and separate with\n\t * one divide.\n\t */\n\tu32 rem;\n\tvalue->tv_sec = div_u64_rem((u64)jiffies * TICK_NSEC,\n\t\t\t\t    NSEC_PER_SEC, &rem);\n\tvalue->tv_nsec = rem;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 570,
        "critical_vars": [
            "nsec"
        ],
        "function": "jiffies_to_timeval",
        "filename": "linux/CVE-2011-3209/CVE-2011-3209_CWE-189_f8bd2258e2d520dff28c855658bd24bdafb5102d_time.c.diff",
        "label": "False",
        "function_code": "\nvoid jiffies_to_timeval(const unsigned long jiffies, struct timeval *value)\n{\n\t/*\n\t * Convert jiffies to nanoseconds and separate with\n\t * one divide.\n\t */\n\tu64 nsec = (u64)jiffies * TICK_NSEC;\n\tlong tv_usec;\n\n\tvalue->tv_sec = div_long_long_rem(nsec, NSEC_PER_SEC, &tv_usec);\n\ttv_usec /= NSEC_PER_USEC;\n\tvalue->tv_usec = tv_usec;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Var-Declaration",
        "line_new": 576,
        "critical_vars": [
            "rem"
        ],
        "function": "jiffies_to_timeval",
        "filename": "linux/CVE-2011-3209/CVE-2011-3209_CWE-189_f8bd2258e2d520dff28c855658bd24bdafb5102d_time.c.diff",
        "label": "True",
        "function_code": "\nvoid jiffies_to_timeval(const unsigned long jiffies, struct timeval *value)\n{\n\t/*\n\t * Convert jiffies to nanoseconds and separate with\n\t * one divide.\n\t */\n\tu32 rem;\n\n\tvalue->tv_sec = div_u64_rem((u64)jiffies * TICK_NSEC,\n\t\t\t\t    NSEC_PER_SEC, &rem);\n\tvalue->tv_usec = rem / NSEC_PER_USEC;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Var-Declaration",
        "line_old": 571,
        "critical_vars": [
            "tv_usec"
        ],
        "function": "jiffies_to_timeval",
        "filename": "linux/CVE-2011-3209/CVE-2011-3209_CWE-189_f8bd2258e2d520dff28c855658bd24bdafb5102d_time.c.diff",
        "label": "False",
        "function_code": "\nvoid jiffies_to_timeval(const unsigned long jiffies, struct timeval *value)\n{\n\t/*\n\t * Convert jiffies to nanoseconds and separate with\n\t * one divide.\n\t */\n\tu64 nsec = (u64)jiffies * TICK_NSEC;\n\tlong tv_usec;\n\n\tvalue->tv_sec = div_long_long_rem(nsec, NSEC_PER_SEC, &tv_usec);\n\ttv_usec /= NSEC_PER_USEC;\n\tvalue->tv_usec = tv_usec;\n}"
    },
    {
        "patch_model": "Replace",
        "change_type": "Assignment",
        "line_old": 573,
        "line_new": 578,
        "critical_vars": [
            "value->tv_sec"
        ],
        "function": "jiffies_to_timeval",
        "filename": "linux/CVE-2011-3209/CVE-2011-3209_CWE-189_f8bd2258e2d520dff28c855658bd24bdafb5102d_time.c.diff",
        "label": "True",
        "function_code": "\nvoid jiffies_to_timeval(const unsigned long jiffies, struct timeval *value)\n{\n\t/*\n\t * Convert jiffies to nanoseconds and separate with\n\t * one divide.\n\t */\n\tu32 rem;\n\n\tvalue->tv_sec = div_u64_rem((u64)jiffies * TICK_NSEC,\n\t\t\t\t    NSEC_PER_SEC, &rem);\n\tvalue->tv_usec = rem / NSEC_PER_USEC;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 574,
        "critical_vars": [
            "tv_usec"
        ],
        "function": "jiffies_to_timeval",
        "filename": "linux/CVE-2011-3209/CVE-2011-3209_CWE-189_f8bd2258e2d520dff28c855658bd24bdafb5102d_time.c.diff",
        "label": "False",
        "function_code": "\nvoid jiffies_to_timeval(const unsigned long jiffies, struct timeval *value)\n{\n\t/*\n\t * Convert jiffies to nanoseconds and separate with\n\t * one divide.\n\t */\n\tu64 nsec = (u64)jiffies * TICK_NSEC;\n\tlong tv_usec;\n\n\tvalue->tv_sec = div_long_long_rem(nsec, NSEC_PER_SEC, &tv_usec);\n\ttv_usec /= NSEC_PER_USEC;\n\tvalue->tv_usec = tv_usec;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 580,
        "critical_vars": [
            "value->tv_usec"
        ],
        "function": "jiffies_to_timeval",
        "filename": "linux/CVE-2011-3209/CVE-2011-3209_CWE-189_f8bd2258e2d520dff28c855658bd24bdafb5102d_time.c.diff",
        "label": "True",
        "function_code": "\nvoid jiffies_to_timeval(const unsigned long jiffies, struct timeval *value)\n{\n\t/*\n\t * Convert jiffies to nanoseconds and separate with\n\t * one divide.\n\t */\n\tu32 rem;\n\n\tvalue->tv_sec = div_u64_rem((u64)jiffies * TICK_NSEC,\n\t\t\t\t    NSEC_PER_SEC, &rem);\n\tvalue->tv_usec = rem / NSEC_PER_USEC;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 575,
        "critical_vars": [
            "value->tv_usec"
        ],
        "function": "jiffies_to_timeval",
        "filename": "linux/CVE-2011-3209/CVE-2011-3209_CWE-189_f8bd2258e2d520dff28c855658bd24bdafb5102d_time.c.diff",
        "label": "False",
        "function_code": "\nvoid jiffies_to_timeval(const unsigned long jiffies, struct timeval *value)\n{\n\t/*\n\t * Convert jiffies to nanoseconds and separate with\n\t * one divide.\n\t */\n\tu64 nsec = (u64)jiffies * TICK_NSEC;\n\tlong tv_usec;\n\n\tvalue->tv_sec = div_long_long_rem(nsec, NSEC_PER_SEC, &tv_usec);\n\ttv_usec /= NSEC_PER_USEC;\n\tvalue->tv_usec = tv_usec;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Var-Declaration",
        "line_old": 3624,
        "critical_vars": [
            "remainder"
        ],
        "function": "list_locations",
        "filename": "linux/CVE-2011-3209/CVE-2011-3209_CWE-189_f8bd2258e2d520dff28c855658bd24bdafb5102d_slub.c.diff",
        "label": "False",
        "function_code": "\nstatic int list_locations(struct kmem_cache *s, char *buf,\n\t\t\t\t\tenum track_item alloc)\n{\n\tint len = 0;\n\tunsigned long i;\n\tstruct loc_track t = { 0, 0, NULL };\n\tint node;\n\n\tif (!alloc_loc_track(&t, PAGE_SIZE / sizeof(struct location),\n\t\t\tGFP_TEMPORARY))\n\t\treturn sprintf(buf, \"Out of memory\\n\");\n\n\t/* Push back cpu slabs */\n\tflush_all(s);\n\n\tfor_each_node_state(node, N_NORMAL_MEMORY) {\n\t\tstruct kmem_cache_node *n = get_node(s, node);\n\t\tunsigned long flags;\n\t\tstruct page *page;\n\n\t\tif (!atomic_long_read(&n->nr_slabs))\n\t\t\tcontinue;\n\n\t\tspin_lock_irqsave(&n->list_lock, flags);\n\t\tlist_for_each_entry(page, &n->partial, lru)\n\t\t\tprocess_slab(&t, s, page, alloc);\n\t\tlist_for_each_entry(page, &n->full, lru)\n\t\t\tprocess_slab(&t, s, page, alloc);\n\t\tspin_unlock_irqrestore(&n->list_lock, flags);\n\t}\n\n\tfor (i = 0; i < t.count; i++) {\n\t\tstruct location *l = &t.loc[i];\n\n\t\tif (len > PAGE_SIZE - 100)\n\t\t\tbreak;\n\t\tlen += sprintf(buf + len, \"%7ld \", l->count);\n\n\t\tif (l->addr)\n\t\t\tlen += sprint_symbol(buf + len, (unsigned long)l->addr);\n\t\telse\n\t\t\tlen += sprintf(buf + len, \"<not-available>\");\n\n\t\tif (l->sum_time != l->min_time) {\n\t\t\tunsigned long remainder;\n\n\t\t\tlen += sprintf(buf + len, \" age=%ld/%ld/%ld\",\n\t\t\tl->min_time,\n\t\t\tdiv_long_long_rem(l->sum_time, l->count, &remainder),\n\t\t\tl->max_time);\n\t\t} else\n\t\t\tlen += sprintf(buf + len, \" age=%ld\",\n\t\t\t\tl->min_time);\n\n\t\tif (l->min_pid != l->max_pid)\n\t\t\tlen += sprintf(buf + len, \" pid=%ld-%ld\",\n\t\t\t\tl->min_pid, l->max_pid);\n\t\telse\n\t\t\tlen += sprintf(buf + len, \" pid=%ld\",\n\t\t\t\tl->min_pid);\n\n\t\tif (num_online_cpus() > 1 && !cpus_empty(l->cpus) &&\n\t\t\t\tlen < PAGE_SIZE - 60) {\n\t\t\tlen += sprintf(buf + len, \" cpus=\");\n\t\t\tlen += cpulist_scnprintf(buf + len, PAGE_SIZE - len - 50,\n\t\t\t\t\tl->cpus);\n\t\t}\n\n\t\tif (num_online_nodes() > 1 && !nodes_empty(l->nodes) &&\n\t\t\t\tlen < PAGE_SIZE - 60) {\n\t\t\tlen += sprintf(buf + len, \" nodes=\");\n\t\t\tlen += nodelist_scnprintf(buf + len, PAGE_SIZE - len - 50,\n\t\t\t\t\tl->nodes);\n\t\t}\n\n\t\tlen += sprintf(buf + len, \"\\n\");\n\t}\n\n\tfree_loc_track(&t);\n\tif (!t.count)\n\t\tlen += sprintf(buf, \"No data\\n\");\n\treturn len;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Fun-Call",
        "line_old": 3627,
        "critical_vars": [
            "l->max_time",
            "l->sum_time",
            "&remainder",
            "l->count"
        ],
        "function": "list_locations",
        "filename": "linux/CVE-2011-3209/CVE-2011-3209_CWE-189_f8bd2258e2d520dff28c855658bd24bdafb5102d_slub.c.diff",
        "label": "False",
        "function_code": "\nstatic int list_locations(struct kmem_cache *s, char *buf,\n\t\t\t\t\tenum track_item alloc)\n{\n\tint len = 0;\n\tunsigned long i;\n\tstruct loc_track t = { 0, 0, NULL };\n\tint node;\n\n\tif (!alloc_loc_track(&t, PAGE_SIZE / sizeof(struct location),\n\t\t\tGFP_TEMPORARY))\n\t\treturn sprintf(buf, \"Out of memory\\n\");\n\n\t/* Push back cpu slabs */\n\tflush_all(s);\n\n\tfor_each_node_state(node, N_NORMAL_MEMORY) {\n\t\tstruct kmem_cache_node *n = get_node(s, node);\n\t\tunsigned long flags;\n\t\tstruct page *page;\n\n\t\tif (!atomic_long_read(&n->nr_slabs))\n\t\t\tcontinue;\n\n\t\tspin_lock_irqsave(&n->list_lock, flags);\n\t\tlist_for_each_entry(page, &n->partial, lru)\n\t\t\tprocess_slab(&t, s, page, alloc);\n\t\tlist_for_each_entry(page, &n->full, lru)\n\t\t\tprocess_slab(&t, s, page, alloc);\n\t\tspin_unlock_irqrestore(&n->list_lock, flags);\n\t}\n\n\tfor (i = 0; i < t.count; i++) {\n\t\tstruct location *l = &t.loc[i];\n\n\t\tif (len > PAGE_SIZE - 100)\n\t\t\tbreak;\n\t\tlen += sprintf(buf + len, \"%7ld \", l->count);\n\n\t\tif (l->addr)\n\t\t\tlen += sprint_symbol(buf + len, (unsigned long)l->addr);\n\t\telse\n\t\t\tlen += sprintf(buf + len, \"<not-available>\");\n\n\t\tif (l->sum_time != l->min_time) {\n\t\t\tunsigned long remainder;\n\n\t\t\tlen += sprintf(buf + len, \" age=%ld/%ld/%ld\",\n\t\t\tl->min_time,\n\t\t\tdiv_long_long_rem(l->sum_time, l->count, &remainder),\n\t\t\tl->max_time);\n\t\t} else\n\t\t\tlen += sprintf(buf + len, \" age=%ld\",\n\t\t\t\tl->min_time);\n\n\t\tif (l->min_pid != l->max_pid)\n\t\t\tlen += sprintf(buf + len, \" pid=%ld-%ld\",\n\t\t\t\tl->min_pid, l->max_pid);\n\t\telse\n\t\t\tlen += sprintf(buf + len, \" pid=%ld\",\n\t\t\t\tl->min_pid);\n\n\t\tif (num_online_cpus() > 1 && !cpus_empty(l->cpus) &&\n\t\t\t\tlen < PAGE_SIZE - 60) {\n\t\t\tlen += sprintf(buf + len, \" cpus=\");\n\t\t\tlen += cpulist_scnprintf(buf + len, PAGE_SIZE - len - 50,\n\t\t\t\t\tl->cpus);\n\t\t}\n\n\t\tif (num_online_nodes() > 1 && !nodes_empty(l->nodes) &&\n\t\t\t\tlen < PAGE_SIZE - 60) {\n\t\t\tlen += sprintf(buf + len, \" nodes=\");\n\t\t\tlen += nodelist_scnprintf(buf + len, PAGE_SIZE - len - 50,\n\t\t\t\t\tl->nodes);\n\t\t}\n\n\t\tlen += sprintf(buf + len, \"\\n\");\n\t}\n\n\tfree_loc_track(&t);\n\tif (!t.count)\n\t\tlen += sprintf(buf, \"No data\\n\");\n\treturn len;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Var-Declaration",
        "line_old": 237,
        "critical_vars": [
            "rem"
        ],
        "function": "notify_cmos_timer",
        "filename": "linux/CVE-2011-3209/CVE-2011-3209_CWE-189_f8bd2258e2d520dff28c855658bd24bdafb5102d_ntp.c.diff",
        "label": "False",
        "function_code": "\nstatic void notify_cmos_timer(void)\n{\n\tif (!no_sync_cmos_clock)\n\t\tmod_timer(&sync_cmos_timer, jiffies + 1);\n}"
    },
    {
        "patch_model": "Replace",
        "change_type": "Assignment",
        "line_old": 348,
        "line_new": 348,
        "critical_vars": [
            "time_offset"
        ],
        "function": "do_adjtimex",
        "filename": "linux/CVE-2011-3209/CVE-2011-3209_CWE-189_f8bd2258e2d520dff28c855658bd24bdafb5102d_ntp.c.diff",
        "label": "True",
        "function_code": "int do_adjtimex(struct timex *txc)\n{\n\tlong mtemp, save_adjust;\n\ts64 freq_adj;\n\tint result;\n\n\t/* In order to modify anything, you gotta be super-user! */\n\tif (txc->modes && !capable(CAP_SYS_TIME))\n\t\treturn -EPERM;\n\n\t/* Now we validate the data before disabling interrupts */\n\n\tif ((txc->modes & ADJ_OFFSET_SINGLESHOT) == ADJ_OFFSET_SINGLESHOT) {\n\t  /* singleshot must not be used with any other mode bits */\n\t\tif (txc->modes != ADJ_OFFSET_SINGLESHOT &&\n\t\t\t\t\ttxc->modes != ADJ_OFFSET_SS_READ)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (txc->modes != ADJ_OFFSET_SINGLESHOT && (txc->modes & ADJ_OFFSET))\n\t  /* adjustment Offset limited to +- .512 seconds */\n\t\tif (txc->offset <= - MAXPHASE || txc->offset >= MAXPHASE )\n\t\t\treturn -EINVAL;\n\n\t/* if the quartz is off by more than 10% something is VERY wrong ! */\n\tif (txc->modes & ADJ_TICK)\n\t\tif (txc->tick <  900000/USER_HZ ||\n\t\t    txc->tick > 1100000/USER_HZ)\n\t\t\treturn -EINVAL;\n\n\twrite_seqlock_irq(&xtime_lock);\n\tresult = time_state;\t/* mostly `TIME_OK' */\n\n\t/* Save for later - semantics of adjtime is to return old value */\n\tsave_adjust = time_adjust;\n\n#if 0\t/* STA_CLOCKERR is never set yet */\n\ttime_status &= ~STA_CLOCKERR;\t\t/* reset STA_CLOCKERR */\n#endif\n\t/* If there are input parameters, then process them */\n\tif (txc->modes)\n\t{\n\t    if (txc->modes & ADJ_STATUS)\t/* only set allowed bits */\n\t\ttime_status =  (txc->status & ~STA_RONLY) |\n\t\t\t      (time_status & STA_RONLY);\n\n\t    if (txc->modes & ADJ_FREQUENCY) {\t/* p. 22 */\n\t\tif (txc->freq > MAXFREQ || txc->freq < -MAXFREQ) {\n\t\t    result = -EINVAL;\n\t\t    goto leave;\n\t\t}\n\t\ttime_freq = ((s64)txc->freq * NSEC_PER_USEC)\n\t\t\t\t>> (SHIFT_USEC - SHIFT_NSEC);\n\t    }\n\n\t    if (txc->modes & ADJ_MAXERROR) {\n\t\tif (txc->maxerror < 0 || txc->maxerror >= NTP_PHASE_LIMIT) {\n\t\t    result = -EINVAL;\n\t\t    goto leave;\n\t\t}\n\t\ttime_maxerror = txc->maxerror;\n\t    }\n\n\t    if (txc->modes & ADJ_ESTERROR) {\n\t\tif (txc->esterror < 0 || txc->esterror >= NTP_PHASE_LIMIT) {\n\t\t    result = -EINVAL;\n\t\t    goto leave;\n\t\t}\n\t\ttime_ester\n... (function end not found)"
    },
    {
        "patch_model": "Replace",
        "change_type": "Var-Declaration",
        "line_old": 107,
        "line_new": 108,
        "critical_vars": [
            "rem"
        ],
        "function": "jiffies_to_compat_timeval",
        "filename": "linux/CVE-2011-3209/CVE-2011-3209_CWE-189_f8bd2258e2d520dff28c855658bd24bdafb5102d_binfmt_elfo32.c.diff",
        "label": "True",
        "function_code": "static inline void\njiffies_to_compat_timeval(unsigned long jiffies, struct compat_timeval *value)\n{\n\t/*\n\t * Convert jiffies to nanoseconds and separate with\n\t * one divide.\n\t */\n\tu64 nsec = (u64)jiffies * TICK_NSEC;\n\tu32 rem;\n\tvalue->tv_sec = div_u64_rem(nsec, NSEC_PER_SEC, &rem);\n\tvalue->tv_usec = rem / NSEC_PER_USEC;\n}"
    },
    {
        "patch_model": "Replace",
        "change_type": "Assignment",
        "line_old": 108,
        "line_new": 109,
        "critical_vars": [
            "value->tv_sec"
        ],
        "function": "jiffies_to_compat_timeval",
        "filename": "linux/CVE-2011-3209/CVE-2011-3209_CWE-189_f8bd2258e2d520dff28c855658bd24bdafb5102d_binfmt_elfo32.c.diff",
        "label": "True",
        "function_code": "static inline void\njiffies_to_compat_timeval(unsigned long jiffies, struct compat_timeval *value)\n{\n\t/*\n\t * Convert jiffies to nanoseconds and separate with\n\t * one divide.\n\t */\n\tu64 nsec = (u64)jiffies * TICK_NSEC;\n\tu32 rem;\n\tvalue->tv_sec = div_u64_rem(nsec, NSEC_PER_SEC, &rem);\n\tvalue->tv_usec = rem / NSEC_PER_USEC;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 475,
        "critical_vars": [
            "tp->tv_sec"
        ],
        "function": "sgi_clock_get",
        "filename": "linux/CVE-2011-3209/CVE-2011-3209_CWE-189_f8bd2258e2d520dff28c855658bd24bdafb5102d_mmtimer.c.diff",
        "label": "False",
        "function_code": "\nstatic int sgi_clock_get(clockid_t clockid, struct timespec *tp)\n{\n\tu64 nsec;\n\n\tnsec = rtc_time() * sgi_clock_period\n\t\t\t+ sgi_clock_offset.tv_nsec;\n\ttp->tv_sec = div_long_long_rem(nsec, NSEC_PER_SEC, &tp->tv_nsec)\n\t\t\t+ sgi_clock_offset.tv_sec;\n\treturn 0;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 477,
        "critical_vars": [
            "*tp"
        ],
        "function": "sgi_clock_get",
        "filename": "linux/CVE-2011-3209/CVE-2011-3209_CWE-189_f8bd2258e2d520dff28c855658bd24bdafb5102d_mmtimer.c.diff",
        "label": "True",
        "function_code": "\nstatic int sgi_clock_get(clockid_t clockid, struct timespec *tp)\n{\n\tu64 nsec;\n\n\tnsec = rtc_time() * sgi_clock_period\n\t\t\t+ sgi_clock_offset.tv_nsec;\n\t*tp = ns_to_timespec(nsec);\n\ttp->tv_sec += sgi_clock_offset.tv_sec;\n\treturn 0;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 478,
        "critical_vars": [
            "tp->tv_sec"
        ],
        "function": "sgi_clock_get",
        "filename": "linux/CVE-2011-3209/CVE-2011-3209_CWE-189_f8bd2258e2d520dff28c855658bd24bdafb5102d_mmtimer.c.diff",
        "label": "True",
        "function_code": "\nstatic int sgi_clock_get(clockid_t clockid, struct timespec *tp)\n{\n\tu64 nsec;\n\n\tnsec = rtc_time() * sgi_clock_period\n\t\t\t+ sgi_clock_offset.tv_nsec;\n\t*tp = ns_to_timespec(nsec);\n\ttp->tv_sec += sgi_clock_offset.tv_sec;\n\treturn 0;\n}"
    },
    {
        "patch_model": "Replace",
        "change_type": "Var-Declaration",
        "line_old": 484,
        "line_new": 486,
        "critical_vars": [
            "rem"
        ],
        "function": "sgi_clock_set",
        "filename": "linux/CVE-2011-3209/CVE-2011-3209_CWE-189_f8bd2258e2d520dff28c855658bd24bdafb5102d_mmtimer.c.diff",
        "label": "True",
        "function_code": "\nstatic int sgi_clock_set(clockid_t clockid, struct timespec *tp)\n{\n\n\tu64 nsec;\n\tu32 rem;\n\n\tnsec = rtc_time() * sgi_clock_period;\n\n\tsgi_clock_offset.tv_sec = tp->tv_sec - div_u64_rem(nsec, NSEC_PER_SEC, &rem);\n\n\tif (rem <= tp->tv_nsec)\n\t\tsgi_clock_offset.tv_nsec = tp->tv_sec - rem;\n\telse {\n\t\tsgi_clock_offset.tv_nsec = tp->tv_sec + NSEC_PER_SEC - rem;\n\t\tsgi_clock_offset.tv_sec--;\n\t}\n\treturn 0;\n}"
    },
    {
        "patch_model": "Replace",
        "change_type": "Assignment",
        "line_old": 488,
        "line_new": 490,
        "critical_vars": [
            "sgi_clock_offset.tv_sec"
        ],
        "function": "sgi_clock_set",
        "filename": "linux/CVE-2011-3209/CVE-2011-3209_CWE-189_f8bd2258e2d520dff28c855658bd24bdafb5102d_mmtimer.c.diff",
        "label": "True",
        "function_code": "\nstatic int sgi_clock_set(clockid_t clockid, struct timespec *tp)\n{\n\n\tu64 nsec;\n\tu32 rem;\n\n\tnsec = rtc_time() * sgi_clock_period;\n\n\tsgi_clock_offset.tv_sec = tp->tv_sec - div_u64_rem(nsec, NSEC_PER_SEC, &rem);\n\n\tif (rem <= tp->tv_nsec)\n\t\tsgi_clock_offset.tv_nsec = tp->tv_sec - rem;\n\telse {\n\t\tsgi_clock_offset.tv_nsec = tp->tv_sec + NSEC_PER_SEC - rem;\n\t\tsgi_clock_offset.tv_sec--;\n\t}\n\treturn 0;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Fun-Call",
        "line_old": 662,
        "critical_vars": [
            "cur_setting->it_interval",
            "timr->it.mmtimer.incr",
            "sgi_clock_period"
        ],
        "function": "sgi_timer_get",
        "filename": "linux/CVE-2011-3209/CVE-2011-3209_CWE-189_f8bd2258e2d520dff28c855658bd24bdafb5102d_mmtimer.c.diff",
        "label": "False",
        "function_code": "static void sgi_timer_get(struct k_itimer *timr, struct itimerspec *cur_setting)\n{\n\n\tif (timr->it.mmtimer.clock == TIMER_OFF) {\n\t\tcur_setting->it_interval.tv_nsec = 0;\n\t\tcur_setting->it_interval.tv_sec = 0;\n\t\tcur_setting->it_value.tv_nsec = 0;\n\t\tcur_setting->it_value.tv_sec =0;\n\t\treturn;\n\t}\n\n\tns_to_timespec(cur_setting->it_interval, timr->it.mmtimer.incr * sgi_clock_period);\n\tns_to_timespec(cur_setting->it_value, (timr->it.mmtimer.expires - rtc_time())* sgi_clock_period);\n\treturn;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 661,
        "critical_vars": [
            "cur_setting->it_interval"
        ],
        "function": "sgi_timer_get",
        "filename": "linux/CVE-2011-3209/CVE-2011-3209_CWE-189_f8bd2258e2d520dff28c855658bd24bdafb5102d_mmtimer.c.diff",
        "label": "True",
        "function_code": "static void sgi_timer_get(struct k_itimer *timr, struct itimerspec *cur_setting)\n{\n\n\tif (timr->it.mmtimer.clock == TIMER_OFF) {\n\t\tcur_setting->it_interval.tv_nsec = 0;\n\t\tcur_setting->it_interval.tv_sec = 0;\n\t\tcur_setting->it_value.tv_nsec = 0;\n\t\tcur_setting->it_value.tv_sec =0;\n\t\treturn;\n\t}\n\n\tcur_setting->it_interval = ns_to_timespec(timr->it.mmtimer.incr * sgi_clock_period);\n\tcur_setting->it_value = ns_to_timespec((timr->it.mmtimer.expires - rtc_time()) * sgi_clock_period);\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Fun-Call",
        "line_old": 663,
        "critical_vars": [
            "sgi_clock_period",
            "cur_setting->it_value"
        ],
        "function": "sgi_timer_get",
        "filename": "linux/CVE-2011-3209/CVE-2011-3209_CWE-189_f8bd2258e2d520dff28c855658bd24bdafb5102d_mmtimer.c.diff",
        "label": "False",
        "function_code": "static void sgi_timer_get(struct k_itimer *timr, struct itimerspec *cur_setting)\n{\n\n\tif (timr->it.mmtimer.clock == TIMER_OFF) {\n\t\tcur_setting->it_interval.tv_nsec = 0;\n\t\tcur_setting->it_interval.tv_sec = 0;\n\t\tcur_setting->it_value.tv_nsec = 0;\n\t\tcur_setting->it_value.tv_sec =0;\n\t\treturn;\n\t}\n\n\tns_to_timespec(cur_setting->it_interval, timr->it.mmtimer.incr * sgi_clock_period);\n\tns_to_timespec(cur_setting->it_value, (timr->it.mmtimer.expires - rtc_time())* sgi_clock_period);\n\treturn;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 662,
        "critical_vars": [
            "cur_setting->it_value"
        ],
        "function": "sgi_timer_get",
        "filename": "linux/CVE-2011-3209/CVE-2011-3209_CWE-189_f8bd2258e2d520dff28c855658bd24bdafb5102d_mmtimer.c.diff",
        "label": "True",
        "function_code": "static void sgi_timer_get(struct k_itimer *timr, struct itimerspec *cur_setting)\n{\n\n\tif (timr->it.mmtimer.clock == TIMER_OFF) {\n\t\tcur_setting->it_interval.tv_nsec = 0;\n\t\tcur_setting->it_interval.tv_sec = 0;\n\t\tcur_setting->it_value.tv_nsec = 0;\n\t\tcur_setting->it_value.tv_sec =0;\n\t\treturn;\n\t}\n\n\tcur_setting->it_interval = ns_to_timespec(timr->it.mmtimer.incr * sgi_clock_period);\n\tcur_setting->it_value = ns_to_timespec((timr->it.mmtimer.expires - rtc_time()) * sgi_clock_period);\n}"
    },
    {
        "patch_model": "Replace",
        "change_type": "Assignment",
        "line_old": 682,
        "line_new": 680,
        "critical_vars": [
            "when"
        ],
        "function": "sgi_timer_set",
        "filename": "linux/CVE-2011-3209/CVE-2011-3209_CWE-189_f8bd2258e2d520dff28c855658bd24bdafb5102d_mmtimer.c.diff",
        "label": "True",
        "function_code": "\n\nstatic int sgi_timer_set(struct k_itimer *timr, int flags,\n\tstruct itimerspec * new_setting,\n\tstruct itimerspec * old_setting)\n{\n\tunsigned long when, period, irqflags;\n\tint err = 0;\n\tcnodeid_t nodeid;\n\tstruct mmtimer *base;\n\tstruct rb_node *n;\n\n\tif (old_setting)\n\t\tsgi_timer_get(timr, old_setting);\n\n\tsgi_timer_del(timr);\n\twhen = timespec_to_ns(&new_setting->it_value);\n\tperiod = timespec_to_ns(&new_setting->it_interval);\n\n\tif (when == 0)\n\t\t/* Clear timer */\n\t\treturn 0;\n\n\tbase = kmalloc(sizeof(struct mmtimer), GFP_KERNEL);\n\tif (base == NULL)\n\t\treturn -ENOMEM;\n\n\tif (flags & TIMER_ABSTIME) {\n\t\tstruct timespec n;\n\t\tunsigned long now;\n\n\t\tgetnstimeofday(&n);\n\t\tnow = timespec_to_ns(&n);\n\t\tif (when > now)\n\t\t\twhen -= now;\n\t\telse\n\t\t\t/* Fire the timer immediately */\n\t\t\twhen = 0;\n\t}\n\n\t/*\n\t * Convert to sgi clock period. Need to keep rtc_time() as near as possible\n\t * to getnstimeofday() in order to be as faithful as possible to the time\n\t * specified.\n\t */\n\twhen = (when + sgi_clock_period - 1) / sgi_clock_period + rtc_time();\n\tperiod = (period + sgi_clock_period - 1)  / sgi_clock_period;\n\n\t/*\n\t * We are allocating a local SHub comparator. If we would be moved to another\n\t * cpu then another SHub may be local to us. Prohibit that by switching off\n\t * preemption.\n\t */\n\tpreempt_disable();\n\n\tnodeid =  cpu_to_node(smp_processor_id());\n\n\t/* Lock the node timer structure */\n\tspin_lock_irqsave(&timers[nodeid].lock, irqflags);\n\n\tbase->timer = timr;\n\tbase->cpu = smp_processor_id();\n\n\ttimr->it.mmtimer.clock = TIMER_SET;\n\ttimr->it.mmtimer.node = nodeid;\n\ttimr->it.mmtimer.incr = period;\n\ttimr->it.mmtimer.expires = when;\n\n\tn = timers[nodeid].next;\n\n\t/* Add the new struct mmtimer to node's timer list */\n\tmmtimer_add_list(base);\n\n\tif (timers[nodeid].next == n) {\n\t\t/* No need to reprogram comparator for now */\n\t\tspin_unlock_irqrestore(&timers[nodeid].lock, irqflags);\n\t\tpreempt_enable();\n\t\treturn err;\n\t}\n\n\t/* We need to reprogram the comparator */\n\tif (n)\n\t\tmmtimer_disable_int(cnodeid_to_nasid(n\n... (function end not found)"
    },
    {
        "patch_model": "Replace",
        "change_type": "Assignment",
        "line_old": 683,
        "line_new": 681,
        "critical_vars": [
            "period"
        ],
        "function": "sgi_timer_set",
        "filename": "linux/CVE-2011-3209/CVE-2011-3209_CWE-189_f8bd2258e2d520dff28c855658bd24bdafb5102d_mmtimer.c.diff",
        "label": "True",
        "function_code": "\n\nstatic int sgi_timer_set(struct k_itimer *timr, int flags,\n\tstruct itimerspec * new_setting,\n\tstruct itimerspec * old_setting)\n{\n\tunsigned long when, period, irqflags;\n\tint err = 0;\n\tcnodeid_t nodeid;\n\tstruct mmtimer *base;\n\tstruct rb_node *n;\n\n\tif (old_setting)\n\t\tsgi_timer_get(timr, old_setting);\n\n\tsgi_timer_del(timr);\n\twhen = timespec_to_ns(&new_setting->it_value);\n\tperiod = timespec_to_ns(&new_setting->it_interval);\n\n\tif (when == 0)\n\t\t/* Clear timer */\n\t\treturn 0;\n\n\tbase = kmalloc(sizeof(struct mmtimer), GFP_KERNEL);\n\tif (base == NULL)\n\t\treturn -ENOMEM;\n\n\tif (flags & TIMER_ABSTIME) {\n\t\tstruct timespec n;\n\t\tunsigned long now;\n\n\t\tgetnstimeofday(&n);\n\t\tnow = timespec_to_ns(&n);\n\t\tif (when > now)\n\t\t\twhen -= now;\n\t\telse\n\t\t\t/* Fire the timer immediately */\n\t\t\twhen = 0;\n\t}\n\n\t/*\n\t * Convert to sgi clock period. Need to keep rtc_time() as near as possible\n\t * to getnstimeofday() in order to be as faithful as possible to the time\n\t * specified.\n\t */\n\twhen = (when + sgi_clock_period - 1) / sgi_clock_period + rtc_time();\n\tperiod = (period + sgi_clock_period - 1)  / sgi_clock_period;\n\n\t/*\n\t * We are allocating a local SHub comparator. If we would be moved to another\n\t * cpu then another SHub may be local to us. Prohibit that by switching off\n\t * preemption.\n\t */\n\tpreempt_disable();\n\n\tnodeid =  cpu_to_node(smp_processor_id());\n\n\t/* Lock the node timer structure */\n\tspin_lock_irqsave(&timers[nodeid].lock, irqflags);\n\n\tbase->timer = timr;\n\tbase->cpu = smp_processor_id();\n\n\ttimr->it.mmtimer.clock = TIMER_SET;\n\ttimr->it.mmtimer.node = nodeid;\n\ttimr->it.mmtimer.incr = period;\n\ttimr->it.mmtimer.expires = when;\n\n\tn = timers[nodeid].next;\n\n\t/* Add the new struct mmtimer to node's timer list */\n\tmmtimer_add_list(base);\n\n\tif (timers[nodeid].next == n) {\n\t\t/* No need to reprogram comparator for now */\n\t\tspin_unlock_irqrestore(&timers[nodeid].lock, irqflags);\n\t\tpreempt_enable();\n\t\treturn err;\n\t}\n\n\t/* We need to reprogram the comparator */\n\tif (n)\n\t\tmmtimer_disable_int(cnodeid_to_nasid(n\n... (function end not found)"
    },
    {
        "patch_model": "Replace",
        "change_type": "Assignment",
        "line_old": 698,
        "line_new": 696,
        "critical_vars": [
            "now"
        ],
        "function": "sgi_timer_set",
        "filename": "linux/CVE-2011-3209/CVE-2011-3209_CWE-189_f8bd2258e2d520dff28c855658bd24bdafb5102d_mmtimer.c.diff",
        "label": "True",
        "function_code": "\n\nstatic int sgi_timer_set(struct k_itimer *timr, int flags,\n\tstruct itimerspec * new_setting,\n\tstruct itimerspec * old_setting)\n{\n\tunsigned long when, period, irqflags;\n\tint err = 0;\n\tcnodeid_t nodeid;\n\tstruct mmtimer *base;\n\tstruct rb_node *n;\n\n\tif (old_setting)\n\t\tsgi_timer_get(timr, old_setting);\n\n\tsgi_timer_del(timr);\n\twhen = timespec_to_ns(&new_setting->it_value);\n\tperiod = timespec_to_ns(&new_setting->it_interval);\n\n\tif (when == 0)\n\t\t/* Clear timer */\n\t\treturn 0;\n\n\tbase = kmalloc(sizeof(struct mmtimer), GFP_KERNEL);\n\tif (base == NULL)\n\t\treturn -ENOMEM;\n\n\tif (flags & TIMER_ABSTIME) {\n\t\tstruct timespec n;\n\t\tunsigned long now;\n\n\t\tgetnstimeofday(&n);\n\t\tnow = timespec_to_ns(&n);\n\t\tif (when > now)\n\t\t\twhen -= now;\n\t\telse\n\t\t\t/* Fire the timer immediately */\n\t\t\twhen = 0;\n\t}\n\n\t/*\n\t * Convert to sgi clock period. Need to keep rtc_time() as near as possible\n\t * to getnstimeofday() in order to be as faithful as possible to the time\n\t * specified.\n\t */\n\twhen = (when + sgi_clock_period - 1) / sgi_clock_period + rtc_time();\n\tperiod = (period + sgi_clock_period - 1)  / sgi_clock_period;\n\n\t/*\n\t * We are allocating a local SHub comparator. If we would be moved to another\n\t * cpu then another SHub may be local to us. Prohibit that by switching off\n\t * preemption.\n\t */\n\tpreempt_disable();\n\n\tnodeid =  cpu_to_node(smp_processor_id());\n\n\t/* Lock the node timer structure */\n\tspin_lock_irqsave(&timers[nodeid].lock, irqflags);\n\n\tbase->timer = timr;\n\tbase->cpu = smp_processor_id();\n\n\ttimr->it.mmtimer.clock = TIMER_SET;\n\ttimr->it.mmtimer.node = nodeid;\n\ttimr->it.mmtimer.incr = period;\n\ttimr->it.mmtimer.expires = when;\n\n\tn = timers[nodeid].next;\n\n\t/* Add the new struct mmtimer to node's timer list */\n\tmmtimer_add_list(base);\n\n\tif (timers[nodeid].next == n) {\n\t\t/* No need to reprogram comparator for now */\n\t\tspin_unlock_irqrestore(&timers[nodeid].lock, irqflags);\n\t\tpreempt_enable();\n\t\treturn err;\n\t}\n\n\t/* We need to reprogram the comparator */\n\tif (n)\n\t\tmmtimer_disable_int(cnodeid_to_nasid(n\n... (function end not found)"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 314,
        "critical_vars": [
            "nsops"
        ],
        "function": "sys_oabi_semtimedop",
        "filename": "linux/CVE-2011-1759/CVE-2011-1759_CWE-189_0f22072ab50cac7983f9660d33974b45184da4f9_sys_oabi-compat.c.diff",
        "label": "True",
        "function_code": "\nasmlinkage long sys_oabi_semtimedop(int semid,\n\t\t\t\t    struct oabi_sembuf __user *tsops,\n\t\t\t\t    unsigned nsops,\n\t\t\t\t    const struct timespec __user *timeout)\n{\n\tstruct sembuf *sops;\n\tstruct timespec local_timeout;\n\tlong err;\n\tint i;\n\n\tif (nsops < 1 || nsops > SEMOPM)\n\t\treturn -EINVAL;\n\tsops = kmalloc(sizeof(*sops) * nsops, GFP_KERNEL);\n\tif (!sops)\n\t\treturn -ENOMEM;\n\terr = 0;\n\tfor (i = 0; i < nsops; i++) {\n\t\t__get_user_error(sops[i].sem_num, &tsops->sem_num, err);\n\t\t__get_user_error(sops[i].sem_op,  &tsops->sem_op,  err);\n\t\t__get_user_error(sops[i].sem_flg, &tsops->sem_flg, err);\n\t\ttsops++;\n\t}\n\tif (timeout) {\n\t\t/* copy this as well before changing domain protection */\n\t\terr |= copy_from_user(&local_timeout, timeout, sizeof(*timeout));\n\t\ttimeout = &local_timeout;\n\t}\n\tif (err) {\n\t\terr = -EFAULT;\n\t} else {\n\t\tmm_segment_t fs = get_fs();\n\t\tset_fs(KERNEL_DS);\n\t\terr = sys_semtimedop(semid, sops, nsops, timeout);\n\t\tset_fs(fs);\n\t}\n\tkfree(sops);\n\treturn err;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 916,
        "critical_vars": [
            "pdata"
        ],
        "function": "lbs_debugfs_write",
        "filename": "linux/CVE-2013-6378/CVE-2013-6378_CWE-189_a497e47d4aec37aaf8f13509f3ef3d1f6a717d88_debugfs.c.diff",
        "label": "False",
        "function_code": "static ssize_t lbs_debugfs_write(struct file *f, const char __user *buf,\n\t\t\t    size_t cnt, loff_t *ppos)\n{\n\tint r, i;\n\tchar *pdata;\n\tchar *p;\n\tchar *p0;\n\tchar *p1;\n\tchar *p2;\n\tstruct debug_data *d = f->private_data;\n\n\tpdata = kmalloc(cnt, GFP_KERNEL);\n\tif (pdata == NULL)\n\t\treturn 0;\n\n\tif (copy_from_user(pdata, buf, cnt)) {\n\t\tlbs_deb_debugfs(\"Copy from user failed\\n\");\n\t\tkfree(pdata);\n\t\treturn 0;\n\t}\n\n\tp0 = pdata;\n\tfor (i = 0; i < num_of_items; i++) {\n\t\tdo {\n\t\t\tp = strstr(p0, d[i].name);\n\t\t\tif (p == NULL)\n\t\t\t\tbreak;\n\t\t\tp1 = strchr(p, '\\n');\n\t\t\tif (p1 == NULL)\n\t\t\t\tbreak;\n\t\t\tp0 = p1++;\n\t\t\tp2 = strchr(p, '=');\n\t\t\tif (!p2)\n\t\t\t\tbreak;\n\t\t\tp2++;\n\t\t\tr = simple_strtoul(p2, NULL, 0);\n\t\t\tif (d[i].size == 1)\n\t\t\t\t*((u8 *) d[i].addr) = (u8) r;\n\t\t\telse if (d[i].size == 2)\n\t\t\t\t*((u16 *) d[i].addr) = (u16) r;\n\t\t\telse if (d[i].size == 4)\n\t\t\t\t*((u32 *) d[i].addr) = (u32) r;\n\t\t\telse if (d[i].size == 8)\n\t\t\t\t*((u64 *) d[i].addr) = (u64) r;\n\t\t\tbreak;\n\t\t} while (1);\n\t}\n\tkfree(pdata);\n\n\treturn (ssize_t)cnt;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 916,
        "critical_vars": [
            "cnt"
        ],
        "function": "lbs_debugfs_write",
        "filename": "linux/CVE-2013-6378/CVE-2013-6378_CWE-189_a497e47d4aec37aaf8f13509f3ef3d1f6a717d88_debugfs.c.diff",
        "label": "True",
        "function_code": "static ssize_t lbs_debugfs_write(struct file *f, const char __user *buf,\n\t\t\t    size_t cnt, loff_t *ppos)\n{\n\tint r, i;\n\tchar *pdata;\n\tchar *p;\n\tchar *p0;\n\tchar *p1;\n\tchar *p2;\n\tstruct debug_data *d = f->private_data;\n\n\tif (cnt == 0)\n\t\treturn 0;\n\n\tpdata = kmalloc(cnt + 1, GFP_KERNEL);\n\tif (pdata == NULL)\n\t\treturn 0;\n\n\tif (copy_from_user(pdata, buf, cnt)) {\n\t\tlbs_deb_debugfs(\"Copy from user failed\\n\");\n\t\tkfree(pdata);\n\t\treturn 0;\n\t}\n\tpdata[cnt] = '\\0';\n\n\tp0 = pdata;\n\tfor (i = 0; i < num_of_items; i++) {\n\t\tdo {\n\t\t\tp = strstr(p0, d[i].name);\n\t\t\tif (p == NULL)\n\t\t\t\tbreak;\n\t\t\tp1 = strchr(p, '\\n');\n\t\t\tif (p1 == NULL)\n\t\t\t\tbreak;\n\t\t\tp0 = p1++;\n\t\t\tp2 = strchr(p, '=');\n\t\t\tif (!p2)\n\t\t\t\tbreak;\n\t\t\tp2++;\n\t\t\tr = simple_strtoul(p2, NULL, 0);\n\t\t\tif (d[i].size == 1)\n\t\t\t\t*((u8 *) d[i].addr) = (u8) r;\n\t\t\telse if (d[i].size == 2)\n\t\t\t\t*((u16 *) d[i].addr) = (u16) r;\n\t\t\telse if (d[i].size == 4)\n\t\t\t\t*((u32 *) d[i].addr) = (u32) r;\n\t\t\telse if (d[i].size == 8)\n\t\t\t\t*((u64 *) d[i].addr) = (u64) r;\n\t\t\tbreak;\n\t\t} while (1);\n\t}\n\tkfree(pdata);\n\n\treturn (ssize_t)cnt;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 919,
        "critical_vars": [
            "pdata"
        ],
        "function": "lbs_debugfs_write",
        "filename": "linux/CVE-2013-6378/CVE-2013-6378_CWE-189_a497e47d4aec37aaf8f13509f3ef3d1f6a717d88_debugfs.c.diff",
        "label": "True",
        "function_code": "static ssize_t lbs_debugfs_write(struct file *f, const char __user *buf,\n\t\t\t    size_t cnt, loff_t *ppos)\n{\n\tint r, i;\n\tchar *pdata;\n\tchar *p;\n\tchar *p0;\n\tchar *p1;\n\tchar *p2;\n\tstruct debug_data *d = f->private_data;\n\n\tif (cnt == 0)\n\t\treturn 0;\n\n\tpdata = kmalloc(cnt + 1, GFP_KERNEL);\n\tif (pdata == NULL)\n\t\treturn 0;\n\n\tif (copy_from_user(pdata, buf, cnt)) {\n\t\tlbs_deb_debugfs(\"Copy from user failed\\n\");\n\t\tkfree(pdata);\n\t\treturn 0;\n\t}\n\tpdata[cnt] = '\\0';\n\n\tp0 = pdata;\n\tfor (i = 0; i < num_of_items; i++) {\n\t\tdo {\n\t\t\tp = strstr(p0, d[i].name);\n\t\t\tif (p == NULL)\n\t\t\t\tbreak;\n\t\t\tp1 = strchr(p, '\\n');\n\t\t\tif (p1 == NULL)\n\t\t\t\tbreak;\n\t\t\tp0 = p1++;\n\t\t\tp2 = strchr(p, '=');\n\t\t\tif (!p2)\n\t\t\t\tbreak;\n\t\t\tp2++;\n\t\t\tr = simple_strtoul(p2, NULL, 0);\n\t\t\tif (d[i].size == 1)\n\t\t\t\t*((u8 *) d[i].addr) = (u8) r;\n\t\t\telse if (d[i].size == 2)\n\t\t\t\t*((u16 *) d[i].addr) = (u16) r;\n\t\t\telse if (d[i].size == 4)\n\t\t\t\t*((u32 *) d[i].addr) = (u32) r;\n\t\t\telse if (d[i].size == 8)\n\t\t\t\t*((u64 *) d[i].addr) = (u64) r;\n\t\t\tbreak;\n\t\t} while (1);\n\t}\n\tkfree(pdata);\n\n\treturn (ssize_t)cnt;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 928,
        "critical_vars": [
            "pdata[cnt]"
        ],
        "function": "lbs_debugfs_write",
        "filename": "linux/CVE-2013-6378/CVE-2013-6378_CWE-189_a497e47d4aec37aaf8f13509f3ef3d1f6a717d88_debugfs.c.diff",
        "label": "True",
        "function_code": "static ssize_t lbs_debugfs_write(struct file *f, const char __user *buf,\n\t\t\t    size_t cnt, loff_t *ppos)\n{\n\tint r, i;\n\tchar *pdata;\n\tchar *p;\n\tchar *p0;\n\tchar *p1;\n\tchar *p2;\n\tstruct debug_data *d = f->private_data;\n\n\tif (cnt == 0)\n\t\treturn 0;\n\n\tpdata = kmalloc(cnt + 1, GFP_KERNEL);\n\tif (pdata == NULL)\n\t\treturn 0;\n\n\tif (copy_from_user(pdata, buf, cnt)) {\n\t\tlbs_deb_debugfs(\"Copy from user failed\\n\");\n\t\tkfree(pdata);\n\t\treturn 0;\n\t}\n\tpdata[cnt] = '\\0';\n\n\tp0 = pdata;\n\tfor (i = 0; i < num_of_items; i++) {\n\t\tdo {\n\t\t\tp = strstr(p0, d[i].name);\n\t\t\tif (p == NULL)\n\t\t\t\tbreak;\n\t\t\tp1 = strchr(p, '\\n');\n\t\t\tif (p1 == NULL)\n\t\t\t\tbreak;\n\t\t\tp0 = p1++;\n\t\t\tp2 = strchr(p, '=');\n\t\t\tif (!p2)\n\t\t\t\tbreak;\n\t\t\tp2++;\n\t\t\tr = simple_strtoul(p2, NULL, 0);\n\t\t\tif (d[i].size == 1)\n\t\t\t\t*((u8 *) d[i].addr) = (u8) r;\n\t\t\telse if (d[i].size == 2)\n\t\t\t\t*((u16 *) d[i].addr) = (u16) r;\n\t\t\telse if (d[i].size == 4)\n\t\t\t\t*((u32 *) d[i].addr) = (u32) r;\n\t\t\telse if (d[i].size == 8)\n\t\t\t\t*((u64 *) d[i].addr) = (u64) r;\n\t\t\tbreak;\n\t\t} while (1);\n\t}\n\tkfree(pdata);\n\n\treturn (ssize_t)cnt;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 713,
        "critical_vars": [
            "dst_state->speculative"
        ],
        "function": "copy_verifier_state",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic int copy_verifier_state(struct bpf_verifier_state *dst_state,\n\t\t\t       const struct bpf_verifier_state *src)\n{\n\tstruct bpf_func_state *dst;\n\tint i, err;\n\n\t/* if dst has more stack frames then src frame, free them */\n\tfor (i = src->curframe + 1; i <= dst_state->curframe; i++) {\n\t\tfree_func_state(dst_state->frame[i]);\n\t\tdst_state->frame[i] = NULL;\n\t}\n\tdst_state->speculative = src->speculative;\n\tdst_state->curframe = src->curframe;\n\tfor (i = 0; i <= src->curframe; i++) {\n\t\tdst = dst_state->frame[i];\n\t\tif (!dst) {\n\t\t\tdst = kzalloc(sizeof(*dst), GFP_KERNEL);\n\t\t\tif (!dst)\n\t\t\t\treturn -ENOMEM;\n\t\t\tdst_state->frame[i] = dst;\n\t\t}\n\t\terr = copy_func_state(dst, src->frame[i]);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\treturn 0;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Var-Declaration",
        "line_new": 758,
        "critical_vars": [
            "speculative"
        ],
        "function": "pop_stack",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic int pop_stack(struct bpf_verifier_env *env, int *prev_insn_idx,\n\t\t     int *insn_idx)\n{\n\tstruct bpf_verifier_state *cur = env->cur_state;\n\tstruct bpf_verifier_stack_elem *elem, *head = env->head;\n\tint err;\n\n\tif (env->head == NULL)\n\t\treturn -ENOENT;\n\n\tif (cur) {\n\t\terr = copy_verifier_state(cur, &head->st);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\tif (insn_idx)\n\t\t*insn_idx = head->insn_idx;\n\tif (prev_insn_idx)\n\t\t*prev_insn_idx = head->prev_insn_idx;\n\telem = head->next;\n\tfree_verifier_state(&head->st, false);\n\tkfree(head);\n\tenv->head = elem;\n\tenv->stack_size--;\n\treturn 0;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 777,
        "critical_vars": [
            "elem->st.speculative"
        ],
        "function": "*push_stack",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic struct bpf_verifier_state *push_stack(struct bpf_verifier_env *env,\n\t\t\t\t\t     int insn_idx, int prev_insn_idx,\n\t\t\t\t\t     bool speculative)\n{\n\tstruct bpf_verifier_state *cur = env->cur_state;\n\tstruct bpf_verifier_stack_elem *elem;\n\tint err;\n\n\telem = kzalloc(sizeof(struct bpf_verifier_stack_elem), GFP_KERNEL);\n\tif (!elem)\n\t\tgoto err;\n\n\telem->insn_idx = insn_idx;\n\telem->prev_insn_idx = prev_insn_idx;\n\telem->next = env->head;\n\tenv->head = elem;\n\tenv->stack_size++;\n\terr = copy_verifier_state(&elem->st, cur);\n\tif (err)\n\t\tgoto err;\n\telem->st.speculative |= speculative;\n\tif (env->stack_size > BPF_COMPLEXITY_LIMIT_STACK) {\n\t\tverbose(env, \"BPF program is too complex\\n\");\n\t\tgoto err;\n\t}\n\treturn &elem->st;\nerr:\n\tfree_verifier_state(env->cur_state, true);\n\tenv->cur_state = NULL;\n\t/* pop all elements and return */\n\twhile (!pop_stack(env, NULL, NULL));\n\treturn NULL;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Head",
        "line_new": 3073,
        "critical_vars": [
            "*env"
        ],
        "function": "check_reg_sane_offset",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic bool check_reg_sane_offset(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  enum bpf_reg_type type)\n{\n\tbool known = tnum_is_const(reg->var_off);\n\ts64 val = reg->var_off.value;\n\ts64 smin = reg->smin_value;\n\n\tif (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {\n\t\tverbose(env, \"math between %s pointer and %lld is not allowed\\n\",\n\t\t\treg_type_str[type], val);\n\t\treturn false;\n\t}\n\n\tif (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"%s pointer offset %d is not allowed\\n\",\n\t\t\treg_type_str[type], reg->off);\n\t\treturn false;\n\t}\n\n\tif (smin == S64_MIN) {\n\t\tverbose(env, \"math between %s pointer and register with unbounded min value is not allowed\\n\",\n\t\t\treg_type_str[type]);\n\t\treturn false;\n\t}\n\n\tif (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"value %lld makes %s pointer be out of bounds\\n\",\n\t\t\tsmin, reg_type_str[type]);\n\t\treturn false;\n\t}\n\n\treturn true;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Head",
        "line_new": 3078,
        "critical_vars": [
            "*ptr_reg",
            "off_is_neg",
            "*ptr_limit",
            "opcode"
        ],
        "function": "check_reg_sane_offset",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic bool check_reg_sane_offset(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  enum bpf_reg_type type)\n{\n\tbool known = tnum_is_const(reg->var_off);\n\ts64 val = reg->var_off.value;\n\ts64 smin = reg->smin_value;\n\n\tif (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {\n\t\tverbose(env, \"math between %s pointer and %lld is not allowed\\n\",\n\t\t\treg_type_str[type], val);\n\t\treturn false;\n\t}\n\n\tif (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"%s pointer offset %d is not allowed\\n\",\n\t\t\treg_type_str[type], reg->off);\n\t\treturn false;\n\t}\n\n\tif (smin == S64_MIN) {\n\t\tverbose(env, \"math between %s pointer and register with unbounded min value is not allowed\\n\",\n\t\t\treg_type_str[type]);\n\t\treturn false;\n\t}\n\n\tif (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"value %lld makes %s pointer be out of bounds\\n\",\n\t\t\tsmin, reg_type_str[type]);\n\t\treturn false;\n\t}\n\n\treturn true;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 3081,
        "critical_vars": [
            "mask_to_left"
        ],
        "function": "check_reg_sane_offset",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic bool check_reg_sane_offset(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  enum bpf_reg_type type)\n{\n\tbool known = tnum_is_const(reg->var_off);\n\ts64 val = reg->var_off.value;\n\ts64 smin = reg->smin_value;\n\n\tif (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {\n\t\tverbose(env, \"math between %s pointer and %lld is not allowed\\n\",\n\t\t\treg_type_str[type], val);\n\t\treturn false;\n\t}\n\n\tif (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"%s pointer offset %d is not allowed\\n\",\n\t\t\treg_type_str[type], reg->off);\n\t\treturn false;\n\t}\n\n\tif (smin == S64_MIN) {\n\t\tverbose(env, \"math between %s pointer and register with unbounded min value is not allowed\\n\",\n\t\t\treg_type_str[type]);\n\t\treturn false;\n\t}\n\n\tif (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"value %lld makes %s pointer be out of bounds\\n\",\n\t\t\tsmin, reg_type_str[type]);\n\t\treturn false;\n\t}\n\n\treturn true;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Var-Declaration",
        "line_new": 3083,
        "critical_vars": [
            "off"
        ],
        "function": "check_reg_sane_offset",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic bool check_reg_sane_offset(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  enum bpf_reg_type type)\n{\n\tbool known = tnum_is_const(reg->var_off);\n\ts64 val = reg->var_off.value;\n\ts64 smin = reg->smin_value;\n\n\tif (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {\n\t\tverbose(env, \"math between %s pointer and %lld is not allowed\\n\",\n\t\t\treg_type_str[type], val);\n\t\treturn false;\n\t}\n\n\tif (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"%s pointer offset %d is not allowed\\n\",\n\t\t\treg_type_str[type], reg->off);\n\t\treturn false;\n\t}\n\n\tif (smin == S64_MIN) {\n\t\tverbose(env, \"math between %s pointer and register with unbounded min value is not allowed\\n\",\n\t\t\treg_type_str[type]);\n\t\treturn false;\n\t}\n\n\tif (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"value %lld makes %s pointer be out of bounds\\n\",\n\t\t\tsmin, reg_type_str[type]);\n\t\treturn false;\n\t}\n\n\treturn true;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 3085,
        "critical_vars": [
            "ptr_reg->type"
        ],
        "function": "check_reg_sane_offset",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic bool check_reg_sane_offset(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  enum bpf_reg_type type)\n{\n\tbool known = tnum_is_const(reg->var_off);\n\ts64 val = reg->var_off.value;\n\ts64 smin = reg->smin_value;\n\n\tif (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {\n\t\tverbose(env, \"math between %s pointer and %lld is not allowed\\n\",\n\t\t\treg_type_str[type], val);\n\t\treturn false;\n\t}\n\n\tif (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"%s pointer offset %d is not allowed\\n\",\n\t\t\treg_type_str[type], reg->off);\n\t\treturn false;\n\t}\n\n\tif (smin == S64_MIN) {\n\t\tverbose(env, \"math between %s pointer and register with unbounded min value is not allowed\\n\",\n\t\t\treg_type_str[type]);\n\t\treturn false;\n\t}\n\n\tif (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"value %lld makes %s pointer be out of bounds\\n\",\n\t\t\tsmin, reg_type_str[type]);\n\t\treturn false;\n\t}\n\n\treturn true;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 3088,
        "critical_vars": [
            "mask_to_left"
        ],
        "function": "check_reg_sane_offset",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic bool check_reg_sane_offset(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  enum bpf_reg_type type)\n{\n\tbool known = tnum_is_const(reg->var_off);\n\ts64 val = reg->var_off.value;\n\ts64 smin = reg->smin_value;\n\n\tif (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {\n\t\tverbose(env, \"math between %s pointer and %lld is not allowed\\n\",\n\t\t\treg_type_str[type], val);\n\t\treturn false;\n\t}\n\n\tif (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"%s pointer offset %d is not allowed\\n\",\n\t\t\treg_type_str[type], reg->off);\n\t\treturn false;\n\t}\n\n\tif (smin == S64_MIN) {\n\t\tverbose(env, \"math between %s pointer and register with unbounded min value is not allowed\\n\",\n\t\t\treg_type_str[type]);\n\t\treturn false;\n\t}\n\n\tif (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"value %lld makes %s pointer be out of bounds\\n\",\n\t\t\tsmin, reg_type_str[type]);\n\t\treturn false;\n\t}\n\n\treturn true;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 3089,
        "critical_vars": [
            "*ptr_limit"
        ],
        "function": "check_reg_sane_offset",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic bool check_reg_sane_offset(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  enum bpf_reg_type type)\n{\n\tbool known = tnum_is_const(reg->var_off);\n\ts64 val = reg->var_off.value;\n\ts64 smin = reg->smin_value;\n\n\tif (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {\n\t\tverbose(env, \"math between %s pointer and %lld is not allowed\\n\",\n\t\t\treg_type_str[type], val);\n\t\treturn false;\n\t}\n\n\tif (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"%s pointer offset %d is not allowed\\n\",\n\t\t\treg_type_str[type], reg->off);\n\t\treturn false;\n\t}\n\n\tif (smin == S64_MIN) {\n\t\tverbose(env, \"math between %s pointer and register with unbounded min value is not allowed\\n\",\n\t\t\treg_type_str[type]);\n\t\treturn false;\n\t}\n\n\tif (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"value %lld makes %s pointer be out of bounds\\n\",\n\t\t\tsmin, reg_type_str[type]);\n\t\treturn false;\n\t}\n\n\treturn true;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 3091,
        "critical_vars": [
            "*ptr_limit"
        ],
        "function": "check_reg_sane_offset",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic bool check_reg_sane_offset(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  enum bpf_reg_type type)\n{\n\tbool known = tnum_is_const(reg->var_off);\n\ts64 val = reg->var_off.value;\n\ts64 smin = reg->smin_value;\n\n\tif (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {\n\t\tverbose(env, \"math between %s pointer and %lld is not allowed\\n\",\n\t\t\treg_type_str[type], val);\n\t\treturn false;\n\t}\n\n\tif (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"%s pointer offset %d is not allowed\\n\",\n\t\t\treg_type_str[type], reg->off);\n\t\treturn false;\n\t}\n\n\tif (smin == S64_MIN) {\n\t\tverbose(env, \"math between %s pointer and register with unbounded min value is not allowed\\n\",\n\t\t\treg_type_str[type]);\n\t\treturn false;\n\t}\n\n\tif (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"value %lld makes %s pointer be out of bounds\\n\",\n\t\t\tsmin, reg_type_str[type]);\n\t\treturn false;\n\t}\n\n\treturn true;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 3095,
        "critical_vars": [
            "*ptr_limit"
        ],
        "function": "check_reg_sane_offset",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic bool check_reg_sane_offset(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  enum bpf_reg_type type)\n{\n\tbool known = tnum_is_const(reg->var_off);\n\ts64 val = reg->var_off.value;\n\ts64 smin = reg->smin_value;\n\n\tif (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {\n\t\tverbose(env, \"math between %s pointer and %lld is not allowed\\n\",\n\t\t\treg_type_str[type], val);\n\t\treturn false;\n\t}\n\n\tif (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"%s pointer offset %d is not allowed\\n\",\n\t\t\treg_type_str[type], reg->off);\n\t\treturn false;\n\t}\n\n\tif (smin == S64_MIN) {\n\t\tverbose(env, \"math between %s pointer and register with unbounded min value is not allowed\\n\",\n\t\t\treg_type_str[type]);\n\t\treturn false;\n\t}\n\n\tif (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"value %lld makes %s pointer be out of bounds\\n\",\n\t\t\tsmin, reg_type_str[type]);\n\t\treturn false;\n\t}\n\n\treturn true;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 3097,
        "critical_vars": [
            "off"
        ],
        "function": "check_reg_sane_offset",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic bool check_reg_sane_offset(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  enum bpf_reg_type type)\n{\n\tbool known = tnum_is_const(reg->var_off);\n\ts64 val = reg->var_off.value;\n\ts64 smin = reg->smin_value;\n\n\tif (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {\n\t\tverbose(env, \"math between %s pointer and %lld is not allowed\\n\",\n\t\t\treg_type_str[type], val);\n\t\treturn false;\n\t}\n\n\tif (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"%s pointer offset %d is not allowed\\n\",\n\t\t\treg_type_str[type], reg->off);\n\t\treturn false;\n\t}\n\n\tif (smin == S64_MIN) {\n\t\tverbose(env, \"math between %s pointer and register with unbounded min value is not allowed\\n\",\n\t\t\treg_type_str[type]);\n\t\treturn false;\n\t}\n\n\tif (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"value %lld makes %s pointer be out of bounds\\n\",\n\t\t\tsmin, reg_type_str[type]);\n\t\treturn false;\n\t}\n\n\treturn true;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 3098,
        "critical_vars": [
            "*ptr_limit"
        ],
        "function": "check_reg_sane_offset",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic bool check_reg_sane_offset(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  enum bpf_reg_type type)\n{\n\tbool known = tnum_is_const(reg->var_off);\n\ts64 val = reg->var_off.value;\n\ts64 smin = reg->smin_value;\n\n\tif (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {\n\t\tverbose(env, \"math between %s pointer and %lld is not allowed\\n\",\n\t\t\treg_type_str[type], val);\n\t\treturn false;\n\t}\n\n\tif (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"%s pointer offset %d is not allowed\\n\",\n\t\t\treg_type_str[type], reg->off);\n\t\treturn false;\n\t}\n\n\tif (smin == S64_MIN) {\n\t\tverbose(env, \"math between %s pointer and register with unbounded min value is not allowed\\n\",\n\t\t\treg_type_str[type]);\n\t\treturn false;\n\t}\n\n\tif (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"value %lld makes %s pointer be out of bounds\\n\",\n\t\t\tsmin, reg_type_str[type]);\n\t\treturn false;\n\t}\n\n\treturn true;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Head",
        "line_new": 3106,
        "critical_vars": [
            "*ptr_reg",
            "off_is_neg",
            "*env",
            "*insn",
            "*dst_reg"
        ],
        "function": "check_reg_sane_offset",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic bool check_reg_sane_offset(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  enum bpf_reg_type type)\n{\n\tbool known = tnum_is_const(reg->var_off);\n\ts64 val = reg->var_off.value;\n\ts64 smin = reg->smin_value;\n\n\tif (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {\n\t\tverbose(env, \"math between %s pointer and %lld is not allowed\\n\",\n\t\t\treg_type_str[type], val);\n\t\treturn false;\n\t}\n\n\tif (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"%s pointer offset %d is not allowed\\n\",\n\t\t\treg_type_str[type], reg->off);\n\t\treturn false;\n\t}\n\n\tif (smin == S64_MIN) {\n\t\tverbose(env, \"math between %s pointer and register with unbounded min value is not allowed\\n\",\n\t\t\treg_type_str[type]);\n\t\treturn false;\n\t}\n\n\tif (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"value %lld makes %s pointer be out of bounds\\n\",\n\t\t\tsmin, reg_type_str[type]);\n\t\treturn false;\n\t}\n\n\treturn true;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 3112,
        "critical_vars": [
            "*vstate"
        ],
        "function": "check_reg_sane_offset",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic bool check_reg_sane_offset(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  enum bpf_reg_type type)\n{\n\tbool known = tnum_is_const(reg->var_off);\n\ts64 val = reg->var_off.value;\n\ts64 smin = reg->smin_value;\n\n\tif (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {\n\t\tverbose(env, \"math between %s pointer and %lld is not allowed\\n\",\n\t\t\treg_type_str[type], val);\n\t\treturn false;\n\t}\n\n\tif (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"%s pointer offset %d is not allowed\\n\",\n\t\t\treg_type_str[type], reg->off);\n\t\treturn false;\n\t}\n\n\tif (smin == S64_MIN) {\n\t\tverbose(env, \"math between %s pointer and register with unbounded min value is not allowed\\n\",\n\t\t\treg_type_str[type]);\n\t\treturn false;\n\t}\n\n\tif (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"value %lld makes %s pointer be out of bounds\\n\",\n\t\t\tsmin, reg_type_str[type]);\n\t\treturn false;\n\t}\n\n\treturn true;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 3113,
        "critical_vars": [
            "*aux"
        ],
        "function": "check_reg_sane_offset",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic bool check_reg_sane_offset(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  enum bpf_reg_type type)\n{\n\tbool known = tnum_is_const(reg->var_off);\n\ts64 val = reg->var_off.value;\n\ts64 smin = reg->smin_value;\n\n\tif (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {\n\t\tverbose(env, \"math between %s pointer and %lld is not allowed\\n\",\n\t\t\treg_type_str[type], val);\n\t\treturn false;\n\t}\n\n\tif (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"%s pointer offset %d is not allowed\\n\",\n\t\t\treg_type_str[type], reg->off);\n\t\treturn false;\n\t}\n\n\tif (smin == S64_MIN) {\n\t\tverbose(env, \"math between %s pointer and register with unbounded min value is not allowed\\n\",\n\t\t\treg_type_str[type]);\n\t\treturn false;\n\t}\n\n\tif (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"value %lld makes %s pointer be out of bounds\\n\",\n\t\t\tsmin, reg_type_str[type]);\n\t\treturn false;\n\t}\n\n\treturn true;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 3114,
        "critical_vars": [
            "ptr_is_dst_reg"
        ],
        "function": "check_reg_sane_offset",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic bool check_reg_sane_offset(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  enum bpf_reg_type type)\n{\n\tbool known = tnum_is_const(reg->var_off);\n\ts64 val = reg->var_off.value;\n\ts64 smin = reg->smin_value;\n\n\tif (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {\n\t\tverbose(env, \"math between %s pointer and %lld is not allowed\\n\",\n\t\t\treg_type_str[type], val);\n\t\treturn false;\n\t}\n\n\tif (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"%s pointer offset %d is not allowed\\n\",\n\t\t\treg_type_str[type], reg->off);\n\t\treturn false;\n\t}\n\n\tif (smin == S64_MIN) {\n\t\tverbose(env, \"math between %s pointer and register with unbounded min value is not allowed\\n\",\n\t\t\treg_type_str[type]);\n\t\treturn false;\n\t}\n\n\tif (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"value %lld makes %s pointer be out of bounds\\n\",\n\t\t\tsmin, reg_type_str[type]);\n\t\treturn false;\n\t}\n\n\treturn true;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 3115,
        "critical_vars": [
            "opcode"
        ],
        "function": "check_reg_sane_offset",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic bool check_reg_sane_offset(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  enum bpf_reg_type type)\n{\n\tbool known = tnum_is_const(reg->var_off);\n\ts64 val = reg->var_off.value;\n\ts64 smin = reg->smin_value;\n\n\tif (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {\n\t\tverbose(env, \"math between %s pointer and %lld is not allowed\\n\",\n\t\t\treg_type_str[type], val);\n\t\treturn false;\n\t}\n\n\tif (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"%s pointer offset %d is not allowed\\n\",\n\t\t\treg_type_str[type], reg->off);\n\t\treturn false;\n\t}\n\n\tif (smin == S64_MIN) {\n\t\tverbose(env, \"math between %s pointer and register with unbounded min value is not allowed\\n\",\n\t\t\treg_type_str[type]);\n\t\treturn false;\n\t}\n\n\tif (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"value %lld makes %s pointer be out of bounds\\n\",\n\t\t\tsmin, reg_type_str[type]);\n\t\treturn false;\n\t}\n\n\treturn true;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Var-Declaration",
        "line_new": 3116,
        "critical_vars": [
            "alu_state",
            "alu_limit"
        ],
        "function": "check_reg_sane_offset",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic bool check_reg_sane_offset(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  enum bpf_reg_type type)\n{\n\tbool known = tnum_is_const(reg->var_off);\n\ts64 val = reg->var_off.value;\n\ts64 smin = reg->smin_value;\n\n\tif (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {\n\t\tverbose(env, \"math between %s pointer and %lld is not allowed\\n\",\n\t\t\treg_type_str[type], val);\n\t\treturn false;\n\t}\n\n\tif (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"%s pointer offset %d is not allowed\\n\",\n\t\t\treg_type_str[type], reg->off);\n\t\treturn false;\n\t}\n\n\tif (smin == S64_MIN) {\n\t\tverbose(env, \"math between %s pointer and register with unbounded min value is not allowed\\n\",\n\t\t\treg_type_str[type]);\n\t\treturn false;\n\t}\n\n\tif (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"value %lld makes %s pointer be out of bounds\\n\",\n\t\t\tsmin, reg_type_str[type]);\n\t\treturn false;\n\t}\n\n\treturn true;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Var-Declaration",
        "line_new": 3117,
        "critical_vars": [
            "tmp"
        ],
        "function": "check_reg_sane_offset",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic bool check_reg_sane_offset(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  enum bpf_reg_type type)\n{\n\tbool known = tnum_is_const(reg->var_off);\n\ts64 val = reg->var_off.value;\n\ts64 smin = reg->smin_value;\n\n\tif (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {\n\t\tverbose(env, \"math between %s pointer and %lld is not allowed\\n\",\n\t\t\treg_type_str[type], val);\n\t\treturn false;\n\t}\n\n\tif (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"%s pointer offset %d is not allowed\\n\",\n\t\t\treg_type_str[type], reg->off);\n\t\treturn false;\n\t}\n\n\tif (smin == S64_MIN) {\n\t\tverbose(env, \"math between %s pointer and register with unbounded min value is not allowed\\n\",\n\t\t\treg_type_str[type]);\n\t\treturn false;\n\t}\n\n\tif (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"value %lld makes %s pointer be out of bounds\\n\",\n\t\t\tsmin, reg_type_str[type]);\n\t\treturn false;\n\t}\n\n\treturn true;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Var-Declaration",
        "line_new": 3118,
        "critical_vars": [
            "ret"
        ],
        "function": "check_reg_sane_offset",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic bool check_reg_sane_offset(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  enum bpf_reg_type type)\n{\n\tbool known = tnum_is_const(reg->var_off);\n\ts64 val = reg->var_off.value;\n\ts64 smin = reg->smin_value;\n\n\tif (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {\n\t\tverbose(env, \"math between %s pointer and %lld is not allowed\\n\",\n\t\t\treg_type_str[type], val);\n\t\treturn false;\n\t}\n\n\tif (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"%s pointer offset %d is not allowed\\n\",\n\t\t\treg_type_str[type], reg->off);\n\t\treturn false;\n\t}\n\n\tif (smin == S64_MIN) {\n\t\tverbose(env, \"math between %s pointer and register with unbounded min value is not allowed\\n\",\n\t\t\treg_type_str[type]);\n\t\treturn false;\n\t}\n\n\tif (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"value %lld makes %s pointer be out of bounds\\n\",\n\t\t\tsmin, reg_type_str[type]);\n\t\treturn false;\n\t}\n\n\treturn true;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 3120,
        "critical_vars": [
            "env->allow_ptr_leaks",
            "insn->code"
        ],
        "function": "check_reg_sane_offset",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic bool check_reg_sane_offset(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  enum bpf_reg_type type)\n{\n\tbool known = tnum_is_const(reg->var_off);\n\ts64 val = reg->var_off.value;\n\ts64 smin = reg->smin_value;\n\n\tif (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {\n\t\tverbose(env, \"math between %s pointer and %lld is not allowed\\n\",\n\t\t\treg_type_str[type], val);\n\t\treturn false;\n\t}\n\n\tif (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"%s pointer offset %d is not allowed\\n\",\n\t\t\treg_type_str[type], reg->off);\n\t\treturn false;\n\t}\n\n\tif (smin == S64_MIN) {\n\t\tverbose(env, \"math between %s pointer and register with unbounded min value is not allowed\\n\",\n\t\t\treg_type_str[type]);\n\t\treturn false;\n\t}\n\n\tif (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"value %lld makes %s pointer be out of bounds\\n\",\n\t\t\tsmin, reg_type_str[type]);\n\t\treturn false;\n\t}\n\n\treturn true;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 3127,
        "critical_vars": [
            "vstate->speculative"
        ],
        "function": "check_reg_sane_offset",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic bool check_reg_sane_offset(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  enum bpf_reg_type type)\n{\n\tbool known = tnum_is_const(reg->var_off);\n\ts64 val = reg->var_off.value;\n\ts64 smin = reg->smin_value;\n\n\tif (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {\n\t\tverbose(env, \"math between %s pointer and %lld is not allowed\\n\",\n\t\t\treg_type_str[type], val);\n\t\treturn false;\n\t}\n\n\tif (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"%s pointer offset %d is not allowed\\n\",\n\t\t\treg_type_str[type], reg->off);\n\t\treturn false;\n\t}\n\n\tif (smin == S64_MIN) {\n\t\tverbose(env, \"math between %s pointer and register with unbounded min value is not allowed\\n\",\n\t\t\treg_type_str[type]);\n\t\treturn false;\n\t}\n\n\tif (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"value %lld makes %s pointer be out of bounds\\n\",\n\t\t\tsmin, reg_type_str[type]);\n\t\treturn false;\n\t}\n\n\treturn true;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 3130,
        "critical_vars": [
            "alu_state"
        ],
        "function": "check_reg_sane_offset",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic bool check_reg_sane_offset(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  enum bpf_reg_type type)\n{\n\tbool known = tnum_is_const(reg->var_off);\n\ts64 val = reg->var_off.value;\n\ts64 smin = reg->smin_value;\n\n\tif (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {\n\t\tverbose(env, \"math between %s pointer and %lld is not allowed\\n\",\n\t\t\treg_type_str[type], val);\n\t\treturn false;\n\t}\n\n\tif (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"%s pointer offset %d is not allowed\\n\",\n\t\t\treg_type_str[type], reg->off);\n\t\treturn false;\n\t}\n\n\tif (smin == S64_MIN) {\n\t\tverbose(env, \"math between %s pointer and register with unbounded min value is not allowed\\n\",\n\t\t\treg_type_str[type]);\n\t\treturn false;\n\t}\n\n\tif (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"value %lld makes %s pointer be out of bounds\\n\",\n\t\t\tsmin, reg_type_str[type]);\n\t\treturn false;\n\t}\n\n\treturn true;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 3131,
        "critical_vars": [
            "alu_state"
        ],
        "function": "check_reg_sane_offset",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic bool check_reg_sane_offset(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  enum bpf_reg_type type)\n{\n\tbool known = tnum_is_const(reg->var_off);\n\ts64 val = reg->var_off.value;\n\ts64 smin = reg->smin_value;\n\n\tif (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {\n\t\tverbose(env, \"math between %s pointer and %lld is not allowed\\n\",\n\t\t\treg_type_str[type], val);\n\t\treturn false;\n\t}\n\n\tif (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"%s pointer offset %d is not allowed\\n\",\n\t\t\treg_type_str[type], reg->off);\n\t\treturn false;\n\t}\n\n\tif (smin == S64_MIN) {\n\t\tverbose(env, \"math between %s pointer and register with unbounded min value is not allowed\\n\",\n\t\t\treg_type_str[type]);\n\t\treturn false;\n\t}\n\n\tif (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"value %lld makes %s pointer be out of bounds\\n\",\n\t\t\tsmin, reg_type_str[type]);\n\t\treturn false;\n\t}\n\n\treturn true;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 3134,
        "critical_vars": [
            "opcode",
            "ptr_reg",
            "&alu_limit",
            "off_is_neg"
        ],
        "function": "check_reg_sane_offset",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic bool check_reg_sane_offset(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  enum bpf_reg_type type)\n{\n\tbool known = tnum_is_const(reg->var_off);\n\ts64 val = reg->var_off.value;\n\ts64 smin = reg->smin_value;\n\n\tif (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {\n\t\tverbose(env, \"math between %s pointer and %lld is not allowed\\n\",\n\t\t\treg_type_str[type], val);\n\t\treturn false;\n\t}\n\n\tif (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"%s pointer offset %d is not allowed\\n\",\n\t\t\treg_type_str[type], reg->off);\n\t\treturn false;\n\t}\n\n\tif (smin == S64_MIN) {\n\t\tverbose(env, \"math between %s pointer and register with unbounded min value is not allowed\\n\",\n\t\t\treg_type_str[type]);\n\t\treturn false;\n\t}\n\n\tif (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"value %lld makes %s pointer be out of bounds\\n\",\n\t\t\tsmin, reg_type_str[type]);\n\t\treturn false;\n\t}\n\n\treturn true;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 3140,
        "critical_vars": [
            "aux->alu_state",
            "aux->alu_limit"
        ],
        "function": "check_reg_sane_offset",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic bool check_reg_sane_offset(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  enum bpf_reg_type type)\n{\n\tbool known = tnum_is_const(reg->var_off);\n\ts64 val = reg->var_off.value;\n\ts64 smin = reg->smin_value;\n\n\tif (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {\n\t\tverbose(env, \"math between %s pointer and %lld is not allowed\\n\",\n\t\t\treg_type_str[type], val);\n\t\treturn false;\n\t}\n\n\tif (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"%s pointer offset %d is not allowed\\n\",\n\t\t\treg_type_str[type], reg->off);\n\t\treturn false;\n\t}\n\n\tif (smin == S64_MIN) {\n\t\tverbose(env, \"math between %s pointer and register with unbounded min value is not allowed\\n\",\n\t\t\treg_type_str[type]);\n\t\treturn false;\n\t}\n\n\tif (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"value %lld makes %s pointer be out of bounds\\n\",\n\t\t\tsmin, reg_type_str[type]);\n\t\treturn false;\n\t}\n\n\treturn true;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 3146,
        "critical_vars": [
            "aux->alu_state"
        ],
        "function": "check_reg_sane_offset",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic bool check_reg_sane_offset(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  enum bpf_reg_type type)\n{\n\tbool known = tnum_is_const(reg->var_off);\n\ts64 val = reg->var_off.value;\n\ts64 smin = reg->smin_value;\n\n\tif (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {\n\t\tverbose(env, \"math between %s pointer and %lld is not allowed\\n\",\n\t\t\treg_type_str[type], val);\n\t\treturn false;\n\t}\n\n\tif (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"%s pointer offset %d is not allowed\\n\",\n\t\t\treg_type_str[type], reg->off);\n\t\treturn false;\n\t}\n\n\tif (smin == S64_MIN) {\n\t\tverbose(env, \"math between %s pointer and register with unbounded min value is not allowed\\n\",\n\t\t\treg_type_str[type]);\n\t\treturn false;\n\t}\n\n\tif (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"value %lld makes %s pointer be out of bounds\\n\",\n\t\t\tsmin, reg_type_str[type]);\n\t\treturn false;\n\t}\n\n\treturn true;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 3147,
        "critical_vars": [
            "aux->alu_limit"
        ],
        "function": "check_reg_sane_offset",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic bool check_reg_sane_offset(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  enum bpf_reg_type type)\n{\n\tbool known = tnum_is_const(reg->var_off);\n\ts64 val = reg->var_off.value;\n\ts64 smin = reg->smin_value;\n\n\tif (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {\n\t\tverbose(env, \"math between %s pointer and %lld is not allowed\\n\",\n\t\t\treg_type_str[type], val);\n\t\treturn false;\n\t}\n\n\tif (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"%s pointer offset %d is not allowed\\n\",\n\t\t\treg_type_str[type], reg->off);\n\t\treturn false;\n\t}\n\n\tif (smin == S64_MIN) {\n\t\tverbose(env, \"math between %s pointer and register with unbounded min value is not allowed\\n\",\n\t\t\treg_type_str[type]);\n\t\treturn false;\n\t}\n\n\tif (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"value %lld makes %s pointer be out of bounds\\n\",\n\t\t\tsmin, reg_type_str[type]);\n\t\treturn false;\n\t}\n\n\treturn true;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 3149,
        "critical_vars": [
            "ptr_is_dst_reg"
        ],
        "function": "check_reg_sane_offset",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic bool check_reg_sane_offset(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  enum bpf_reg_type type)\n{\n\tbool known = tnum_is_const(reg->var_off);\n\ts64 val = reg->var_off.value;\n\ts64 smin = reg->smin_value;\n\n\tif (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {\n\t\tverbose(env, \"math between %s pointer and %lld is not allowed\\n\",\n\t\t\treg_type_str[type], val);\n\t\treturn false;\n\t}\n\n\tif (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"%s pointer offset %d is not allowed\\n\",\n\t\t\treg_type_str[type], reg->off);\n\t\treturn false;\n\t}\n\n\tif (smin == S64_MIN) {\n\t\tverbose(env, \"math between %s pointer and register with unbounded min value is not allowed\\n\",\n\t\t\treg_type_str[type]);\n\t\treturn false;\n\t}\n\n\tif (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"value %lld makes %s pointer be out of bounds\\n\",\n\t\t\tsmin, reg_type_str[type]);\n\t\treturn false;\n\t}\n\n\treturn true;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 3160,
        "critical_vars": [
            "tmp"
        ],
        "function": "check_reg_sane_offset",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic bool check_reg_sane_offset(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  enum bpf_reg_type type)\n{\n\tbool known = tnum_is_const(reg->var_off);\n\ts64 val = reg->var_off.value;\n\ts64 smin = reg->smin_value;\n\n\tif (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {\n\t\tverbose(env, \"math between %s pointer and %lld is not allowed\\n\",\n\t\t\treg_type_str[type], val);\n\t\treturn false;\n\t}\n\n\tif (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"%s pointer offset %d is not allowed\\n\",\n\t\t\treg_type_str[type], reg->off);\n\t\treturn false;\n\t}\n\n\tif (smin == S64_MIN) {\n\t\tverbose(env, \"math between %s pointer and register with unbounded min value is not allowed\\n\",\n\t\t\treg_type_str[type]);\n\t\treturn false;\n\t}\n\n\tif (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"value %lld makes %s pointer be out of bounds\\n\",\n\t\t\tsmin, reg_type_str[type]);\n\t\treturn false;\n\t}\n\n\treturn true;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 3161,
        "critical_vars": [
            "*dst_reg"
        ],
        "function": "check_reg_sane_offset",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic bool check_reg_sane_offset(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  enum bpf_reg_type type)\n{\n\tbool known = tnum_is_const(reg->var_off);\n\ts64 val = reg->var_off.value;\n\ts64 smin = reg->smin_value;\n\n\tif (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {\n\t\tverbose(env, \"math between %s pointer and %lld is not allowed\\n\",\n\t\t\treg_type_str[type], val);\n\t\treturn false;\n\t}\n\n\tif (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"%s pointer offset %d is not allowed\\n\",\n\t\t\treg_type_str[type], reg->off);\n\t\treturn false;\n\t}\n\n\tif (smin == S64_MIN) {\n\t\tverbose(env, \"math between %s pointer and register with unbounded min value is not allowed\\n\",\n\t\t\treg_type_str[type]);\n\t\treturn false;\n\t}\n\n\tif (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"value %lld makes %s pointer be out of bounds\\n\",\n\t\t\tsmin, reg_type_str[type]);\n\t\treturn false;\n\t}\n\n\treturn true;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 3163,
        "critical_vars": [
            "ret"
        ],
        "function": "check_reg_sane_offset",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic bool check_reg_sane_offset(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  enum bpf_reg_type type)\n{\n\tbool known = tnum_is_const(reg->var_off);\n\ts64 val = reg->var_off.value;\n\ts64 smin = reg->smin_value;\n\n\tif (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {\n\t\tverbose(env, \"math between %s pointer and %lld is not allowed\\n\",\n\t\t\treg_type_str[type], val);\n\t\treturn false;\n\t}\n\n\tif (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"%s pointer offset %d is not allowed\\n\",\n\t\t\treg_type_str[type], reg->off);\n\t\treturn false;\n\t}\n\n\tif (smin == S64_MIN) {\n\t\tverbose(env, \"math between %s pointer and register with unbounded min value is not allowed\\n\",\n\t\t\treg_type_str[type]);\n\t\treturn false;\n\t}\n\n\tif (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"value %lld makes %s pointer be out of bounds\\n\",\n\t\t\tsmin, reg_type_str[type]);\n\t\treturn false;\n\t}\n\n\treturn true;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 3164,
        "critical_vars": [
            "ptr_is_dst_reg"
        ],
        "function": "check_reg_sane_offset",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic bool check_reg_sane_offset(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  enum bpf_reg_type type)\n{\n\tbool known = tnum_is_const(reg->var_off);\n\ts64 val = reg->var_off.value;\n\ts64 smin = reg->smin_value;\n\n\tif (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {\n\t\tverbose(env, \"math between %s pointer and %lld is not allowed\\n\",\n\t\t\treg_type_str[type], val);\n\t\treturn false;\n\t}\n\n\tif (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"%s pointer offset %d is not allowed\\n\",\n\t\t\treg_type_str[type], reg->off);\n\t\treturn false;\n\t}\n\n\tif (smin == S64_MIN) {\n\t\tverbose(env, \"math between %s pointer and register with unbounded min value is not allowed\\n\",\n\t\t\treg_type_str[type]);\n\t\treturn false;\n\t}\n\n\tif (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"value %lld makes %s pointer be out of bounds\\n\",\n\t\t\tsmin, reg_type_str[type]);\n\t\treturn false;\n\t}\n\n\treturn true;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 3165,
        "critical_vars": [
            "*dst_reg"
        ],
        "function": "check_reg_sane_offset",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic bool check_reg_sane_offset(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  enum bpf_reg_type type)\n{\n\tbool known = tnum_is_const(reg->var_off);\n\ts64 val = reg->var_off.value;\n\ts64 smin = reg->smin_value;\n\n\tif (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {\n\t\tverbose(env, \"math between %s pointer and %lld is not allowed\\n\",\n\t\t\treg_type_str[type], val);\n\t\treturn false;\n\t}\n\n\tif (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"%s pointer offset %d is not allowed\\n\",\n\t\t\treg_type_str[type], reg->off);\n\t\treturn false;\n\t}\n\n\tif (smin == S64_MIN) {\n\t\tverbose(env, \"math between %s pointer and register with unbounded min value is not allowed\\n\",\n\t\t\treg_type_str[type]);\n\t\treturn false;\n\t}\n\n\tif (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"value %lld makes %s pointer be out of bounds\\n\",\n\t\t\tsmin, reg_type_str[type]);\n\t\treturn false;\n\t}\n\n\treturn true;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Var-Declaration",
        "line_new": 3189,
        "critical_vars": [
            "ret"
        ],
        "function": "adjust_ptr_min_max_vals",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "static int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn,\n\t\t\t\t   const struct bpf_reg_state *ptr_reg,\n\t\t\t\t   const struct bpf_reg_state *off_reg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg;\n\tbool known = tnum_is_const(off_reg->var_off);\n\ts64 smin_val = off_reg->smin_value, smax_val = off_reg->smax_value,\n\t    smin_ptr = ptr_reg->smin_value, smax_ptr = ptr_reg->smax_value;\n\tu64 umin_val = off_reg->umin_value, umax_val = off_reg->umax_value,\n\t    umin_ptr = ptr_reg->umin_value, umax_ptr = ptr_reg->umax_value;\n\tu32 dst = insn->dst_reg, src = insn->src_reg;\n\tu8 opcode = BPF_OP(insn->code);\n\tint ret;\n\n\tdst_reg = &regs[dst];\n\n\tif ((known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops on pointers produce (meaningless) scalars */\n\t\tverbose(env,\n\t\t\t\"R%d 32-bit pointer arithmetic prohibited\\n\",\n\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\n\tswitch (ptr_reg->type) {\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited, null-check it first\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tcase CONST_PTR_TO_MAP:\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (!env->allow_ptr_leaks && !known && (smin_val < 0) != (smax_val < 0)) {\n\t\t\tverbose(env, \"R%d has unknown scalar with mixed signed bounds, pointer arithmetic with it prohibited for !root\\n\",\n\t\t\t\toff_reg == dst_reg ? dst : src);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* fall-through */\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* In case of 'scalar += pointer', dst_reg inherits pointer type and id.\n\t * The id may be overwritten later if we create a new variable offset.\n\t */\n\tdst_reg->type = ptr_reg->type;\n\tdst_reg->id = ptr_reg->id;\n\n\tif (!check_reg_sane_offset(env, off_reg, ptr_reg->type) ||\n\t    !check_reg_sane_offset(env, ptr_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, dst_reg, smin_val < 0);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to add from different maps or paths\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\t/* We can take a fixed offset as long as it doesn't overflow\n\t\t * the s32 'off' field\n\t\t */\n\t\tif (known && (ptr_reg->off + smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off + smin_val))) {\n\t\t\t/* pointer += K.  Accumulate it into fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->off = ptr_reg->off + smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  Note that off_reg->off\n\t\t * == 0, since it's a scalar.\n\t\t * dst_reg gets the pointer type and since some positive\n\t\t * integer value was added to the pointer, give it a new 'id'\n\t\t * if it's a PTR_TO_PACKET.\n\t\t * this creates a new 'base' pointer, off_reg (variable) gets\n\t\t * added into the variable offset, and we copy the fixed offset\n\t\t * from ptr_reg.\n\t\t */\n\t\tif (signed_add_overflows(smin_ptr, smin_val) ||\n\t\t    signed_add_overflows(smax_ptr, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr + smin_val;\n\t\t\tdst_reg->smax_value = smax_ptr + smax_val;\n\t\t}\n\t\tif (umin_ptr + umin_val < umin_ptr ||\n\t\t    umax_ptr + umax_val < umax_ptr) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value = umin_ptr + umin_val;\n\t\t\tdst_reg->umax_value = umax_ptr + umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tdst_reg->raw = 0;\n\t\t}\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, dst_reg, smin_val < 0);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to sub from different maps or paths\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\tif (dst_reg == off_reg) {\n\t\t\t/* scalar -= pointer.  Creates an unknown scalar */\n\t\t\tverbose(env, \"R%d tried to subtract pointer from scalar\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* We don't allow subtraction from FP, because (according to\n\t\t * test_verifier.c test \"invalid fp arithmetic\", JITs might not\n\t\t * be able to deal with it.\n\t\t */\n\t\tif (ptr_reg->type == PTR_TO_STACK) {\n\t\t\tverbose(env, \"R%d subtraction from stack pointer prohibited\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (known && (ptr_reg->off - smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off - smin_val))) {\n\t\t\t/* pointer -= K.  Subtract it from fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->id = ptr_reg->id;\n\t\t\tdst_reg->off = ptr_reg->off - smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  If the subtrahend is known\n\t\t * nonnegative, then any reg->range we had before is still good.\n\t\t */\n\t\tif (signed_sub_overflows(smin_ptr, smax_val) ||\n\t\t    signed_sub_overflows(smax_ptr, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr - smax_val;\n\t\t\tdst_reg->smax_value = smax_ptr - smin_val;\n\t\t}\n\t\tif (umin_ptr < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value = umin_ptr - umax_val;\n\t\t\tdst_reg->umax_value = umax_ptr - umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tif (smin_val < 0)\n\t\t\t\tdst_reg->raw = 0;\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\tcase BPF_OR:\n\tcase BPF_XOR:\n\t\t/* bitwise ops on pointers are troublesome, prohibit. */\n\t\tverbose(env, \"R%d bitwise operator %s on pointer prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\tdefault:\n\t\t/* other operators (e.g. MUL,LSH) produce non-pointer results */\n\t\tverbose(env, \"R%d pointer arithmetic with %s operator prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!check_reg_sane_offset(env, dst_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t__update_reg_bounds(dst_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\n\t/* For unprivileged we require that resulting offset must be in bounds\n\t * in order to be able to sanitize access later on.\n\t */\n\tif (!env->allow_ptr_leaks) {\n\t\tif (dst_reg->type == PTR_TO_MAP_VALUE &&\n\t\t    check_map_access(env, dst, dst_reg->off, 1, false)) {\n\t\t\tverbose(env, \"R%d pointer arithmetic of map value goes out of range, \"\n\t\t\t\t\"prohibited for !root\\n\", dst);\n\t\t\treturn -EACCES;\n\t\t} else if (dst_reg->type == PTR_TO_STACK &&\n\t\t\t   check_stack_access(env, dst_reg, dst_reg->off +\n\t\t\t\t\t      dst_reg->var_off.value, 1)) {\n\t\t\tverbose(env, \"R%d stack pointer arithmetic goes out of range, \"\n\t\t\t\t\"prohibited for !root\\n\", dst);\n\t\t\treturn -EACCES;\n\t\t}\n\t}\n\n\treturn 0;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 3245,
        "critical_vars": [
            "ret"
        ],
        "function": "adjust_ptr_min_max_vals",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "static int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn,\n\t\t\t\t   const struct bpf_reg_state *ptr_reg,\n\t\t\t\t   const struct bpf_reg_state *off_reg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg;\n\tbool known = tnum_is_const(off_reg->var_off);\n\ts64 smin_val = off_reg->smin_value, smax_val = off_reg->smax_value,\n\t    smin_ptr = ptr_reg->smin_value, smax_ptr = ptr_reg->smax_value;\n\tu64 umin_val = off_reg->umin_value, umax_val = off_reg->umax_value,\n\t    umin_ptr = ptr_reg->umin_value, umax_ptr = ptr_reg->umax_value;\n\tu32 dst = insn->dst_reg, src = insn->src_reg;\n\tu8 opcode = BPF_OP(insn->code);\n\tint ret;\n\n\tdst_reg = &regs[dst];\n\n\tif ((known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops on pointers produce (meaningless) scalars */\n\t\tverbose(env,\n\t\t\t\"R%d 32-bit pointer arithmetic prohibited\\n\",\n\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\n\tswitch (ptr_reg->type) {\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited, null-check it first\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tcase CONST_PTR_TO_MAP:\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (!env->allow_ptr_leaks && !known && (smin_val < 0) != (smax_val < 0)) {\n\t\t\tverbose(env, \"R%d has unknown scalar with mixed signed bounds, pointer arithmetic with it prohibited for !root\\n\",\n\t\t\t\toff_reg == dst_reg ? dst : src);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* fall-through */\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* In case of 'scalar += pointer', dst_reg inherits pointer type and id.\n\t * The id may be overwritten later if we create a new variable offset.\n\t */\n\tdst_reg->type = ptr_reg->type;\n\tdst_reg->id = ptr_reg->id;\n\n\tif (!check_reg_sane_offset(env, off_reg, ptr_reg->type) ||\n\t    !check_reg_sane_offset(env, ptr_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, dst_reg, smin_val < 0);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to add from different maps or paths\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\t/* We can take a fixed offset as long as it doesn't overflow\n\t\t * the s32 'off' field\n\t\t */\n\t\tif (known && (ptr_reg->off + smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off + smin_val))) {\n\t\t\t/* pointer += K.  Accumulate it into fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->off = ptr_reg->off + smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  Note that off_reg->off\n\t\t * == 0, since it's a scalar.\n\t\t * dst_reg gets the pointer type and since some positive\n\t\t * integer value was added to the pointer, give it a new 'id'\n\t\t * if it's a PTR_TO_PACKET.\n\t\t * this creates a new 'base' pointer, off_reg (variable) gets\n\t\t * added into the variable offset, and we copy the fixed offset\n\t\t * from ptr_reg.\n\t\t */\n\t\tif (signed_add_overflows(smin_ptr, smin_val) ||\n\t\t    signed_add_overflows(smax_ptr, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr + smin_val;\n\t\t\tdst_reg->smax_value = smax_ptr + smax_val;\n\t\t}\n\t\tif (umin_ptr + umin_val < umin_ptr ||\n\t\t    umax_ptr + umax_val < umax_ptr) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value = umin_ptr + umin_val;\n\t\t\tdst_reg->umax_value = umax_ptr + umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tdst_reg->raw = 0;\n\t\t}\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, dst_reg, smin_val < 0);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to sub from different maps or paths\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\tif (dst_reg == off_reg) {\n\t\t\t/* scalar -= pointer.  Creates an unknown scalar */\n\t\t\tverbose(env, \"R%d tried to subtract pointer from scalar\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* We don't allow subtraction from FP, because (according to\n\t\t * test_verifier.c test \"invalid fp arithmetic\", JITs might not\n\t\t * be able to deal with it.\n\t\t */\n\t\tif (ptr_reg->type == PTR_TO_STACK) {\n\t\t\tverbose(env, \"R%d subtraction from stack pointer prohibited\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (known && (ptr_reg->off - smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off - smin_val))) {\n\t\t\t/* pointer -= K.  Subtract it from fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->id = ptr_reg->id;\n\t\t\tdst_reg->off = ptr_reg->off - smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  If the subtrahend is known\n\t\t * nonnegative, then any reg->range we had before is still good.\n\t\t */\n\t\tif (signed_sub_overflows(smin_ptr, smax_val) ||\n\t\t    signed_sub_overflows(smax_ptr, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr - smax_val;\n\t\t\tdst_reg->smax_value = smax_ptr - smin_val;\n\t\t}\n\t\tif (umin_ptr < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value = umin_ptr - umax_val;\n\t\t\tdst_reg->umax_value = umax_ptr - umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tif (smin_val < 0)\n\t\t\t\tdst_reg->raw = 0;\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\tcase BPF_OR:\n\tcase BPF_XOR:\n\t\t/* bitwise ops on pointers are troublesome, prohibit. */\n\t\tverbose(env, \"R%d bitwise operator %s on pointer prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\tdefault:\n\t\t/* other operators (e.g. MUL,LSH) produce non-pointer results */\n\t\tverbose(env, \"R%d pointer arithmetic with %s operator prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!check_reg_sane_offset(env, dst_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t__update_reg_bounds(dst_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\n\t/* For unprivileged we require that resulting offset must be in bounds\n\t * in order to be able to sanitize access later on.\n\t */\n\tif (!env->allow_ptr_leaks) {\n\t\tif (dst_reg->type == PTR_TO_MAP_VALUE &&\n\t\t    check_map_access(env, dst, dst_reg->off, 1, false)) {\n\t\t\tverbose(env, \"R%d pointer arithmetic of map value goes out of range, \"\n\t\t\t\t\"prohibited for !root\\n\", dst);\n\t\t\treturn -EACCES;\n\t\t} else if (dst_reg->type == PTR_TO_STACK &&\n\t\t\t   check_stack_access(env, dst_reg, dst_reg->off +\n\t\t\t\t\t      dst_reg->var_off.value, 1)) {\n\t\t\tverbose(env, \"R%d stack pointer arithmetic goes out of range, \"\n\t\t\t\t\"prohibited for !root\\n\", dst);\n\t\t\treturn -EACCES;\n\t\t}\n\t}\n\n\treturn 0;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 3246,
        "critical_vars": [
            "ret"
        ],
        "function": "adjust_ptr_min_max_vals",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "static int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn,\n\t\t\t\t   const struct bpf_reg_state *ptr_reg,\n\t\t\t\t   const struct bpf_reg_state *off_reg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg;\n\tbool known = tnum_is_const(off_reg->var_off);\n\ts64 smin_val = off_reg->smin_value, smax_val = off_reg->smax_value,\n\t    smin_ptr = ptr_reg->smin_value, smax_ptr = ptr_reg->smax_value;\n\tu64 umin_val = off_reg->umin_value, umax_val = off_reg->umax_value,\n\t    umin_ptr = ptr_reg->umin_value, umax_ptr = ptr_reg->umax_value;\n\tu32 dst = insn->dst_reg, src = insn->src_reg;\n\tu8 opcode = BPF_OP(insn->code);\n\tint ret;\n\n\tdst_reg = &regs[dst];\n\n\tif ((known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops on pointers produce (meaningless) scalars */\n\t\tverbose(env,\n\t\t\t\"R%d 32-bit pointer arithmetic prohibited\\n\",\n\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\n\tswitch (ptr_reg->type) {\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited, null-check it first\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tcase CONST_PTR_TO_MAP:\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (!env->allow_ptr_leaks && !known && (smin_val < 0) != (smax_val < 0)) {\n\t\t\tverbose(env, \"R%d has unknown scalar with mixed signed bounds, pointer arithmetic with it prohibited for !root\\n\",\n\t\t\t\toff_reg == dst_reg ? dst : src);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* fall-through */\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* In case of 'scalar += pointer', dst_reg inherits pointer type and id.\n\t * The id may be overwritten later if we create a new variable offset.\n\t */\n\tdst_reg->type = ptr_reg->type;\n\tdst_reg->id = ptr_reg->id;\n\n\tif (!check_reg_sane_offset(env, off_reg, ptr_reg->type) ||\n\t    !check_reg_sane_offset(env, ptr_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, dst_reg, smin_val < 0);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to add from different maps or paths\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\t/* We can take a fixed offset as long as it doesn't overflow\n\t\t * the s32 'off' field\n\t\t */\n\t\tif (known && (ptr_reg->off + smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off + smin_val))) {\n\t\t\t/* pointer += K.  Accumulate it into fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->off = ptr_reg->off + smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  Note that off_reg->off\n\t\t * == 0, since it's a scalar.\n\t\t * dst_reg gets the pointer type and since some positive\n\t\t * integer value was added to the pointer, give it a new 'id'\n\t\t * if it's a PTR_TO_PACKET.\n\t\t * this creates a new 'base' pointer, off_reg (variable) gets\n\t\t * added into the variable offset, and we copy the fixed offset\n\t\t * from ptr_reg.\n\t\t */\n\t\tif (signed_add_overflows(smin_ptr, smin_val) ||\n\t\t    signed_add_overflows(smax_ptr, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr + smin_val;\n\t\t\tdst_reg->smax_value = smax_ptr + smax_val;\n\t\t}\n\t\tif (umin_ptr + umin_val < umin_ptr ||\n\t\t    umax_ptr + umax_val < umax_ptr) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value = umin_ptr + umin_val;\n\t\t\tdst_reg->umax_value = umax_ptr + umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tdst_reg->raw = 0;\n\t\t}\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, dst_reg, smin_val < 0);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to sub from different maps or paths\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\tif (dst_reg == off_reg) {\n\t\t\t/* scalar -= pointer.  Creates an unknown scalar */\n\t\t\tverbose(env, \"R%d tried to subtract pointer from scalar\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* We don't allow subtraction from FP, because (according to\n\t\t * test_verifier.c test \"invalid fp arithmetic\", JITs might not\n\t\t * be able to deal with it.\n\t\t */\n\t\tif (ptr_reg->type == PTR_TO_STACK) {\n\t\t\tverbose(env, \"R%d subtraction from stack pointer prohibited\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (known && (ptr_reg->off - smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off - smin_val))) {\n\t\t\t/* pointer -= K.  Subtract it from fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->id = ptr_reg->id;\n\t\t\tdst_reg->off = ptr_reg->off - smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  If the subtrahend is known\n\t\t * nonnegative, then any reg->range we had before is still good.\n\t\t */\n\t\tif (signed_sub_overflows(smin_ptr, smax_val) ||\n\t\t    signed_sub_overflows(smax_ptr, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr - smax_val;\n\t\t\tdst_reg->smax_value = smax_ptr - smin_val;\n\t\t}\n\t\tif (umin_ptr < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value = umin_ptr - umax_val;\n\t\t\tdst_reg->umax_value = umax_ptr - umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tif (smin_val < 0)\n\t\t\t\tdst_reg->raw = 0;\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\tcase BPF_OR:\n\tcase BPF_XOR:\n\t\t/* bitwise ops on pointers are troublesome, prohibit. */\n\t\tverbose(env, \"R%d bitwise operator %s on pointer prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\tdefault:\n\t\t/* other operators (e.g. MUL,LSH) produce non-pointer results */\n\t\tverbose(env, \"R%d pointer arithmetic with %s operator prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!check_reg_sane_offset(env, dst_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t__update_reg_bounds(dst_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\n\t/* For unprivileged we require that resulting offset must be in bounds\n\t * in order to be able to sanitize access later on.\n\t */\n\tif (!env->allow_ptr_leaks) {\n\t\tif (dst_reg->type == PTR_TO_MAP_VALUE &&\n\t\t    check_map_access(env, dst, dst_reg->off, 1, false)) {\n\t\t\tverbose(env, \"R%d pointer arithmetic of map value goes out of range, \"\n\t\t\t\t\"prohibited for !root\\n\", dst);\n\t\t\treturn -EACCES;\n\t\t} else if (dst_reg->type == PTR_TO_STACK &&\n\t\t\t   check_stack_access(env, dst_reg, dst_reg->off +\n\t\t\t\t\t      dst_reg->var_off.value, 1)) {\n\t\t\tverbose(env, \"R%d stack pointer arithmetic goes out of range, \"\n\t\t\t\t\"prohibited for !root\\n\", dst);\n\t\t\treturn -EACCES;\n\t\t}\n\t}\n\n\treturn 0;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 3247,
        "critical_vars": [
            "dst",
            "env"
        ],
        "function": "adjust_ptr_min_max_vals",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "static int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn,\n\t\t\t\t   const struct bpf_reg_state *ptr_reg,\n\t\t\t\t   const struct bpf_reg_state *off_reg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg;\n\tbool known = tnum_is_const(off_reg->var_off);\n\ts64 smin_val = off_reg->smin_value, smax_val = off_reg->smax_value,\n\t    smin_ptr = ptr_reg->smin_value, smax_ptr = ptr_reg->smax_value;\n\tu64 umin_val = off_reg->umin_value, umax_val = off_reg->umax_value,\n\t    umin_ptr = ptr_reg->umin_value, umax_ptr = ptr_reg->umax_value;\n\tu32 dst = insn->dst_reg, src = insn->src_reg;\n\tu8 opcode = BPF_OP(insn->code);\n\tint ret;\n\n\tdst_reg = &regs[dst];\n\n\tif ((known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops on pointers produce (meaningless) scalars */\n\t\tverbose(env,\n\t\t\t\"R%d 32-bit pointer arithmetic prohibited\\n\",\n\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\n\tswitch (ptr_reg->type) {\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited, null-check it first\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tcase CONST_PTR_TO_MAP:\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (!env->allow_ptr_leaks && !known && (smin_val < 0) != (smax_val < 0)) {\n\t\t\tverbose(env, \"R%d has unknown scalar with mixed signed bounds, pointer arithmetic with it prohibited for !root\\n\",\n\t\t\t\toff_reg == dst_reg ? dst : src);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* fall-through */\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* In case of 'scalar += pointer', dst_reg inherits pointer type and id.\n\t * The id may be overwritten later if we create a new variable offset.\n\t */\n\tdst_reg->type = ptr_reg->type;\n\tdst_reg->id = ptr_reg->id;\n\n\tif (!check_reg_sane_offset(env, off_reg, ptr_reg->type) ||\n\t    !check_reg_sane_offset(env, ptr_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, dst_reg, smin_val < 0);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to add from different maps or paths\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\t/* We can take a fixed offset as long as it doesn't overflow\n\t\t * the s32 'off' field\n\t\t */\n\t\tif (known && (ptr_reg->off + smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off + smin_val))) {\n\t\t\t/* pointer += K.  Accumulate it into fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->off = ptr_reg->off + smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  Note that off_reg->off\n\t\t * == 0, since it's a scalar.\n\t\t * dst_reg gets the pointer type and since some positive\n\t\t * integer value was added to the pointer, give it a new 'id'\n\t\t * if it's a PTR_TO_PACKET.\n\t\t * this creates a new 'base' pointer, off_reg (variable) gets\n\t\t * added into the variable offset, and we copy the fixed offset\n\t\t * from ptr_reg.\n\t\t */\n\t\tif (signed_add_overflows(smin_ptr, smin_val) ||\n\t\t    signed_add_overflows(smax_ptr, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr + smin_val;\n\t\t\tdst_reg->smax_value = smax_ptr + smax_val;\n\t\t}\n\t\tif (umin_ptr + umin_val < umin_ptr ||\n\t\t    umax_ptr + umax_val < umax_ptr) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value = umin_ptr + umin_val;\n\t\t\tdst_reg->umax_value = umax_ptr + umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tdst_reg->raw = 0;\n\t\t}\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, dst_reg, smin_val < 0);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to sub from different maps or paths\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\tif (dst_reg == off_reg) {\n\t\t\t/* scalar -= pointer.  Creates an unknown scalar */\n\t\t\tverbose(env, \"R%d tried to subtract pointer from scalar\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* We don't allow subtraction from FP, because (according to\n\t\t * test_verifier.c test \"invalid fp arithmetic\", JITs might not\n\t\t * be able to deal with it.\n\t\t */\n\t\tif (ptr_reg->type == PTR_TO_STACK) {\n\t\t\tverbose(env, \"R%d subtraction from stack pointer prohibited\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (known && (ptr_reg->off - smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off - smin_val))) {\n\t\t\t/* pointer -= K.  Subtract it from fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->id = ptr_reg->id;\n\t\t\tdst_reg->off = ptr_reg->off - smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  If the subtrahend is known\n\t\t * nonnegative, then any reg->range we had before is still good.\n\t\t */\n\t\tif (signed_sub_overflows(smin_ptr, smax_val) ||\n\t\t    signed_sub_overflows(smax_ptr, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr - smax_val;\n\t\t\tdst_reg->smax_value = smax_ptr - smin_val;\n\t\t}\n\t\tif (umin_ptr < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value = umin_ptr - umax_val;\n\t\t\tdst_reg->umax_value = umax_ptr - umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tif (smin_val < 0)\n\t\t\t\tdst_reg->raw = 0;\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\tcase BPF_OR:\n\tcase BPF_XOR:\n\t\t/* bitwise ops on pointers are troublesome, prohibit. */\n\t\tverbose(env, \"R%d bitwise operator %s on pointer prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\tdefault:\n\t\t/* other operators (e.g. MUL,LSH) produce non-pointer results */\n\t\tverbose(env, \"R%d pointer arithmetic with %s operator prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!check_reg_sane_offset(env, dst_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t__update_reg_bounds(dst_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\n\t/* For unprivileged we require that resulting offset must be in bounds\n\t * in order to be able to sanitize access later on.\n\t */\n\tif (!env->allow_ptr_leaks) {\n\t\tif (dst_reg->type == PTR_TO_MAP_VALUE &&\n\t\t    check_map_access(env, dst, dst_reg->off, 1, false)) {\n\t\t\tverbose(env, \"R%d pointer arithmetic of map value goes out of range, \"\n\t\t\t\t\"prohibited for !root\\n\", dst);\n\t\t\treturn -EACCES;\n\t\t} else if (dst_reg->type == PTR_TO_STACK &&\n\t\t\t   check_stack_access(env, dst_reg, dst_reg->off +\n\t\t\t\t\t      dst_reg->var_off.value, 1)) {\n\t\t\tverbose(env, \"R%d stack pointer arithmetic goes out of range, \"\n\t\t\t\t\"prohibited for !root\\n\", dst);\n\t\t\treturn -EACCES;\n\t\t}\n\t}\n\n\treturn 0;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 3300,
        "critical_vars": [
            "ret"
        ],
        "function": "adjust_ptr_min_max_vals",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "static int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn,\n\t\t\t\t   const struct bpf_reg_state *ptr_reg,\n\t\t\t\t   const struct bpf_reg_state *off_reg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg;\n\tbool known = tnum_is_const(off_reg->var_off);\n\ts64 smin_val = off_reg->smin_value, smax_val = off_reg->smax_value,\n\t    smin_ptr = ptr_reg->smin_value, smax_ptr = ptr_reg->smax_value;\n\tu64 umin_val = off_reg->umin_value, umax_val = off_reg->umax_value,\n\t    umin_ptr = ptr_reg->umin_value, umax_ptr = ptr_reg->umax_value;\n\tu32 dst = insn->dst_reg, src = insn->src_reg;\n\tu8 opcode = BPF_OP(insn->code);\n\tint ret;\n\n\tdst_reg = &regs[dst];\n\n\tif ((known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops on pointers produce (meaningless) scalars */\n\t\tverbose(env,\n\t\t\t\"R%d 32-bit pointer arithmetic prohibited\\n\",\n\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\n\tswitch (ptr_reg->type) {\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited, null-check it first\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tcase CONST_PTR_TO_MAP:\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (!env->allow_ptr_leaks && !known && (smin_val < 0) != (smax_val < 0)) {\n\t\t\tverbose(env, \"R%d has unknown scalar with mixed signed bounds, pointer arithmetic with it prohibited for !root\\n\",\n\t\t\t\toff_reg == dst_reg ? dst : src);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* fall-through */\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* In case of 'scalar += pointer', dst_reg inherits pointer type and id.\n\t * The id may be overwritten later if we create a new variable offset.\n\t */\n\tdst_reg->type = ptr_reg->type;\n\tdst_reg->id = ptr_reg->id;\n\n\tif (!check_reg_sane_offset(env, off_reg, ptr_reg->type) ||\n\t    !check_reg_sane_offset(env, ptr_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, dst_reg, smin_val < 0);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to add from different maps or paths\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\t/* We can take a fixed offset as long as it doesn't overflow\n\t\t * the s32 'off' field\n\t\t */\n\t\tif (known && (ptr_reg->off + smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off + smin_val))) {\n\t\t\t/* pointer += K.  Accumulate it into fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->off = ptr_reg->off + smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  Note that off_reg->off\n\t\t * == 0, since it's a scalar.\n\t\t * dst_reg gets the pointer type and since some positive\n\t\t * integer value was added to the pointer, give it a new 'id'\n\t\t * if it's a PTR_TO_PACKET.\n\t\t * this creates a new 'base' pointer, off_reg (variable) gets\n\t\t * added into the variable offset, and we copy the fixed offset\n\t\t * from ptr_reg.\n\t\t */\n\t\tif (signed_add_overflows(smin_ptr, smin_val) ||\n\t\t    signed_add_overflows(smax_ptr, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr + smin_val;\n\t\t\tdst_reg->smax_value = smax_ptr + smax_val;\n\t\t}\n\t\tif (umin_ptr + umin_val < umin_ptr ||\n\t\t    umax_ptr + umax_val < umax_ptr) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value = umin_ptr + umin_val;\n\t\t\tdst_reg->umax_value = umax_ptr + umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tdst_reg->raw = 0;\n\t\t}\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, dst_reg, smin_val < 0);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to sub from different maps or paths\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\tif (dst_reg == off_reg) {\n\t\t\t/* scalar -= pointer.  Creates an unknown scalar */\n\t\t\tverbose(env, \"R%d tried to subtract pointer from scalar\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* We don't allow subtraction from FP, because (according to\n\t\t * test_verifier.c test \"invalid fp arithmetic\", JITs might not\n\t\t * be able to deal with it.\n\t\t */\n\t\tif (ptr_reg->type == PTR_TO_STACK) {\n\t\t\tverbose(env, \"R%d subtraction from stack pointer prohibited\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (known && (ptr_reg->off - smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off - smin_val))) {\n\t\t\t/* pointer -= K.  Subtract it from fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->id = ptr_reg->id;\n\t\t\tdst_reg->off = ptr_reg->off - smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  If the subtrahend is known\n\t\t * nonnegative, then any reg->range we had before is still good.\n\t\t */\n\t\tif (signed_sub_overflows(smin_ptr, smax_val) ||\n\t\t    signed_sub_overflows(smax_ptr, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr - smax_val;\n\t\t\tdst_reg->smax_value = smax_ptr - smin_val;\n\t\t}\n\t\tif (umin_ptr < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value = umin_ptr - umax_val;\n\t\t\tdst_reg->umax_value = umax_ptr - umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tif (smin_val < 0)\n\t\t\t\tdst_reg->raw = 0;\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\tcase BPF_OR:\n\tcase BPF_XOR:\n\t\t/* bitwise ops on pointers are troublesome, prohibit. */\n\t\tverbose(env, \"R%d bitwise operator %s on pointer prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\tdefault:\n\t\t/* other operators (e.g. MUL,LSH) produce non-pointer results */\n\t\tverbose(env, \"R%d pointer arithmetic with %s operator prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!check_reg_sane_offset(env, dst_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t__update_reg_bounds(dst_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\n\t/* For unprivileged we require that resulting offset must be in bounds\n\t * in order to be able to sanitize access later on.\n\t */\n\tif (!env->allow_ptr_leaks) {\n\t\tif (dst_reg->type == PTR_TO_MAP_VALUE &&\n\t\t    check_map_access(env, dst, dst_reg->off, 1, false)) {\n\t\t\tverbose(env, \"R%d pointer arithmetic of map value goes out of range, \"\n\t\t\t\t\"prohibited for !root\\n\", dst);\n\t\t\treturn -EACCES;\n\t\t} else if (dst_reg->type == PTR_TO_STACK &&\n\t\t\t   check_stack_access(env, dst_reg, dst_reg->off +\n\t\t\t\t\t      dst_reg->var_off.value, 1)) {\n\t\t\tverbose(env, \"R%d stack pointer arithmetic goes out of range, \"\n\t\t\t\t\"prohibited for !root\\n\", dst);\n\t\t\treturn -EACCES;\n\t\t}\n\t}\n\n\treturn 0;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 3301,
        "critical_vars": [
            "ret"
        ],
        "function": "adjust_ptr_min_max_vals",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "static int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn,\n\t\t\t\t   const struct bpf_reg_state *ptr_reg,\n\t\t\t\t   const struct bpf_reg_state *off_reg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg;\n\tbool known = tnum_is_const(off_reg->var_off);\n\ts64 smin_val = off_reg->smin_value, smax_val = off_reg->smax_value,\n\t    smin_ptr = ptr_reg->smin_value, smax_ptr = ptr_reg->smax_value;\n\tu64 umin_val = off_reg->umin_value, umax_val = off_reg->umax_value,\n\t    umin_ptr = ptr_reg->umin_value, umax_ptr = ptr_reg->umax_value;\n\tu32 dst = insn->dst_reg, src = insn->src_reg;\n\tu8 opcode = BPF_OP(insn->code);\n\tint ret;\n\n\tdst_reg = &regs[dst];\n\n\tif ((known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops on pointers produce (meaningless) scalars */\n\t\tverbose(env,\n\t\t\t\"R%d 32-bit pointer arithmetic prohibited\\n\",\n\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\n\tswitch (ptr_reg->type) {\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited, null-check it first\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tcase CONST_PTR_TO_MAP:\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (!env->allow_ptr_leaks && !known && (smin_val < 0) != (smax_val < 0)) {\n\t\t\tverbose(env, \"R%d has unknown scalar with mixed signed bounds, pointer arithmetic with it prohibited for !root\\n\",\n\t\t\t\toff_reg == dst_reg ? dst : src);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* fall-through */\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* In case of 'scalar += pointer', dst_reg inherits pointer type and id.\n\t * The id may be overwritten later if we create a new variable offset.\n\t */\n\tdst_reg->type = ptr_reg->type;\n\tdst_reg->id = ptr_reg->id;\n\n\tif (!check_reg_sane_offset(env, off_reg, ptr_reg->type) ||\n\t    !check_reg_sane_offset(env, ptr_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, dst_reg, smin_val < 0);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to add from different maps or paths\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\t/* We can take a fixed offset as long as it doesn't overflow\n\t\t * the s32 'off' field\n\t\t */\n\t\tif (known && (ptr_reg->off + smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off + smin_val))) {\n\t\t\t/* pointer += K.  Accumulate it into fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->off = ptr_reg->off + smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  Note that off_reg->off\n\t\t * == 0, since it's a scalar.\n\t\t * dst_reg gets the pointer type and since some positive\n\t\t * integer value was added to the pointer, give it a new 'id'\n\t\t * if it's a PTR_TO_PACKET.\n\t\t * this creates a new 'base' pointer, off_reg (variable) gets\n\t\t * added into the variable offset, and we copy the fixed offset\n\t\t * from ptr_reg.\n\t\t */\n\t\tif (signed_add_overflows(smin_ptr, smin_val) ||\n\t\t    signed_add_overflows(smax_ptr, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr + smin_val;\n\t\t\tdst_reg->smax_value = smax_ptr + smax_val;\n\t\t}\n\t\tif (umin_ptr + umin_val < umin_ptr ||\n\t\t    umax_ptr + umax_val < umax_ptr) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value = umin_ptr + umin_val;\n\t\t\tdst_reg->umax_value = umax_ptr + umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tdst_reg->raw = 0;\n\t\t}\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, dst_reg, smin_val < 0);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to sub from different maps or paths\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\tif (dst_reg == off_reg) {\n\t\t\t/* scalar -= pointer.  Creates an unknown scalar */\n\t\t\tverbose(env, \"R%d tried to subtract pointer from scalar\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* We don't allow subtraction from FP, because (according to\n\t\t * test_verifier.c test \"invalid fp arithmetic\", JITs might not\n\t\t * be able to deal with it.\n\t\t */\n\t\tif (ptr_reg->type == PTR_TO_STACK) {\n\t\t\tverbose(env, \"R%d subtraction from stack pointer prohibited\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (known && (ptr_reg->off - smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off - smin_val))) {\n\t\t\t/* pointer -= K.  Subtract it from fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->id = ptr_reg->id;\n\t\t\tdst_reg->off = ptr_reg->off - smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  If the subtrahend is known\n\t\t * nonnegative, then any reg->range we had before is still good.\n\t\t */\n\t\tif (signed_sub_overflows(smin_ptr, smax_val) ||\n\t\t    signed_sub_overflows(smax_ptr, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr - smax_val;\n\t\t\tdst_reg->smax_value = smax_ptr - smin_val;\n\t\t}\n\t\tif (umin_ptr < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value = umin_ptr - umax_val;\n\t\t\tdst_reg->umax_value = umax_ptr - umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tif (smin_val < 0)\n\t\t\t\tdst_reg->raw = 0;\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\tcase BPF_OR:\n\tcase BPF_XOR:\n\t\t/* bitwise ops on pointers are troublesome, prohibit. */\n\t\tverbose(env, \"R%d bitwise operator %s on pointer prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\tdefault:\n\t\t/* other operators (e.g. MUL,LSH) produce non-pointer results */\n\t\tverbose(env, \"R%d pointer arithmetic with %s operator prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!check_reg_sane_offset(env, dst_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t__update_reg_bounds(dst_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\n\t/* For unprivileged we require that resulting offset must be in bounds\n\t * in order to be able to sanitize access later on.\n\t */\n\tif (!env->allow_ptr_leaks) {\n\t\tif (dst_reg->type == PTR_TO_MAP_VALUE &&\n\t\t    check_map_access(env, dst, dst_reg->off, 1, false)) {\n\t\t\tverbose(env, \"R%d pointer arithmetic of map value goes out of range, \"\n\t\t\t\t\"prohibited for !root\\n\", dst);\n\t\t\treturn -EACCES;\n\t\t} else if (dst_reg->type == PTR_TO_STACK &&\n\t\t\t   check_stack_access(env, dst_reg, dst_reg->off +\n\t\t\t\t\t      dst_reg->var_off.value, 1)) {\n\t\t\tverbose(env, \"R%d stack pointer arithmetic goes out of range, \"\n\t\t\t\t\"prohibited for !root\\n\", dst);\n\t\t\treturn -EACCES;\n\t\t}\n\t}\n\n\treturn 0;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 3302,
        "critical_vars": [
            "dst",
            "env"
        ],
        "function": "adjust_ptr_min_max_vals",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "static int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn,\n\t\t\t\t   const struct bpf_reg_state *ptr_reg,\n\t\t\t\t   const struct bpf_reg_state *off_reg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg;\n\tbool known = tnum_is_const(off_reg->var_off);\n\ts64 smin_val = off_reg->smin_value, smax_val = off_reg->smax_value,\n\t    smin_ptr = ptr_reg->smin_value, smax_ptr = ptr_reg->smax_value;\n\tu64 umin_val = off_reg->umin_value, umax_val = off_reg->umax_value,\n\t    umin_ptr = ptr_reg->umin_value, umax_ptr = ptr_reg->umax_value;\n\tu32 dst = insn->dst_reg, src = insn->src_reg;\n\tu8 opcode = BPF_OP(insn->code);\n\tint ret;\n\n\tdst_reg = &regs[dst];\n\n\tif ((known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops on pointers produce (meaningless) scalars */\n\t\tverbose(env,\n\t\t\t\"R%d 32-bit pointer arithmetic prohibited\\n\",\n\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\n\tswitch (ptr_reg->type) {\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited, null-check it first\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tcase CONST_PTR_TO_MAP:\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (!env->allow_ptr_leaks && !known && (smin_val < 0) != (smax_val < 0)) {\n\t\t\tverbose(env, \"R%d has unknown scalar with mixed signed bounds, pointer arithmetic with it prohibited for !root\\n\",\n\t\t\t\toff_reg == dst_reg ? dst : src);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* fall-through */\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* In case of 'scalar += pointer', dst_reg inherits pointer type and id.\n\t * The id may be overwritten later if we create a new variable offset.\n\t */\n\tdst_reg->type = ptr_reg->type;\n\tdst_reg->id = ptr_reg->id;\n\n\tif (!check_reg_sane_offset(env, off_reg, ptr_reg->type) ||\n\t    !check_reg_sane_offset(env, ptr_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, dst_reg, smin_val < 0);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to add from different maps or paths\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\t/* We can take a fixed offset as long as it doesn't overflow\n\t\t * the s32 'off' field\n\t\t */\n\t\tif (known && (ptr_reg->off + smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off + smin_val))) {\n\t\t\t/* pointer += K.  Accumulate it into fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->off = ptr_reg->off + smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  Note that off_reg->off\n\t\t * == 0, since it's a scalar.\n\t\t * dst_reg gets the pointer type and since some positive\n\t\t * integer value was added to the pointer, give it a new 'id'\n\t\t * if it's a PTR_TO_PACKET.\n\t\t * this creates a new 'base' pointer, off_reg (variable) gets\n\t\t * added into the variable offset, and we copy the fixed offset\n\t\t * from ptr_reg.\n\t\t */\n\t\tif (signed_add_overflows(smin_ptr, smin_val) ||\n\t\t    signed_add_overflows(smax_ptr, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr + smin_val;\n\t\t\tdst_reg->smax_value = smax_ptr + smax_val;\n\t\t}\n\t\tif (umin_ptr + umin_val < umin_ptr ||\n\t\t    umax_ptr + umax_val < umax_ptr) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value = umin_ptr + umin_val;\n\t\t\tdst_reg->umax_value = umax_ptr + umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tdst_reg->raw = 0;\n\t\t}\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, dst_reg, smin_val < 0);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to sub from different maps or paths\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\tif (dst_reg == off_reg) {\n\t\t\t/* scalar -= pointer.  Creates an unknown scalar */\n\t\t\tverbose(env, \"R%d tried to subtract pointer from scalar\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* We don't allow subtraction from FP, because (according to\n\t\t * test_verifier.c test \"invalid fp arithmetic\", JITs might not\n\t\t * be able to deal with it.\n\t\t */\n\t\tif (ptr_reg->type == PTR_TO_STACK) {\n\t\t\tverbose(env, \"R%d subtraction from stack pointer prohibited\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (known && (ptr_reg->off - smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off - smin_val))) {\n\t\t\t/* pointer -= K.  Subtract it from fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->id = ptr_reg->id;\n\t\t\tdst_reg->off = ptr_reg->off - smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  If the subtrahend is known\n\t\t * nonnegative, then any reg->range we had before is still good.\n\t\t */\n\t\tif (signed_sub_overflows(smin_ptr, smax_val) ||\n\t\t    signed_sub_overflows(smax_ptr, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr - smax_val;\n\t\t\tdst_reg->smax_value = smax_ptr - smin_val;\n\t\t}\n\t\tif (umin_ptr < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value = umin_ptr - umax_val;\n\t\t\tdst_reg->umax_value = umax_ptr - umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tif (smin_val < 0)\n\t\t\t\tdst_reg->raw = 0;\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\tcase BPF_OR:\n\tcase BPF_XOR:\n\t\t/* bitwise ops on pointers are troublesome, prohibit. */\n\t\tverbose(env, \"R%d bitwise operator %s on pointer prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\tdefault:\n\t\t/* other operators (e.g. MUL,LSH) produce non-pointer results */\n\t\tverbose(env, \"R%d pointer arithmetic with %s operator prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!check_reg_sane_offset(env, dst_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t__update_reg_bounds(dst_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\n\t/* For unprivileged we require that resulting offset must be in bounds\n\t * in order to be able to sanitize access later on.\n\t */\n\tif (!env->allow_ptr_leaks) {\n\t\tif (dst_reg->type == PTR_TO_MAP_VALUE &&\n\t\t    check_map_access(env, dst, dst_reg->off, 1, false)) {\n\t\t\tverbose(env, \"R%d pointer arithmetic of map value goes out of range, \"\n\t\t\t\t\"prohibited for !root\\n\", dst);\n\t\t\treturn -EACCES;\n\t\t} else if (dst_reg->type == PTR_TO_STACK &&\n\t\t\t   check_stack_access(env, dst_reg, dst_reg->off +\n\t\t\t\t\t      dst_reg->var_off.value, 1)) {\n\t\t\tverbose(env, \"R%d stack pointer arithmetic goes out of range, \"\n\t\t\t\t\"prohibited for !root\\n\", dst);\n\t\t\treturn -EACCES;\n\t\t}\n\t}\n\n\treturn 0;\n}"
    },
    {
        "patch_model": "Replace",
        "change_type": "Assignment",
        "line_old": 4392,
        "line_new": 4502,
        "critical_vars": [
            "other_branch"
        ],
        "function": "check_cond_jmp_op",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic int check_cond_jmp_op(struct bpf_verifier_env *env,\n\t\t\t     struct bpf_insn *insn, int *insn_idx)\n{\n\tstruct bpf_verifier_state *this_branch = env->cur_state;\n\tstruct bpf_verifier_state *other_branch;\n\tstruct bpf_reg_state *regs = this_branch->frame[this_branch->curframe]->regs;\n\tstruct bpf_reg_state *dst_reg, *other_branch_regs;\n\tu8 opcode = BPF_OP(insn->code);\n\tint err;\n\n\tif (opcode > BPF_JSLE) {\n\t\tverbose(env, \"invalid BPF_JMP opcode %x\\n\", opcode);\n\t\treturn -EINVAL;\n\t}\n\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tif (insn->imm != 0) {\n\t\t\tverbose(env, \"BPF_JMP uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/* check src1 operand */\n\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (is_pointer_value(env, insn->src_reg)) {\n\t\t\tverbose(env, \"R%d pointer comparison prohibited\\n\",\n\t\t\t\tinsn->src_reg);\n\t\t\treturn -EACCES;\n\t\t}\n\t} else {\n\t\tif (insn->src_reg != BPF_REG_0) {\n\t\t\tverbose(env, \"BPF_JMP uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t/* check src2 operand */\n\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tdst_reg = &regs[insn->dst_reg];\n\n\tif (BPF_SRC(insn->code) == BPF_K) {\n\t\tint pred = is_branch_taken(dst_reg, insn->imm, opcode);\n\n\t\tif (pred == 1) {\n\t\t\t /* only follow the goto, ignore fall-through */\n\t\t\t*insn_idx += insn->off;\n\t\t\treturn 0;\n\t\t} else if (pred == 0) {\n\t\t\t/* only follow fall-through branch, since\n\t\t\t * that's where the program will go\n\t\t\t */\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tother_branch = push_stack(env, *insn_idx + insn->off + 1, *insn_idx,\n\t\t\t\t  false);\n\tif (!other_branch)\n\t\treturn -EFAULT;\n\tother_branch_regs = other_branch->frame[other_branch->curframe]->regs;\n\n\t/* detect if we are comparing against a constant value so we can adjust\n\t * our min/max values for our dst register.\n\t * this is only legit if both are scalars (or pointers to the same\n\t * object, I suppose, but we don't support that right now), because\n\t * otherwise the different base pointers mean the offsets aren't\n\t * comparable.\n\t */\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tif (dst_reg->type == SCALAR_VALUE &&\n\t\t    regs[insn->src_reg].type == SCALAR_VALUE) {\n\t\t\tif (tnum_is_const(regs[insn->src_reg].var_off))\n\t\t\t\treg_set_min_max(&other_branch_regs[insn->dst_reg],\n\t\t\t\t\t\tdst_reg, regs[insn->src_reg].var_off.value,\n\t\t\t\t\t\topcode);\n\t\t\telse if (tnum_is_const(dst_reg->var_off))\n\t\t\t\treg_set_min_max_inv(&other_branch_regs[insn->src_reg],\n\t\t\t\t\t\t    &regs[insn->src_reg],\n\t\t\t\t\t\t    dst_reg->var_off.value, opcode);\n\t\t\telse if (opcode == BPF_JEQ || opcode == BPF_JNE)\n\t\t\t\t/* Comparing for equality, we can combine knowledge */\n\t\t\t\treg_combine_min_max(&other_branch_regs[insn->src_reg],\n\t\t\t\t\t\t    &other_branch_regs[insn->dst_reg],\n\t\t\t\t\t\t    &regs[insn->src_reg],\n\t\t\t\t\t\t    &regs[insn->dst_reg], opcode);\n\t\t}\n\t} else if (dst_reg->type == SCALAR_VALUE) {\n\t\treg_set_min_max(&other_branch_regs[insn->dst_reg],\n\t\t\t\t\tdst_reg, insn->imm, opcode);\n\t}\n\n\t/* detect if R == 0 where R is returned from bpf_map_lookup_elem() */\n\tif (BPF_SRC(insn->code) == BPF_K &&\n\t    insn->imm == 0 && (opcode == BPF_JEQ || opcode == BPF_JNE) &&\n\t    reg_type_may_be_null(dst_reg->type)) {\n\t\t/* Mark all identical registers in each branch as either\n\t\t * safe or unknown depending R == 0 or R != 0 conditional.\n\t\t */\n\t\tmark_ptr_or_null_regs(this_branch, insn->dst_reg,\n\t\t\t\t      opcode == BPF_JNE);\n\t\tmark_ptr_or_null_regs(other_branch, insn->dst_reg,\n\t\t\t\t      opcode == BPF_JEQ);\n\t} else if (!try_match_pkt_pointers(insn, dst_reg, &regs[insn->src_reg],\n\t\t\t\t\t   this_branch, other_branch) &&\n\t\t   is_pointer_value(env, insn->dst_reg)) {\n\t\tverbose(env, \"R%d pointer comparison prohibited\\n\",\n\t\t\tinsn->dst_reg);\n\t\treturn -EACCES;\n\t}\n\tif (env->log.level)\n\t\tprint_verifier_state(env, this_branch->frame[this_branch->curframe]);\n\treturn 0;\n}\n\n/* return the map pointer stored inside BPF_LD_IMM64 instruction */\nstatic struct bpf_map *ld_imm64_to_map_ptr(struct bpf_insn *insn)\n{\n\tu64 imm64 = ((u64) (u32) insn[0].imm) | ((u64) (u32) insn[1].imm) << 32;\n\n\treturn (struct bpf_map *) (unsigned long) imm64;\n}\n\n/* verify BPF_LD_IMM64 instruction */\nstatic int check_ld_imm(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tint err;\n\n\tif (BPF_SIZE(insn->code) != BPF_DW) {\n\t\tverbose(env, \"invalid BPF_LD_IMM insn\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (insn->off != 0) {\n\t\tverbose(env, \"BPF_LD_IMM64 uses reserved fields\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\terr = check_reg_arg(env, insn->dst_reg, DST_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (insn->src_reg == 0) {\n\t\tu64 imm = ((u64)(insn + 1)->imm << 32) | (u32)insn->imm;\n\n\t\tregs[insn->dst_reg].type = SCALAR_VALUE;\n\t\t__mark_reg_known(&regs[insn->dst_reg], imm);\n\t\treturn 0;\n\t}\n\n\t/* replace_map_fd_with_map_ptr() should have caught bad ld_imm64 */\n\tBUG_ON(insn->src_reg != BPF_PSEUDO_MAP_FD);\n\n\tregs[insn->dst_reg].type = CONST_PTR_TO_MAP;\n\tregs[insn->dst_reg].map_ptr = ld_imm64_to_map_ptr(insn);\n\treturn 0;\n}\n\nstatic bool may_access_skb(enum bpf_prog_type type)\n{\n\tswitch (type) {\n\tcase BPF_PROG_TYPE_SOCKET_FILTER:\n\tcase BPF_PROG_TYPE_SCHED_CLS:\n\tcase BPF_PROG_TYPE_SCHED_ACT:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n/* verify safety of LD_ABS|LD_IND instructions:\n * - they can only appear in the programs where ctx == skb\n * - since they are wrappers of function calls, they scratch R1-R5 registers,\n *   preserve R6-R9, and store return value into R0\n *\n * Implicit input:\n *   ctx == skb == R6 == CTX\n *\n * Explicit input:\n *   SRC == any register\n *   IMM == 32-bit immediate\n *\n * Output:\n *   R0 - 8/16/32-bit skb data converted to cpu endianness\n */\nstatic int check_ld_abs(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 mode = BPF_MODE(insn->code);\n\tint i, err;\n\n\tif (!may_access_skb(env->prog->type)) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] instructions not allowed for this program type\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!env->ops->gen_ld_abs) {\n\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (env->subprog_cnt > 1) {\n\t\t/* when program has LD_ABS insn JITs and interpreter assume\n\t\t * that r1 == ctx == skb which is not the case for callees\n\t\t * that can have arbitrary arguments. It's problematic\n\t\t * for main prog as well since JITs would need to analyze\n\t\t * all functions in order to make proper register save/restore\n\t\t * decisions in the main prog. Hence disallow LD_ABS with calls\n\t\t */\n\t\tverbose(env, \"BPF_LD_[ABS|IND] instructions cannot be mixed with bpf-to-bpf calls\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (insn->dst_reg != BPF_REG_0 || insn->off != 0 ||\n\t    BPF_SIZE(insn->code) == BPF_DW ||\n\t    (mode == BPF_ABS && insn->src_reg != BPF_REG_0)) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] uses reserved fields\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* check whether implicit source operand (register R6) is readable */\n\terr = check_reg_arg(env, BPF_REG_6, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\t/* Disallow usage of BPF_LD_[ABS|IND] with reference tracking, as\n\t * gen_ld_abs() may terminate the program at runtime, leading to\n\t * reference leak.\n\t */\n\terr = check_reference_leak(env);\n\tif (err) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] cannot be mixed with socket references\\n\");\n\t\treturn err;\n\t}\n\n\tif (regs[BPF_REG_6].type != PTR_TO_CTX) {\n\t\tverbose(env,\n\t\t\t\"at the time of BPF_LD_ABS|IND R6 != pointer to skb\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (mode == BPF_IND) {\n\t\t/* check explicit source operand */\n\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\t/* reset caller saved regs to unreadable */\n\tfor (i = 0; i < CALLER_SAVED_REGS; i++) {\n\t\tmark_reg_not_init(env, regs, caller_saved[i]);\n\t\tcheck_reg_arg(env, caller_saved[i], DST_OP_NO_MARK);\n\t}\n\n\t/* mark destination R0 register as readable, since it contains\n\t * the value fetched from the packet.\n\t * Already marked as written above.\n\t */\n\tmark_reg_unknown(env, regs, BPF_REG_0);\n\treturn 0;\n}\n\nstatic int check_return_code(struct bpf_verifier_env *env)\n{\n\tstruct bpf_reg_state *reg;\n\tstruct tnum range = tnum_range(0, 1);\n\n\tswitch (env->prog->type) {\n\tcase BPF_PROG_TYPE_CGROUP_SKB:\n\tcase BPF_PROG_TYPE_CGROUP_SOCK:\n\tcase BPF_PROG_TYPE_CGROUP_SOCK_ADDR:\n\tcase BPF_PROG_TYPE_SOCK_OPS:\n\tcase BPF_PROG_TYPE_CGROUP_DEVICE:\n\t\tbreak;\n\tdefault:\n\t\treturn 0;\n\t}\n\n\treg = cur_regs(env) + BPF_REG_0;\n\tif (reg->type != SCALAR_VALUE) {\n\t\tverbose(env, \"At program exit the register R0 is not a known value (%s)\\n\",\n\t\t\treg_type_str[reg->type]);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!tnum_in(range, reg->var_off)) {\n\t\tverbose(env, \"At program exit the register R0 \");\n\t\tif (!tnum_is_unknown(reg->var_off)) {\n\t\t\tchar tn_buf[48];\n\n\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\tverbose(env, \"has value %s\", tn_buf);\n\t\t} else {\n\t\t\tverbose(env, \"has unknown scalar value\");\n\t\t}\n\t\tverbose(env, \" should have been 0 or 1\\n\");\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\n/* non-recursive DFS pseudo code\n * 1  procedure DFS-iterative(G,v):\n * 2      label v as discovered\n * 3      let S be a stack\n * 4      S.push(v)\n * 5      while S is not empty\n * 6            t <- S.pop()\n * 7            if t is what we're looking for:\n * 8                return t\n * 9            for all edges e in G.adjacentEdges(t) do\n * 10               if edge e is already labelled\n * 11                   continue with the next edge\n * 12               w <- G.adjacentVertex(t,e)\n * 13               if vertex w is not discovered and not explored\n * 14                   label e as tree-edge\n * 15                   label w as discovered\n * 16                   S.push(w)\n * 17                   continue at 5\n * 18               else if vertex w is discovered\n * 19                   label e as back-edge\n * 20               else\n * 21                   // vertex w is explored\n * 22                   label e as forward- or cross-edge\n * 23           label t as explored\n * 24           S.pop()\n *\n * convention:\n * 0x10 - discovered\n * 0x11 - discovered and fall-through edge labelled\n * 0x12 - discovered and fall-through and branch edges labelled\n * 0x20 - explored\n */\n\nenum {\n\tDISCOVERED = 0x10,\n\tEXPLORED = 0x20,\n\tFALLTHROUGH = 1,\n\tBRANCH = 2,\n};\n\n#define STATE_LIST_MARK ((struct bpf_verifier_state_list *) -1L)\n\nstatic int *insn_stack;\t/* stack of insns to process */\nstatic int cur_stack;\t/* current stack index */\nstatic int *insn_state;\n\n/* t, w, e - match pseudo-code above:\n * t - index of current instruction\n * w - next instruction\n * e - edge\n */\nstatic int push_insn(int t, int w, int e, struct bpf_verifier_env *env)\n{\n\tif (e == FALLTHROUGH && insn_state[t] >= (DISCOVERED | FALLTHROUGH))\n\t\treturn 0;\n\n\tif (e == BRANCH && insn_state[t] >= (DISCOVERED | BRANCH))\n\t\treturn 0;\n\n\tif (w < 0 || w >= env->prog->len) {\n\t\tverbose_linfo(env, t, \"%d: \", t);\n\t\tverbose(env, \"jump out of range from insn %d to %d\\n\", t, w);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e == BRANCH)\n\t\t/* mark branch target for state pruning */\n\t\tenv->explored_states[w] = STATE_LIST_MARK;\n\n\tif (insn_state[w] == 0) {\n\t\t/* tree-edge */\n\t\tinsn_state[t] = DISCOVERED | e;\n\t\tinsn_state[w] = DISCOVERED;\n\t\tif (cur_stack >= env->prog->len)\n\t\t\treturn -E2BIG;\n\t\tinsn_stack[cur_stack++] = w;\n\t\treturn 1;\n\t} else if ((insn_state[w] & 0xF0) == DISCOVERED) {\n\t\tverbose_linfo(env, t, \"%d: \", t);\n\t\tverbose_linfo(env, w, \"%d: \", w);\n\t\tverbose(env, \"back-edge from insn %d to %d\\n\", t, w);\n\t\treturn -EINVAL;\n\t} else if (insn_state[w] == EXPLORED) {\n\t\t/* forward- or cross-edge */\n\t\tinsn_state[t] = DISCOVERED | e;\n\t} else {\n\t\tverbose(env, \"insn state internal bug\\n\");\n\t\treturn -EFAULT;\n\t}\n\treturn 0;\n}\n\n/* non-recursive depth-first-search to detect loops in BPF program\n * loop == back-edge in directed graph\n */\nstatic int check_cfg(struct bpf_verifier_env *env)\n{\n\tstruct bpf_insn *insns = env->prog->insnsi;\n\tint insn_cnt = env->prog->len;\n\tint ret = 0;\n\tint i, t;\n\n\tinsn_state = kcalloc(insn_cnt, sizeof(int), GFP_KERNEL);\n\tif (!insn_state)\n\t\treturn -ENOMEM;\n\n\tinsn_stack = kcalloc(insn_cnt, sizeof(int), GFP_KERNEL);\n\tif (!insn_stack) {\n\t\tkfree(insn_state);\n\t\treturn -ENOMEM;\n\t}\n\n\tinsn_state[0] = DISCOVERED; /* mark 1st insn as discovered */\n\tinsn_stack[0] = 0; /* 0 is the first instruction */\n\tcur_stack = 1;\n\npeek_stack:\n\tif (cur_stack == 0)\n\t\tgoto check_state;\n\tt = insn_stack[cur_stack - 1];\n\n\tif (BPF_CLASS(insns[t].code) == BPF_JMP) {\n\t\tu8 opcode = BPF_OP(insns[t].code);\n\n\t\tif (opcode == BPF_EXIT) {\n\t\t\tgoto mark_explored;\n\t\t} else if (opcode == BPF_CALL) {\n\t\t\tret = push_insn(t, t + 1, FALLTHROUGH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t\tif (t + 1 < insn_cnt)\n\t\t\t\tenv->explored_states[t + 1] = STATE_LIST_MARK;\n\t\t\tif (insns[t].src_reg == BPF_PSEUDO_CALL) {\n\t\t\t\tenv->explored_states[t] = STATE_LIST_MARK;\n\t\t\t\tret = push_insn(t, t + insns[t].imm + 1, BRANCH, env);\n\t\t\t\tif (ret == 1)\n\t\t\t\t\tgoto peek_stack;\n\t\t\t\telse if (ret < 0)\n\t\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t} else if (opcode == BPF_JA) {\n\t\t\tif (BPF_SRC(insns[t].code) != BPF_K) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t\t/* unconditional jump with single edge */\n\t\t\tret = push_insn(t, t + insns[t].off + 1,\n\t\t\t\t\tFALLTHROUGH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t\t/* tell verifier to check for equivalent states\n\t\t\t * after every call and jump\n\t\t\t */\n\t\t\tif (t + 1 < insn_cnt)\n\t\t\t\tenv->explored_states[t + 1] = STATE_LIST_MARK;\n\t\t} else {\n\t\t\t/* conditional jump with two edges */\n\t\t\tenv->explored_states[t] = STATE_LIST_MARK;\n\t\t\tret = push_insn(t, t + 1, FALLTHROUGH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\n\t\t\tret = push_insn(t, t + insns[t].off + 1, BRANCH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t}\n\t} else {\n\t\t/* all other non-branch instructions with single\n\t\t * fall-through edge\n\t\t */\n\t\tret = push_insn(t, t + 1, FALLTHROUGH, env);\n\t\tif (ret == 1)\n\t\t\tgoto peek_stack;\n\t\telse if (ret < 0)\n\t\t\tgoto err_free;\n\t}\n\nmark_explored:\n\tinsn_state[t] = EXPLORED;\n\tif (cur_stack-- <= 0) {\n\t\tverbose(env, \"pop stack internal bug\\n\");\n\t\tret = -EFAULT;\n\t\tgoto err_free;\n\t}\n\tgoto peek_stack;\n\ncheck_state:\n\tfor (i = 0; i < insn_cnt; i++) {\n\t\tif (insn_state[i] != EXPLORED) {\n\t\t\tverbose(env, \"unreachable insn %d\\n\", i);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\t}\n\tret = 0; /* cfg looks good */\n\nerr_free:\n\tkfree(insn_state);\n\tkfree(insn_stack);\n\treturn ret;\n}\n\n/* The minimum supported BTF func info size */\n#define MIN_BPF_FUNCINFO_SIZE\t8\n#define MAX_FUNCINFO_REC_SIZE\t252\n\nstatic int check_btf_func(struct bpf_verifier_env *env,\n\t\t\t  const union bpf_attr *attr,\n\t\t\t  union bpf_attr __user *uattr)\n{\n\tu32 i, nfuncs, urec_size, min_size, prev_offset;\n\tu32 krec_size = sizeof(struct bpf_func_info);\n\tstruct bpf_func_info *krecord;\n\tconst struct btf_type *type;\n\tstruct bpf_prog *prog;\n\tconst struct btf *btf;\n\tvoid __user *urecord;\n\tint ret = 0;\n\n\tnfuncs = attr->func_info_cnt;\n\tif (!nfuncs)\n\t\treturn 0;\n\n\tif (nfuncs != env->subprog_cnt) {\n\t\tverbose(env, \"number of funcs in func_info doesn't match number of subprogs\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\turec_size = attr->func_info_rec_size;\n\tif (urec_size < MIN_BPF_FUNCINFO_SIZE ||\n\t    urec_size > MAX_FUNCINFO_REC_SIZE ||\n\t    urec_size % sizeof(u32)) {\n\t\tverbose(env, \"invalid func info rec size %u\\n\", urec_size);\n\t\treturn -EINVAL;\n\t}\n\n\tprog = env->prog;\n\tbtf = prog->aux->btf;\n\n\turecord = u64_to_user_ptr(attr->func_info);\n\tmin_size = min_t(u32, krec_size, urec_size);\n\n\tkrecord = kvcalloc(nfuncs, krec_size, GFP_KERNEL | __GFP_NOWARN);\n\tif (!krecord)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < nfuncs; i++) {\n\t\tret = bpf_check_uarg_tail_zero(urecord, krec_size, urec_size);\n\t\tif (ret) {\n\t\t\tif (ret == -E2BIG) {\n\t\t\t\tverbose(env, \"nonzero tailing record in func info\");\n\t\t\t\t/* set the size kernel expects so loader can zero\n\t\t\t\t * out the rest of the record.\n\t\t\t\t */\n\t\t\t\tif (put_user(min_size, &uattr->func_info_rec_size))\n\t\t\t\t\tret = -EFAULT;\n\t\t\t}\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (copy_from_user(&krecord[i], urecord, min_size)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/* check insn_off */\n\t\tif (i == 0) {\n\t\t\tif (krecord[i].insn_off) {\n\t\t\t\tverbose(env,\n\t\t\t\t\t\"nonzero insn_off %u for the first func info record\",\n\t\t\t\t\tkrecord[i].insn_off);\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t} else if (krecord[i].insn_off <= prev_offset) {\n\t\t\tverbose(env,\n\t\t\t\t\"same or smaller insn offset (%u) than previous func info record (%u)\",\n\t\t\t\tkrecord[i].insn_off, prev_offset);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (env->subprog_info[i].start != krecord[i].insn_off) {\n\t\t\tverbose(env, \"func_info BTF section doesn't match subprog layout in BPF program\\n\");\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/* check type_id */\n\t\ttype = btf_type_by_id(btf, krecord[i].type_id);\n\t\tif (!type || BTF_INFO_KIND(type->info) != BTF_KIND_FUNC) {\n\t\t\tverbose(env, \"invalid type id %d in func info\",\n\t\t\t\tkrecord[i].type_id);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tprev_offset = krecord[i].insn_off;\n\t\turecord += urec_size;\n\t}\n\n\tprog->aux->func_info = krecord;\n\tprog->aux->func_info_cnt = nfuncs;\n\treturn 0;\n\nerr_free:\n\tkvfree(krecord);\n\treturn ret;\n}\n\nstatic void adjust_btf_func(struct bpf_verifier_env *env)\n{\n\tint i;\n\n\tif (!env->prog->aux->func_info)\n\t\treturn;\n\n\tfor (i = 0; i < env->subprog_cnt; i++)\n\t\tenv->prog->aux->func_info[i].insn_off = env->subprog_info[i].start;\n}\n\n#define MIN_BPF_LINEINFO_SIZE\t(offsetof(struct bpf_line_info, line_col) + \\\n\t\tsizeof(((struct bpf_line_info *)(0))->line_col))\n#define MAX_LINEINFO_REC_SIZE\tMAX_FUNCINFO_REC_SIZE\n\nstatic int check_btf_line(struct bpf_verifier_env *env,\n\t\t\t  const union bpf_attr *attr,\n\t\t\t  union bpf_attr __user *uattr)\n{\n\tu32 i, s, nr_linfo, ncopy, expected_size, rec_size, prev_offset = 0;\n\tstruct bpf_subprog_info *sub;\n\tstruct bpf_line_info *linfo;\n\tstruct bpf_prog *prog;\n\tconst struct btf *btf;\n\tvoid __user *ulinfo;\n\tint err;\n\n\tnr_linfo = attr->line_info_cnt;\n\tif (!nr_linfo)\n\t\treturn 0;\n\n\trec_size = attr->line_info_rec_size;\n\tif (rec_size < MIN_BPF_LINEINFO_SIZE ||\n\t    rec_size > MAX_LINEINFO_REC_SIZE ||\n\t    rec_size & (sizeof(u32) - 1))\n\t\treturn -EINVAL;\n\n\t/* Need to zero it in case the userspace may\n\t * pass in a smaller bpf_line_info object.\n\t */\n\tlinfo = kvcalloc(nr_linfo, sizeof(struct bpf_line_info),\n\t\t\t GFP_KERNEL | __GFP_NOWARN);\n\tif (!linfo)\n\t\treturn -ENOMEM;\n\n\tprog = env->prog;\n\tbtf = prog->aux->btf;\n\n\ts = 0;\n\tsub = env->subprog_info;\n\tulinfo = u64_to_user_ptr(attr->line_info);\n\texpected_size = sizeof(struct bpf_line_info);\n\tncopy = min_t(u32, expected_size, rec_size);\n\tfor (i = 0; i < nr_linfo; i++) {\n\t\terr = bpf_check_uarg_tail_zero(ulinfo, expected_size, rec_size);\n\t\tif (err) {\n\t\t\tif (err == -E2BIG) {\n\t\t\t\tverbose(env, \"nonzero tailing record in line_info\");\n\t\t\t\tif (put_user(expected_size,\n\t\t\t\t\t     &uattr->line_info_rec_size))\n\t\t\t\t\terr = -EFAULT;\n\t\t\t}\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (copy_from_user(&linfo[i], ulinfo, ncopy)) {\n\t\t\terr = -EFAULT;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/*\n\t\t * Check insn_off to ensure\n\t\t * 1) strictly increasing AND\n\t\t * 2) bounded by prog->len\n\t\t *\n\t\t * The linfo[0].insn_off == 0 check logically falls into\n\t\t * the later \"missing bpf_line_info for func...\" case\n\t\t * because the first linfo[0].insn_off must be the\n\t\t * first sub also and the first sub must have\n\t\t * subprog_info[0].start == 0.\n\t\t */\n\t\tif ((i && linfo[i].insn_off <= prev_offset) ||\n\t\t    linfo[i].insn_off >= prog->len) {\n\t\t\tverbose(env, \"Invalid line_info[%u].insn_off:%u (prev_offset:%u prog->len:%u)\\n\",\n\t\t\t\ti, linfo[i].insn_off, prev_offset,\n\t\t\t\tprog->len);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (!prog->insnsi[linfo[i].insn_off].code) {\n\t\t\tverbose(env,\n\t\t\t\t\"Invalid insn code at line_info[%u].insn_off\\n\",\n\t\t\t\ti);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (!btf_name_by_offset(btf, linfo[i].line_off) ||\n\t\t    !btf_name_by_offset(btf, linfo[i].file_name_off)) {\n\t\t\tverbose(env, \"Invalid line_info[%u].line_off or .file_name_off\\n\", i);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (s != env->subprog_cnt) {\n\t\t\tif (linfo[i].insn_off == sub[s].start) {\n\t\t\t\tsub[s].linfo_idx = i;\n\t\t\t\ts++;\n\t\t\t} else if (sub[s].start < linfo[i].insn_off) {\n\t\t\t\tverbose(env, \"missing bpf_line_info for func#%u\\n\", s);\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t}\n\n\t\tprev_offset = linfo[i].insn_off;\n\t\tulinfo += rec_size;\n\t}\n\n\tif (s != env->subprog_cnt) {\n\t\tverbose(env, \"missing bpf_line_info for %u funcs starting from func#%u\\n\",\n\t\t\tenv->subprog_cnt - s, s);\n\t\terr = -EINVAL;\n\t\tgoto err_free;\n\t}\n\n\tprog->aux->linfo = linfo;\n\tprog->aux->nr_linfo = nr_linfo;\n\n\treturn 0;\n\nerr_free:\n\tkvfree(linfo);\n\treturn err;\n}\n\nstatic int check_btf_info(struct bpf_verifier_env *env,\n\t\t\t  const union bpf_attr *attr,\n\t\t\t  union bpf_attr __user *uattr)\n{\n\tstruct btf *btf;\n\tint err;\n\n\tif (!attr->func_info_cnt && !attr->line_info_cnt)\n\t\treturn 0;\n\n\tbtf = btf_get_by_fd(attr->prog_btf_fd);\n\tif (IS_ERR(btf))\n\t\treturn PTR_ERR(btf);\n\tenv->prog->aux->btf = btf;\n\n\terr = check_btf_func(env, attr, uattr);\n\tif (err)\n\t\treturn err;\n\n\terr = check_btf_line(env, attr, uattr);\n\tif (err)\n\t\treturn err;\n\n\treturn 0;\n}\n\n/* check %cur's range satisfies %old's */\nstatic bool range_within(struct bpf_reg_state *old,\n\t\t\t struct bpf_reg_state *cur)\n{\n\treturn old->umin_value <= cur->umin_value &&\n\t       old->umax_value >= cur->umax_value &&\n\t       old->smin_value <= cur->smin_value &&\n\t       old->smax_value >= cur->smax_value;\n}\n\n/* Maximum number of register states that can exist at once */\n#define ID_MAP_SIZE\t(MAX_BPF_REG + MAX_BPF_STACK / BPF_REG_SIZE)\nstruct idpair {\n\tu32 old;\n\tu32 cur;\n};\n\n/* If in the old state two registers had the same id, then they need to have\n * the same id in the new state as well.  But that id could be different from\n * the old state, so we need to track the mapping from old to new ids.\n * Once we have seen that, say, a reg with old id 5 had new id 9, any subsequent\n * regs with old id 5 must also have new id 9 for the new state to be safe.  But\n * regs with a different old id could still have new id 9, we don't care about\n * that.\n * So we look through our idmap to see if this old id has been seen before.  If\n * so, we require the new id to match; otherwise, we add the id pair to the map.\n */\nstatic bool check_ids(u32 old_id, u32 cur_id, struct idpair *idmap)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < ID_MAP_SIZE; i++) {\n\t\tif (!idmap[i].old) {\n\t\t\t/* Reached an empty slot; haven't seen this id before */\n\t\t\tidmap[i].old = old_id;\n\t\t\tidmap[i].cur = cur_id;\n\t\t\treturn true;\n\t\t}\n\t\tif (idmap[i].old == old_id)\n\t\t\treturn idmap[i].cur == cur_id;\n\t}\n\t/* We ran out of idmap slots, which should be impossible */\n\tWARN_ON_ONCE(1);\n\treturn false;\n}\n\nstatic void clean_func_state(struct bpf_verifier_env *env,\n\t\t\t     struct bpf_func_state *st)\n{\n\tenum bpf_reg_liveness live;\n\tint i, j;\n\n\tfor (i = 0; i < BPF_REG_FP; i++) {\n\t\tlive = st->regs[i].live;\n\t\t/* liveness must not touch this register anymore */\n\t\tst->regs[i].live |= REG_LIVE_DONE;\n\t\tif (!(live & REG_LIVE_READ))\n\t\t\t/* since the register is unused, clear its state\n\t\t\t * to make further comparison simpler\n\t\t\t */\n\t\t\t__mark_reg_not_init(&st->regs[i]);\n\t}\n\n\tfor (i = 0; i < st->allocated_stack / BPF_REG_SIZE; i++) {\n\t\tlive = st->stack[i].spilled_ptr.live;\n\t\t/* liveness must not touch this stack slot anymore */\n\t\tst->stack[i].spilled_ptr.live |= REG_LIVE_DONE;\n\t\tif (!(live & REG_LIVE_READ)) {\n\t\t\t__mark_reg_not_init(&st->stack[i].spilled_ptr);\n\t\t\tfor (j = 0; j < BPF_REG_SIZE; j++)\n\t\t\t\tst->stack[i].slot_type[j] = STACK_INVALID;\n\t\t}\n\t}\n}\n\nstatic void clean_verifier_state(struct bpf_verifier_env *env,\n\t\t\t\t struct bpf_verifier_state *st)\n{\n\tint i;\n\n\tif (st->frame[0]->regs[0].live & REG_LIVE_DONE)\n\t\t/* all regs in this state in all frames were already marked */\n\t\treturn;\n\n\tfor (i = 0; i <= st->curframe; i++)\n\t\tclean_func_state(env, st->frame[i]);\n}\n\n/* the parentage chains form a tree.\n * the verifier states are added to state lists at given insn and\n * pushed into state stack for future exploration.\n * when the verifier reaches bpf_exit insn some of the verifer states\n * stored in the state lists have their final liveness state already,\n * but a lot of states will get revised from liveness point of view when\n * the verifier explores other branches.\n * Example:\n * 1: r0 = 1\n * 2: if r1 == 100 goto pc+1\n * 3: r0 = 2\n * 4: exit\n * when the verifier reaches exit insn the register r0 in the state list of\n * insn 2 will be seen as !REG_LIVE_READ. Then the verifier pops the other_branch\n * of insn 2 and goes exploring further. At the insn 4 it will walk the\n * parentage chain from insn 4 into insn 2 and will mark r0 as REG_LIVE_READ.\n *\n * Since the verifier pushes the branch states as it sees them while exploring\n * the program the condition of walking the branch instruction for the second\n * time means that all states below this branch were already explored and\n * their final liveness markes are already propagated.\n * Hence when the verifier completes the search of state list in is_state_visited()\n * we can call this clean_live_states() function to mark all liveness states\n * as REG_LIVE_DONE to indicate that 'parent' pointers of 'struct bpf_reg_state'\n * will not be used.\n * This function also clears the registers and stack for states that !READ\n * to simplify state merging.\n *\n * Important note here that walking the same branch instruction in the callee\n * doesn't meant that the states are DONE. The verifier has to compare\n * the callsites\n */\nstatic void clean_live_states(struct bpf_verifier_env *env, int insn,\n\t\t\t      struct bpf_verifier_state *cur)\n{\n\tstruct bpf_verifier_state_list *sl;\n\tint i;\n\n\tsl = env->explored_states[insn];\n\tif (!sl)\n\t\treturn;\n\n\twhile (sl != STATE_LIST_MARK) {\n\t\tif (sl->state.curframe != cur->curframe)\n\t\t\tgoto next;\n\t\tfor (i = 0; i <= cur->curframe; i++)\n\t\t\tif (sl->state.frame[i]->callsite != cur->frame[i]->callsite)\n\t\t\t\tgoto next;\n\t\tclean_verifier_state(env, &sl->state);\nnext:\n\t\tsl = sl->next;\n\t}\n}\n\n/* Returns true if (rold safe implies rcur safe) */\nstatic bool regsafe(struct bpf_reg_state *rold, struct bpf_reg_state *rcur,\n\t\t    struct idpair *idmap)\n{\n\tbool equal;\n\n\tif (!(rold->live & REG_LIVE_READ))\n\t\t/* explored state didn't use this */\n\t\treturn true;\n\n\tequal = memcmp(rold, rcur, offsetof(struct bpf_reg_state, parent)) == 0;\n\n\tif (rold->type == PTR_TO_STACK)\n\t\t/* two stack pointers are equal only if they're pointing to\n\t\t * the same stack frame, since fp-8 in foo != fp-8 in bar\n\t\t */\n\t\treturn equal && rold->frameno == rcur->frameno;\n\n\tif (equal)\n\t\treturn true;\n\n\tif (rold->type == NOT_INIT)\n\t\t/* explored state can't have used this */\n\t\treturn true;\n\tif (rcur->type == NOT_INIT)\n\t\treturn false;\n\tswitch (rold->type) {\n\tcase SCALAR_VALUE:\n\t\tif (rcur->type == SCALAR_VALUE) {\n\t\t\t/* new val must satisfy old val knowledge */\n\t\t\treturn range_within(rold, rcur) &&\n\t\t\t       tnum_in(rold->var_off, rcur->var_off);\n\t\t} else {\n\t\t\t/* We're trying to use a pointer in place of a scalar.\n\t\t\t * Even if the scalar was unbounded, this could lead to\n\t\t\t * pointer leaks because scalars are allowed to leak\n\t\t\t * while pointers are not. We could make this safe in\n\t\t\t * special cases if root is calling us, but it's\n\t\t\t * probably not worth the hassle.\n\t\t\t */\n\t\t\treturn false;\n\t\t}\n\tcase PTR_TO_MAP_VALUE:\n\t\t/* If the new min/max/var_off satisfy the old ones and\n\t\t * everything else matches, we are OK.\n\t\t * We don't care about the 'id' value, because nothing\n\t\t * uses it for PTR_TO_MAP_VALUE (only for ..._OR_NULL)\n\t\t */\n\t\treturn memcmp(rold, rcur, offsetof(struct bpf_reg_state, id)) == 0 &&\n\t\t       range_within(rold, rcur) &&\n\t\t       tnum_in(rold->var_off, rcur->var_off);\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\t\t/* a PTR_TO_MAP_VALUE could be safe to use as a\n\t\t * PTR_TO_MAP_VALUE_OR_NULL into the same map.\n\t\t * However, if the old PTR_TO_MAP_VALUE_OR_NULL then got NULL-\n\t\t * checked, doing so could have affected others with the same\n\t\t * id, and we can't check for that because we lost the id when\n\t\t * we converted to a PTR_TO_MAP_VALUE.\n\t\t */\n\t\tif (rcur->type != PTR_TO_MAP_VALUE_OR_NULL)\n\t\t\treturn false;\n\t\tif (memcmp(rold, rcur, offsetof(struct bpf_reg_state, id)))\n\t\t\treturn false;\n\t\t/* Check our ids match any regs they're supposed to */\n\t\treturn check_ids(rold->id, rcur->id, idmap);\n\tcase PTR_TO_PACKET_META:\n\tcase PTR_TO_PACKET:\n\t\tif (rcur->type != rold->type)\n\t\t\treturn false;\n\t\t/* We must have at least as much range as the old ptr\n\t\t * did, so that any accesses which were safe before are\n\t\t * still safe.  This is true even if old range < old off,\n\t\t * since someone could have accessed through (ptr - k), or\n\t\t * even done ptr -= k in a register, to get a safe access.\n\t\t */\n\t\tif (rold->range > rcur->range)\n\t\t\treturn false;\n\t\t/* If the offsets don't match, we can't trust our alignment;\n\t\t * nor can we be sure that we won't fall out of range.\n\t\t */\n\t\tif (rold->off != rcur->off)\n\t\t\treturn false;\n\t\t/* id relations must be preserved */\n\t\tif (rold->id && !check_ids(rold->id, rcur->id, idmap))\n\t\t\treturn false;\n\t\t/* new val must satisfy old val knowledge */\n\t\treturn range_within(rold, rcur) &&\n\t\t       tnum_in(rold->var_off, rcur->var_off);\n\tcase PTR_TO_CTX:\n\tcase CONST_PTR_TO_MAP:\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_FLOW_KEYS:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\t\t/* Only valid matches are exact, which memcmp() above\n\t\t * would have accepted\n\t\t */\n\tdefault:\n\t\t/* Don't know what's going on, just say it's not safe */\n\t\treturn false;\n\t}\n\n\t/* Shouldn't get here; if we do, say it's not safe */\n\tWARN_ON_ONCE(1);\n\treturn false;\n}\n\nstatic bool stacksafe(struct bpf_func_state *old,\n\t\t      struct bpf_func_state *cur,\n\t\t      struct idpair *idmap)\n{\n\tint i, spi;\n\n\t/* walk slots of the explored stack and ignore any additional\n\t * slots in the current stack, since explored(safe) state\n\t * didn't use them\n\t */\n\tfor (i = 0; i < old->allocated_stack; i++) {\n\t\tspi = i / BPF_REG_SIZE;\n\n\t\tif (!(old->stack[spi].spilled_ptr.live & REG_LIVE_READ)) {\n\t\t\ti += BPF_REG_SIZE - 1;\n\t\t\t/* explored state didn't use this */\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (old->stack[spi].slot_type[i % BPF_REG_SIZE] == STACK_INVALID)\n\t\t\tcontinue;\n\n\t\t/* explored stack has more populated slots than current stack\n\t\t * and these slots were used\n\t\t */\n\t\tif (i >= cur->allocated_stack)\n\t\t\treturn false;\n\n\t\t/* if old state was safe with misc data in the stack\n\t\t * it will be safe with zero-initialized stack.\n\t\t * The opposite is not true\n\t\t */\n\t\tif (old->stack[spi].slot_type[i % BPF_REG_SIZE] == STACK_MISC &&\n\t\t    cur->stack[spi].slot_type[i % BPF_REG_SIZE] == STACK_ZERO)\n\t\t\tcontinue;\n\t\tif (old->stack[spi].slot_type[i % BPF_REG_SIZE] !=\n\t\t    cur->stack[spi].slot_type[i % BPF_REG_SIZE])\n\t\t\t/* Ex: old explored (safe) state has STACK_SPILL in\n\t\t\t * this stack slot, but current has has STACK_MISC ->\n\t\t\t * this verifier states are not equivalent,\n\t\t\t * return false to continue verification of this path\n\t\t\t */\n\t\t\treturn false;\n\t\tif (i % BPF_REG_SIZE)\n\t\t\tcontinue;\n\t\tif (old->stack[spi].slot_type[0] != STACK_SPILL)\n\t\t\tcontinue;\n\t\tif (!regsafe(&old->stack[spi].spilled_ptr,\n\t\t\t     &cur->stack[spi].spilled_ptr,\n\t\t\t     idmap))\n\t\t\t/* when explored and current stack slot are both storing\n\t\t\t * spilled registers, check that stored pointers types\n\t\t\t * are the same as well.\n\t\t\t * Ex: explored safe path could have stored\n\t\t\t * (bpf_reg_state) {.type = PTR_TO_STACK, .off = -8}\n\t\t\t * but current path has stored:\n\t\t\t * (bpf_reg_state) {.type = PTR_TO_STACK, .off = -16}\n\t\t\t * such verifier states are not equivalent.\n\t\t\t * return false to continue verification of this path\n\t\t\t */\n\t\t\treturn false;\n\t}\n\treturn true;\n}\n\nstatic bool refsafe(struct bpf_func_state *old, struct bpf_func_state *cur)\n{\n\tif (old->acquired_refs != cur->acquired_refs)\n\t\treturn false;\n\treturn !memcmp(old->refs, cur->refs,\n\t\t       sizeof(*old->refs) * old->acquired_refs);\n}\n\n/* compare two verifier states\n *\n * all states stored in state_list are known to be valid, since\n * verifier reached 'bpf_exit' instruction through them\n *\n * this function is called when verifier exploring different branches of\n * execution popped from the state stack. If it sees an old state that has\n * more strict register state and more strict stack state then this execution\n * branch doesn't need to be explored further, since verifier already\n * concluded that more strict state leads to valid finish.\n *\n * Therefore two states are equivalent if register state is more conservative\n * and explored stack state is more conservative than the current one.\n * Example:\n *       explored                   current\n * (slot1=INV slot2=MISC) == (slot1=MISC slot2=MISC)\n * (slot1=MISC slot2=MISC) != (slot1=INV slot2=MISC)\n *\n * In other words if current stack state (one being explored) has more\n * valid slots than old one that already passed validation, it means\n * the verifier can stop exploring and conclude that current state is valid too\n *\n * Similarly with registers. If explored state has register type as invalid\n * whereas register type in current state is meaningful, it means that\n * the current state will reach 'bpf_exit' instruction safely\n */\nstatic bool func_states_equal(struct bpf_func_state *old,\n\t\t\t      struct bpf_func_state *cur)\n{\n\tstruct idpair *idmap;\n\tbool ret = false;\n\tint i;\n\n\tidmap = kcalloc(ID_MAP_SIZE, sizeof(struct idpair), GFP_KERNEL);\n\t/* If we failed to allocate the idmap, just say it's not safe */\n\tif (!idmap)\n\t\treturn false;\n\n\tfor (i = 0; i < MAX_BPF_REG; i++) {\n\t\tif (!regsafe(&old->regs[i], &cur->regs[i], idmap))\n\t\t\tgoto out_free;\n\t}\n\n\tif (!stacksafe(old, cur, idmap))\n\t\tgoto out_free;\n\n\tif (!refsafe(old, cur))\n\t\tgoto out_free;\n\tret = true;\nout_free:\n\tkfree(idmap);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 5616,
        "critical_vars": [
            "cur->speculative",
            "old->speculative"
        ],
        "function": "states_equal",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic bool states_equal(struct bpf_verifier_env *env,\n\t\t\t struct bpf_verifier_state *old,\n\t\t\t struct bpf_verifier_state *cur)\n{\n\tint i;\n\n\tif (old->curframe != cur->curframe)\n\t\treturn false;\n\n\t/* Verification state from speculative execution simulation\n\t * must never prune a non-speculative execution one.\n\t */\n\tif (old->speculative && !cur->speculative)\n\t\treturn false;\n\n\t/* for states to be equal callsites have to be the same\n\t * and all frame states need to be equivalent\n\t */\n\tfor (i = 0; i <= old->curframe; i++) {\n\t\tif (old->frame[i]->callsite != cur->frame[i]->callsite)\n\t\t\treturn false;\n\t\tif (!func_states_equal(old->frame[i], cur->frame[i]))\n\t\t\treturn false;\n\t}\n\treturn true;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 5820,
        "critical_vars": [
            "state->speculative"
        ],
        "function": "do_check",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic int do_check(struct bpf_verifier_env *env)\n{\n\tstruct bpf_verifier_state *state;\n\tstruct bpf_insn *insns = env->prog->insnsi;\n\tstruct bpf_reg_state *regs;\n\tint insn_cnt = env->prog->len, i;\n\tint insn_processed = 0;\n\tbool do_print_state = false;\n\n\tenv->prev_linfo = NULL;\n\n\tstate = kzalloc(sizeof(struct bpf_verifier_state), GFP_KERNEL);\n\tif (!state)\n\t\treturn -ENOMEM;\n\tstate->curframe = 0;\n\tstate->speculative = false;\n\tstate->frame[0] = kzalloc(sizeof(struct bpf_func_state), GFP_KERNEL);\n\tif (!state->frame[0]) {\n\t\tkfree(state);\n\t\treturn -ENOMEM;\n\t}\n\tenv->cur_state = state;\n\tinit_func_state(env, state->frame[0],\n\t\t\tBPF_MAIN_FUNC /* callsite */,\n\t\t\t0 /* frameno */,\n\t\t\t0 /* subprogno, zero == main subprog */);\n\n\tfor (;;) {\n\t\tstruct bpf_insn *insn;\n\t\tu8 class;\n\t\tint err;\n\n\t\tif (env->insn_idx >= insn_cnt) {\n\t\t\tverbose(env, \"invalid insn idx %d insn_cnt %d\\n\",\n\t\t\t\tenv->insn_idx, insn_cnt);\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\tinsn = &insns[env->insn_idx];\n\t\tclass = BPF_CLASS(insn->code);\n\n\t\tif (++insn_processed > BPF_COMPLEXITY_LIMIT_INSNS) {\n\t\t\tverbose(env,\n\t\t\t\t\"BPF program is too large. Processed %d insn\\n\",\n\t\t\t\tinsn_processed);\n\t\t\treturn -E2BIG;\n\t\t}\n\n\t\terr = is_state_visited(env, env->insn_idx);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t\tif (err == 1) {\n\t\t\t/* found equivalent state, can prune the search */\n\t\t\tif (env->log.level) {\n\t\t\t\tif (do_print_state)\n\t\t\t\t\tverbose(env, \"\\nfrom %d to %d%s: safe\\n\",\n\t\t\t\t\t\tenv->prev_insn_idx, env->insn_idx,\n\t\t\t\t\t\tenv->cur_state->speculative ?\n\t\t\t\t\t\t\" (speculative execution)\" : \"\");\n\t\t\t\telse\n\t\t\t\t\tverbose(env, \"%d: safe\\n\", env->insn_idx);\n\t\t\t}\n\t\t\tgoto process_bpf_exit;\n\t\t}\n\n\t\tif (signal_pending(current))\n\t\t\treturn -EAGAIN;\n\n\t\tif (need_resched())\n\t\t\tcond_resched();\n\n\t\tif (env->log.level > 1 || (env->log.level && do_print_state)) {\n\t\t\tif (env->log.level > 1)\n\t\t\t\tverbose(env, \"%d:\", env->insn_idx);\n\t\t\telse\n\t\t\t\tverbose(env, \"\\nfrom %d to %d%s:\",\n\t\t\t\t\tenv->prev_insn_idx, env->insn_idx,\n\t\t\t\t\tenv->cur_state->speculative ?\n\t\t\t\t\t\" (speculative execution)\" : \"\");\n\n... (function end not found)"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 5860,
        "critical_vars": [
            "env->cur_state->speculative"
        ],
        "function": "do_check",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic int do_check(struct bpf_verifier_env *env)\n{\n\tstruct bpf_verifier_state *state;\n\tstruct bpf_insn *insns = env->prog->insnsi;\n\tstruct bpf_reg_state *regs;\n\tint insn_cnt = env->prog->len, i;\n\tint insn_processed = 0;\n\tbool do_print_state = false;\n\n\tenv->prev_linfo = NULL;\n\n\tstate = kzalloc(sizeof(struct bpf_verifier_state), GFP_KERNEL);\n\tif (!state)\n\t\treturn -ENOMEM;\n\tstate->curframe = 0;\n\tstate->speculative = false;\n\tstate->frame[0] = kzalloc(sizeof(struct bpf_func_state), GFP_KERNEL);\n\tif (!state->frame[0]) {\n\t\tkfree(state);\n\t\treturn -ENOMEM;\n\t}\n\tenv->cur_state = state;\n\tinit_func_state(env, state->frame[0],\n\t\t\tBPF_MAIN_FUNC /* callsite */,\n\t\t\t0 /* frameno */,\n\t\t\t0 /* subprogno, zero == main subprog */);\n\n\tfor (;;) {\n\t\tstruct bpf_insn *insn;\n\t\tu8 class;\n\t\tint err;\n\n\t\tif (env->insn_idx >= insn_cnt) {\n\t\t\tverbose(env, \"invalid insn idx %d insn_cnt %d\\n\",\n\t\t\t\tenv->insn_idx, insn_cnt);\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\tinsn = &insns[env->insn_idx];\n\t\tclass = BPF_CLASS(insn->code);\n\n\t\tif (++insn_processed > BPF_COMPLEXITY_LIMIT_INSNS) {\n\t\t\tverbose(env,\n\t\t\t\t\"BPF program is too large. Processed %d insn\\n\",\n\t\t\t\tinsn_processed);\n\t\t\treturn -E2BIG;\n\t\t}\n\n\t\terr = is_state_visited(env, env->insn_idx);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t\tif (err == 1) {\n\t\t\t/* found equivalent state, can prune the search */\n\t\t\tif (env->log.level) {\n\t\t\t\tif (do_print_state)\n\t\t\t\t\tverbose(env, \"\\nfrom %d to %d%s: safe\\n\",\n\t\t\t\t\t\tenv->prev_insn_idx, env->insn_idx,\n\t\t\t\t\t\tenv->cur_state->speculative ?\n\t\t\t\t\t\t\" (speculative execution)\" : \"\");\n\t\t\t\telse\n\t\t\t\t\tverbose(env, \"%d: safe\\n\", env->insn_idx);\n\t\t\t}\n\t\t\tgoto process_bpf_exit;\n\t\t}\n\n\t\tif (signal_pending(current))\n\t\t\treturn -EAGAIN;\n\n\t\tif (need_resched())\n\t\t\tcond_resched();\n\n\t\tif (env->log.level > 1 || (env->log.level && do_print_state)) {\n\t\t\tif (env->log.level > 1)\n\t\t\t\tverbose(env, \"%d:\", env->insn_idx);\n\t\t\telse\n\t\t\t\tverbose(env, \"\\nfrom %d to %d%s:\",\n\t\t\t\t\tenv->prev_insn_idx, env->insn_idx,\n\t\t\t\t\tenv->cur_state->speculative ?\n\t\t\t\t\t\" (speculative execution)\" : \"\");\n\n... (function end not found)"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 5880,
        "critical_vars": [
            "env->cur_state->speculative"
        ],
        "function": "do_check",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic int do_check(struct bpf_verifier_env *env)\n{\n\tstruct bpf_verifier_state *state;\n\tstruct bpf_insn *insns = env->prog->insnsi;\n\tstruct bpf_reg_state *regs;\n\tint insn_cnt = env->prog->len, i;\n\tint insn_processed = 0;\n\tbool do_print_state = false;\n\n\tenv->prev_linfo = NULL;\n\n\tstate = kzalloc(sizeof(struct bpf_verifier_state), GFP_KERNEL);\n\tif (!state)\n\t\treturn -ENOMEM;\n\tstate->curframe = 0;\n\tstate->speculative = false;\n\tstate->frame[0] = kzalloc(sizeof(struct bpf_func_state), GFP_KERNEL);\n\tif (!state->frame[0]) {\n\t\tkfree(state);\n\t\treturn -ENOMEM;\n\t}\n\tenv->cur_state = state;\n\tinit_func_state(env, state->frame[0],\n\t\t\tBPF_MAIN_FUNC /* callsite */,\n\t\t\t0 /* frameno */,\n\t\t\t0 /* subprogno, zero == main subprog */);\n\n\tfor (;;) {\n\t\tstruct bpf_insn *insn;\n\t\tu8 class;\n\t\tint err;\n\n\t\tif (env->insn_idx >= insn_cnt) {\n\t\t\tverbose(env, \"invalid insn idx %d insn_cnt %d\\n\",\n\t\t\t\tenv->insn_idx, insn_cnt);\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\tinsn = &insns[env->insn_idx];\n\t\tclass = BPF_CLASS(insn->code);\n\n\t\tif (++insn_processed > BPF_COMPLEXITY_LIMIT_INSNS) {\n\t\t\tverbose(env,\n\t\t\t\t\"BPF program is too large. Processed %d insn\\n\",\n\t\t\t\tinsn_processed);\n\t\t\treturn -E2BIG;\n\t\t}\n\n\t\terr = is_state_visited(env, env->insn_idx);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t\tif (err == 1) {\n\t\t\t/* found equivalent state, can prune the search */\n\t\t\tif (env->log.level) {\n\t\t\t\tif (do_print_state)\n\t\t\t\t\tverbose(env, \"\\nfrom %d to %d%s: safe\\n\",\n\t\t\t\t\t\tenv->prev_insn_idx, env->insn_idx,\n\t\t\t\t\t\tenv->cur_state->speculative ?\n\t\t\t\t\t\t\" (speculative execution)\" : \"\");\n\t\t\t\telse\n\t\t\t\t\tverbose(env, \"%d: safe\\n\", env->insn_idx);\n\t\t\t}\n\t\t\tgoto process_bpf_exit;\n\t\t}\n\n\t\tif (signal_pending(current))\n\t\t\treturn -EAGAIN;\n\n\t\tif (need_resched())\n\t\t\tcond_resched();\n\n\t\tif (env->log.level > 1 || (env->log.level && do_print_state)) {\n\t\t\tif (env->log.level > 1)\n\t\t\t\tverbose(env, \"%d:\", env->insn_idx);\n\t\t\telse\n\t\t\t\tverbose(env, \"\\nfrom %d to %d%s:\",\n\t\t\t\t\tenv->prev_insn_idx, env->insn_idx,\n\t\t\t\t\tenv->cur_state->speculative ?\n\t\t\t\t\t\" (speculative execution)\" : \"\");\n\n... (function end not found)"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 6875,
        "critical_vars": [
            "insn->code"
        ],
        "function": "fixup_bpf_calls",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "static int fixup_bpf_calls(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tstruct bpf_insn *insn = prog->insnsi;\n\tconst struct bpf_func_proto *fn;\n\tconst int insn_cnt = prog->len;\n\tconst struct bpf_map_ops *ops;\n\tstruct bpf_insn_aux_data *aux;\n\tstruct bpf_insn insn_buf[16];\n\tstruct bpf_prog *new_prog;\n\tstruct bpf_map *map_ptr;\n\tint i, cnt, delta = 0;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (insn->code == (BPF_ALU64 | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\tbool is64 = BPF_CLASS(insn->code) == BPF_ALU64;\n\t\t\tstruct bpf_insn mask_and_div[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx div 0 -> 0 */\n\t\t\t\tBPF_JMP_IMM(BPF_JNE, insn->src_reg, 0, 2),\n\t\t\t\tBPF_ALU32_REG(BPF_XOR, insn->dst_reg, insn->dst_reg),\n\t\t\t\tBPF_JMP_IMM(BPF_JA, 0, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn mask_and_mod[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx mod 0 -> Rx */\n\t\t\t\tBPF_JMP_IMM(BPF_JEQ, insn->src_reg, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn *patchlet;\n\n\t\t\tif (insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\t\tpatchlet = mask_and_div + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_div) - (is64 ? 1 : 0);\n\t\t\t} else {\n\t\t\t\tpatchlet = mask_and_mod + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_mod) - (is64 ? 1 : 0);\n\t\t\t}\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, patchlet, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (BPF_CLASS(insn->code) == BPF_LD &&\n\t\t    (BPF_MODE(insn->code) == BPF_ABS ||\n\t\t     BPF_MODE(insn->code) == BPF_IND)) {\n\t\t\tcnt = env->ops->gen_ld_abs(insn, insn_buf);\n\t\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\n... (function end not found)"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 6877,
        "critical_vars": [
            "code_add"
        ],
        "function": "fixup_bpf_calls",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "static int fixup_bpf_calls(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tstruct bpf_insn *insn = prog->insnsi;\n\tconst struct bpf_func_proto *fn;\n\tconst int insn_cnt = prog->len;\n\tconst struct bpf_map_ops *ops;\n\tstruct bpf_insn_aux_data *aux;\n\tstruct bpf_insn insn_buf[16];\n\tstruct bpf_prog *new_prog;\n\tstruct bpf_map *map_ptr;\n\tint i, cnt, delta = 0;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (insn->code == (BPF_ALU64 | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\tbool is64 = BPF_CLASS(insn->code) == BPF_ALU64;\n\t\t\tstruct bpf_insn mask_and_div[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx div 0 -> 0 */\n\t\t\t\tBPF_JMP_IMM(BPF_JNE, insn->src_reg, 0, 2),\n\t\t\t\tBPF_ALU32_REG(BPF_XOR, insn->dst_reg, insn->dst_reg),\n\t\t\t\tBPF_JMP_IMM(BPF_JA, 0, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn mask_and_mod[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx mod 0 -> Rx */\n\t\t\t\tBPF_JMP_IMM(BPF_JEQ, insn->src_reg, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn *patchlet;\n\n\t\t\tif (insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\t\tpatchlet = mask_and_div + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_div) - (is64 ? 1 : 0);\n\t\t\t} else {\n\t\t\t\tpatchlet = mask_and_mod + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_mod) - (is64 ? 1 : 0);\n\t\t\t}\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, patchlet, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (BPF_CLASS(insn->code) == BPF_LD &&\n\t\t    (BPF_MODE(insn->code) == BPF_ABS ||\n\t\t     BPF_MODE(insn->code) == BPF_IND)) {\n\t\t\tcnt = env->ops->gen_ld_abs(insn, insn_buf);\n\t\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\n... (function end not found)"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 6878,
        "critical_vars": [
            "code_sub"
        ],
        "function": "fixup_bpf_calls",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "static int fixup_bpf_calls(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tstruct bpf_insn *insn = prog->insnsi;\n\tconst struct bpf_func_proto *fn;\n\tconst int insn_cnt = prog->len;\n\tconst struct bpf_map_ops *ops;\n\tstruct bpf_insn_aux_data *aux;\n\tstruct bpf_insn insn_buf[16];\n\tstruct bpf_prog *new_prog;\n\tstruct bpf_map *map_ptr;\n\tint i, cnt, delta = 0;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (insn->code == (BPF_ALU64 | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\tbool is64 = BPF_CLASS(insn->code) == BPF_ALU64;\n\t\t\tstruct bpf_insn mask_and_div[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx div 0 -> 0 */\n\t\t\t\tBPF_JMP_IMM(BPF_JNE, insn->src_reg, 0, 2),\n\t\t\t\tBPF_ALU32_REG(BPF_XOR, insn->dst_reg, insn->dst_reg),\n\t\t\t\tBPF_JMP_IMM(BPF_JA, 0, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn mask_and_mod[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx mod 0 -> Rx */\n\t\t\t\tBPF_JMP_IMM(BPF_JEQ, insn->src_reg, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn *patchlet;\n\n\t\t\tif (insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\t\tpatchlet = mask_and_div + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_div) - (is64 ? 1 : 0);\n\t\t\t} else {\n\t\t\t\tpatchlet = mask_and_mod + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_mod) - (is64 ? 1 : 0);\n\t\t\t}\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, patchlet, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (BPF_CLASS(insn->code) == BPF_LD &&\n\t\t    (BPF_MODE(insn->code) == BPF_ABS ||\n\t\t     BPF_MODE(insn->code) == BPF_IND)) {\n\t\t\tcnt = env->ops->gen_ld_abs(insn, insn_buf);\n\t\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\n... (function end not found)"
    },
    {
        "patch_model": "Add",
        "change_type": "Var-Declaration",
        "line_new": 6879,
        "critical_vars": [
            "insn_buf"
        ],
        "function": "fixup_bpf_calls",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "static int fixup_bpf_calls(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tstruct bpf_insn *insn = prog->insnsi;\n\tconst struct bpf_func_proto *fn;\n\tconst int insn_cnt = prog->len;\n\tconst struct bpf_map_ops *ops;\n\tstruct bpf_insn_aux_data *aux;\n\tstruct bpf_insn insn_buf[16];\n\tstruct bpf_prog *new_prog;\n\tstruct bpf_map *map_ptr;\n\tint i, cnt, delta = 0;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (insn->code == (BPF_ALU64 | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\tbool is64 = BPF_CLASS(insn->code) == BPF_ALU64;\n\t\t\tstruct bpf_insn mask_and_div[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx div 0 -> 0 */\n\t\t\t\tBPF_JMP_IMM(BPF_JNE, insn->src_reg, 0, 2),\n\t\t\t\tBPF_ALU32_REG(BPF_XOR, insn->dst_reg, insn->dst_reg),\n\t\t\t\tBPF_JMP_IMM(BPF_JA, 0, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn mask_and_mod[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx mod 0 -> Rx */\n\t\t\t\tBPF_JMP_IMM(BPF_JEQ, insn->src_reg, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn *patchlet;\n\n\t\t\tif (insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\t\tpatchlet = mask_and_div + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_div) - (is64 ? 1 : 0);\n\t\t\t} else {\n\t\t\t\tpatchlet = mask_and_mod + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_mod) - (is64 ? 1 : 0);\n\t\t\t}\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, patchlet, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (BPF_CLASS(insn->code) == BPF_LD &&\n\t\t    (BPF_MODE(insn->code) == BPF_ABS ||\n\t\t     BPF_MODE(insn->code) == BPF_IND)) {\n\t\t\tcnt = env->ops->gen_ld_abs(insn, insn_buf);\n\t\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\n... (function end not found)"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 6880,
        "critical_vars": [
            "*patch"
        ],
        "function": "fixup_bpf_calls",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "static int fixup_bpf_calls(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tstruct bpf_insn *insn = prog->insnsi;\n\tconst struct bpf_func_proto *fn;\n\tconst int insn_cnt = prog->len;\n\tconst struct bpf_map_ops *ops;\n\tstruct bpf_insn_aux_data *aux;\n\tstruct bpf_insn insn_buf[16];\n\tstruct bpf_prog *new_prog;\n\tstruct bpf_map *map_ptr;\n\tint i, cnt, delta = 0;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (insn->code == (BPF_ALU64 | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\tbool is64 = BPF_CLASS(insn->code) == BPF_ALU64;\n\t\t\tstruct bpf_insn mask_and_div[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx div 0 -> 0 */\n\t\t\t\tBPF_JMP_IMM(BPF_JNE, insn->src_reg, 0, 2),\n\t\t\t\tBPF_ALU32_REG(BPF_XOR, insn->dst_reg, insn->dst_reg),\n\t\t\t\tBPF_JMP_IMM(BPF_JA, 0, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn mask_and_mod[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx mod 0 -> Rx */\n\t\t\t\tBPF_JMP_IMM(BPF_JEQ, insn->src_reg, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn *patchlet;\n\n\t\t\tif (insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\t\tpatchlet = mask_and_div + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_div) - (is64 ? 1 : 0);\n\t\t\t} else {\n\t\t\t\tpatchlet = mask_and_mod + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_mod) - (is64 ? 1 : 0);\n\t\t\t}\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, patchlet, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (BPF_CLASS(insn->code) == BPF_LD &&\n\t\t    (BPF_MODE(insn->code) == BPF_ABS ||\n\t\t     BPF_MODE(insn->code) == BPF_IND)) {\n\t\t\tcnt = env->ops->gen_ld_abs(insn, insn_buf);\n\t\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\n... (function end not found)"
    },
    {
        "patch_model": "Add",
        "change_type": "Var-Declaration",
        "line_new": 6881,
        "critical_vars": [
            "issrc",
            "isneg"
        ],
        "function": "fixup_bpf_calls",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "static int fixup_bpf_calls(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tstruct bpf_insn *insn = prog->insnsi;\n\tconst struct bpf_func_proto *fn;\n\tconst int insn_cnt = prog->len;\n\tconst struct bpf_map_ops *ops;\n\tstruct bpf_insn_aux_data *aux;\n\tstruct bpf_insn insn_buf[16];\n\tstruct bpf_prog *new_prog;\n\tstruct bpf_map *map_ptr;\n\tint i, cnt, delta = 0;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (insn->code == (BPF_ALU64 | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\tbool is64 = BPF_CLASS(insn->code) == BPF_ALU64;\n\t\t\tstruct bpf_insn mask_and_div[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx div 0 -> 0 */\n\t\t\t\tBPF_JMP_IMM(BPF_JNE, insn->src_reg, 0, 2),\n\t\t\t\tBPF_ALU32_REG(BPF_XOR, insn->dst_reg, insn->dst_reg),\n\t\t\t\tBPF_JMP_IMM(BPF_JA, 0, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn mask_and_mod[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx mod 0 -> Rx */\n\t\t\t\tBPF_JMP_IMM(BPF_JEQ, insn->src_reg, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn *patchlet;\n\n\t\t\tif (insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\t\tpatchlet = mask_and_div + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_div) - (is64 ? 1 : 0);\n\t\t\t} else {\n\t\t\t\tpatchlet = mask_and_mod + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_mod) - (is64 ? 1 : 0);\n\t\t\t}\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, patchlet, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (BPF_CLASS(insn->code) == BPF_LD &&\n\t\t    (BPF_MODE(insn->code) == BPF_ABS ||\n\t\t     BPF_MODE(insn->code) == BPF_IND)) {\n\t\t\tcnt = env->ops->gen_ld_abs(insn, insn_buf);\n\t\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\n... (function end not found)"
    },
    {
        "patch_model": "Add",
        "change_type": "Var-Declaration",
        "line_new": 6882,
        "critical_vars": [
            "off_reg"
        ],
        "function": "fixup_bpf_calls",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "static int fixup_bpf_calls(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tstruct bpf_insn *insn = prog->insnsi;\n\tconst struct bpf_func_proto *fn;\n\tconst int insn_cnt = prog->len;\n\tconst struct bpf_map_ops *ops;\n\tstruct bpf_insn_aux_data *aux;\n\tstruct bpf_insn insn_buf[16];\n\tstruct bpf_prog *new_prog;\n\tstruct bpf_map *map_ptr;\n\tint i, cnt, delta = 0;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (insn->code == (BPF_ALU64 | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\tbool is64 = BPF_CLASS(insn->code) == BPF_ALU64;\n\t\t\tstruct bpf_insn mask_and_div[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx div 0 -> 0 */\n\t\t\t\tBPF_JMP_IMM(BPF_JNE, insn->src_reg, 0, 2),\n\t\t\t\tBPF_ALU32_REG(BPF_XOR, insn->dst_reg, insn->dst_reg),\n\t\t\t\tBPF_JMP_IMM(BPF_JA, 0, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn mask_and_mod[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx mod 0 -> Rx */\n\t\t\t\tBPF_JMP_IMM(BPF_JEQ, insn->src_reg, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn *patchlet;\n\n\t\t\tif (insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\t\tpatchlet = mask_and_div + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_div) - (is64 ? 1 : 0);\n\t\t\t} else {\n\t\t\t\tpatchlet = mask_and_mod + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_mod) - (is64 ? 1 : 0);\n\t\t\t}\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, patchlet, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (BPF_CLASS(insn->code) == BPF_LD &&\n\t\t    (BPF_MODE(insn->code) == BPF_ABS ||\n\t\t     BPF_MODE(insn->code) == BPF_IND)) {\n\t\t\tcnt = env->ops->gen_ld_abs(insn, insn_buf);\n\t\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\n... (function end not found)"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 6884,
        "critical_vars": [
            "aux"
        ],
        "function": "fixup_bpf_calls",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "static int fixup_bpf_calls(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tstruct bpf_insn *insn = prog->insnsi;\n\tconst struct bpf_func_proto *fn;\n\tconst int insn_cnt = prog->len;\n\tconst struct bpf_map_ops *ops;\n\tstruct bpf_insn_aux_data *aux;\n\tstruct bpf_insn insn_buf[16];\n\tstruct bpf_prog *new_prog;\n\tstruct bpf_map *map_ptr;\n\tint i, cnt, delta = 0;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (insn->code == (BPF_ALU64 | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\tbool is64 = BPF_CLASS(insn->code) == BPF_ALU64;\n\t\t\tstruct bpf_insn mask_and_div[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx div 0 -> 0 */\n\t\t\t\tBPF_JMP_IMM(BPF_JNE, insn->src_reg, 0, 2),\n\t\t\t\tBPF_ALU32_REG(BPF_XOR, insn->dst_reg, insn->dst_reg),\n\t\t\t\tBPF_JMP_IMM(BPF_JA, 0, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn mask_and_mod[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx mod 0 -> Rx */\n\t\t\t\tBPF_JMP_IMM(BPF_JEQ, insn->src_reg, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn *patchlet;\n\n\t\t\tif (insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\t\tpatchlet = mask_and_div + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_div) - (is64 ? 1 : 0);\n\t\t\t} else {\n\t\t\t\tpatchlet = mask_and_mod + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_mod) - (is64 ? 1 : 0);\n\t\t\t}\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, patchlet, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (BPF_CLASS(insn->code) == BPF_LD &&\n\t\t    (BPF_MODE(insn->code) == BPF_ABS ||\n\t\t     BPF_MODE(insn->code) == BPF_IND)) {\n\t\t\tcnt = env->ops->gen_ld_abs(insn, insn_buf);\n\t\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\n... (function end not found)"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 6885,
        "critical_vars": [
            "aux->alu_state"
        ],
        "function": "fixup_bpf_calls",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "static int fixup_bpf_calls(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tstruct bpf_insn *insn = prog->insnsi;\n\tconst struct bpf_func_proto *fn;\n\tconst int insn_cnt = prog->len;\n\tconst struct bpf_map_ops *ops;\n\tstruct bpf_insn_aux_data *aux;\n\tstruct bpf_insn insn_buf[16];\n\tstruct bpf_prog *new_prog;\n\tstruct bpf_map *map_ptr;\n\tint i, cnt, delta = 0;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (insn->code == (BPF_ALU64 | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\tbool is64 = BPF_CLASS(insn->code) == BPF_ALU64;\n\t\t\tstruct bpf_insn mask_and_div[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx div 0 -> 0 */\n\t\t\t\tBPF_JMP_IMM(BPF_JNE, insn->src_reg, 0, 2),\n\t\t\t\tBPF_ALU32_REG(BPF_XOR, insn->dst_reg, insn->dst_reg),\n\t\t\t\tBPF_JMP_IMM(BPF_JA, 0, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn mask_and_mod[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx mod 0 -> Rx */\n\t\t\t\tBPF_JMP_IMM(BPF_JEQ, insn->src_reg, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn *patchlet;\n\n\t\t\tif (insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\t\tpatchlet = mask_and_div + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_div) - (is64 ? 1 : 0);\n\t\t\t} else {\n\t\t\t\tpatchlet = mask_and_mod + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_mod) - (is64 ? 1 : 0);\n\t\t\t}\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, patchlet, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (BPF_CLASS(insn->code) == BPF_LD &&\n\t\t    (BPF_MODE(insn->code) == BPF_ABS ||\n\t\t     BPF_MODE(insn->code) == BPF_IND)) {\n\t\t\tcnt = env->ops->gen_ld_abs(insn, insn_buf);\n\t\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\n... (function end not found)"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 6888,
        "critical_vars": [
            "isneg"
        ],
        "function": "fixup_bpf_calls",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "static int fixup_bpf_calls(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tstruct bpf_insn *insn = prog->insnsi;\n\tconst struct bpf_func_proto *fn;\n\tconst int insn_cnt = prog->len;\n\tconst struct bpf_map_ops *ops;\n\tstruct bpf_insn_aux_data *aux;\n\tstruct bpf_insn insn_buf[16];\n\tstruct bpf_prog *new_prog;\n\tstruct bpf_map *map_ptr;\n\tint i, cnt, delta = 0;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (insn->code == (BPF_ALU64 | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\tbool is64 = BPF_CLASS(insn->code) == BPF_ALU64;\n\t\t\tstruct bpf_insn mask_and_div[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx div 0 -> 0 */\n\t\t\t\tBPF_JMP_IMM(BPF_JNE, insn->src_reg, 0, 2),\n\t\t\t\tBPF_ALU32_REG(BPF_XOR, insn->dst_reg, insn->dst_reg),\n\t\t\t\tBPF_JMP_IMM(BPF_JA, 0, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn mask_and_mod[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx mod 0 -> Rx */\n\t\t\t\tBPF_JMP_IMM(BPF_JEQ, insn->src_reg, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn *patchlet;\n\n\t\t\tif (insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\t\tpatchlet = mask_and_div + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_div) - (is64 ? 1 : 0);\n\t\t\t} else {\n\t\t\t\tpatchlet = mask_and_mod + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_mod) - (is64 ? 1 : 0);\n\t\t\t}\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, patchlet, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (BPF_CLASS(insn->code) == BPF_LD &&\n\t\t    (BPF_MODE(insn->code) == BPF_ABS ||\n\t\t     BPF_MODE(insn->code) == BPF_IND)) {\n\t\t\tcnt = env->ops->gen_ld_abs(insn, insn_buf);\n\t\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\n... (function end not found)"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 6889,
        "critical_vars": [
            "issrc"
        ],
        "function": "fixup_bpf_calls",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "static int fixup_bpf_calls(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tstruct bpf_insn *insn = prog->insnsi;\n\tconst struct bpf_func_proto *fn;\n\tconst int insn_cnt = prog->len;\n\tconst struct bpf_map_ops *ops;\n\tstruct bpf_insn_aux_data *aux;\n\tstruct bpf_insn insn_buf[16];\n\tstruct bpf_prog *new_prog;\n\tstruct bpf_map *map_ptr;\n\tint i, cnt, delta = 0;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (insn->code == (BPF_ALU64 | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\tbool is64 = BPF_CLASS(insn->code) == BPF_ALU64;\n\t\t\tstruct bpf_insn mask_and_div[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx div 0 -> 0 */\n\t\t\t\tBPF_JMP_IMM(BPF_JNE, insn->src_reg, 0, 2),\n\t\t\t\tBPF_ALU32_REG(BPF_XOR, insn->dst_reg, insn->dst_reg),\n\t\t\t\tBPF_JMP_IMM(BPF_JA, 0, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn mask_and_mod[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx mod 0 -> Rx */\n\t\t\t\tBPF_JMP_IMM(BPF_JEQ, insn->src_reg, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn *patchlet;\n\n\t\t\tif (insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\t\tpatchlet = mask_and_div + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_div) - (is64 ? 1 : 0);\n\t\t\t} else {\n\t\t\t\tpatchlet = mask_and_mod + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_mod) - (is64 ? 1 : 0);\n\t\t\t}\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, patchlet, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (BPF_CLASS(insn->code) == BPF_LD &&\n\t\t    (BPF_MODE(insn->code) == BPF_ABS ||\n\t\t     BPF_MODE(insn->code) == BPF_IND)) {\n\t\t\tcnt = env->ops->gen_ld_abs(insn, insn_buf);\n\t\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\n... (function end not found)"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 6892,
        "critical_vars": [
            "off_reg"
        ],
        "function": "fixup_bpf_calls",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "static int fixup_bpf_calls(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tstruct bpf_insn *insn = prog->insnsi;\n\tconst struct bpf_func_proto *fn;\n\tconst int insn_cnt = prog->len;\n\tconst struct bpf_map_ops *ops;\n\tstruct bpf_insn_aux_data *aux;\n\tstruct bpf_insn insn_buf[16];\n\tstruct bpf_prog *new_prog;\n\tstruct bpf_map *map_ptr;\n\tint i, cnt, delta = 0;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (insn->code == (BPF_ALU64 | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\tbool is64 = BPF_CLASS(insn->code) == BPF_ALU64;\n\t\t\tstruct bpf_insn mask_and_div[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx div 0 -> 0 */\n\t\t\t\tBPF_JMP_IMM(BPF_JNE, insn->src_reg, 0, 2),\n\t\t\t\tBPF_ALU32_REG(BPF_XOR, insn->dst_reg, insn->dst_reg),\n\t\t\t\tBPF_JMP_IMM(BPF_JA, 0, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn mask_and_mod[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx mod 0 -> Rx */\n\t\t\t\tBPF_JMP_IMM(BPF_JEQ, insn->src_reg, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn *patchlet;\n\n\t\t\tif (insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\t\tpatchlet = mask_and_div + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_div) - (is64 ? 1 : 0);\n\t\t\t} else {\n\t\t\t\tpatchlet = mask_and_mod + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_mod) - (is64 ? 1 : 0);\n\t\t\t}\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, patchlet, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (BPF_CLASS(insn->code) == BPF_LD &&\n\t\t    (BPF_MODE(insn->code) == BPF_ABS ||\n\t\t     BPF_MODE(insn->code) == BPF_IND)) {\n\t\t\tcnt = env->ops->gen_ld_abs(insn, insn_buf);\n\t\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\n... (function end not found)"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 6893,
        "critical_vars": [
            "isneg"
        ],
        "function": "fixup_bpf_calls",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "static int fixup_bpf_calls(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tstruct bpf_insn *insn = prog->insnsi;\n\tconst struct bpf_func_proto *fn;\n\tconst int insn_cnt = prog->len;\n\tconst struct bpf_map_ops *ops;\n\tstruct bpf_insn_aux_data *aux;\n\tstruct bpf_insn insn_buf[16];\n\tstruct bpf_prog *new_prog;\n\tstruct bpf_map *map_ptr;\n\tint i, cnt, delta = 0;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (insn->code == (BPF_ALU64 | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\tbool is64 = BPF_CLASS(insn->code) == BPF_ALU64;\n\t\t\tstruct bpf_insn mask_and_div[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx div 0 -> 0 */\n\t\t\t\tBPF_JMP_IMM(BPF_JNE, insn->src_reg, 0, 2),\n\t\t\t\tBPF_ALU32_REG(BPF_XOR, insn->dst_reg, insn->dst_reg),\n\t\t\t\tBPF_JMP_IMM(BPF_JA, 0, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn mask_and_mod[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx mod 0 -> Rx */\n\t\t\t\tBPF_JMP_IMM(BPF_JEQ, insn->src_reg, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn *patchlet;\n\n\t\t\tif (insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\t\tpatchlet = mask_and_div + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_div) - (is64 ? 1 : 0);\n\t\t\t} else {\n\t\t\t\tpatchlet = mask_and_mod + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_mod) - (is64 ? 1 : 0);\n\t\t\t}\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, patchlet, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (BPF_CLASS(insn->code) == BPF_LD &&\n\t\t    (BPF_MODE(insn->code) == BPF_ABS ||\n\t\t     BPF_MODE(insn->code) == BPF_IND)) {\n\t\t\tcnt = env->ops->gen_ld_abs(insn, insn_buf);\n\t\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\n... (function end not found)"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 6894,
        "critical_vars": [
            "*patch"
        ],
        "function": "fixup_bpf_calls",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "static int fixup_bpf_calls(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tstruct bpf_insn *insn = prog->insnsi;\n\tconst struct bpf_func_proto *fn;\n\tconst int insn_cnt = prog->len;\n\tconst struct bpf_map_ops *ops;\n\tstruct bpf_insn_aux_data *aux;\n\tstruct bpf_insn insn_buf[16];\n\tstruct bpf_prog *new_prog;\n\tstruct bpf_map *map_ptr;\n\tint i, cnt, delta = 0;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (insn->code == (BPF_ALU64 | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\tbool is64 = BPF_CLASS(insn->code) == BPF_ALU64;\n\t\t\tstruct bpf_insn mask_and_div[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx div 0 -> 0 */\n\t\t\t\tBPF_JMP_IMM(BPF_JNE, insn->src_reg, 0, 2),\n\t\t\t\tBPF_ALU32_REG(BPF_XOR, insn->dst_reg, insn->dst_reg),\n\t\t\t\tBPF_JMP_IMM(BPF_JA, 0, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn mask_and_mod[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx mod 0 -> Rx */\n\t\t\t\tBPF_JMP_IMM(BPF_JEQ, insn->src_reg, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn *patchlet;\n\n\t\t\tif (insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\t\tpatchlet = mask_and_div + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_div) - (is64 ? 1 : 0);\n\t\t\t} else {\n\t\t\t\tpatchlet = mask_and_mod + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_mod) - (is64 ? 1 : 0);\n\t\t\t}\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, patchlet, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (BPF_CLASS(insn->code) == BPF_LD &&\n\t\t    (BPF_MODE(insn->code) == BPF_ABS ||\n\t\t     BPF_MODE(insn->code) == BPF_IND)) {\n\t\t\tcnt = env->ops->gen_ld_abs(insn, insn_buf);\n\t\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\n... (function end not found)"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 6895,
        "critical_vars": [
            "*patch"
        ],
        "function": "fixup_bpf_calls",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "static int fixup_bpf_calls(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tstruct bpf_insn *insn = prog->insnsi;\n\tconst struct bpf_func_proto *fn;\n\tconst int insn_cnt = prog->len;\n\tconst struct bpf_map_ops *ops;\n\tstruct bpf_insn_aux_data *aux;\n\tstruct bpf_insn insn_buf[16];\n\tstruct bpf_prog *new_prog;\n\tstruct bpf_map *map_ptr;\n\tint i, cnt, delta = 0;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (insn->code == (BPF_ALU64 | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\tbool is64 = BPF_CLASS(insn->code) == BPF_ALU64;\n\t\t\tstruct bpf_insn mask_and_div[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx div 0 -> 0 */\n\t\t\t\tBPF_JMP_IMM(BPF_JNE, insn->src_reg, 0, 2),\n\t\t\t\tBPF_ALU32_REG(BPF_XOR, insn->dst_reg, insn->dst_reg),\n\t\t\t\tBPF_JMP_IMM(BPF_JA, 0, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn mask_and_mod[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx mod 0 -> Rx */\n\t\t\t\tBPF_JMP_IMM(BPF_JEQ, insn->src_reg, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn *patchlet;\n\n\t\t\tif (insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\t\tpatchlet = mask_and_div + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_div) - (is64 ? 1 : 0);\n\t\t\t} else {\n\t\t\t\tpatchlet = mask_and_mod + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_mod) - (is64 ? 1 : 0);\n\t\t\t}\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, patchlet, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (BPF_CLASS(insn->code) == BPF_LD &&\n\t\t    (BPF_MODE(insn->code) == BPF_ABS ||\n\t\t     BPF_MODE(insn->code) == BPF_IND)) {\n\t\t\tcnt = env->ops->gen_ld_abs(insn, insn_buf);\n\t\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\n... (function end not found)"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 6896,
        "critical_vars": [
            "*patch"
        ],
        "function": "fixup_bpf_calls",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "static int fixup_bpf_calls(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tstruct bpf_insn *insn = prog->insnsi;\n\tconst struct bpf_func_proto *fn;\n\tconst int insn_cnt = prog->len;\n\tconst struct bpf_map_ops *ops;\n\tstruct bpf_insn_aux_data *aux;\n\tstruct bpf_insn insn_buf[16];\n\tstruct bpf_prog *new_prog;\n\tstruct bpf_map *map_ptr;\n\tint i, cnt, delta = 0;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (insn->code == (BPF_ALU64 | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\tbool is64 = BPF_CLASS(insn->code) == BPF_ALU64;\n\t\t\tstruct bpf_insn mask_and_div[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx div 0 -> 0 */\n\t\t\t\tBPF_JMP_IMM(BPF_JNE, insn->src_reg, 0, 2),\n\t\t\t\tBPF_ALU32_REG(BPF_XOR, insn->dst_reg, insn->dst_reg),\n\t\t\t\tBPF_JMP_IMM(BPF_JA, 0, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn mask_and_mod[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx mod 0 -> Rx */\n\t\t\t\tBPF_JMP_IMM(BPF_JEQ, insn->src_reg, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn *patchlet;\n\n\t\t\tif (insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\t\tpatchlet = mask_and_div + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_div) - (is64 ? 1 : 0);\n\t\t\t} else {\n\t\t\t\tpatchlet = mask_and_mod + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_mod) - (is64 ? 1 : 0);\n\t\t\t}\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, patchlet, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (BPF_CLASS(insn->code) == BPF_LD &&\n\t\t    (BPF_MODE(insn->code) == BPF_ABS ||\n\t\t     BPF_MODE(insn->code) == BPF_IND)) {\n\t\t\tcnt = env->ops->gen_ld_abs(insn, insn_buf);\n\t\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\n... (function end not found)"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 6897,
        "critical_vars": [
            "*patch"
        ],
        "function": "fixup_bpf_calls",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "static int fixup_bpf_calls(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tstruct bpf_insn *insn = prog->insnsi;\n\tconst struct bpf_func_proto *fn;\n\tconst int insn_cnt = prog->len;\n\tconst struct bpf_map_ops *ops;\n\tstruct bpf_insn_aux_data *aux;\n\tstruct bpf_insn insn_buf[16];\n\tstruct bpf_prog *new_prog;\n\tstruct bpf_map *map_ptr;\n\tint i, cnt, delta = 0;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (insn->code == (BPF_ALU64 | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\tbool is64 = BPF_CLASS(insn->code) == BPF_ALU64;\n\t\t\tstruct bpf_insn mask_and_div[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx div 0 -> 0 */\n\t\t\t\tBPF_JMP_IMM(BPF_JNE, insn->src_reg, 0, 2),\n\t\t\t\tBPF_ALU32_REG(BPF_XOR, insn->dst_reg, insn->dst_reg),\n\t\t\t\tBPF_JMP_IMM(BPF_JA, 0, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn mask_and_mod[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx mod 0 -> Rx */\n\t\t\t\tBPF_JMP_IMM(BPF_JEQ, insn->src_reg, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn *patchlet;\n\n\t\t\tif (insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\t\tpatchlet = mask_and_div + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_div) - (is64 ? 1 : 0);\n\t\t\t} else {\n\t\t\t\tpatchlet = mask_and_mod + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_mod) - (is64 ? 1 : 0);\n\t\t\t}\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, patchlet, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (BPF_CLASS(insn->code) == BPF_LD &&\n\t\t    (BPF_MODE(insn->code) == BPF_ABS ||\n\t\t     BPF_MODE(insn->code) == BPF_IND)) {\n\t\t\tcnt = env->ops->gen_ld_abs(insn, insn_buf);\n\t\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\n... (function end not found)"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 6898,
        "critical_vars": [
            "*patch"
        ],
        "function": "fixup_bpf_calls",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "static int fixup_bpf_calls(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tstruct bpf_insn *insn = prog->insnsi;\n\tconst struct bpf_func_proto *fn;\n\tconst int insn_cnt = prog->len;\n\tconst struct bpf_map_ops *ops;\n\tstruct bpf_insn_aux_data *aux;\n\tstruct bpf_insn insn_buf[16];\n\tstruct bpf_prog *new_prog;\n\tstruct bpf_map *map_ptr;\n\tint i, cnt, delta = 0;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (insn->code == (BPF_ALU64 | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\tbool is64 = BPF_CLASS(insn->code) == BPF_ALU64;\n\t\t\tstruct bpf_insn mask_and_div[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx div 0 -> 0 */\n\t\t\t\tBPF_JMP_IMM(BPF_JNE, insn->src_reg, 0, 2),\n\t\t\t\tBPF_ALU32_REG(BPF_XOR, insn->dst_reg, insn->dst_reg),\n\t\t\t\tBPF_JMP_IMM(BPF_JA, 0, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn mask_and_mod[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx mod 0 -> Rx */\n\t\t\t\tBPF_JMP_IMM(BPF_JEQ, insn->src_reg, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn *patchlet;\n\n\t\t\tif (insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\t\tpatchlet = mask_and_div + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_div) - (is64 ? 1 : 0);\n\t\t\t} else {\n\t\t\t\tpatchlet = mask_and_mod + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_mod) - (is64 ? 1 : 0);\n\t\t\t}\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, patchlet, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (BPF_CLASS(insn->code) == BPF_LD &&\n\t\t    (BPF_MODE(insn->code) == BPF_ABS ||\n\t\t     BPF_MODE(insn->code) == BPF_IND)) {\n\t\t\tcnt = env->ops->gen_ld_abs(insn, insn_buf);\n\t\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\n... (function end not found)"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 6899,
        "critical_vars": [
            "*patch"
        ],
        "function": "fixup_bpf_calls",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "static int fixup_bpf_calls(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tstruct bpf_insn *insn = prog->insnsi;\n\tconst struct bpf_func_proto *fn;\n\tconst int insn_cnt = prog->len;\n\tconst struct bpf_map_ops *ops;\n\tstruct bpf_insn_aux_data *aux;\n\tstruct bpf_insn insn_buf[16];\n\tstruct bpf_prog *new_prog;\n\tstruct bpf_map *map_ptr;\n\tint i, cnt, delta = 0;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (insn->code == (BPF_ALU64 | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\tbool is64 = BPF_CLASS(insn->code) == BPF_ALU64;\n\t\t\tstruct bpf_insn mask_and_div[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx div 0 -> 0 */\n\t\t\t\tBPF_JMP_IMM(BPF_JNE, insn->src_reg, 0, 2),\n\t\t\t\tBPF_ALU32_REG(BPF_XOR, insn->dst_reg, insn->dst_reg),\n\t\t\t\tBPF_JMP_IMM(BPF_JA, 0, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn mask_and_mod[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx mod 0 -> Rx */\n\t\t\t\tBPF_JMP_IMM(BPF_JEQ, insn->src_reg, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn *patchlet;\n\n\t\t\tif (insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\t\tpatchlet = mask_and_div + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_div) - (is64 ? 1 : 0);\n\t\t\t} else {\n\t\t\t\tpatchlet = mask_and_mod + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_mod) - (is64 ? 1 : 0);\n\t\t\t}\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, patchlet, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (BPF_CLASS(insn->code) == BPF_LD &&\n\t\t    (BPF_MODE(insn->code) == BPF_ABS ||\n\t\t     BPF_MODE(insn->code) == BPF_IND)) {\n\t\t\tcnt = env->ops->gen_ld_abs(insn, insn_buf);\n\t\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\n... (function end not found)"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 6900,
        "critical_vars": [
            "issrc"
        ],
        "function": "fixup_bpf_calls",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "static int fixup_bpf_calls(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tstruct bpf_insn *insn = prog->insnsi;\n\tconst struct bpf_func_proto *fn;\n\tconst int insn_cnt = prog->len;\n\tconst struct bpf_map_ops *ops;\n\tstruct bpf_insn_aux_data *aux;\n\tstruct bpf_insn insn_buf[16];\n\tstruct bpf_prog *new_prog;\n\tstruct bpf_map *map_ptr;\n\tint i, cnt, delta = 0;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (insn->code == (BPF_ALU64 | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\tbool is64 = BPF_CLASS(insn->code) == BPF_ALU64;\n\t\t\tstruct bpf_insn mask_and_div[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx div 0 -> 0 */\n\t\t\t\tBPF_JMP_IMM(BPF_JNE, insn->src_reg, 0, 2),\n\t\t\t\tBPF_ALU32_REG(BPF_XOR, insn->dst_reg, insn->dst_reg),\n\t\t\t\tBPF_JMP_IMM(BPF_JA, 0, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn mask_and_mod[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx mod 0 -> Rx */\n\t\t\t\tBPF_JMP_IMM(BPF_JEQ, insn->src_reg, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn *patchlet;\n\n\t\t\tif (insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\t\tpatchlet = mask_and_div + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_div) - (is64 ? 1 : 0);\n\t\t\t} else {\n\t\t\t\tpatchlet = mask_and_mod + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_mod) - (is64 ? 1 : 0);\n\t\t\t}\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, patchlet, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (BPF_CLASS(insn->code) == BPF_LD &&\n\t\t    (BPF_MODE(insn->code) == BPF_ABS ||\n\t\t     BPF_MODE(insn->code) == BPF_IND)) {\n\t\t\tcnt = env->ops->gen_ld_abs(insn, insn_buf);\n\t\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\n... (function end not found)"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 6901,
        "critical_vars": [
            "*patch"
        ],
        "function": "fixup_bpf_calls",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "static int fixup_bpf_calls(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tstruct bpf_insn *insn = prog->insnsi;\n\tconst struct bpf_func_proto *fn;\n\tconst int insn_cnt = prog->len;\n\tconst struct bpf_map_ops *ops;\n\tstruct bpf_insn_aux_data *aux;\n\tstruct bpf_insn insn_buf[16];\n\tstruct bpf_prog *new_prog;\n\tstruct bpf_map *map_ptr;\n\tint i, cnt, delta = 0;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (insn->code == (BPF_ALU64 | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\tbool is64 = BPF_CLASS(insn->code) == BPF_ALU64;\n\t\t\tstruct bpf_insn mask_and_div[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx div 0 -> 0 */\n\t\t\t\tBPF_JMP_IMM(BPF_JNE, insn->src_reg, 0, 2),\n\t\t\t\tBPF_ALU32_REG(BPF_XOR, insn->dst_reg, insn->dst_reg),\n\t\t\t\tBPF_JMP_IMM(BPF_JA, 0, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn mask_and_mod[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx mod 0 -> Rx */\n\t\t\t\tBPF_JMP_IMM(BPF_JEQ, insn->src_reg, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn *patchlet;\n\n\t\t\tif (insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\t\tpatchlet = mask_and_div + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_div) - (is64 ? 1 : 0);\n\t\t\t} else {\n\t\t\t\tpatchlet = mask_and_mod + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_mod) - (is64 ? 1 : 0);\n\t\t\t}\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, patchlet, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (BPF_CLASS(insn->code) == BPF_LD &&\n\t\t    (BPF_MODE(insn->code) == BPF_ABS ||\n\t\t     BPF_MODE(insn->code) == BPF_IND)) {\n\t\t\tcnt = env->ops->gen_ld_abs(insn, insn_buf);\n\t\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\n... (function end not found)"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 6903,
        "critical_vars": [
            "insn->src_reg"
        ],
        "function": "fixup_bpf_calls",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "static int fixup_bpf_calls(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tstruct bpf_insn *insn = prog->insnsi;\n\tconst struct bpf_func_proto *fn;\n\tconst int insn_cnt = prog->len;\n\tconst struct bpf_map_ops *ops;\n\tstruct bpf_insn_aux_data *aux;\n\tstruct bpf_insn insn_buf[16];\n\tstruct bpf_prog *new_prog;\n\tstruct bpf_map *map_ptr;\n\tint i, cnt, delta = 0;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (insn->code == (BPF_ALU64 | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\tbool is64 = BPF_CLASS(insn->code) == BPF_ALU64;\n\t\t\tstruct bpf_insn mask_and_div[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx div 0 -> 0 */\n\t\t\t\tBPF_JMP_IMM(BPF_JNE, insn->src_reg, 0, 2),\n\t\t\t\tBPF_ALU32_REG(BPF_XOR, insn->dst_reg, insn->dst_reg),\n\t\t\t\tBPF_JMP_IMM(BPF_JA, 0, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn mask_and_mod[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx mod 0 -> Rx */\n\t\t\t\tBPF_JMP_IMM(BPF_JEQ, insn->src_reg, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn *patchlet;\n\n\t\t\tif (insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\t\tpatchlet = mask_and_div + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_div) - (is64 ? 1 : 0);\n\t\t\t} else {\n\t\t\t\tpatchlet = mask_and_mod + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_mod) - (is64 ? 1 : 0);\n\t\t\t}\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, patchlet, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (BPF_CLASS(insn->code) == BPF_LD &&\n\t\t    (BPF_MODE(insn->code) == BPF_ABS ||\n\t\t     BPF_MODE(insn->code) == BPF_IND)) {\n\t\t\tcnt = env->ops->gen_ld_abs(insn, insn_buf);\n\t\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\n... (function end not found)"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 6905,
        "critical_vars": [
            "*patch"
        ],
        "function": "fixup_bpf_calls",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "static int fixup_bpf_calls(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tstruct bpf_insn *insn = prog->insnsi;\n\tconst struct bpf_func_proto *fn;\n\tconst int insn_cnt = prog->len;\n\tconst struct bpf_map_ops *ops;\n\tstruct bpf_insn_aux_data *aux;\n\tstruct bpf_insn insn_buf[16];\n\tstruct bpf_prog *new_prog;\n\tstruct bpf_map *map_ptr;\n\tint i, cnt, delta = 0;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (insn->code == (BPF_ALU64 | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\tbool is64 = BPF_CLASS(insn->code) == BPF_ALU64;\n\t\t\tstruct bpf_insn mask_and_div[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx div 0 -> 0 */\n\t\t\t\tBPF_JMP_IMM(BPF_JNE, insn->src_reg, 0, 2),\n\t\t\t\tBPF_ALU32_REG(BPF_XOR, insn->dst_reg, insn->dst_reg),\n\t\t\t\tBPF_JMP_IMM(BPF_JA, 0, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn mask_and_mod[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx mod 0 -> Rx */\n\t\t\t\tBPF_JMP_IMM(BPF_JEQ, insn->src_reg, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn *patchlet;\n\n\t\t\tif (insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\t\tpatchlet = mask_and_div + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_div) - (is64 ? 1 : 0);\n\t\t\t} else {\n\t\t\t\tpatchlet = mask_and_mod + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_mod) - (is64 ? 1 : 0);\n\t\t\t}\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, patchlet, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (BPF_CLASS(insn->code) == BPF_LD &&\n\t\t    (BPF_MODE(insn->code) == BPF_ABS ||\n\t\t     BPF_MODE(insn->code) == BPF_IND)) {\n\t\t\tcnt = env->ops->gen_ld_abs(insn, insn_buf);\n\t\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\n... (function end not found)"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 6908,
        "critical_vars": [
            "isneg"
        ],
        "function": "fixup_bpf_calls",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "static int fixup_bpf_calls(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tstruct bpf_insn *insn = prog->insnsi;\n\tconst struct bpf_func_proto *fn;\n\tconst int insn_cnt = prog->len;\n\tconst struct bpf_map_ops *ops;\n\tstruct bpf_insn_aux_data *aux;\n\tstruct bpf_insn insn_buf[16];\n\tstruct bpf_prog *new_prog;\n\tstruct bpf_map *map_ptr;\n\tint i, cnt, delta = 0;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (insn->code == (BPF_ALU64 | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\tbool is64 = BPF_CLASS(insn->code) == BPF_ALU64;\n\t\t\tstruct bpf_insn mask_and_div[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx div 0 -> 0 */\n\t\t\t\tBPF_JMP_IMM(BPF_JNE, insn->src_reg, 0, 2),\n\t\t\t\tBPF_ALU32_REG(BPF_XOR, insn->dst_reg, insn->dst_reg),\n\t\t\t\tBPF_JMP_IMM(BPF_JA, 0, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn mask_and_mod[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx mod 0 -> Rx */\n\t\t\t\tBPF_JMP_IMM(BPF_JEQ, insn->src_reg, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn *patchlet;\n\n\t\t\tif (insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\t\tpatchlet = mask_and_div + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_div) - (is64 ? 1 : 0);\n\t\t\t} else {\n\t\t\t\tpatchlet = mask_and_mod + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_mod) - (is64 ? 1 : 0);\n\t\t\t}\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, patchlet, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (BPF_CLASS(insn->code) == BPF_LD &&\n\t\t    (BPF_MODE(insn->code) == BPF_ABS ||\n\t\t     BPF_MODE(insn->code) == BPF_IND)) {\n\t\t\tcnt = env->ops->gen_ld_abs(insn, insn_buf);\n\t\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\n... (function end not found)"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 6909,
        "critical_vars": [
            "insn->code"
        ],
        "function": "fixup_bpf_calls",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "static int fixup_bpf_calls(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tstruct bpf_insn *insn = prog->insnsi;\n\tconst struct bpf_func_proto *fn;\n\tconst int insn_cnt = prog->len;\n\tconst struct bpf_map_ops *ops;\n\tstruct bpf_insn_aux_data *aux;\n\tstruct bpf_insn insn_buf[16];\n\tstruct bpf_prog *new_prog;\n\tstruct bpf_map *map_ptr;\n\tint i, cnt, delta = 0;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (insn->code == (BPF_ALU64 | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\tbool is64 = BPF_CLASS(insn->code) == BPF_ALU64;\n\t\t\tstruct bpf_insn mask_and_div[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx div 0 -> 0 */\n\t\t\t\tBPF_JMP_IMM(BPF_JNE, insn->src_reg, 0, 2),\n\t\t\t\tBPF_ALU32_REG(BPF_XOR, insn->dst_reg, insn->dst_reg),\n\t\t\t\tBPF_JMP_IMM(BPF_JA, 0, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn mask_and_mod[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx mod 0 -> Rx */\n\t\t\t\tBPF_JMP_IMM(BPF_JEQ, insn->src_reg, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn *patchlet;\n\n\t\t\tif (insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\t\tpatchlet = mask_and_div + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_div) - (is64 ? 1 : 0);\n\t\t\t} else {\n\t\t\t\tpatchlet = mask_and_mod + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_mod) - (is64 ? 1 : 0);\n\t\t\t}\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, patchlet, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (BPF_CLASS(insn->code) == BPF_LD &&\n\t\t    (BPF_MODE(insn->code) == BPF_ABS ||\n\t\t     BPF_MODE(insn->code) == BPF_IND)) {\n\t\t\tcnt = env->ops->gen_ld_abs(insn, insn_buf);\n\t\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\n... (function end not found)"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 6911,
        "critical_vars": [
            "*patch"
        ],
        "function": "fixup_bpf_calls",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "static int fixup_bpf_calls(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tstruct bpf_insn *insn = prog->insnsi;\n\tconst struct bpf_func_proto *fn;\n\tconst int insn_cnt = prog->len;\n\tconst struct bpf_map_ops *ops;\n\tstruct bpf_insn_aux_data *aux;\n\tstruct bpf_insn insn_buf[16];\n\tstruct bpf_prog *new_prog;\n\tstruct bpf_map *map_ptr;\n\tint i, cnt, delta = 0;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (insn->code == (BPF_ALU64 | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\tbool is64 = BPF_CLASS(insn->code) == BPF_ALU64;\n\t\t\tstruct bpf_insn mask_and_div[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx div 0 -> 0 */\n\t\t\t\tBPF_JMP_IMM(BPF_JNE, insn->src_reg, 0, 2),\n\t\t\t\tBPF_ALU32_REG(BPF_XOR, insn->dst_reg, insn->dst_reg),\n\t\t\t\tBPF_JMP_IMM(BPF_JA, 0, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn mask_and_mod[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx mod 0 -> Rx */\n\t\t\t\tBPF_JMP_IMM(BPF_JEQ, insn->src_reg, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn *patchlet;\n\n\t\t\tif (insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\t\tpatchlet = mask_and_div + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_div) - (is64 ? 1 : 0);\n\t\t\t} else {\n\t\t\t\tpatchlet = mask_and_mod + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_mod) - (is64 ? 1 : 0);\n\t\t\t}\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, patchlet, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (BPF_CLASS(insn->code) == BPF_LD &&\n\t\t    (BPF_MODE(insn->code) == BPF_ABS ||\n\t\t     BPF_MODE(insn->code) == BPF_IND)) {\n\t\t\tcnt = env->ops->gen_ld_abs(insn, insn_buf);\n\t\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\n... (function end not found)"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 6912,
        "critical_vars": [
            "issrc",
            "isneg"
        ],
        "function": "fixup_bpf_calls",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "static int fixup_bpf_calls(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tstruct bpf_insn *insn = prog->insnsi;\n\tconst struct bpf_func_proto *fn;\n\tconst int insn_cnt = prog->len;\n\tconst struct bpf_map_ops *ops;\n\tstruct bpf_insn_aux_data *aux;\n\tstruct bpf_insn insn_buf[16];\n\tstruct bpf_prog *new_prog;\n\tstruct bpf_map *map_ptr;\n\tint i, cnt, delta = 0;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (insn->code == (BPF_ALU64 | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\tbool is64 = BPF_CLASS(insn->code) == BPF_ALU64;\n\t\t\tstruct bpf_insn mask_and_div[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx div 0 -> 0 */\n\t\t\t\tBPF_JMP_IMM(BPF_JNE, insn->src_reg, 0, 2),\n\t\t\t\tBPF_ALU32_REG(BPF_XOR, insn->dst_reg, insn->dst_reg),\n\t\t\t\tBPF_JMP_IMM(BPF_JA, 0, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn mask_and_mod[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx mod 0 -> Rx */\n\t\t\t\tBPF_JMP_IMM(BPF_JEQ, insn->src_reg, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn *patchlet;\n\n\t\t\tif (insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\t\tpatchlet = mask_and_div + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_div) - (is64 ? 1 : 0);\n\t\t\t} else {\n\t\t\t\tpatchlet = mask_and_mod + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_mod) - (is64 ? 1 : 0);\n\t\t\t}\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, patchlet, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (BPF_CLASS(insn->code) == BPF_LD &&\n\t\t    (BPF_MODE(insn->code) == BPF_ABS ||\n\t\t     BPF_MODE(insn->code) == BPF_IND)) {\n\t\t\tcnt = env->ops->gen_ld_abs(insn, insn_buf);\n\t\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\n... (function end not found)"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 6913,
        "critical_vars": [
            "*patch"
        ],
        "function": "fixup_bpf_calls",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "static int fixup_bpf_calls(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tstruct bpf_insn *insn = prog->insnsi;\n\tconst struct bpf_func_proto *fn;\n\tconst int insn_cnt = prog->len;\n\tconst struct bpf_map_ops *ops;\n\tstruct bpf_insn_aux_data *aux;\n\tstruct bpf_insn insn_buf[16];\n\tstruct bpf_prog *new_prog;\n\tstruct bpf_map *map_ptr;\n\tint i, cnt, delta = 0;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (insn->code == (BPF_ALU64 | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\tbool is64 = BPF_CLASS(insn->code) == BPF_ALU64;\n\t\t\tstruct bpf_insn mask_and_div[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx div 0 -> 0 */\n\t\t\t\tBPF_JMP_IMM(BPF_JNE, insn->src_reg, 0, 2),\n\t\t\t\tBPF_ALU32_REG(BPF_XOR, insn->dst_reg, insn->dst_reg),\n\t\t\t\tBPF_JMP_IMM(BPF_JA, 0, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn mask_and_mod[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx mod 0 -> Rx */\n\t\t\t\tBPF_JMP_IMM(BPF_JEQ, insn->src_reg, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn *patchlet;\n\n\t\t\tif (insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\t\tpatchlet = mask_and_div + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_div) - (is64 ? 1 : 0);\n\t\t\t} else {\n\t\t\t\tpatchlet = mask_and_mod + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_mod) - (is64 ? 1 : 0);\n\t\t\t}\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, patchlet, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (BPF_CLASS(insn->code) == BPF_LD &&\n\t\t    (BPF_MODE(insn->code) == BPF_ABS ||\n\t\t     BPF_MODE(insn->code) == BPF_IND)) {\n\t\t\tcnt = env->ops->gen_ld_abs(insn, insn_buf);\n\t\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\n... (function end not found)"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 6914,
        "critical_vars": [
            "cnt"
        ],
        "function": "fixup_bpf_calls",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "static int fixup_bpf_calls(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tstruct bpf_insn *insn = prog->insnsi;\n\tconst struct bpf_func_proto *fn;\n\tconst int insn_cnt = prog->len;\n\tconst struct bpf_map_ops *ops;\n\tstruct bpf_insn_aux_data *aux;\n\tstruct bpf_insn insn_buf[16];\n\tstruct bpf_prog *new_prog;\n\tstruct bpf_map *map_ptr;\n\tint i, cnt, delta = 0;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (insn->code == (BPF_ALU64 | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\tbool is64 = BPF_CLASS(insn->code) == BPF_ALU64;\n\t\t\tstruct bpf_insn mask_and_div[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx div 0 -> 0 */\n\t\t\t\tBPF_JMP_IMM(BPF_JNE, insn->src_reg, 0, 2),\n\t\t\t\tBPF_ALU32_REG(BPF_XOR, insn->dst_reg, insn->dst_reg),\n\t\t\t\tBPF_JMP_IMM(BPF_JA, 0, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn mask_and_mod[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx mod 0 -> Rx */\n\t\t\t\tBPF_JMP_IMM(BPF_JEQ, insn->src_reg, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn *patchlet;\n\n\t\t\tif (insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\t\tpatchlet = mask_and_div + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_div) - (is64 ? 1 : 0);\n\t\t\t} else {\n\t\t\t\tpatchlet = mask_and_mod + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_mod) - (is64 ? 1 : 0);\n\t\t\t}\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, patchlet, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (BPF_CLASS(insn->code) == BPF_LD &&\n\t\t    (BPF_MODE(insn->code) == BPF_ABS ||\n\t\t     BPF_MODE(insn->code) == BPF_IND)) {\n\t\t\tcnt = env->ops->gen_ld_abs(insn, insn_buf);\n\t\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\n... (function end not found)"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 6916,
        "critical_vars": [
            "new_prog"
        ],
        "function": "fixup_bpf_calls",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "static int fixup_bpf_calls(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tstruct bpf_insn *insn = prog->insnsi;\n\tconst struct bpf_func_proto *fn;\n\tconst int insn_cnt = prog->len;\n\tconst struct bpf_map_ops *ops;\n\tstruct bpf_insn_aux_data *aux;\n\tstruct bpf_insn insn_buf[16];\n\tstruct bpf_prog *new_prog;\n\tstruct bpf_map *map_ptr;\n\tint i, cnt, delta = 0;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (insn->code == (BPF_ALU64 | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\tbool is64 = BPF_CLASS(insn->code) == BPF_ALU64;\n\t\t\tstruct bpf_insn mask_and_div[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx div 0 -> 0 */\n\t\t\t\tBPF_JMP_IMM(BPF_JNE, insn->src_reg, 0, 2),\n\t\t\t\tBPF_ALU32_REG(BPF_XOR, insn->dst_reg, insn->dst_reg),\n\t\t\t\tBPF_JMP_IMM(BPF_JA, 0, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn mask_and_mod[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx mod 0 -> Rx */\n\t\t\t\tBPF_JMP_IMM(BPF_JEQ, insn->src_reg, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn *patchlet;\n\n\t\t\tif (insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\t\tpatchlet = mask_and_div + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_div) - (is64 ? 1 : 0);\n\t\t\t} else {\n\t\t\t\tpatchlet = mask_and_mod + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_mod) - (is64 ? 1 : 0);\n\t\t\t}\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, patchlet, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (BPF_CLASS(insn->code) == BPF_LD &&\n\t\t    (BPF_MODE(insn->code) == BPF_ABS ||\n\t\t     BPF_MODE(insn->code) == BPF_IND)) {\n\t\t\tcnt = env->ops->gen_ld_abs(insn, insn_buf);\n\t\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\n... (function end not found)"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 6917,
        "critical_vars": [
            "new_prog"
        ],
        "function": "fixup_bpf_calls",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "static int fixup_bpf_calls(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tstruct bpf_insn *insn = prog->insnsi;\n\tconst struct bpf_func_proto *fn;\n\tconst int insn_cnt = prog->len;\n\tconst struct bpf_map_ops *ops;\n\tstruct bpf_insn_aux_data *aux;\n\tstruct bpf_insn insn_buf[16];\n\tstruct bpf_prog *new_prog;\n\tstruct bpf_map *map_ptr;\n\tint i, cnt, delta = 0;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (insn->code == (BPF_ALU64 | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\tbool is64 = BPF_CLASS(insn->code) == BPF_ALU64;\n\t\t\tstruct bpf_insn mask_and_div[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx div 0 -> 0 */\n\t\t\t\tBPF_JMP_IMM(BPF_JNE, insn->src_reg, 0, 2),\n\t\t\t\tBPF_ALU32_REG(BPF_XOR, insn->dst_reg, insn->dst_reg),\n\t\t\t\tBPF_JMP_IMM(BPF_JA, 0, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn mask_and_mod[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx mod 0 -> Rx */\n\t\t\t\tBPF_JMP_IMM(BPF_JEQ, insn->src_reg, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn *patchlet;\n\n\t\t\tif (insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\t\tpatchlet = mask_and_div + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_div) - (is64 ? 1 : 0);\n\t\t\t} else {\n\t\t\t\tpatchlet = mask_and_mod + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_mod) - (is64 ? 1 : 0);\n\t\t\t}\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, patchlet, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (BPF_CLASS(insn->code) == BPF_LD &&\n\t\t    (BPF_MODE(insn->code) == BPF_ABS ||\n\t\t     BPF_MODE(insn->code) == BPF_IND)) {\n\t\t\tcnt = env->ops->gen_ld_abs(insn, insn_buf);\n\t\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\n... (function end not found)"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 6920,
        "critical_vars": [
            "delta"
        ],
        "function": "fixup_bpf_calls",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "static int fixup_bpf_calls(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tstruct bpf_insn *insn = prog->insnsi;\n\tconst struct bpf_func_proto *fn;\n\tconst int insn_cnt = prog->len;\n\tconst struct bpf_map_ops *ops;\n\tstruct bpf_insn_aux_data *aux;\n\tstruct bpf_insn insn_buf[16];\n\tstruct bpf_prog *new_prog;\n\tstruct bpf_map *map_ptr;\n\tint i, cnt, delta = 0;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (insn->code == (BPF_ALU64 | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\tbool is64 = BPF_CLASS(insn->code) == BPF_ALU64;\n\t\t\tstruct bpf_insn mask_and_div[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx div 0 -> 0 */\n\t\t\t\tBPF_JMP_IMM(BPF_JNE, insn->src_reg, 0, 2),\n\t\t\t\tBPF_ALU32_REG(BPF_XOR, insn->dst_reg, insn->dst_reg),\n\t\t\t\tBPF_JMP_IMM(BPF_JA, 0, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn mask_and_mod[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx mod 0 -> Rx */\n\t\t\t\tBPF_JMP_IMM(BPF_JEQ, insn->src_reg, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn *patchlet;\n\n\t\t\tif (insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\t\tpatchlet = mask_and_div + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_div) - (is64 ? 1 : 0);\n\t\t\t} else {\n\t\t\t\tpatchlet = mask_and_mod + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_mod) - (is64 ? 1 : 0);\n\t\t\t}\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, patchlet, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (BPF_CLASS(insn->code) == BPF_LD &&\n\t\t    (BPF_MODE(insn->code) == BPF_ABS ||\n\t\t     BPF_MODE(insn->code) == BPF_IND)) {\n\t\t\tcnt = env->ops->gen_ld_abs(insn, insn_buf);\n\t\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\n... (function end not found)"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 6921,
        "critical_vars": [
            "env->prog"
        ],
        "function": "fixup_bpf_calls",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "static int fixup_bpf_calls(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tstruct bpf_insn *insn = prog->insnsi;\n\tconst struct bpf_func_proto *fn;\n\tconst int insn_cnt = prog->len;\n\tconst struct bpf_map_ops *ops;\n\tstruct bpf_insn_aux_data *aux;\n\tstruct bpf_insn insn_buf[16];\n\tstruct bpf_prog *new_prog;\n\tstruct bpf_map *map_ptr;\n\tint i, cnt, delta = 0;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (insn->code == (BPF_ALU64 | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\tbool is64 = BPF_CLASS(insn->code) == BPF_ALU64;\n\t\t\tstruct bpf_insn mask_and_div[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx div 0 -> 0 */\n\t\t\t\tBPF_JMP_IMM(BPF_JNE, insn->src_reg, 0, 2),\n\t\t\t\tBPF_ALU32_REG(BPF_XOR, insn->dst_reg, insn->dst_reg),\n\t\t\t\tBPF_JMP_IMM(BPF_JA, 0, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn mask_and_mod[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx mod 0 -> Rx */\n\t\t\t\tBPF_JMP_IMM(BPF_JEQ, insn->src_reg, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn *patchlet;\n\n\t\t\tif (insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\t\tpatchlet = mask_and_div + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_div) - (is64 ? 1 : 0);\n\t\t\t} else {\n\t\t\t\tpatchlet = mask_and_mod + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_mod) - (is64 ? 1 : 0);\n\t\t\t}\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, patchlet, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (BPF_CLASS(insn->code) == BPF_LD &&\n\t\t    (BPF_MODE(insn->code) == BPF_ABS ||\n\t\t     BPF_MODE(insn->code) == BPF_IND)) {\n\t\t\tcnt = env->ops->gen_ld_abs(insn, insn_buf);\n\t\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\n... (function end not found)"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 6922,
        "critical_vars": [
            "insn"
        ],
        "function": "fixup_bpf_calls",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_979d63d50c0c0f7bc537bf821e056cc9fe5abd38_verifier.c.diff",
        "label": "True",
        "function_code": "static int fixup_bpf_calls(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tstruct bpf_insn *insn = prog->insnsi;\n\tconst struct bpf_func_proto *fn;\n\tconst int insn_cnt = prog->len;\n\tconst struct bpf_map_ops *ops;\n\tstruct bpf_insn_aux_data *aux;\n\tstruct bpf_insn insn_buf[16];\n\tstruct bpf_prog *new_prog;\n\tstruct bpf_map *map_ptr;\n\tint i, cnt, delta = 0;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (insn->code == (BPF_ALU64 | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\tbool is64 = BPF_CLASS(insn->code) == BPF_ALU64;\n\t\t\tstruct bpf_insn mask_and_div[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx div 0 -> 0 */\n\t\t\t\tBPF_JMP_IMM(BPF_JNE, insn->src_reg, 0, 2),\n\t\t\t\tBPF_ALU32_REG(BPF_XOR, insn->dst_reg, insn->dst_reg),\n\t\t\t\tBPF_JMP_IMM(BPF_JA, 0, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn mask_and_mod[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx mod 0 -> Rx */\n\t\t\t\tBPF_JMP_IMM(BPF_JEQ, insn->src_reg, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn *patchlet;\n\n\t\t\tif (insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\t\tpatchlet = mask_and_div + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_div) - (is64 ? 1 : 0);\n\t\t\t} else {\n\t\t\t\tpatchlet = mask_and_mod + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_mod) - (is64 ? 1 : 0);\n\t\t\t}\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, patchlet, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (BPF_CLASS(insn->code) == BPF_LD &&\n\t\t    (BPF_MODE(insn->code) == BPF_ABS ||\n\t\t     BPF_MODE(insn->code) == BPF_IND)) {\n\t\t\tcnt = env->ops->gen_ld_abs(insn, insn_buf);\n\t\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\n... (function end not found)"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 3106,
        "critical_vars": [
            "bpf_verifier_env",
            "bpf_insn",
            "env",
            "insn"
        ],
        "function": "retrieve_ptr_limit",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_d3bd7413e0ca40b60cf60d4003246d067cafdeda_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic int retrieve_ptr_limit(const struct bpf_reg_state *ptr_reg,\n\t\t\t      u32 *ptr_limit, u8 opcode, bool off_is_neg)\n{\n\tbool mask_to_left = (opcode == BPF_ADD &&  off_is_neg) ||\n\t\t\t    (opcode == BPF_SUB && !off_is_neg);\n\tu32 off;\n\n\tswitch (ptr_reg->type) {\n\tcase PTR_TO_STACK:\n\t\toff = ptr_reg->off + ptr_reg->var_off.value;\n\t\tif (mask_to_left)\n\t\t\t*ptr_limit = MAX_BPF_STACK + off;\n\t\telse\n\t\t\t*ptr_limit = -off;\n\t\treturn 0;\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (mask_to_left) {\n\t\t\t*ptr_limit = ptr_reg->umax_value + ptr_reg->off;\n\t\t} else {\n\t\t\toff = ptr_reg->smin_value + ptr_reg->off;\n\t\t\t*ptr_limit = ptr_reg->map_ptr->value_size - off;\n\t\t}\n\t\treturn 0;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Head",
        "line_new": 3112,
        "critical_vars": [
            "alu_state",
            "*aux",
            "alu_limit"
        ],
        "function": "retrieve_ptr_limit",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_d3bd7413e0ca40b60cf60d4003246d067cafdeda_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic int retrieve_ptr_limit(const struct bpf_reg_state *ptr_reg,\n\t\t\t      u32 *ptr_limit, u8 opcode, bool off_is_neg)\n{\n\tbool mask_to_left = (opcode == BPF_ADD &&  off_is_neg) ||\n\t\t\t    (opcode == BPF_SUB && !off_is_neg);\n\tu32 off;\n\n\tswitch (ptr_reg->type) {\n\tcase PTR_TO_STACK:\n\t\toff = ptr_reg->off + ptr_reg->var_off.value;\n\t\tif (mask_to_left)\n\t\t\t*ptr_limit = MAX_BPF_STACK + off;\n\t\telse\n\t\t\t*ptr_limit = -off;\n\t\treturn 0;\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (mask_to_left) {\n\t\t\t*ptr_limit = ptr_reg->umax_value + ptr_reg->off;\n\t\t} else {\n\t\t\toff = ptr_reg->smin_value + ptr_reg->off;\n\t\t\t*ptr_limit = ptr_reg->map_ptr->value_size - off;\n\t\t}\n\t\treturn 0;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 3118,
        "critical_vars": [
            "aux->alu_state",
            "aux->alu_limit"
        ],
        "function": "retrieve_ptr_limit",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_d3bd7413e0ca40b60cf60d4003246d067cafdeda_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic int retrieve_ptr_limit(const struct bpf_reg_state *ptr_reg,\n\t\t\t      u32 *ptr_limit, u8 opcode, bool off_is_neg)\n{\n\tbool mask_to_left = (opcode == BPF_ADD &&  off_is_neg) ||\n\t\t\t    (opcode == BPF_SUB && !off_is_neg);\n\tu32 off;\n\n\tswitch (ptr_reg->type) {\n\tcase PTR_TO_STACK:\n\t\toff = ptr_reg->off + ptr_reg->var_off.value;\n\t\tif (mask_to_left)\n\t\t\t*ptr_limit = MAX_BPF_STACK + off;\n\t\telse\n\t\t\t*ptr_limit = -off;\n\t\treturn 0;\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (mask_to_left) {\n\t\t\t*ptr_limit = ptr_reg->umax_value + ptr_reg->off;\n\t\t} else {\n\t\t\toff = ptr_reg->smin_value + ptr_reg->off;\n\t\t\t*ptr_limit = ptr_reg->map_ptr->value_size - off;\n\t\t}\n\t\treturn 0;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 3124,
        "critical_vars": [
            "aux->alu_state"
        ],
        "function": "retrieve_ptr_limit",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_d3bd7413e0ca40b60cf60d4003246d067cafdeda_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic int retrieve_ptr_limit(const struct bpf_reg_state *ptr_reg,\n\t\t\t      u32 *ptr_limit, u8 opcode, bool off_is_neg)\n{\n\tbool mask_to_left = (opcode == BPF_ADD &&  off_is_neg) ||\n\t\t\t    (opcode == BPF_SUB && !off_is_neg);\n\tu32 off;\n\n\tswitch (ptr_reg->type) {\n\tcase PTR_TO_STACK:\n\t\toff = ptr_reg->off + ptr_reg->var_off.value;\n\t\tif (mask_to_left)\n\t\t\t*ptr_limit = MAX_BPF_STACK + off;\n\t\telse\n\t\t\t*ptr_limit = -off;\n\t\treturn 0;\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (mask_to_left) {\n\t\t\t*ptr_limit = ptr_reg->umax_value + ptr_reg->off;\n\t\t} else {\n\t\t\toff = ptr_reg->smin_value + ptr_reg->off;\n\t\t\t*ptr_limit = ptr_reg->map_ptr->value_size - off;\n\t\t}\n\t\treturn 0;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 3125,
        "critical_vars": [
            "aux->alu_limit"
        ],
        "function": "retrieve_ptr_limit",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_d3bd7413e0ca40b60cf60d4003246d067cafdeda_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic int retrieve_ptr_limit(const struct bpf_reg_state *ptr_reg,\n\t\t\t      u32 *ptr_limit, u8 opcode, bool off_is_neg)\n{\n\tbool mask_to_left = (opcode == BPF_ADD &&  off_is_neg) ||\n\t\t\t    (opcode == BPF_SUB && !off_is_neg);\n\tu32 off;\n\n\tswitch (ptr_reg->type) {\n\tcase PTR_TO_STACK:\n\t\toff = ptr_reg->off + ptr_reg->var_off.value;\n\t\tif (mask_to_left)\n\t\t\t*ptr_limit = MAX_BPF_STACK + off;\n\t\telse\n\t\t\t*ptr_limit = -off;\n\t\treturn 0;\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (mask_to_left) {\n\t\t\t*ptr_limit = ptr_reg->umax_value + ptr_reg->off;\n\t\t} else {\n\t\t\toff = ptr_reg->smin_value + ptr_reg->off;\n\t\t\t*ptr_limit = ptr_reg->map_ptr->value_size - off;\n\t\t}\n\t\treturn 0;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Head",
        "line_new": 3129,
        "critical_vars": [
            "*insn",
            "*env"
        ],
        "function": "retrieve_ptr_limit",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_d3bd7413e0ca40b60cf60d4003246d067cafdeda_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic int retrieve_ptr_limit(const struct bpf_reg_state *ptr_reg,\n\t\t\t      u32 *ptr_limit, u8 opcode, bool off_is_neg)\n{\n\tbool mask_to_left = (opcode == BPF_ADD &&  off_is_neg) ||\n\t\t\t    (opcode == BPF_SUB && !off_is_neg);\n\tu32 off;\n\n\tswitch (ptr_reg->type) {\n\tcase PTR_TO_STACK:\n\t\toff = ptr_reg->off + ptr_reg->var_off.value;\n\t\tif (mask_to_left)\n\t\t\t*ptr_limit = MAX_BPF_STACK + off;\n\t\telse\n\t\t\t*ptr_limit = -off;\n\t\treturn 0;\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (mask_to_left) {\n\t\t\t*ptr_limit = ptr_reg->umax_value + ptr_reg->off;\n\t\t} else {\n\t\t\toff = ptr_reg->smin_value + ptr_reg->off;\n\t\t\t*ptr_limit = ptr_reg->map_ptr->value_size - off;\n\t\t}\n\t\treturn 0;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 3132,
        "critical_vars": [
            "*aux"
        ],
        "function": "retrieve_ptr_limit",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_d3bd7413e0ca40b60cf60d4003246d067cafdeda_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic int retrieve_ptr_limit(const struct bpf_reg_state *ptr_reg,\n\t\t\t      u32 *ptr_limit, u8 opcode, bool off_is_neg)\n{\n\tbool mask_to_left = (opcode == BPF_ADD &&  off_is_neg) ||\n\t\t\t    (opcode == BPF_SUB && !off_is_neg);\n\tu32 off;\n\n\tswitch (ptr_reg->type) {\n\tcase PTR_TO_STACK:\n\t\toff = ptr_reg->off + ptr_reg->var_off.value;\n\t\tif (mask_to_left)\n\t\t\t*ptr_limit = MAX_BPF_STACK + off;\n\t\telse\n\t\t\t*ptr_limit = -off;\n\t\treturn 0;\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (mask_to_left) {\n\t\t\t*ptr_limit = ptr_reg->umax_value + ptr_reg->off;\n\t\t} else {\n\t\t\toff = ptr_reg->smin_value + ptr_reg->off;\n\t\t\t*ptr_limit = ptr_reg->map_ptr->value_size - off;\n\t\t}\n\t\treturn 0;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 3134,
        "critical_vars": [
            "env",
            "insn"
        ],
        "function": "retrieve_ptr_limit",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_d3bd7413e0ca40b60cf60d4003246d067cafdeda_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic int retrieve_ptr_limit(const struct bpf_reg_state *ptr_reg,\n\t\t\t      u32 *ptr_limit, u8 opcode, bool off_is_neg)\n{\n\tbool mask_to_left = (opcode == BPF_ADD &&  off_is_neg) ||\n\t\t\t    (opcode == BPF_SUB && !off_is_neg);\n\tu32 off;\n\n\tswitch (ptr_reg->type) {\n\tcase PTR_TO_STACK:\n\t\toff = ptr_reg->off + ptr_reg->var_off.value;\n\t\tif (mask_to_left)\n\t\t\t*ptr_limit = MAX_BPF_STACK + off;\n\t\telse\n\t\t\t*ptr_limit = -off;\n\t\treturn 0;\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (mask_to_left) {\n\t\t\t*ptr_limit = ptr_reg->umax_value + ptr_reg->off;\n\t\t} else {\n\t\t\toff = ptr_reg->smin_value + ptr_reg->off;\n\t\t\t*ptr_limit = ptr_reg->map_ptr->value_size - off;\n\t\t}\n\t\treturn 0;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "if-Condition",
        "line_old": 3120,
        "critical_vars": [
            "env->allow_ptr_leaks",
            "insn->code"
        ],
        "function": "sanitize_ptr_alu",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_d3bd7413e0ca40b60cf60d4003246d067cafdeda_verifier.c.diff",
        "label": "False",
        "function_code": "\nstatic int sanitize_ptr_alu(struct bpf_verifier_env *env,\n\t\t\t    struct bpf_insn *insn,\n\t\t\t    const struct bpf_reg_state *ptr_reg,\n\t\t\t    struct bpf_reg_state *dst_reg,\n\t\t\t    bool off_is_neg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_insn_aux_data *aux = cur_aux(env);\n\tbool ptr_is_dst_reg = ptr_reg == dst_reg;\n\tu8 opcode = BPF_OP(insn->code);\n\tu32 alu_state, alu_limit;\n\tstruct bpf_reg_state tmp;\n\tbool ret;\n\n\tif (env->allow_ptr_leaks || BPF_SRC(insn->code) == BPF_K)\n\t\treturn 0;\n\n\t/* We already marked aux for masking from non-speculative\n\t * paths, thus we got here in the first place. We only care\n\t * to explore bad access from here.\n\t */\n\tif (vstate->speculative)\n\t\tgoto do_sim;\n\n\talu_state  = off_is_neg ? BPF_ALU_NEG_VALUE : 0;\n\talu_state |= ptr_is_dst_reg ?\n\t\t     BPF_ALU_SANITIZE_SRC : BPF_ALU_SANITIZE_DST;\n\n\tif (retrieve_ptr_limit(ptr_reg, &alu_limit, opcode, off_is_neg))\n\t\treturn 0;\n\n\t/* If we arrived here from different branches with different\n\t * limits to sanitize, then this won't work.\n\t */\n\tif (aux->alu_state &&\n\t    (aux->alu_state != alu_state ||\n\t     aux->alu_limit != alu_limit))\n\t\treturn -EACCES;\n\n\t/* Corresponding fixup done in fixup_bpf_calls(). */\n\taux->alu_state = alu_state;\n\taux->alu_limit = alu_limit;\n\ndo_sim:\n\t/* Simulate and find potential out-of-bounds access under\n\t * speculative execution from truncation as a result of\n\t * masking when off was not within expected range. If off\n\t * sits in dst, then we temporarily need to move ptr there\n\t * to simulate dst (== 0) +/-= ptr. Needed, for example,\n\t * for cases where we use K-based arithmetic in one direction\n\t * and truncated reg-based in the other in order to explore\n\t * bad access.\n\t */\n\tif (!ptr_is_dst_reg) {\n\t\ttmp = *dst_reg;\n\t\t*dst_reg = *ptr_reg;\n\t}\n\tret = push_stack(env, env->insn_idx + 1, env->insn_idx, true);\n\tif (!ptr_is_dst_reg)\n\t\t*dst_reg = tmp;\n\treturn !ret ? -EFAULT : 0;\n}\n\n/* Handles arithmetic on a pointer and a scalar: computes new min/max and var_off.\n * Caller should also handle BPF_MOV case separately.\n * If we return -EACCES, caller may want to try again treating pointer as a\n * scalar.  So we only emit a diagnostic if !env->allow_ptr_leaks.\n */\nstatic int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn,\n\t\t\t\t   const struct bpf_reg_state *ptr_reg,\n\t\t\t\t   const struct bpf_reg_state *off_reg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg;\n\tbool known = tnum_is_const(off_reg->var_off);\n\ts64 smin_val = off_reg->smin_value, smax_val = off_reg->smax_value,\n\t    smin_ptr = ptr_reg->smin_value, smax_ptr = ptr_reg->smax_value;\n\tu64 umin_val = off_reg->umin_value, umax_val = off_reg->umax_value,\n\t    umin_ptr = ptr_reg->umin_value, umax_ptr = ptr_reg->umax_value;\n\tu32 dst = insn->dst_reg, src = insn->src_reg;\n\tu8 opcode = BPF_OP(insn->code);\n\tint ret;\n\n\tdst_reg = &regs[dst];\n\n\tif ((known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops on pointers produce (meaningless) scalars */\n\t\tverbose(env,\n\t\t\t\"R%d 32-bit pointer arithmetic prohibited\\n\",\n\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\n\tswitch (ptr_reg->type) {\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited, null-check it first\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tcase CONST_PTR_TO_MAP:\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (!env->allow_ptr_leaks && !known && (smin_val < 0) != (smax_val < 0)) {\n\t\t\tverbose(env, \"R%d has unknown scalar with mixed signed bounds, pointer arithmetic with it prohibited for !root\\n\",\n\t\t\t\toff_reg == dst_reg ? dst : src);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* fall-through */\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* In case of 'scalar += pointer', dst_reg inherits pointer type and id.\n\t * The id may be overwritten later if we create a new variable offset.\n\t */\n\tdst_reg->type = ptr_reg->type;\n\tdst_reg->id = ptr_reg->id;\n\n\tif (!check_reg_sane_offset(env, off_reg, ptr_reg->type) ||\n\t    !check_reg_sane_offset(env, ptr_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, dst_reg, smin_val < 0);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to add from different maps or paths\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\t/* We can take a fixed offset as long as it doesn't overflow\n\t\t * the s32 'off' field\n\t\t */\n\t\tif (known && (ptr_reg->off + smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off + smin_val))) {\n\t\t\t/* pointer += K.  Accumulate it into fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->off = ptr_reg->off + smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  Note that off_reg->off\n\t\t * == 0, since it's a scalar.\n\t\t * dst_reg gets the pointer type and since some positive\n\t\t * integer value was added to the pointer, give it a new 'id'\n\t\t * if it's a PTR_TO_PACKET.\n\t\t * this creates a new 'base' pointer, off_reg (variable) gets\n\t\t * added into the variable offset, and we copy the fixed offset\n\t\t * from ptr_reg.\n\t\t */\n\t\tif (signed_add_overflows(smin_ptr, smin_val) ||\n\t\t    signed_add_overflows(smax_ptr, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr + smin_val;\n\t\t\tdst_reg->smax_value = smax_ptr + smax_val;\n\t\t}\n\t\tif (umin_ptr + umin_val < umin_ptr ||\n\t\t    umax_ptr + umax_val < umax_ptr) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value = umin_ptr + umin_val;\n\t\t\tdst_reg->umax_value = umax_ptr + umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tdst_reg->raw = 0;\n\t\t}\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, dst_reg, smin_val < 0);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to sub from different maps or paths\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\tif (dst_reg == off_reg) {\n\t\t\t/* scalar -= pointer.  Creates an unknown scalar */\n\t\t\tverbose(env, \"R%d tried to subtract pointer from scalar\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* We don't allow subtraction from FP, because (according to\n\t\t * test_verifier.c test \"invalid fp arithmetic\", JITs might not\n\t\t * be able to deal with it.\n\t\t */\n\t\tif (ptr_reg->type == PTR_TO_STACK) {\n\t\t\tverbose(env, \"R%d subtraction from stack pointer prohibited\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (known && (ptr_reg->off - smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off - smin_val))) {\n\t\t\t/* pointer -= K.  Subtract it from fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->id = ptr_reg->id;\n\t\t\tdst_reg->off = ptr_reg->off - smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  If the subtrahend is known\n\t\t * nonnegative, then any reg->range we had before is still good.\n\t\t */\n\t\tif (signed_sub_overflows(smin_ptr, smax_val) ||\n\t\t    signed_sub_overflows(smax_ptr, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr - smax_val;\n\t\t\tdst_reg->smax_value = smax_ptr - smin_val;\n\t\t}\n\t\tif (umin_ptr < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value = umin_ptr - umax_val;\n\t\t\tdst_reg->umax_value = umax_ptr - umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tif (smin_val < 0)\n\t\t\t\tdst_reg->raw = 0;\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\tcase BPF_OR:\n\tcase BPF_XOR:\n\t\t/* bitwise ops on pointers are troublesome, prohibit. */\n\t\tverbose(env, \"R%d bitwise operator %s on pointer prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\tdefault:\n\t\t/* other operators (e.g. MUL,LSH) produce non-pointer results */\n\t\tverbose(env, \"R%d pointer arithmetic with %s operator prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!check_reg_sane_offset(env, dst_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t__update_reg_bounds(dst_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\n\t/* For unprivileged we require that resulting offset must be in bounds\n\t * in order to be able to sanitize access later on.\n\t */\n\tif (!env->allow_ptr_leaks) {\n\t\tif (dst_reg->type == PTR_TO_MAP_VALUE &&\n\t\t    check_map_access(env, dst, dst_reg->off, 1, false)) {\n\t\t\tverbose(env, \"R%d pointer arithmetic of map value goes out of range, \"\n\t\t\t\t\"prohibited for !root\\n\", dst);\n\t\t\treturn -EACCES;\n\t\t} else if (dst_reg->type == PTR_TO_STACK &&\n\t\t\t   check_stack_access(env, dst_reg, dst_reg->off +\n\t\t\t\t\t      dst_reg->var_off.value, 1)) {\n\t\t\tverbose(env, \"R%d stack pointer arithmetic goes out of range, \"\n\t\t\t\t\"prohibited for !root\\n\", dst);\n\t\t\treturn -EACCES;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/* WARNING: This function does calculations on 64-bit values, but the actual\n * execution may occur on 32-bit values. Therefore, things like bitshifts\n * need extra checks in the 32-bit case.\n */\nstatic int adjust_scalar_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t      struct bpf_insn *insn,\n\t\t\t\t      struct bpf_reg_state *dst_reg,\n\t\t\t\t      struct bpf_reg_state src_reg)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 opcode = BPF_OP(insn->code);\n\tbool src_known, dst_known;\n\ts64 smin_val, smax_val;\n\tu64 umin_val, umax_val;\n\tu64 insn_bitness = (BPF_CLASS(insn->code) == BPF_ALU64) ? 64 : 32;\n\n\tif (insn_bitness == 32) {\n\t\t/* Relevant for 32-bit RSH: Information can propagate towards\n\t\t * LSB, so it isn't sufficient to only truncate the output to\n\t\t * 32 bits.\n\t\t */\n\t\tcoerce_reg_to_size(dst_reg, 4);\n\t\tcoerce_reg_to_size(&src_reg, 4);\n\t}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 3154,
        "critical_vars": [
            "env",
            "insn"
        ],
        "function": "sanitize_ptr_alu",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_d3bd7413e0ca40b60cf60d4003246d067cafdeda_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic int sanitize_ptr_alu(struct bpf_verifier_env *env,\n\t\t\t    struct bpf_insn *insn,\n\t\t\t    const struct bpf_reg_state *ptr_reg,\n\t\t\t    struct bpf_reg_state *dst_reg,\n\t\t\t    bool off_is_neg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_insn_aux_data *aux = cur_aux(env);\n\tbool ptr_is_dst_reg = ptr_reg == dst_reg;\n\tu8 opcode = BPF_OP(insn->code);\n\tu32 alu_state, alu_limit;\n\tstruct bpf_reg_state tmp;\n\tbool ret;\n\n\tif (can_skip_alu_sanitation(env, insn))\n\t\treturn 0;\n\n\t/* We already marked aux for masking from non-speculative\n\t * paths, thus we got here in the first place. We only care\n\t * to explore bad access from here.\n\t */\n\tif (vstate->speculative)\n\t\tgoto do_sim;\n\n\talu_state  = off_is_neg ? BPF_ALU_NEG_VALUE : 0;\n\talu_state |= ptr_is_dst_reg ?\n\t\t     BPF_ALU_SANITIZE_SRC : BPF_ALU_SANITIZE_DST;\n\n\tif (retrieve_ptr_limit(ptr_reg, &alu_limit, opcode, off_is_neg))\n\t\treturn 0;\n\tif (update_alu_sanitation_state(aux, alu_state, alu_limit))\n\t\treturn -EACCES;\ndo_sim:\n\t/* Simulate and find potential out-of-bounds access under\n\t * speculative execution from truncation as a result of\n\t * masking when off was not within expected range. If off\n\t * sits in dst, then we temporarily need to move ptr there\n\t * to simulate dst (== 0) +/-= ptr. Needed, for example,\n\t * for cases where we use K-based arithmetic in one direction\n\t * and truncated reg-based in the other in order to explore\n\t * bad access.\n\t */\n\tif (!ptr_is_dst_reg) {\n\t\ttmp = *dst_reg;\n\t\t*dst_reg = *ptr_reg;\n\t}\n\tret = push_stack(env, env->insn_idx + 1, env->insn_idx, true);\n\tif (!ptr_is_dst_reg)\n\t\t*dst_reg = tmp;\n\treturn !ret ? -EFAULT : 0;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "if-Condition",
        "line_old": 3140,
        "critical_vars": [
            "aux->alu_state",
            "aux->alu_limit"
        ],
        "function": "sanitize_ptr_alu",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_d3bd7413e0ca40b60cf60d4003246d067cafdeda_verifier.c.diff",
        "label": "False",
        "function_code": "\nstatic int sanitize_ptr_alu(struct bpf_verifier_env *env,\n\t\t\t    struct bpf_insn *insn,\n\t\t\t    const struct bpf_reg_state *ptr_reg,\n\t\t\t    struct bpf_reg_state *dst_reg,\n\t\t\t    bool off_is_neg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_insn_aux_data *aux = cur_aux(env);\n\tbool ptr_is_dst_reg = ptr_reg == dst_reg;\n\tu8 opcode = BPF_OP(insn->code);\n\tu32 alu_state, alu_limit;\n\tstruct bpf_reg_state tmp;\n\tbool ret;\n\n\tif (env->allow_ptr_leaks || BPF_SRC(insn->code) == BPF_K)\n\t\treturn 0;\n\n\t/* We already marked aux for masking from non-speculative\n\t * paths, thus we got here in the first place. We only care\n\t * to explore bad access from here.\n\t */\n\tif (vstate->speculative)\n\t\tgoto do_sim;\n\n\talu_state  = off_is_neg ? BPF_ALU_NEG_VALUE : 0;\n\talu_state |= ptr_is_dst_reg ?\n\t\t     BPF_ALU_SANITIZE_SRC : BPF_ALU_SANITIZE_DST;\n\n\tif (retrieve_ptr_limit(ptr_reg, &alu_limit, opcode, off_is_neg))\n\t\treturn 0;\n\n\t/* If we arrived here from different branches with different\n\t * limits to sanitize, then this won't work.\n\t */\n\tif (aux->alu_state &&\n\t    (aux->alu_state != alu_state ||\n\t     aux->alu_limit != alu_limit))\n\t\treturn -EACCES;\n\n\t/* Corresponding fixup done in fixup_bpf_calls(). */\n\taux->alu_state = alu_state;\n\taux->alu_limit = alu_limit;\n\ndo_sim:\n\t/* Simulate and find potential out-of-bounds access under\n\t * speculative execution from truncation as a result of\n\t * masking when off was not within expected range. If off\n\t * sits in dst, then we temporarily need to move ptr there\n\t * to simulate dst (== 0) +/-= ptr. Needed, for example,\n\t * for cases where we use K-based arithmetic in one direction\n\t * and truncated reg-based in the other in order to explore\n\t * bad access.\n\t */\n\tif (!ptr_is_dst_reg) {\n\t\ttmp = *dst_reg;\n\t\t*dst_reg = *ptr_reg;\n\t}\n\tret = push_stack(env, env->insn_idx + 1, env->insn_idx, true);\n\tif (!ptr_is_dst_reg)\n\t\t*dst_reg = tmp;\n\treturn !ret ? -EFAULT : 0;\n}\n\n/* Handles arithmetic on a pointer and a scalar: computes new min/max and var_off.\n * Caller should also handle BPF_MOV case separately.\n * If we return -EACCES, caller may want to try again treating pointer as a\n * scalar.  So we only emit a diagnostic if !env->allow_ptr_leaks.\n */\nstatic int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn,\n\t\t\t\t   const struct bpf_reg_state *ptr_reg,\n\t\t\t\t   const struct bpf_reg_state *off_reg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg;\n\tbool known = tnum_is_const(off_reg->var_off);\n\ts64 smin_val = off_reg->smin_value, smax_val = off_reg->smax_value,\n\t    smin_ptr = ptr_reg->smin_value, smax_ptr = ptr_reg->smax_value;\n\tu64 umin_val = off_reg->umin_value, umax_val = off_reg->umax_value,\n\t    umin_ptr = ptr_reg->umin_value, umax_ptr = ptr_reg->umax_value;\n\tu32 dst = insn->dst_reg, src = insn->src_reg;\n\tu8 opcode = BPF_OP(insn->code);\n\tint ret;\n\n\tdst_reg = &regs[dst];\n\n\tif ((known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops on pointers produce (meaningless) scalars */\n\t\tverbose(env,\n\t\t\t\"R%d 32-bit pointer arithmetic prohibited\\n\",\n\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\n\tswitch (ptr_reg->type) {\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited, null-check it first\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tcase CONST_PTR_TO_MAP:\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (!env->allow_ptr_leaks && !known && (smin_val < 0) != (smax_val < 0)) {\n\t\t\tverbose(env, \"R%d has unknown scalar with mixed signed bounds, pointer arithmetic with it prohibited for !root\\n\",\n\t\t\t\toff_reg == dst_reg ? dst : src);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* fall-through */\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* In case of 'scalar += pointer', dst_reg inherits pointer type and id.\n\t * The id may be overwritten later if we create a new variable offset.\n\t */\n\tdst_reg->type = ptr_reg->type;\n\tdst_reg->id = ptr_reg->id;\n\n\tif (!check_reg_sane_offset(env, off_reg, ptr_reg->type) ||\n\t    !check_reg_sane_offset(env, ptr_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, dst_reg, smin_val < 0);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to add from different maps or paths\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\t/* We can take a fixed offset as long as it doesn't overflow\n\t\t * the s32 'off' field\n\t\t */\n\t\tif (known && (ptr_reg->off + smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off + smin_val))) {\n\t\t\t/* pointer += K.  Accumulate it into fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->off = ptr_reg->off + smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  Note that off_reg->off\n\t\t * == 0, since it's a scalar.\n\t\t * dst_reg gets the pointer type and since some positive\n\t\t * integer value was added to the pointer, give it a new 'id'\n\t\t * if it's a PTR_TO_PACKET.\n\t\t * this creates a new 'base' pointer, off_reg (variable) gets\n\t\t * added into the variable offset, and we copy the fixed offset\n\t\t * from ptr_reg.\n\t\t */\n\t\tif (signed_add_overflows(smin_ptr, smin_val) ||\n\t\t    signed_add_overflows(smax_ptr, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr + smin_val;\n\t\t\tdst_reg->smax_value = smax_ptr + smax_val;\n\t\t}\n\t\tif (umin_ptr + umin_val < umin_ptr ||\n\t\t    umax_ptr + umax_val < umax_ptr) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value = umin_ptr + umin_val;\n\t\t\tdst_reg->umax_value = umax_ptr + umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tdst_reg->raw = 0;\n\t\t}\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, dst_reg, smin_val < 0);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to sub from different maps or paths\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\tif (dst_reg == off_reg) {\n\t\t\t/* scalar -= pointer.  Creates an unknown scalar */\n\t\t\tverbose(env, \"R%d tried to subtract pointer from scalar\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* We don't allow subtraction from FP, because (according to\n\t\t * test_verifier.c test \"invalid fp arithmetic\", JITs might not\n\t\t * be able to deal with it.\n\t\t */\n\t\tif (ptr_reg->type == PTR_TO_STACK) {\n\t\t\tverbose(env, \"R%d subtraction from stack pointer prohibited\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (known && (ptr_reg->off - smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off - smin_val))) {\n\t\t\t/* pointer -= K.  Subtract it from fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->id = ptr_reg->id;\n\t\t\tdst_reg->off = ptr_reg->off - smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  If the subtrahend is known\n\t\t * nonnegative, then any reg->range we had before is still good.\n\t\t */\n\t\tif (signed_sub_overflows(smin_ptr, smax_val) ||\n\t\t    signed_sub_overflows(smax_ptr, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr - smax_val;\n\t\t\tdst_reg->smax_value = smax_ptr - smin_val;\n\t\t}\n\t\tif (umin_ptr < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value = umin_ptr - umax_val;\n\t\t\tdst_reg->umax_value = umax_ptr - umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tif (smin_val < 0)\n\t\t\t\tdst_reg->raw = 0;\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\tcase BPF_OR:\n\tcase BPF_XOR:\n\t\t/* bitwise ops on pointers are troublesome, prohibit. */\n\t\tverbose(env, \"R%d bitwise operator %s on pointer prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\tdefault:\n\t\t/* other operators (e.g. MUL,LSH) produce non-pointer results */\n\t\tverbose(env, \"R%d pointer arithmetic with %s operator prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!check_reg_sane_offset(env, dst_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t__update_reg_bounds(dst_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\n\t/* For unprivileged we require that resulting offset must be in bounds\n\t * in order to be able to sanitize access later on.\n\t */\n\tif (!env->allow_ptr_leaks) {\n\t\tif (dst_reg->type == PTR_TO_MAP_VALUE &&\n\t\t    check_map_access(env, dst, dst_reg->off, 1, false)) {\n\t\t\tverbose(env, \"R%d pointer arithmetic of map value goes out of range, \"\n\t\t\t\t\"prohibited for !root\\n\", dst);\n\t\t\treturn -EACCES;\n\t\t} else if (dst_reg->type == PTR_TO_STACK &&\n\t\t\t   check_stack_access(env, dst_reg, dst_reg->off +\n\t\t\t\t\t      dst_reg->var_off.value, 1)) {\n\t\t\tverbose(env, \"R%d stack pointer arithmetic goes out of range, \"\n\t\t\t\t\"prohibited for !root\\n\", dst);\n\t\t\treturn -EACCES;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/* WARNING: This function does calculations on 64-bit values, but the actual\n * execution may occur on 32-bit values. Therefore, things like bitshifts\n * need extra checks in the 32-bit case.\n */\nstatic int adjust_scalar_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t      struct bpf_insn *insn,\n\t\t\t\t      struct bpf_reg_state *dst_reg,\n\t\t\t\t      struct bpf_reg_state src_reg)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 opcode = BPF_OP(insn->code);\n\tbool src_known, dst_known;\n\ts64 smin_val, smax_val;\n\tu64 umin_val, umax_val;\n\tu64 insn_bitness = (BPF_CLASS(insn->code) == BPF_ALU64) ? 64 : 32;\n\n\tif (insn_bitness == 32) {\n\t\t/* Relevant for 32-bit RSH: Information can propagate towards\n\t\t * LSB, so it isn't sufficient to only truncate the output to\n\t\t * 32 bits.\n\t\t */\n\t\tcoerce_reg_to_size(dst_reg, 4);\n\t\tcoerce_reg_to_size(&src_reg, 4);\n\t}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 3170,
        "critical_vars": [
            "alu_state",
            "alu_limit",
            "aux"
        ],
        "function": "sanitize_ptr_alu",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_d3bd7413e0ca40b60cf60d4003246d067cafdeda_verifier.c.diff",
        "label": "True",
        "function_code": "\nstatic int sanitize_ptr_alu(struct bpf_verifier_env *env,\n\t\t\t    struct bpf_insn *insn,\n\t\t\t    const struct bpf_reg_state *ptr_reg,\n\t\t\t    struct bpf_reg_state *dst_reg,\n\t\t\t    bool off_is_neg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_insn_aux_data *aux = cur_aux(env);\n\tbool ptr_is_dst_reg = ptr_reg == dst_reg;\n\tu8 opcode = BPF_OP(insn->code);\n\tu32 alu_state, alu_limit;\n\tstruct bpf_reg_state tmp;\n\tbool ret;\n\n\tif (can_skip_alu_sanitation(env, insn))\n\t\treturn 0;\n\n\t/* We already marked aux for masking from non-speculative\n\t * paths, thus we got here in the first place. We only care\n\t * to explore bad access from here.\n\t */\n\tif (vstate->speculative)\n\t\tgoto do_sim;\n\n\talu_state  = off_is_neg ? BPF_ALU_NEG_VALUE : 0;\n\talu_state |= ptr_is_dst_reg ?\n\t\t     BPF_ALU_SANITIZE_SRC : BPF_ALU_SANITIZE_DST;\n\n\tif (retrieve_ptr_limit(ptr_reg, &alu_limit, opcode, off_is_neg))\n\t\treturn 0;\n\tif (update_alu_sanitation_state(aux, alu_state, alu_limit))\n\t\treturn -EACCES;\ndo_sim:\n\t/* Simulate and find potential out-of-bounds access under\n\t * speculative execution from truncation as a result of\n\t * masking when off was not within expected range. If off\n\t * sits in dst, then we temporarily need to move ptr there\n\t * to simulate dst (== 0) +/-= ptr. Needed, for example,\n\t * for cases where we use K-based arithmetic in one direction\n\t * and truncated reg-based in the other in order to explore\n\t * bad access.\n\t */\n\tif (!ptr_is_dst_reg) {\n\t\ttmp = *dst_reg;\n\t\t*dst_reg = *ptr_reg;\n\t}\n\tret = push_stack(env, env->insn_idx + 1, env->insn_idx, true);\n\tif (!ptr_is_dst_reg)\n\t\t*dst_reg = tmp;\n\treturn !ret ? -EFAULT : 0;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 3146,
        "critical_vars": [
            "aux->alu_state"
        ],
        "function": "sanitize_ptr_alu",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_d3bd7413e0ca40b60cf60d4003246d067cafdeda_verifier.c.diff",
        "label": "False",
        "function_code": "\nstatic int sanitize_ptr_alu(struct bpf_verifier_env *env,\n\t\t\t    struct bpf_insn *insn,\n\t\t\t    const struct bpf_reg_state *ptr_reg,\n\t\t\t    struct bpf_reg_state *dst_reg,\n\t\t\t    bool off_is_neg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_insn_aux_data *aux = cur_aux(env);\n\tbool ptr_is_dst_reg = ptr_reg == dst_reg;\n\tu8 opcode = BPF_OP(insn->code);\n\tu32 alu_state, alu_limit;\n\tstruct bpf_reg_state tmp;\n\tbool ret;\n\n\tif (env->allow_ptr_leaks || BPF_SRC(insn->code) == BPF_K)\n\t\treturn 0;\n\n\t/* We already marked aux for masking from non-speculative\n\t * paths, thus we got here in the first place. We only care\n\t * to explore bad access from here.\n\t */\n\tif (vstate->speculative)\n\t\tgoto do_sim;\n\n\talu_state  = off_is_neg ? BPF_ALU_NEG_VALUE : 0;\n\talu_state |= ptr_is_dst_reg ?\n\t\t     BPF_ALU_SANITIZE_SRC : BPF_ALU_SANITIZE_DST;\n\n\tif (retrieve_ptr_limit(ptr_reg, &alu_limit, opcode, off_is_neg))\n\t\treturn 0;\n\n\t/* If we arrived here from different branches with different\n\t * limits to sanitize, then this won't work.\n\t */\n\tif (aux->alu_state &&\n\t    (aux->alu_state != alu_state ||\n\t     aux->alu_limit != alu_limit))\n\t\treturn -EACCES;\n\n\t/* Corresponding fixup done in fixup_bpf_calls(). */\n\taux->alu_state = alu_state;\n\taux->alu_limit = alu_limit;\n\ndo_sim:\n\t/* Simulate and find potential out-of-bounds access under\n\t * speculative execution from truncation as a result of\n\t * masking when off was not within expected range. If off\n\t * sits in dst, then we temporarily need to move ptr there\n\t * to simulate dst (== 0) +/-= ptr. Needed, for example,\n\t * for cases where we use K-based arithmetic in one direction\n\t * and truncated reg-based in the other in order to explore\n\t * bad access.\n\t */\n\tif (!ptr_is_dst_reg) {\n\t\ttmp = *dst_reg;\n\t\t*dst_reg = *ptr_reg;\n\t}\n\tret = push_stack(env, env->insn_idx + 1, env->insn_idx, true);\n\tif (!ptr_is_dst_reg)\n\t\t*dst_reg = tmp;\n\treturn !ret ? -EFAULT : 0;\n}\n\n/* Handles arithmetic on a pointer and a scalar: computes new min/max and var_off.\n * Caller should also handle BPF_MOV case separately.\n * If we return -EACCES, caller may want to try again treating pointer as a\n * scalar.  So we only emit a diagnostic if !env->allow_ptr_leaks.\n */\nstatic int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn,\n\t\t\t\t   const struct bpf_reg_state *ptr_reg,\n\t\t\t\t   const struct bpf_reg_state *off_reg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg;\n\tbool known = tnum_is_const(off_reg->var_off);\n\ts64 smin_val = off_reg->smin_value, smax_val = off_reg->smax_value,\n\t    smin_ptr = ptr_reg->smin_value, smax_ptr = ptr_reg->smax_value;\n\tu64 umin_val = off_reg->umin_value, umax_val = off_reg->umax_value,\n\t    umin_ptr = ptr_reg->umin_value, umax_ptr = ptr_reg->umax_value;\n\tu32 dst = insn->dst_reg, src = insn->src_reg;\n\tu8 opcode = BPF_OP(insn->code);\n\tint ret;\n\n\tdst_reg = &regs[dst];\n\n\tif ((known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops on pointers produce (meaningless) scalars */\n\t\tverbose(env,\n\t\t\t\"R%d 32-bit pointer arithmetic prohibited\\n\",\n\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\n\tswitch (ptr_reg->type) {\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited, null-check it first\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tcase CONST_PTR_TO_MAP:\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (!env->allow_ptr_leaks && !known && (smin_val < 0) != (smax_val < 0)) {\n\t\t\tverbose(env, \"R%d has unknown scalar with mixed signed bounds, pointer arithmetic with it prohibited for !root\\n\",\n\t\t\t\toff_reg == dst_reg ? dst : src);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* fall-through */\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* In case of 'scalar += pointer', dst_reg inherits pointer type and id.\n\t * The id may be overwritten later if we create a new variable offset.\n\t */\n\tdst_reg->type = ptr_reg->type;\n\tdst_reg->id = ptr_reg->id;\n\n\tif (!check_reg_sane_offset(env, off_reg, ptr_reg->type) ||\n\t    !check_reg_sane_offset(env, ptr_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, dst_reg, smin_val < 0);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to add from different maps or paths\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\t/* We can take a fixed offset as long as it doesn't overflow\n\t\t * the s32 'off' field\n\t\t */\n\t\tif (known && (ptr_reg->off + smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off + smin_val))) {\n\t\t\t/* pointer += K.  Accumulate it into fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->off = ptr_reg->off + smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  Note that off_reg->off\n\t\t * == 0, since it's a scalar.\n\t\t * dst_reg gets the pointer type and since some positive\n\t\t * integer value was added to the pointer, give it a new 'id'\n\t\t * if it's a PTR_TO_PACKET.\n\t\t * this creates a new 'base' pointer, off_reg (variable) gets\n\t\t * added into the variable offset, and we copy the fixed offset\n\t\t * from ptr_reg.\n\t\t */\n\t\tif (signed_add_overflows(smin_ptr, smin_val) ||\n\t\t    signed_add_overflows(smax_ptr, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr + smin_val;\n\t\t\tdst_reg->smax_value = smax_ptr + smax_val;\n\t\t}\n\t\tif (umin_ptr + umin_val < umin_ptr ||\n\t\t    umax_ptr + umax_val < umax_ptr) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value = umin_ptr + umin_val;\n\t\t\tdst_reg->umax_value = umax_ptr + umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tdst_reg->raw = 0;\n\t\t}\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, dst_reg, smin_val < 0);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to sub from different maps or paths\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\tif (dst_reg == off_reg) {\n\t\t\t/* scalar -= pointer.  Creates an unknown scalar */\n\t\t\tverbose(env, \"R%d tried to subtract pointer from scalar\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* We don't allow subtraction from FP, because (according to\n\t\t * test_verifier.c test \"invalid fp arithmetic\", JITs might not\n\t\t * be able to deal with it.\n\t\t */\n\t\tif (ptr_reg->type == PTR_TO_STACK) {\n\t\t\tverbose(env, \"R%d subtraction from stack pointer prohibited\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (known && (ptr_reg->off - smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off - smin_val))) {\n\t\t\t/* pointer -= K.  Subtract it from fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->id = ptr_reg->id;\n\t\t\tdst_reg->off = ptr_reg->off - smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  If the subtrahend is known\n\t\t * nonnegative, then any reg->range we had before is still good.\n\t\t */\n\t\tif (signed_sub_overflows(smin_ptr, smax_val) ||\n\t\t    signed_sub_overflows(smax_ptr, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr - smax_val;\n\t\t\tdst_reg->smax_value = smax_ptr - smin_val;\n\t\t}\n\t\tif (umin_ptr < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value = umin_ptr - umax_val;\n\t\t\tdst_reg->umax_value = umax_ptr - umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tif (smin_val < 0)\n\t\t\t\tdst_reg->raw = 0;\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\tcase BPF_OR:\n\tcase BPF_XOR:\n\t\t/* bitwise ops on pointers are troublesome, prohibit. */\n\t\tverbose(env, \"R%d bitwise operator %s on pointer prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\tdefault:\n\t\t/* other operators (e.g. MUL,LSH) produce non-pointer results */\n\t\tverbose(env, \"R%d pointer arithmetic with %s operator prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!check_reg_sane_offset(env, dst_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t__update_reg_bounds(dst_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\n\t/* For unprivileged we require that resulting offset must be in bounds\n\t * in order to be able to sanitize access later on.\n\t */\n\tif (!env->allow_ptr_leaks) {\n\t\tif (dst_reg->type == PTR_TO_MAP_VALUE &&\n\t\t    check_map_access(env, dst, dst_reg->off, 1, false)) {\n\t\t\tverbose(env, \"R%d pointer arithmetic of map value goes out of range, \"\n\t\t\t\t\"prohibited for !root\\n\", dst);\n\t\t\treturn -EACCES;\n\t\t} else if (dst_reg->type == PTR_TO_STACK &&\n\t\t\t   check_stack_access(env, dst_reg, dst_reg->off +\n\t\t\t\t\t      dst_reg->var_off.value, 1)) {\n\t\t\tverbose(env, \"R%d stack pointer arithmetic goes out of range, \"\n\t\t\t\t\"prohibited for !root\\n\", dst);\n\t\t\treturn -EACCES;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/* WARNING: This function does calculations on 64-bit values, but the actual\n * execution may occur on 32-bit values. Therefore, things like bitshifts\n * need extra checks in the 32-bit case.\n */\nstatic int adjust_scalar_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t      struct bpf_insn *insn,\n\t\t\t\t      struct bpf_reg_state *dst_reg,\n\t\t\t\t      struct bpf_reg_state src_reg)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 opcode = BPF_OP(insn->code);\n\tbool src_known, dst_known;\n\ts64 smin_val, smax_val;\n\tu64 umin_val, umax_val;\n\tu64 insn_bitness = (BPF_CLASS(insn->code) == BPF_ALU64) ? 64 : 32;\n\n\tif (insn_bitness == 32) {\n\t\t/* Relevant for 32-bit RSH: Information can propagate towards\n\t\t * LSB, so it isn't sufficient to only truncate the output to\n\t\t * 32 bits.\n\t\t */\n\t\tcoerce_reg_to_size(dst_reg, 4);\n\t\tcoerce_reg_to_size(&src_reg, 4);\n\t}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 3147,
        "critical_vars": [
            "aux->alu_limit"
        ],
        "function": "sanitize_ptr_alu",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_d3bd7413e0ca40b60cf60d4003246d067cafdeda_verifier.c.diff",
        "label": "False",
        "function_code": "\nstatic int sanitize_ptr_alu(struct bpf_verifier_env *env,\n\t\t\t    struct bpf_insn *insn,\n\t\t\t    const struct bpf_reg_state *ptr_reg,\n\t\t\t    struct bpf_reg_state *dst_reg,\n\t\t\t    bool off_is_neg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_insn_aux_data *aux = cur_aux(env);\n\tbool ptr_is_dst_reg = ptr_reg == dst_reg;\n\tu8 opcode = BPF_OP(insn->code);\n\tu32 alu_state, alu_limit;\n\tstruct bpf_reg_state tmp;\n\tbool ret;\n\n\tif (env->allow_ptr_leaks || BPF_SRC(insn->code) == BPF_K)\n\t\treturn 0;\n\n\t/* We already marked aux for masking from non-speculative\n\t * paths, thus we got here in the first place. We only care\n\t * to explore bad access from here.\n\t */\n\tif (vstate->speculative)\n\t\tgoto do_sim;\n\n\talu_state  = off_is_neg ? BPF_ALU_NEG_VALUE : 0;\n\talu_state |= ptr_is_dst_reg ?\n\t\t     BPF_ALU_SANITIZE_SRC : BPF_ALU_SANITIZE_DST;\n\n\tif (retrieve_ptr_limit(ptr_reg, &alu_limit, opcode, off_is_neg))\n\t\treturn 0;\n\n\t/* If we arrived here from different branches with different\n\t * limits to sanitize, then this won't work.\n\t */\n\tif (aux->alu_state &&\n\t    (aux->alu_state != alu_state ||\n\t     aux->alu_limit != alu_limit))\n\t\treturn -EACCES;\n\n\t/* Corresponding fixup done in fixup_bpf_calls(). */\n\taux->alu_state = alu_state;\n\taux->alu_limit = alu_limit;\n\ndo_sim:\n\t/* Simulate and find potential out-of-bounds access under\n\t * speculative execution from truncation as a result of\n\t * masking when off was not within expected range. If off\n\t * sits in dst, then we temporarily need to move ptr there\n\t * to simulate dst (== 0) +/-= ptr. Needed, for example,\n\t * for cases where we use K-based arithmetic in one direction\n\t * and truncated reg-based in the other in order to explore\n\t * bad access.\n\t */\n\tif (!ptr_is_dst_reg) {\n\t\ttmp = *dst_reg;\n\t\t*dst_reg = *ptr_reg;\n\t}\n\tret = push_stack(env, env->insn_idx + 1, env->insn_idx, true);\n\tif (!ptr_is_dst_reg)\n\t\t*dst_reg = tmp;\n\treturn !ret ? -EFAULT : 0;\n}\n\n/* Handles arithmetic on a pointer and a scalar: computes new min/max and var_off.\n * Caller should also handle BPF_MOV case separately.\n * If we return -EACCES, caller may want to try again treating pointer as a\n * scalar.  So we only emit a diagnostic if !env->allow_ptr_leaks.\n */\nstatic int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn,\n\t\t\t\t   const struct bpf_reg_state *ptr_reg,\n\t\t\t\t   const struct bpf_reg_state *off_reg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg;\n\tbool known = tnum_is_const(off_reg->var_off);\n\ts64 smin_val = off_reg->smin_value, smax_val = off_reg->smax_value,\n\t    smin_ptr = ptr_reg->smin_value, smax_ptr = ptr_reg->smax_value;\n\tu64 umin_val = off_reg->umin_value, umax_val = off_reg->umax_value,\n\t    umin_ptr = ptr_reg->umin_value, umax_ptr = ptr_reg->umax_value;\n\tu32 dst = insn->dst_reg, src = insn->src_reg;\n\tu8 opcode = BPF_OP(insn->code);\n\tint ret;\n\n\tdst_reg = &regs[dst];\n\n\tif ((known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops on pointers produce (meaningless) scalars */\n\t\tverbose(env,\n\t\t\t\"R%d 32-bit pointer arithmetic prohibited\\n\",\n\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\n\tswitch (ptr_reg->type) {\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited, null-check it first\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tcase CONST_PTR_TO_MAP:\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (!env->allow_ptr_leaks && !known && (smin_val < 0) != (smax_val < 0)) {\n\t\t\tverbose(env, \"R%d has unknown scalar with mixed signed bounds, pointer arithmetic with it prohibited for !root\\n\",\n\t\t\t\toff_reg == dst_reg ? dst : src);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* fall-through */\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* In case of 'scalar += pointer', dst_reg inherits pointer type and id.\n\t * The id may be overwritten later if we create a new variable offset.\n\t */\n\tdst_reg->type = ptr_reg->type;\n\tdst_reg->id = ptr_reg->id;\n\n\tif (!check_reg_sane_offset(env, off_reg, ptr_reg->type) ||\n\t    !check_reg_sane_offset(env, ptr_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, dst_reg, smin_val < 0);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to add from different maps or paths\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\t/* We can take a fixed offset as long as it doesn't overflow\n\t\t * the s32 'off' field\n\t\t */\n\t\tif (known && (ptr_reg->off + smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off + smin_val))) {\n\t\t\t/* pointer += K.  Accumulate it into fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->off = ptr_reg->off + smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  Note that off_reg->off\n\t\t * == 0, since it's a scalar.\n\t\t * dst_reg gets the pointer type and since some positive\n\t\t * integer value was added to the pointer, give it a new 'id'\n\t\t * if it's a PTR_TO_PACKET.\n\t\t * this creates a new 'base' pointer, off_reg (variable) gets\n\t\t * added into the variable offset, and we copy the fixed offset\n\t\t * from ptr_reg.\n\t\t */\n\t\tif (signed_add_overflows(smin_ptr, smin_val) ||\n\t\t    signed_add_overflows(smax_ptr, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr + smin_val;\n\t\t\tdst_reg->smax_value = smax_ptr + smax_val;\n\t\t}\n\t\tif (umin_ptr + umin_val < umin_ptr ||\n\t\t    umax_ptr + umax_val < umax_ptr) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value = umin_ptr + umin_val;\n\t\t\tdst_reg->umax_value = umax_ptr + umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tdst_reg->raw = 0;\n\t\t}\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, dst_reg, smin_val < 0);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to sub from different maps or paths\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\tif (dst_reg == off_reg) {\n\t\t\t/* scalar -= pointer.  Creates an unknown scalar */\n\t\t\tverbose(env, \"R%d tried to subtract pointer from scalar\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* We don't allow subtraction from FP, because (according to\n\t\t * test_verifier.c test \"invalid fp arithmetic\", JITs might not\n\t\t * be able to deal with it.\n\t\t */\n\t\tif (ptr_reg->type == PTR_TO_STACK) {\n\t\t\tverbose(env, \"R%d subtraction from stack pointer prohibited\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (known && (ptr_reg->off - smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off - smin_val))) {\n\t\t\t/* pointer -= K.  Subtract it from fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->id = ptr_reg->id;\n\t\t\tdst_reg->off = ptr_reg->off - smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  If the subtrahend is known\n\t\t * nonnegative, then any reg->range we had before is still good.\n\t\t */\n\t\tif (signed_sub_overflows(smin_ptr, smax_val) ||\n\t\t    signed_sub_overflows(smax_ptr, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr - smax_val;\n\t\t\tdst_reg->smax_value = smax_ptr - smin_val;\n\t\t}\n\t\tif (umin_ptr < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value = umin_ptr - umax_val;\n\t\t\tdst_reg->umax_value = umax_ptr - umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tif (smin_val < 0)\n\t\t\t\tdst_reg->raw = 0;\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\tcase BPF_OR:\n\tcase BPF_XOR:\n\t\t/* bitwise ops on pointers are troublesome, prohibit. */\n\t\tverbose(env, \"R%d bitwise operator %s on pointer prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\tdefault:\n\t\t/* other operators (e.g. MUL,LSH) produce non-pointer results */\n\t\tverbose(env, \"R%d pointer arithmetic with %s operator prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!check_reg_sane_offset(env, dst_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t__update_reg_bounds(dst_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\n\t/* For unprivileged we require that resulting offset must be in bounds\n\t * in order to be able to sanitize access later on.\n\t */\n\tif (!env->allow_ptr_leaks) {\n\t\tif (dst_reg->type == PTR_TO_MAP_VALUE &&\n\t\t    check_map_access(env, dst, dst_reg->off, 1, false)) {\n\t\t\tverbose(env, \"R%d pointer arithmetic of map value goes out of range, \"\n\t\t\t\t\"prohibited for !root\\n\", dst);\n\t\t\treturn -EACCES;\n\t\t} else if (dst_reg->type == PTR_TO_STACK &&\n\t\t\t   check_stack_access(env, dst_reg, dst_reg->off +\n\t\t\t\t\t      dst_reg->var_off.value, 1)) {\n\t\t\tverbose(env, \"R%d stack pointer arithmetic goes out of range, \"\n\t\t\t\t\"prohibited for !root\\n\", dst);\n\t\t\treturn -EACCES;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/* WARNING: This function does calculations on 64-bit values, but the actual\n * execution may occur on 32-bit values. Therefore, things like bitshifts\n * need extra checks in the 32-bit case.\n */\nstatic int adjust_scalar_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t      struct bpf_insn *insn,\n\t\t\t\t      struct bpf_reg_state *dst_reg,\n\t\t\t\t      struct bpf_reg_state src_reg)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 opcode = BPF_OP(insn->code);\n\tbool src_known, dst_known;\n\ts64 smin_val, smax_val;\n\tu64 umin_val, umax_val;\n\tu64 insn_bitness = (BPF_CLASS(insn->code) == BPF_ALU64) ? 64 : 32;\n\n\tif (insn_bitness == 32) {\n\t\t/* Relevant for 32-bit RSH: Information can propagate towards\n\t\t * LSB, so it isn't sufficient to only truncate the output to\n\t\t * 32 bits.\n\t\t */\n\t\tcoerce_reg_to_size(dst_reg, 4);\n\t\tcoerce_reg_to_size(&src_reg, 4);\n\t}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 3444,
        "critical_vars": [
            "dst"
        ],
        "function": "adjust_scalar_min_max_vals",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_d3bd7413e0ca40b60cf60d4003246d067cafdeda_verifier.c.diff",
        "label": "True",
        "function_code": "static int adjust_scalar_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t      struct bpf_insn *insn,\n\t\t\t\t      struct bpf_reg_state *dst_reg,\n\t\t\t\t      struct bpf_reg_state src_reg)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 opcode = BPF_OP(insn->code);\n\tbool src_known, dst_known;\n\ts64 smin_val, smax_val;\n\tu64 umin_val, umax_val;\n\tu64 insn_bitness = (BPF_CLASS(insn->code) == BPF_ALU64) ? 64 : 32;\n\tu32 dst = insn->dst_reg;\n\tint ret;\n\n\tif (insn_bitness == 32) {\n\t\t/* Relevant for 32-bit RSH: Information can propagate towards\n\t\t * LSB, so it isn't sufficient to only truncate the output to\n\t\t * 32 bits.\n\t\t */\n\t\tcoerce_reg_to_size(dst_reg, 4);\n\t\tcoerce_reg_to_size(&src_reg, 4);\n\t}\n\n\tsmin_val = src_reg.smin_value;\n\tsmax_val = src_reg.smax_value;\n\tumin_val = src_reg.umin_value;\n\tumax_val = src_reg.umax_value;\n\tsrc_known = tnum_is_const(src_reg.var_off);\n\tdst_known = tnum_is_const(dst_reg->var_off);\n\n\tif ((src_known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (!src_known &&\n\t    opcode != BPF_ADD && opcode != BPF_SUB && opcode != BPF_AND) {\n\t\t__mark_reg_unknown(dst_reg);\n\t\treturn 0;\n\t}\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\tret = sanitize_val_alu(env, insn);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to add from different pointers or scalars\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\tif (signed_add_overflows(dst_reg->smin_value, smin_val) ||\n\t\t    signed_add_overflows(dst_reg->smax_value, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value += smin_val;\n\t\t\tdst_reg->smax_value += smax_val;\n\t\t}\n\t\tif (dst_reg->umin_value + umin_val < umin_val ||\n\t\t    dst_reg->umax_value + umax_val < umax_val) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value += umin_val;\n\t\t\tdst_reg->umax_value += umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(dst_reg->var_off, src_reg.var_off);\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tret = sanitize_val_alu(env, insn);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to sub from different pointers or scalars\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\tif (signed_sub_overflows(dst_reg->smin_value, smax_val) ||\n\t\t    signed_sub_overflows(dst_reg->smax_value, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value -= smax_val;\n\t\t\tdst_reg->smax_value -= smin_val;\n\t\t}\n\t\tif (dst_reg->umin_value < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value -= umax_val;\n\t\t\tdst_reg->umax_value -= umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(dst_reg->var_off, src_reg.var_off);\n\t\tbreak;\n\tcase BPF_MUL:\n\t\tdst_reg->var_off = tnum_mul(dst_reg->var_off, src_reg.var_off);\n\t\tif (smin_val < 0 || dst_reg->smin_value < 0) {\n\t\t\t/* Ain't nobody got time to multiply that sign */\n\t\t\t__mark_reg_unbounded(dst_reg);\n\t\t\t__update_reg_bounds(dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\t/* Both values are positive, so we can work with unsigned and\n\t\t * copy the result to signed (unless it exceeds S64_MAX).\n\t\t */\n\t\tif (umax_val > U32_MAX || dst_reg->umax_value > U32_MAX) {\n\t\t\t/* Potential overflow, we know nothing */\n\t\t\t__mark_reg_unbounded(dst_reg);\n\t\t\t/* (except what we can learn from the var_off) */\n\t\t\t__update_reg_bounds(dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\tdst_reg->umin_value *= umin_val;\n\t\tdst_reg->umax_value *= umax_val;\n\t\tif (dst_reg->umax_value > S64_MAX) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = dst_reg->umin_value;\n\t\t\tdst_reg->smax_value = dst_reg->umax_value;\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\t\tif (src_known && dst_known) {\n\t\t\t__mark_reg_known(dst_reg, dst_reg->var_off.value &\n\t\t\t\t\t\t  src_reg.var_off.value);\n\t\t\tbreak;\n\t\t}\n\t\t/* We get our minimum from the var_off, since that's inherently\n\t\t * bitwise.  Our maximum is the minimum of the operands' maxima.\n\t\t */\n\t\tdst_reg->var_off = tnum_and(dst_reg->var_off, src_reg.var_off);\n\t\tdst_reg->umin_value = dst_reg->var_off.value;\n\t\tdst_reg->umax_value = min(dst_reg->umax_value, umax_val);\n\t\tif (dst_reg->smin_value < 0 || smin_val < 0) {\n\t\t\t/* Lose signed bounds when ANDing negative numbers,\n\t\t\t * ain't nobody got time for that.\n\t\t\t */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\t/* ANDing two positives gives a positive, so safe to\n\t\t\t * cast result into s64.\n\t\t\t */\n\t\t\tdst_reg->smin_value = dst_reg->umin_value;\n\t\t\tdst_reg->smax_value = dst_reg->umax_value;\n\t\t}\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tcase BPF_OR:\n\t\tif (src_known && dst_known) {\n\t\t\t__mark_reg_known(dst_reg, dst_reg->var_off.value |\n\t\t\t\t\t\t  src_reg.var_off.value);\n\t\t\tbreak;\n\t\t}\n\t\t/* We get our maximum from the var_off, and our minimum is the\n\t\t * maximum of the operands' minima\n\t\t */\n\t\tdst_reg->var_off = tnum_or(dst_reg->var_off, src_reg.var_off);\n\t\tdst_reg->umin_value = max(dst_reg->umin_value, umin_val);\n\t\tdst_reg->umax_value = dst_reg->var_off.value |\n\t\t\t\t      dst_reg->var_off.mask;\n\t\tif (dst_reg->smin_value < 0 || smin_val < 0) {\n\t\t\t/* Lose signed bounds when ORing negative numbers,\n\t\t\t * ain't nobody got time for that.\n\t\t\t */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\t/* ORing two positives gives a positive, so safe to\n\t\t\t * cast result into s64.\n\t\t\t */\n\t\t\tdst_reg->smin_value = dst_reg->umin_value;\n\t\t\tdst_reg->smax_value = dst_reg->umax_value;\n\t\t}\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tcase BPF_LSH:\n\t\tif (umax_val >= insn_bitness) {\n\t\t\t/* Shifts greater than 31 or 63 are undefined.\n\t\t\t * This includes shifts by a negative number.\n\t\t\t */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\t/* We lose all sign bit information (except what we can pick\n\t\t * up from var_off)\n\t\t */\n\t\tdst_reg->smin_value = S64_MIN;\n\t\tdst_reg->smax_value = S64_MAX;\n\t\t/* If we might shift our top bit out, then we know nothing */\n\t\tif (dst_reg->umax_value > 1ULL << (63 - umax_val)) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value <<= umin_val;\n\t\t\tdst_reg->umax_value <<= umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_lshift(dst_reg->var_off, umin_val);\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tcase BPF_RSH:\n\t\tif (umax_val >= insn_bitness) {\n\t\t\t/* Shifts greater than 31 or 63 are undefined.\n\t\t\t * This includes shifts by a negative number.\n\t\t\t */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\t/* BPF_RSH is an unsigned shift.  If the value in dst_reg might\n\t\t * be negative, then either:\n\t\t * 1) src_reg might be zero, so the sign bit of the result is\n\t\t *    unknown, so we lose our signed bounds\n\t\t * 2) it's known negative, thus the unsigned bounds capture the\n\t\t *    signed bounds\n\t\t * 3) the signed bounds cross zero, so they tell us nothing\n\t\t *    about the result\n\t\t * If the value in dst_reg is known nonnegative, then again the\n\t\t * unsigned bounts capture the signed bounds.\n\t\t * Thus, in all cases it suffices to blow away our signed bounds\n\t\t * and rely on inferring new ones from the unsigned bounds and\n\t\t * var_off of the result.\n\t\t */\n\t\tdst_reg->smin_value = S64_MIN;\n\t\tdst_reg->smax_value = S64_MAX;\n\t\tdst_reg->var_off = tnum_rshift(dst_reg->var_off, umin_val);\n\t\tdst_reg->umin_value >>= umax_val;\n\t\tdst_reg->umax_value >>= umin_val;\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tcase BPF_ARSH:\n\t\tif (umax_val >= insn_bitness) {\n\t\t\t/* Shifts greater than 31 or 63 are undefined.\n\t\t\t * This includes shifts by a negative number.\n\t\t\t */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Upon reaching here, src_known is true and\n\t\t * umax_val is equal to umin_val.\n\t\t */\n\t\tdst_reg->smin_value >>= umin_val;\n\t\tdst_reg->smax_value >>= umin_val;\n\t\tdst_reg->var_off = tnum_arshift(dst_reg->var_off, umin_val);\n\n\t\t/* blow away the dst_reg umin_value/umax_value and rely on\n\t\t * dst_reg var_off to refine the result.\n\t\t */\n\t\tdst_reg->umin_value = 0;\n\t\tdst_reg->umax_value = U64_MAX;\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tdefault:\n\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\tbreak;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops are (32,32)->32 */\n\t\tcoerce_reg_to_size(dst_reg, 4);\n\t}\n\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\treturn 0;\n}\n\n/* Handles ALU ops other than BPF_END, BPF_NEG and BPF_MOV: computes new min/max\n * and var_off.\n */\nstatic int adjust_reg_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg, *src_reg;\n\tstruct bpf_reg_state *ptr_reg = NULL, off_reg = {0};\n\tu8 opcode = BPF_OP(insn->code);\n\n\tdst_reg = &regs[insn->dst_reg];\n\tsrc_reg = NULL;\n\tif (dst_reg->type != SCALAR_VALUE)\n\t\tptr_reg = dst_reg;\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tsrc_reg = &regs[insn->src_reg];\n\t\tif (src_reg->type != SCALAR_VALUE) {\n\t\t\tif (dst_reg->type != SCALAR_VALUE) {\n\t\t\t\t/* Combining two pointers by any ALU op yields\n\t\t\t\t * an arbitrary scalar. Disallow all math except\n\t\t\t\t * pointer subtraction\n\t\t\t\t */\n\t\t\t\tif (opcode == BPF_SUB && env->allow_ptr_leaks) {\n\t\t\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t\tverbose(env, \"R%d pointer %s pointer prohibited\\n\",\n\t\t\t\t\tinsn->dst_reg,\n\t\t\t\t\tbpf_alu_string[opcode >> 4]);\n\t\t\t\treturn -EACCES;\n\t\t\t} else {\n\t\t\t\t/* scalar += pointer\n\t\t\t\t * This is legal, but we have to reverse our\n\t\t\t\t * src/dest handling in computing the range\n\t\t\t\t */\n\t\t\t\treturn adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t\t       src_reg, dst_reg);\n\t\t\t}\n\t\t} else if (ptr_reg) {\n\t\t\t/* pointer += scalar */\n\t\t\treturn adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t       dst_reg, src_reg);\n\t\t}\n\t} else {\n\t\t/* Pretend the src is a reg with a known value, since we only\n\t\t * need to be able to read from this state.\n\t\t */\n\t\toff_reg.type = SCALAR_VALUE;\n\t\t__mark_reg_known(&off_reg, insn->imm);\n\t\tsrc_reg = &off_reg;\n\t\tif (ptr_reg) /* pointer += K */\n\t\t\treturn adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t       ptr_reg, src_reg);\n\t}\n\n\t/* Got here implies adding two SCALAR_VALUEs */\n\tif (WARN_ON_ONCE(ptr_reg)) {\n\t\tprint_verifier_state(env, state);\n\t\tverbose(env, \"verifier internal error: unexpected ptr_reg\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (WARN_ON(!src_reg)) {\n\t\tprint_verifier_state(env, state);\n\t\tverbose(env, \"verifier internal error: no src_reg\\n\");\n\t\treturn -EINVAL;\n\t}\n\treturn adjust_scalar_min_max_vals(env, insn, dst_reg, *src_reg);\n}\n\n/* check validity of 32-bit and 64-bit arithmetic operations */\nstatic int check_alu_op(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 opcode = BPF_OP(insn->code);\n\tint err;\n\n\tif (opcode == BPF_END || opcode == BPF_NEG) {\n\t\tif (opcode == BPF_NEG) {\n\t\t\tif (BPF_SRC(insn->code) != 0 ||\n\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t    insn->off != 0 || insn->imm != 0) {\n\t\t\t\tverbose(env, \"BPF_NEG uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else {\n\t\t\tif (insn->src_reg != BPF_REG_0 || insn->off != 0 ||\n\t\t\t    (insn->imm != 16 && insn->imm != 32 && insn->imm != 64) ||\n\t\t\t    BPF_CLASS(insn->code) == BPF_ALU64) {\n\t\t\t\tverbose(env, \"BPF_END uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check src operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (is_pointer_value(env, insn->dst_reg)) {\n\t\t\tverbose(env, \"R%d pointer arithmetic prohibited\\n\",\n\t\t\t\tinsn->dst_reg);\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\t/* check dest operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t} else if (opcode == BPF_MOV) {\n\n\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\tif (insn->imm != 0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_MOV uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\t/* check src operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t} else {\n\t\t\tif (insn->src_reg != BPF_REG_0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_MOV uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check dest operand, mark as required later */\n\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP_NO_MARK);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\tstruct bpf_reg_state *src_reg = regs + insn->src_reg;\n\t\t\tstruct bpf_reg_state *dst_reg = regs + insn->dst_reg;\n\n\t\t\tif (BPF_CLASS(insn->code) == BPF_ALU64) {\n\t\t\t\t/* case: R1 = R2\n\t\t\t\t * copy register state to dest reg\n\t\t\t\t */\n\t\t\t\t*dst_reg = *src_reg;\n\t\t\t\tdst_reg->live |= REG_LIVE_WRITTEN;\n\t\t\t} else {\n\t\t\t\t/* R1 = (u32) R2 */\n\t\t\t\tif (is_pointer_value(env, insn->src_reg)) {\n\t\t\t\t\tverbose(env,\n\t\t\t\t\t\t\"R%d partial copy of pointer\\n\",\n\t\t\t\t\t\tinsn->src_reg);\n\t\t\t\t\treturn -EACCES;\n\t\t\t\t} else if (src_reg->type == SCALAR_VALUE) {\n\t\t\t\t\t*dst_reg = *src_reg;\n\t\t\t\t\tdst_reg->live |= REG_LIVE_WRITTEN;\n\t\t\t\t} else {\n\t\t\t\t\tmark_reg_unknown(env, regs,\n\t\t\t\t\t\t\t insn->dst_reg);\n\t\t\t\t}\n\t\t\t\tcoerce_reg_to_size(dst_reg, 4);\n\t\t\t}\n\t\t} else {\n\t\t\t/* case: R = imm\n\t\t\t * remember the value we stored into this reg\n\t\t\t */\n\t\t\t/* clear any state __mark_reg_known doesn't set */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tregs[insn->dst_reg].type = SCALAR_VALUE;\n\t\t\tif (BPF_CLASS(insn->code) == BPF_ALU64) {\n\t\t\t\t__mark_reg_known(regs + insn->dst_reg,\n\t\t\t\t\t\t insn->imm);\n\t\t\t} else {\n\t\t\t\t__mark_reg_known(regs + insn->dst_reg,\n\t\t\t\t\t\t (u32)insn->imm);\n\t\t\t}\n\t\t}\n\n\t} else if (opcode > BPF_END) {\n\t\tverbose(env, \"invalid BPF_ALU opcode %x\\n\", opcode);\n\t\treturn -EINVAL;\n\n\t} else {\t/* all other ALU ops: and, sub, xor, add, ... */\n\n\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\tif (insn->imm != 0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_ALU uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\t/* check src1 operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t} else {\n\t\t\tif (insn->src_reg != BPF_REG_0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_ALU uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check src2 operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif ((opcode == BPF_MOD || opcode == BPF_DIV) &&\n\t\t    BPF_SRC(insn->code) == BPF_K && insn->imm == 0) {\n\t\t\tverbose(env, \"div by zero\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif ((opcode == BPF_LSH || opcode == BPF_RSH ||\n\t\t     opcode == BPF_ARSH) && BPF_SRC(insn->code) == BPF_K) {\n\t\t\tint size = BPF_CLASS(insn->code) == BPF_ALU64 ? 64 : 32;\n\n\t\t\tif (insn->imm < 0 || insn->imm >= size) {\n\t\t\t\tverbose(env, \"invalid shift %d\\n\", insn->imm);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check dest operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP_NO_MARK);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\treturn adjust_reg_min_max_vals(env, insn);\n\t}\n\n\treturn 0;\n}\n\nstatic void find_good_pkt_pointers(struct bpf_verifier_state *vstate,\n\t\t\t\t   struct bpf_reg_state *dst_reg,\n\t\t\t\t   enum bpf_reg_type type,\n\t\t\t\t   bool range_right_open)\n{\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *reg;\n\tu16 new_range;\n\tint i, j;\n\n\tif (dst_reg->off < 0 ||\n\t    (dst_reg->off == 0 && range_right_open))\n\t\t/* This doesn't give us any range */\n\t\treturn;\n\n\tif (dst_reg->umax_value > MAX_PACKET_OFF ||\n\t    dst_reg->umax_value + dst_reg->off > MAX_PACKET_OFF)\n\t\t/* Risk of overflow.  For instance, ptr + (1<<63) may be less\n\t\t * than pkt_end, but that's because it's also less than pkt.\n\t\t */\n\t\treturn;\n\n\tnew_range = dst_reg->off;\n\tif (range_right_open)\n\t\tnew_range--;\n\n\t/* Examples for register markings:\n\t *\n\t * pkt_data in dst register:\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (r2 > pkt_end) goto <handle exception>\n\t *   <access okay>\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (r2 < pkt_end) goto <access okay>\n\t *   <handle exception>\n\t *\n\t *   Where:\n\t *     r2 == dst_reg, pkt_end == src_reg\n\t *     r2=pkt(id=n,off=8,r=0)\n\t *     r3=pkt(id=n,off=0,r=0)\n\t *\n\t * pkt_data in src register:\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (pkt_end >= r2) goto <access okay>\n\t *   <handle exception>\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (pkt_end <= r2) goto <handle exception>\n\t *   <access okay>\n\t *\n\t *   Where:\n\t *     pkt_end == dst_reg, r2 == src_reg\n\t *     r2=pkt(id=n,off=8,r=0)\n\t *     r3=pkt(id=n,off=0,r=0)\n\t *\n\t * Find register r3 and mark its range as r3=pkt(id=n,off=0,r=8)\n\t * or r3=pkt(id=n,off=0,r=8-1), so that range of bytes [r3, r3 + 8)\n\t * and [r3, r3 + 8-1) respectively is safe to access depending on\n\t * the check.\n\t */\n\n\t/* If our ids match, then we must have the same max_value.  And we\n\t * don't care about the other reg's fixed offset, since if it's too big\n\t * the range won't allow anything.\n\t * dst_reg->off is known < MAX_PACKET_OFF, therefore it fits in a u16.\n\t */\n\tfor (i = 0; i < MAX_BPF_REG; i++)\n\t\tif (regs[i].type == type && regs[i].id == dst_reg->id)\n\t\t\t/* keep the maximum range already checked */\n\t\t\tregs[i].range = max(regs[i].range, new_range);\n\n\tfor (j = 0; j <= vstate->curframe; j++) {\n\t\tstate = vstate->frame[j];\n\t\tbpf_for_each_spilled_reg(i, state, reg) {\n\t\t\tif (!reg)\n\t\t\t\tcontinue;\n\t\t\tif (reg->type == type && reg->id == dst_reg->id)\n\t\t\t\treg->range = max(reg->range, new_range);\n\t\t}\n\t}\n}\n\n/* compute branch direction of the expression \"if (reg opcode val) goto target;\"\n * and return:\n *  1 - branch will be taken and \"goto target\" will be executed\n *  0 - branch will not be taken and fall-through to next insn\n * -1 - unknown. Example: \"if (reg < 5)\" is unknown when register value range [0,10]\n */\nstatic int is_branch_taken(struct bpf_reg_state *reg, u64 val, u8 opcode)\n{\n\tif (__is_pointer_value(false, reg))\n\t\treturn -1;\n\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\tif (tnum_is_const(reg->var_off))\n\t\t\treturn !!tnum_equals_const(reg->var_off, val);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\tif (tnum_is_const(reg->var_off))\n\t\t\treturn !tnum_equals_const(reg->var_off, val);\n\t\tbreak;\n\tcase BPF_JSET:\n\t\tif ((~reg->var_off.mask & reg->var_off.value) & val)\n\t\t\treturn 1;\n\t\tif (!((reg->var_off.mask | reg->var_off.value) & val))\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JGT:\n\t\tif (reg->umin_value > val)\n\t\t\treturn 1;\n\t\telse if (reg->umax_value <= val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSGT:\n\t\tif (reg->smin_value > (s64)val)\n\t\t\treturn 1;\n\t\telse if (reg->smax_value < (s64)val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JLT:\n\t\tif (reg->umax_value < val)\n\t\t\treturn 1;\n\t\telse if (reg->umin_value >= val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSLT:\n\t\tif (reg->smax_value < (s64)val)\n\t\t\treturn 1;\n\t\telse if (reg->smin_value >= (s64)val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JGE:\n\t\tif (reg->umin_value >= val)\n\t\t\treturn 1;\n\t\telse if (reg->umax_value < val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSGE:\n\t\tif (reg->smin_value >= (s64)val)\n\t\t\treturn 1;\n\t\telse if (reg->smax_value < (s64)val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JLE:\n\t\tif (reg->umax_value <= val)\n\t\t\treturn 1;\n\t\telse if (reg->umin_value > val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSLE:\n\t\tif (reg->smax_value <= (s64)val)\n\t\t\treturn 1;\n\t\telse if (reg->smin_value > (s64)val)\n\t\t\treturn 0;\n\t\tbreak;\n\t}\n\n\treturn -1;\n}\n\n/* Adjusts the register min/max values in the case that the dst_reg is the\n * variable register that we are working on, and src_reg is a constant or we're\n * simply doing a BPF_K check.\n * In JEQ/JNE cases we also adjust the var_off values.\n */\nstatic void reg_set_min_max(struct bpf_reg_state *true_reg,\n\t\t\t    struct bpf_reg_state *false_reg, u64 val,\n\t\t\t    u8 opcode)\n{\n\t/* If the dst_reg is a pointer, we can't learn anything about its\n\t * variable offset from the compare (unless src_reg were a pointer into\n\t * the same object, but we don't bother with that.\n\t * Since false_reg and true_reg have the same type by construction, we\n\t * only need to check one of them for pointerness.\n\t */\n\tif (__is_pointer_value(false, false_reg))\n\t\treturn;\n\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\t/* If this is false then we know nothing Jon Snow, but if it is\n\t\t * true then we know for sure.\n\t\t */\n\t\t__mark_reg_known(true_reg, val);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\t/* If this is true we know nothing Jon Snow, but if it is false\n\t\t * we know the value for sure;\n\t\t */\n\t\t__mark_reg_known(false_reg, val);\n\t\tbreak;\n\tcase BPF_JSET:\n\t\tfalse_reg->var_off = tnum_and(false_reg->var_off,\n\t\t\t\t\t      tnum_const(~val));\n\t\tif (is_power_of_2(val))\n\t\t\ttrue_reg->var_off = tnum_or(true_reg->var_off,\n\t\t\t\t\t\t    tnum_const(val));\n\t\tbreak;\n\tcase BPF_JGT:\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val);\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JSGT:\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val);\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JLT:\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val);\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val - 1);\n\t\tbreak;\n\tcase BPF_JSLT:\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val);\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val - 1);\n\t\tbreak;\n\tcase BPF_JGE:\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val - 1);\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val);\n\t\tbreak;\n\tcase BPF_JSGE:\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val - 1);\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val);\n\t\tbreak;\n\tcase BPF_JLE:\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val + 1);\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val);\n\t\tbreak;\n\tcase BPF_JSLE:\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val + 1);\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t__reg_deduce_bounds(false_reg);\n\t__reg_deduce_bounds(true_reg);\n\t/* We might have learned some bits from the bounds. */\n\t__reg_bound_offset(false_reg);\n\t__reg_bound_offset(true_reg);\n\t/* Intersecting with the old var_off might have improved our bounds\n\t * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),\n\t * then new var_off is (0; 0x7f...fc) which improves our umax.\n\t */\n\t__update_reg_bounds(false_reg);\n\t__update_reg_bounds(true_reg);\n}\n\n/* Same as above, but for the case that dst_reg holds a constant and src_reg is\n * the variable reg.\n */\nstatic void reg_set_min_max_inv(struct bpf_reg_state *true_reg,\n\t\t\t\tstruct bpf_reg_state *false_reg, u64 val,\n\t\t\t\tu8 opcode)\n{\n\tif (__is_pointer_value(false, false_reg))\n\t\treturn;\n\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\t/* If this is false then we know nothing Jon Snow, but if it is\n\t\t * true then we know for sure.\n\t\t */\n\t\t__mark_reg_known(true_reg, val);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\t/* If this is true we know nothing Jon Snow, but if it is false\n\t\t * we know the value for sure;\n\t\t */\n\t\t__mark_reg_known(false_reg, val);\n\t\tbreak;\n\tcase BPF_JSET:\n\t\tfalse_reg->var_off = tnum_and(false_reg->var_off,\n\t\t\t\t\t      tnum_const(~val));\n\t\tif (is_power_of_2(val))\n\t\t\ttrue_reg->var_off = tnum_or(true_reg->var_off,\n\t\t\t\t\t\t    tnum_const(val));\n\t\tbreak;\n\tcase BPF_JGT:\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val - 1);\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val);\n\t\tbreak;\n\tcase BPF_JSGT:\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val - 1);\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val);\n\t\tbreak;\n\tcase BPF_JLT:\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val + 1);\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val);\n\t\tbreak;\n\tcase BPF_JSLT:\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val + 1);\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val);\n\t\tbreak;\n\tcase BPF_JGE:\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val);\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JSGE:\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val);\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JLE:\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val);\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val - 1);\n\t\tbreak;\n\tcase BPF_JSLE:\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val);\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val - 1);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t__reg_deduce_bounds(false_reg);\n\t__reg_deduce_bounds(true_reg);\n\t/* We might have learned some bits from the bounds. */\n\t__reg_bound_offset(false_reg);\n\t__reg_bound_offset(true_reg);\n\t/* Intersecting with the old var_off might have improved our bounds\n\t * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),\n\t * then new var_off is (0; 0x7f...fc) which improves our umax.\n\t */\n\t__update_reg_bounds(false_reg);\n\t__update_reg_bounds(true_reg);\n}\n\n/* Regs are known to be equal, so intersect their min/max/var_off */\nstatic void __reg_combine_min_max(struct bpf_reg_state *src_reg,\n\t\t\t\t  struct bpf_reg_state *dst_reg)\n{\n\tsrc_reg->umin_value = dst_reg->umin_value = max(src_reg->umin_value,\n\t\t\t\t\t\t\tdst_reg->umin_value);\n\tsrc_reg->umax_value = dst_reg->umax_value = min(src_reg->umax_value,\n\t\t\t\t\t\t\tdst_reg->umax_value);\n\tsrc_reg->smin_value = dst_reg->smin_value = max(src_reg->smin_value,\n\t\t\t\t\t\t\tdst_reg->smin_value);\n\tsrc_reg->smax_value = dst_reg->smax_value = min(src_reg->smax_value,\n\t\t\t\t\t\t\tdst_reg->smax_value);\n\tsrc_reg->var_off = dst_reg->var_off = tnum_intersect(src_reg->var_off,\n\t\t\t\t\t\t\t     dst_reg->var_off);\n\t/* We might have learned new bounds from the var_off. */\n\t__update_reg_bounds(src_reg);\n\t__update_reg_bounds(dst_reg);\n\t/* We might have learned something about the sign bit. */\n\t__reg_deduce_bounds(src_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t/* We might have learned some bits from the bounds. */\n\t__reg_bound_offset(src_reg);\n\t__reg_bound_offset(dst_reg);\n\t/* Intersecting with the old var_off might have improved our bounds\n\t * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),\n\t * then new var_off is (0; 0x7f...fc) which improves our umax.\n\t */\n\t__update_reg_bounds(src_reg);\n\t__update_reg_bounds(dst_reg);\n}\n\nstatic void reg_combine_min_max(struct bpf_reg_state *true_src,\n\t\t\t\tstruct bpf_reg_state *true_dst,\n\t\t\t\tstruct bpf_reg_state *false_src,\n\t\t\t\tstruct bpf_reg_state *false_dst,\n\t\t\t\tu8 opcode)\n{\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\t__reg_combine_min_max(true_src, true_dst);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\t__reg_combine_min_max(false_src, false_dst);\n\t\tbreak;\n\t}\n}\n\nstatic void mark_ptr_or_null_reg(struct bpf_func_state *state,\n\t\t\t\t struct bpf_reg_state *reg, u32 id,\n\t\t\t\t bool is_null)\n{\n\tif (reg_type_may_be_null(reg->type) && reg->id == id) {\n\t\t/* Old offset (both fixed and variable parts) should\n\t\t * have been known-zero, because we don't allow pointer\n\t\t * arithmetic on pointers that might be NULL.\n\t\t */\n\t\tif (WARN_ON_ONCE(reg->smin_value || reg->smax_value ||\n\t\t\t\t !tnum_equals_const(reg->var_off, 0) ||\n\t\t\t\t reg->off)) {\n\t\t\t__mark_reg_known_zero(reg);\n\t\t\treg->off = 0;\n\t\t}\n\t\tif (is_null) {\n\t\t\treg->type = SCALAR_VALUE;\n\t\t} else if (reg->type == PTR_TO_MAP_VALUE_OR_NULL) {\n\t\t\tif (reg->map_ptr->inner_map_meta) {\n\t\t\t\treg->type = CONST_PTR_TO_MAP;\n\t\t\t\treg->map_ptr = reg->map_ptr->inner_map_meta;\n\t\t\t} else {\n\t\t\t\treg->type = PTR_TO_MAP_VALUE;\n\t\t\t}\n\t\t} else if (reg->type == PTR_TO_SOCKET_OR_NULL) {\n\t\t\treg->type = PTR_TO_SOCKET;\n\t\t}\n\t\tif (is_null || !reg_is_refcounted(reg)) {\n\t\t\t/* We don't need id from this point onwards anymore,\n\t\t\t * thus we should better reset it, so that state\n\t\t\t * pruning has chances to take effect.\n\t\t\t */\n\t\t\treg->id = 0;\n\t\t}\n\t}\n}\n\n/* The logic is similar to find_good_pkt_pointers(), both could eventually\n * be folded together at some point.\n */\nstatic void mark_ptr_or_null_regs(struct bpf_verifier_state *vstate, u32 regno,\n\t\t\t\t  bool is_null)\n{\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *reg, *regs = state->regs;\n\tu32 id = regs[regno].id;\n\tint i, j;\n\n\tif (reg_is_refcounted_or_null(&regs[regno]) && is_null)\n\t\t__release_reference_state(state, id);\n\n\tfor (i = 0; i < MAX_BPF_REG; i++)\n\t\tmark_ptr_or_null_reg(state, &regs[i], id, is_null);\n\n\tfor (j = 0; j <= vstate->curframe; j++) {\n\t\tstate = vstate->frame[j];\n\t\tbpf_for_each_spilled_reg(i, state, reg) {\n\t\t\tif (!reg)\n\t\t\t\tcontinue;\n\t\t\tmark_ptr_or_null_reg(state, reg, id, is_null);\n\t\t}\n\t}\n}\n\nstatic bool try_match_pkt_pointers(const struct bpf_insn *insn,\n\t\t\t\t   struct bpf_reg_state *dst_reg,\n\t\t\t\t   struct bpf_reg_state *src_reg,\n\t\t\t\t   struct bpf_verifier_state *this_branch,\n\t\t\t\t   struct bpf_verifier_state *other_branch)\n{\n\tif (BPF_SRC(insn->code) != BPF_X)\n\t\treturn false;\n\n\tswitch (BPF_OP(insn->code)) {\n\tcase BPF_JGT:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' > pkt_end, pkt_meta' > pkt_data */\n\t\t\tfind_good_pkt_pointers(this_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, false);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end > pkt_data', pkt_data > pkt_meta' */\n\t\t\tfind_good_pkt_pointers(other_branch, src_reg,\n\t\t\t\t\t       src_reg->type, true);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase BPF_JLT:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' < pkt_end, pkt_meta' < pkt_data */\n\t\t\tfind_good_pkt_pointers(other_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, true);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end < pkt_data', pkt_data > pkt_meta' */\n\t\t\tfind_good_pkt_pointers(this_branch, src_reg,\n\t\t\t\t\t       src_reg->type, false);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase BPF_JGE:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' >= pkt_end, pkt_meta' >= pkt_data */\n\t\t\tfind_good_pkt_pointers(this_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, true);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end >= pkt_data', pkt_data >= pkt_meta' */\n\t\t\tfind_good_pkt_pointers(other_branch, src_reg,\n\t\t\t\t\t       src_reg->type, false);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase BPF_JLE:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' <= pkt_end, pkt_meta' <= pkt_data */\n\t\t\tfind_good_pkt_pointers(other_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, false);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end <= pkt_data', pkt_data <= pkt_meta' */\n\t\t\tfind_good_pkt_pointers(this_branch, src_reg,\n\t\t\t\t\t       src_reg->type, true);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic int check_cond_jmp_op(struct bpf_verifier_env *env,\n\t\t\t     struct bpf_insn *insn, int *insn_idx)\n{\n\tstruct bpf_verifier_state *this_branch = env->cur_state;\n\tstruct bpf_verifier_state *other_branch;\n\tstruct bpf_reg_state *regs = this_branch->frame[this_branch->curframe]->regs;\n\tstruct bpf_reg_state *dst_reg, *other_branch_regs;\n\tu8 opcode = BPF_OP(insn->code);\n\tint err;\n\n\tif (opcode > BPF_JSLE) {\n\t\tverbose(env, \"invalid BPF_JMP opcode %x\\n\", opcode);\n\t\treturn -EINVAL;\n\t}\n\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tif (insn->imm != 0) {\n\t\t\tverbose(env, \"BPF_JMP uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/* check src1 operand */\n\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (is_pointer_value(env, insn->src_reg)) {\n\t\t\tverbose(env, \"R%d pointer comparison prohibited\\n\",\n\t\t\t\tinsn->src_reg);\n\t\t\treturn -EACCES;\n\t\t}\n\t} else {\n\t\tif (insn->src_reg != BPF_REG_0) {\n\t\t\tverbose(env, \"BPF_JMP uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t/* check src2 operand */\n\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tdst_reg = &regs[insn->dst_reg];\n\n\tif (BPF_SRC(insn->code) == BPF_K) {\n\t\tint pred = is_branch_taken(dst_reg, insn->imm, opcode);\n\n\t\tif (pred == 1) {\n\t\t\t /* only follow the goto, ignore fall-through */\n\t\t\t*insn_idx += insn->off;\n\t\t\treturn 0;\n\t\t} else if (pred == 0) {\n\t\t\t/* only follow fall-through branch, since\n\t\t\t * that's where the program will go\n\t\t\t */\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tother_branch = push_stack(env, *insn_idx + insn->off + 1, *insn_idx,\n\t\t\t\t  false);\n\tif (!other_branch)\n\t\treturn -EFAULT;\n\tother_branch_regs = other_branch->frame[other_branch->curframe]->regs;\n\n\t/* detect if we are comparing against a constant value so we can adjust\n\t * our min/max values for our dst register.\n\t * this is only legit if both are scalars (or pointers to the same\n\t * object, I suppose, but we don't support that right now), because\n\t * otherwise the different base pointers mean the offsets aren't\n\t * comparable.\n\t */\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tif (dst_reg->type == SCALAR_VALUE &&\n\t\t    regs[insn->src_reg].type == SCALAR_VALUE) {\n\t\t\tif (tnum_is_const(regs[insn->src_reg].var_off))\n\t\t\t\treg_set_min_max(&other_branch_regs[insn->dst_reg],\n\t\t\t\t\t\tdst_reg, regs[insn->src_reg].var_off.value,\n\t\t\t\t\t\topcode);\n\t\t\telse if (tnum_is_const(dst_reg->var_off))\n\t\t\t\treg_set_min_max_inv(&other_branch_regs[insn->src_reg],\n\t\t\t\t\t\t    &regs[insn->src_reg],\n\t\t\t\t\t\t    dst_reg->var_off.value, opcode);\n\t\t\telse if (opcode == BPF_JEQ || opcode == BPF_JNE)\n\t\t\t\t/* Comparing for equality, we can combine knowledge */\n\t\t\t\treg_combine_min_max(&other_branch_regs[insn->src_reg],\n\t\t\t\t\t\t    &other_branch_regs[insn->dst_reg],\n\t\t\t\t\t\t    &regs[insn->src_reg],\n\t\t\t\t\t\t    &regs[insn->dst_reg], opcode);\n\t\t}\n\t} else if (dst_reg->type == SCALAR_VALUE) {\n\t\treg_set_min_max(&other_branch_regs[insn->dst_reg],\n\t\t\t\t\tdst_reg, insn->imm, opcode);\n\t}\n\n\t/* detect if R == 0 where R is returned from bpf_map_lookup_elem() */\n\tif (BPF_SRC(insn->code) == BPF_K &&\n\t    insn->imm == 0 && (opcode == BPF_JEQ || opcode == BPF_JNE) &&\n\t    reg_type_may_be_null(dst_reg->type)) {\n\t\t/* Mark all identical registers in each branch as either\n\t\t * safe or unknown depending R == 0 or R != 0 conditional.\n\t\t */\n\t\tmark_ptr_or_null_regs(this_branch, insn->dst_reg,\n\t\t\t\t      opcode == BPF_JNE);\n\t\tmark_ptr_or_null_regs(other_branch, insn->dst_reg,\n\t\t\t\t      opcode == BPF_JEQ);\n\t} else if (!try_match_pkt_pointers(insn, dst_reg, &regs[insn->src_reg],\n\t\t\t\t\t   this_branch, other_branch) &&\n\t\t   is_pointer_value(env, insn->dst_reg)) {\n\t\tverbose(env, \"R%d pointer comparison prohibited\\n\",\n\t\t\tinsn->dst_reg);\n\t\treturn -EACCES;\n\t}\n\tif (env->log.level)\n\t\tprint_verifier_state(env, this_branch->frame[this_branch->curframe]);\n\treturn 0;\n}\n\n/* return the map pointer stored inside BPF_LD_IMM64 instruction */\nstatic struct bpf_map *ld_imm64_to_map_ptr(struct bpf_insn *insn)\n{\n\tu64 imm64 = ((u64) (u32) insn[0].imm) | ((u64) (u32) insn[1].imm) << 32;\n\n\treturn (struct bpf_map *) (unsigned long) imm64;\n}\n\n/* verify BPF_LD_IMM64 instruction */\nstatic int check_ld_imm(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tint err;\n\n\tif (BPF_SIZE(insn->code) != BPF_DW) {\n\t\tverbose(env, \"invalid BPF_LD_IMM insn\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (insn->off != 0) {\n\t\tverbose(env, \"BPF_LD_IMM64 uses reserved fields\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\terr = check_reg_arg(env, insn->dst_reg, DST_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (insn->src_reg == 0) {\n\t\tu64 imm = ((u64)(insn + 1)->imm << 32) | (u32)insn->imm;\n\n\t\tregs[insn->dst_reg].type = SCALAR_VALUE;\n\t\t__mark_reg_known(&regs[insn->dst_reg], imm);\n\t\treturn 0;\n\t}\n\n\t/* replace_map_fd_with_map_ptr() should have caught bad ld_imm64 */\n\tBUG_ON(insn->src_reg != BPF_PSEUDO_MAP_FD);\n\n\tregs[insn->dst_reg].type = CONST_PTR_TO_MAP;\n\tregs[insn->dst_reg].map_ptr = ld_imm64_to_map_ptr(insn);\n\treturn 0;\n}\n\nstatic bool may_access_skb(enum bpf_prog_type type)\n{\n\tswitch (type) {\n\tcase BPF_PROG_TYPE_SOCKET_FILTER:\n\tcase BPF_PROG_TYPE_SCHED_CLS:\n\tcase BPF_PROG_TYPE_SCHED_ACT:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n/* verify safety of LD_ABS|LD_IND instructions:\n * - they can only appear in the programs where ctx == skb\n * - since they are wrappers of function calls, they scratch R1-R5 registers,\n *   preserve R6-R9, and store return value into R0\n *\n * Implicit input:\n *   ctx == skb == R6 == CTX\n *\n * Explicit input:\n *   SRC == any register\n *   IMM == 32-bit immediate\n *\n * Output:\n *   R0 - 8/16/32-bit skb data converted to cpu endianness\n */\nstatic int check_ld_abs(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 mode = BPF_MODE(insn->code);\n\tint i, err;\n\n\tif (!may_access_skb(env->prog->type)) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] instructions not allowed for this program type\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!env->ops->gen_ld_abs) {\n\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (env->subprog_cnt > 1) {\n\t\t/* when program has LD_ABS insn JITs and interpreter assume\n\t\t * that r1 == ctx == skb which is not the case for callees\n\t\t * that can have arbitrary arguments. It's problematic\n\t\t * for main prog as well since JITs would need to analyze\n\t\t * all functions in order to make proper register save/restore\n\t\t * decisions in the main prog. Hence disallow LD_ABS with calls\n\t\t */\n\t\tverbose(env, \"BPF_LD_[ABS|IND] instructions cannot be mixed with bpf-to-bpf calls\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (insn->dst_reg != BPF_REG_0 || insn->off != 0 ||\n\t    BPF_SIZE(insn->code) == BPF_DW ||\n\t    (mode == BPF_ABS && insn->src_reg != BPF_REG_0)) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] uses reserved fields\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* check whether implicit source operand (register R6) is readable */\n\terr = check_reg_arg(env, BPF_REG_6, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\t/* Disallow usage of BPF_LD_[ABS|IND] with reference tracking, as\n\t * gen_ld_abs() may terminate the program at runtime, leading to\n\t * reference leak.\n\t */\n\terr = check_reference_leak(env);\n\tif (err) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] cannot be mixed with socket references\\n\");\n\t\treturn err;\n\t}\n\n\tif (regs[BPF_REG_6].type != PTR_TO_CTX) {\n\t\tverbose(env,\n\t\t\t\"at the time of BPF_LD_ABS|IND R6 != pointer to skb\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (mode == BPF_IND) {\n\t\t/* check explicit source operand */\n\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\t/* reset caller saved regs to unreadable */\n\tfor (i = 0; i < CALLER_SAVED_REGS; i++) {\n\t\tmark_reg_not_init(env, regs, caller_saved[i]);\n\t\tcheck_reg_arg(env, caller_saved[i], DST_OP_NO_MARK);\n\t}\n\n\t/* mark destination R0 register as readable, since it contains\n\t * the value fetched from the packet.\n\t * Already marked as written above.\n\t */\n\tmark_reg_unknown(env, regs, BPF_REG_0);\n\treturn 0;\n}\n\nstatic int check_return_code(struct bpf_verifier_env *env)\n{\n\tstruct bpf_reg_state *reg;\n\tstruct tnum range = tnum_range(0, 1);\n\n\tswitch (env->prog->type) {\n\tcase BPF_PROG_TYPE_CGROUP_SKB:\n\tcase BPF_PROG_TYPE_CGROUP_SOCK:\n\tcase BPF_PROG_TYPE_CGROUP_SOCK_ADDR:\n\tcase BPF_PROG_TYPE_SOCK_OPS:\n\tcase BPF_PROG_TYPE_CGROUP_DEVICE:\n\t\tbreak;\n\tdefault:\n\t\treturn 0;\n\t}\n\n\treg = cur_regs(env) + BPF_REG_0;\n\tif (reg->type != SCALAR_VALUE) {\n\t\tverbose(env, \"At program exit the register R0 is not a known value (%s)\\n\",\n\t\t\treg_type_str[reg->type]);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!tnum_in(range, reg->var_off)) {\n\t\tverbose(env, \"At program exit the register R0 \");\n\t\tif (!tnum_is_unknown(reg->var_off)) {\n\t\t\tchar tn_buf[48];\n\n\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\tverbose(env, \"has value %s\", tn_buf);\n\t\t} else {\n\t\t\tverbose(env, \"has unknown scalar value\");\n\t\t}\n\t\tverbose(env, \" should have been 0 or 1\\n\");\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\n/* non-recursive DFS pseudo code\n * 1  procedure DFS-iterative(G,v):\n * 2      label v as discovered\n * 3      let S be a stack\n * 4      S.push(v)\n * 5      while S is not empty\n * 6            t <- S.pop()\n * 7            if t is what we're looking for:\n * 8                return t\n * 9            for all edges e in G.adjacentEdges(t) do\n * 10               if edge e is already labelled\n * 11                   continue with the next edge\n * 12               w <- G.adjacentVertex(t,e)\n * 13               if vertex w is not discovered and not explored\n * 14                   label e as tree-edge\n * 15                   label w as discovered\n * 16                   S.push(w)\n * 17                   continue at 5\n * 18               else if vertex w is discovered\n * 19                   label e as back-edge\n * 20               else\n * 21                   // vertex w is explored\n * 22                   label e as forward- or cross-edge\n * 23           label t as explored\n * 24           S.pop()\n *\n * convention:\n * 0x10 - discovered\n * 0x11 - discovered and fall-through edge labelled\n * 0x12 - discovered and fall-through and branch edges labelled\n * 0x20 - explored\n */\n\nenum {\n\tDISCOVERED = 0x10,\n\tEXPLORED = 0x20,\n\tFALLTHROUGH = 1,\n\tBRANCH = 2,\n};\n\n#define STATE_LIST_MARK ((struct bpf_verifier_state_list *) -1L)\n\nstatic int *insn_stack;\t/* stack of insns to process */\nstatic int cur_stack;\t/* current stack index */\nstatic int *insn_state;\n\n/* t, w, e - match pseudo-code above:\n * t - index of current instruction\n * w - next instruction\n * e - edge\n */\nstatic int push_insn(int t, int w, int e, struct bpf_verifier_env *env)\n{\n\tif (e == FALLTHROUGH && insn_state[t] >= (DISCOVERED | FALLTHROUGH))\n\t\treturn 0;\n\n\tif (e == BRANCH && insn_state[t] >= (DISCOVERED | BRANCH))\n\t\treturn 0;\n\n\tif (w < 0 || w >= env->prog->len) {\n\t\tverbose_linfo(env, t, \"%d: \", t);\n\t\tverbose(env, \"jump out of range from insn %d to %d\\n\", t, w);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e == BRANCH)\n\t\t/* mark branch target for state pruning */\n\t\tenv->explored_states[w] = STATE_LIST_MARK;\n\n\tif (insn_state[w] == 0) {\n\t\t/* tree-edge */\n\t\tinsn_state[t] = DISCOVERED | e;\n\t\tinsn_state[w] = DISCOVERED;\n\t\tif (cur_stack >= env->prog->len)\n\t\t\treturn -E2BIG;\n\t\tinsn_stack[cur_stack++] = w;\n\t\treturn 1;\n\t} else if ((insn_state[w] & 0xF0) == DISCOVERED) {\n\t\tverbose_linfo(env, t, \"%d: \", t);\n\t\tverbose_linfo(env, w, \"%d: \", w);\n\t\tverbose(env, \"back-edge from insn %d to %d\\n\", t, w);\n\t\treturn -EINVAL;\n\t} else if (insn_state[w] == EXPLORED) {\n\t\t/* forward- or cross-edge */\n\t\tinsn_state[t] = DISCOVERED | e;\n\t} else {\n\t\tverbose(env, \"insn state internal bug\\n\");\n\t\treturn -EFAULT;\n\t}\n\treturn 0;\n}\n\n/* non-recursive depth-first-search to detect loops in BPF program\n * loop == back-edge in directed graph\n */\nstatic int check_cfg(struct bpf_verifier_env *env)\n{\n\tstruct bpf_insn *insns = env->prog->insnsi;\n\tint insn_cnt = env->prog->len;\n\tint ret = 0;\n\tint i, t;\n\n\tinsn_state = kcalloc(insn_cnt, sizeof(int), GFP_KERNEL);\n\tif (!insn_state)\n\t\treturn -ENOMEM;\n\n\tinsn_stack = kcalloc(insn_cnt, sizeof(int), GFP_KERNEL);\n\tif (!insn_stack) {\n\t\tkfree(insn_state);\n\t\treturn -ENOMEM;\n\t}\n\n\tinsn_state[0] = DISCOVERED; /* mark 1st insn as discovered */\n\tinsn_stack[0] = 0; /* 0 is the first instruction */\n\tcur_stack = 1;\n\npeek_stack:\n\tif (cur_stack == 0)\n\t\tgoto check_state;\n\tt = insn_stack[cur_stack - 1];\n\n\tif (BPF_CLASS(insns[t].code) == BPF_JMP) {\n\t\tu8 opcode = BPF_OP(insns[t].code);\n\n\t\tif (opcode == BPF_EXIT) {\n\t\t\tgoto mark_explored;\n\t\t} else if (opcode == BPF_CALL) {\n\t\t\tret = push_insn(t, t + 1, FALLTHROUGH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t\tif (t + 1 < insn_cnt)\n\t\t\t\tenv->explored_states[t + 1] = STATE_LIST_MARK;\n\t\t\tif (insns[t].src_reg == BPF_PSEUDO_CALL) {\n\t\t\t\tenv->explored_states[t] = STATE_LIST_MARK;\n\t\t\t\tret = push_insn(t, t + insns[t].imm + 1, BRANCH, env);\n\t\t\t\tif (ret == 1)\n\t\t\t\t\tgoto peek_stack;\n\t\t\t\telse if (ret < 0)\n\t\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t} else if (opcode == BPF_JA) {\n\t\t\tif (BPF_SRC(insns[t].code) != BPF_K) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t\t/* unconditional jump with single edge */\n\t\t\tret = push_insn(t, t + insns[t].off + 1,\n\t\t\t\t\tFALLTHROUGH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t\t/* tell verifier to check for equivalent states\n\t\t\t * after every call and jump\n\t\t\t */\n\t\t\tif (t + 1 < insn_cnt)\n\t\t\t\tenv->explored_states[t + 1] = STATE_LIST_MARK;\n\t\t} else {\n\t\t\t/* conditional jump with two edges */\n\t\t\tenv->explored_states[t] = STATE_LIST_MARK;\n\t\t\tret = push_insn(t, t + 1, FALLTHROUGH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\n\t\t\tret = push_insn(t, t + insns[t].off + 1, BRANCH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t}\n\t} else {\n\t\t/* all other non-branch instructions with single\n\t\t * fall-through edge\n\t\t */\n\t\tret = push_insn(t, t + 1, FALLTHROUGH, env);\n\t\tif (ret == 1)\n\t\t\tgoto peek_stack;\n\t\telse if (ret < 0)\n\t\t\tgoto err_free;\n\t}\n\nmark_explored:\n\tinsn_state[t] = EXPLORED;\n\tif (cur_stack-- <= 0) {\n\t\tverbose(env, \"pop stack internal bug\\n\");\n\t\tret = -EFAULT;\n\t\tgoto err_free;\n\t}\n\tgoto peek_stack;\n\ncheck_state:\n\tfor (i = 0; i < insn_cnt; i++) {\n\t\tif (insn_state[i] != EXPLORED) {\n\t\t\tverbose(env, \"unreachable insn %d\\n\", i);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\t}\n\tret = 0; /* cfg looks good */\n\nerr_free:\n\tkfree(insn_state);\n\tkfree(insn_stack);\n\treturn ret;\n}\n\n/* The minimum supported BTF func info size */\n#define MIN_BPF_FUNCINFO_SIZE\t8\n#define MAX_FUNCINFO_REC_SIZE\t252\n\nstatic int check_btf_func(struct bpf_verifier_env *env,\n\t\t\t  const union bpf_attr *attr,\n\t\t\t  union bpf_attr __user *uattr)\n{\n\tu32 i, nfuncs, urec_size, min_size, prev_offset;\n\tu32 krec_size = sizeof(struct bpf_func_info);\n\tstruct bpf_func_info *krecord;\n\tconst struct btf_type *type;\n\tstruct bpf_prog *prog;\n\tconst struct btf *btf;\n\tvoid __user *urecord;\n\tint ret = 0;\n\n\tnfuncs = attr->func_info_cnt;\n\tif (!nfuncs)\n\t\treturn 0;\n\n\tif (nfuncs != env->subprog_cnt) {\n\t\tverbose(env, \"number of funcs in func_info doesn't match number of subprogs\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\turec_size = attr->func_info_rec_size;\n\tif (urec_size < MIN_BPF_FUNCINFO_SIZE ||\n\t    urec_size > MAX_FUNCINFO_REC_SIZE ||\n\t    urec_size % sizeof(u32)) {\n\t\tverbose(env, \"invalid func info rec size %u\\n\", urec_size);\n\t\treturn -EINVAL;\n\t}\n\n\tprog = env->prog;\n\tbtf = prog->aux->btf;\n\n\turecord = u64_to_user_ptr(attr->func_info);\n\tmin_size = min_t(u32, krec_size, urec_size);\n\n\tkrecord = kvcalloc(nfuncs, krec_size, GFP_KERNEL | __GFP_NOWARN);\n\tif (!krecord)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < nfuncs; i++) {\n\t\tret = bpf_check_uarg_tail_zero(urecord, krec_size, urec_size);\n\t\tif (ret) {\n\t\t\tif (ret == -E2BIG) {\n\t\t\t\tverbose(env, \"nonzero tailing record in func info\");\n\t\t\t\t/* set the size kernel expects so loader can zero\n\t\t\t\t * out the rest of the record.\n\t\t\t\t */\n\t\t\t\tif (put_user(min_size, &uattr->func_info_rec_size))\n\t\t\t\t\tret = -EFAULT;\n\t\t\t}\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (copy_from_user(&krecord[i], urecord, min_size)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/* check insn_off */\n\t\tif (i == 0) {\n\t\t\tif (krecord[i].insn_off) {\n\t\t\t\tverbose(env,\n\t\t\t\t\t\"nonzero insn_off %u for the first func info record\",\n\t\t\t\t\tkrecord[i].insn_off);\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t} else if (krecord[i].insn_off <= prev_offset) {\n\t\t\tverbose(env,\n\t\t\t\t\"same or smaller insn offset (%u) than previous func info record (%u)\",\n\t\t\t\tkrecord[i].insn_off, prev_offset);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (env->subprog_info[i].start != krecord[i].insn_off) {\n\t\t\tverbose(env, \"func_info BTF section doesn't match subprog layout in BPF program\\n\");\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/* check type_id */\n\t\ttype = btf_type_by_id(btf, krecord[i].type_id);\n\t\tif (!type || BTF_INFO_KIND(type->info) != BTF_KIND_FUNC) {\n\t\t\tverbose(env, \"invalid type id %d in func info\",\n\t\t\t\tkrecord[i].type_id);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tprev_offset = krecord[i].insn_off;\n\t\turecord += urec_size;\n\t}\n\n\tprog->aux->func_info = krecord;\n\tprog->aux->func_info_cnt = nfuncs;\n\treturn 0;\n\nerr_free:\n\tkvfree(krecord);\n\treturn ret;\n}\n\nstatic void adjust_btf_func(struct bpf_verifier_env *env)\n{\n\tint i;\n\n\tif (!env->prog->aux->func_info)\n\t\treturn;\n\n\tfor (i = 0; i < env->subprog_cnt; i++)\n\t\tenv->prog->aux->func_info[i].insn_off = env->subprog_info[i].start;\n}\n\n#define MIN_BPF_LINEINFO_SIZE\t(offsetof(struct bpf_line_info, line_col) + \\\n\t\tsizeof(((struct bpf_line_info *)(0))->line_col))\n#define MAX_LINEINFO_REC_SIZE\tMAX_FUNCINFO_REC_SIZE\n\nstatic int check_btf_line(struct bpf_verifier_env *env,\n\t\t\t  const union bpf_attr *attr,\n\t\t\t  union bpf_attr __user *uattr)\n{\n\tu32 i, s, nr_linfo, ncopy, expected_size, rec_size, prev_offset = 0;\n\tstruct bpf_subprog_info *sub;\n\tstruct bpf_line_info *linfo;\n\tstruct bpf_prog *prog;\n\tconst struct btf *btf;\n\tvoid __user *ulinfo;\n\tint err;\n\n\tnr_linfo = attr->line_info_cnt;\n\tif (!nr_linfo)\n\t\treturn 0;\n\n\trec_size = attr->line_info_rec_size;\n\tif (rec_size < MIN_BPF_LINEINFO_SIZE ||\n\t    rec_size > MAX_LINEINFO_REC_SIZE ||\n\t    rec_size & (sizeof(u32) - 1))\n\t\treturn -EINVAL;\n\n\t/* Need to zero it in case the userspace may\n\t * pass in a smaller bpf_line_info object.\n\t */\n\tlinfo = kvcalloc(nr_linfo, sizeof(struct bpf_line_info),\n\t\t\t GFP_KERNEL | __GFP_NOWARN);\n\tif (!linfo)\n\t\treturn -ENOMEM;\n\n\tprog = env->prog;\n\tbtf = prog->aux->btf;\n\n\ts = 0;\n\tsub = env->subprog_info;\n\tulinfo = u64_to_user_ptr(attr->line_info);\n\texpected_size = sizeof(struct bpf_line_info);\n\tncopy = min_t(u32, expected_size, rec_size);\n\tfor (i = 0; i < nr_linfo; i++) {\n\t\terr = bpf_check_uarg_tail_zero(ulinfo, expected_size, rec_size);\n\t\tif (err) {\n\t\t\tif (err == -E2BIG) {\n\t\t\t\tverbose(env, \"nonzero tailing record in line_info\");\n\t\t\t\tif (put_user(expected_size,\n\t\t\t\t\t     &uattr->line_info_rec_size))\n\t\t\t\t\terr = -EFAULT;\n\t\t\t}\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (copy_from_user(&linfo[i], ulinfo, ncopy)) {\n\t\t\terr = -EFAULT;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/*\n\t\t * Check insn_off to ensure\n\t\t * 1) strictly increasing AND\n\t\t * 2) bounded by prog->len\n\t\t *\n\t\t * The linfo[0].insn_off == 0 check logically falls into\n\t\t * the later \"missing bpf_line_info for func...\" case\n\t\t * because the first linfo[0].insn_off must be the\n\t\t * first sub also and the first sub must have\n\t\t * subprog_info[0].start == 0.\n\t\t */\n\t\tif ((i && linfo[i].insn_off <= prev_offset) ||\n\t\t    linfo[i].insn_off >= prog->len) {\n\t\t\tverbose(env, \"Invalid line_info[%u].insn_off:%u (prev_offset:%u prog->len:%u)\\n\",\n\t\t\t\ti, linfo[i].insn_off, prev_offset,\n\t\t\t\tprog->len);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (!prog->insnsi[linfo[i].insn_off].code) {\n\t\t\tverbose(env,\n\t\t\t\t\"Invalid insn code at line_info[%u].insn_off\\n\",\n\t\t\t\ti);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (!btf_name_by_offset(btf, linfo[i].line_off) ||\n\t\t    !btf_name_by_offset(btf, linfo[i].file_name_off)) {\n\t\t\tverbose(env, \"Invalid line_info[%u].line_off or .file_name_off\\n\", i);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (s != env->subprog_cnt) {\n\t\t\tif (linfo[i].insn_off == sub[s].start) {\n\t\t\t\tsub[s].linfo_idx = i;\n\t\t\t\ts++;\n\t\t\t} else if (sub[s].start < linfo[i].insn_off) {\n\t\t\t\tverbose(env, \"missing bpf_line_info for func#%u\\n\", s);\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t}\n\n\t\tprev_offset = linfo[i].insn_off;\n\t\tulinfo += rec_size;\n\t}\n\n\tif (s != env->subprog_cnt) {\n\t\tverbose(env, \"missing bpf_line_info for %u funcs starting from func#%u\\n\",\n\t\t\tenv->subprog_cnt - s, s);\n\t\terr = -EINVAL;\n\t\tgoto err_free;\n\t}\n\n\tprog->aux->linfo = linfo;\n\tprog->aux->nr_linfo = nr_linfo;\n\n\treturn 0;\n\nerr_free:\n\tkvfree(linfo);\n\treturn err;\n}\n\nstatic int check_btf_info(struct bpf_verifier_env *env,\n\t\t\t  const union bpf_attr *attr,\n\t\t\t  union bpf_attr __user *uattr)\n{\n\tstruct btf *btf;\n\tint err;\n\n\tif (!attr->func_info_cnt && !attr->line_info_cnt)\n\t\treturn 0;\n\n\tbtf = btf_get_by_fd(attr->prog_btf_fd);\n\tif (IS_ERR(btf))\n\t\treturn PTR_ERR(btf);\n\tenv->prog->aux->btf = btf;\n\n\terr = check_btf_func(env, attr, uattr);\n\tif (err)\n\t\treturn err;\n\n\terr = check_btf_line(env, attr, uattr);\n\tif (err)\n\t\treturn err;\n\n\treturn 0;\n}\n\n/* check %cur's range satisfies %old's */\nstatic bool range_within(struct bpf_reg_state *old,\n\t\t\t struct bpf_reg_state *cur)\n{\n\treturn old->umin_value <= cur->umin_value &&\n\t       old->umax_value >= cur->umax_value &&\n\t       old->smin_value <= cur->smin_value &&\n\t       old->smax_value >= cur->smax_value;\n}\n\n/* Maximum number of register states that can exist at once */\n#define ID_MAP_SIZE\t(MAX_BPF_REG + MAX_BPF_STACK / BPF_REG_SIZE)\nstruct idpair {\n\tu32 old;\n\tu32 cur;\n};\n\n/* If in the old state two registers had the same id, then they need to have\n * the same id in the new state as well.  But that id could be different from\n * the old state, so we need to track the mapping from old to new ids.\n * Once we have seen that, say, a reg with old id 5 had new id 9, any subsequent\n * regs with old id 5 must also have new id 9 for the new state to be safe.  But\n * regs with a different old id could still have new id 9, we don't care about\n * that.\n * So we look through our idmap to see if this old id has been seen before.  If\n * so, we require the new id to match; otherwise, we add the id pair to the map.\n */\nstatic bool check_ids(u32 old_id, u32 cur_id, struct idpair *idmap)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < ID_MAP_SIZE; i++) {\n\t\tif (!idmap[i].old) {\n\t\t\t/* Reached an empty slot; haven't seen this id before */\n\t\t\tidmap[i].old = old_id;\n\t\t\tidmap[i].cur = cur_id;\n\t\t\treturn true;\n\t\t}\n\t\tif (idmap[i].old == old_id)\n\t\t\treturn idmap[i].cur == cur_id;\n\t}\n\t/* We ran out of idmap slots, which should be impossible */\n\tWARN_ON_ONCE(1);\n\treturn false;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Var-Declaration",
        "line_new": 3445,
        "critical_vars": [
            "ret"
        ],
        "function": "adjust_scalar_min_max_vals",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_d3bd7413e0ca40b60cf60d4003246d067cafdeda_verifier.c.diff",
        "label": "True",
        "function_code": "static int adjust_scalar_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t      struct bpf_insn *insn,\n\t\t\t\t      struct bpf_reg_state *dst_reg,\n\t\t\t\t      struct bpf_reg_state src_reg)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 opcode = BPF_OP(insn->code);\n\tbool src_known, dst_known;\n\ts64 smin_val, smax_val;\n\tu64 umin_val, umax_val;\n\tu64 insn_bitness = (BPF_CLASS(insn->code) == BPF_ALU64) ? 64 : 32;\n\tu32 dst = insn->dst_reg;\n\tint ret;\n\n\tif (insn_bitness == 32) {\n\t\t/* Relevant for 32-bit RSH: Information can propagate towards\n\t\t * LSB, so it isn't sufficient to only truncate the output to\n\t\t * 32 bits.\n\t\t */\n\t\tcoerce_reg_to_size(dst_reg, 4);\n\t\tcoerce_reg_to_size(&src_reg, 4);\n\t}\n\n\tsmin_val = src_reg.smin_value;\n\tsmax_val = src_reg.smax_value;\n\tumin_val = src_reg.umin_value;\n\tumax_val = src_reg.umax_value;\n\tsrc_known = tnum_is_const(src_reg.var_off);\n\tdst_known = tnum_is_const(dst_reg->var_off);\n\n\tif ((src_known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (!src_known &&\n\t    opcode != BPF_ADD && opcode != BPF_SUB && opcode != BPF_AND) {\n\t\t__mark_reg_unknown(dst_reg);\n\t\treturn 0;\n\t}\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\tret = sanitize_val_alu(env, insn);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to add from different pointers or scalars\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\tif (signed_add_overflows(dst_reg->smin_value, smin_val) ||\n\t\t    signed_add_overflows(dst_reg->smax_value, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value += smin_val;\n\t\t\tdst_reg->smax_value += smax_val;\n\t\t}\n\t\tif (dst_reg->umin_value + umin_val < umin_val ||\n\t\t    dst_reg->umax_value + umax_val < umax_val) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value += umin_val;\n\t\t\tdst_reg->umax_value += umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(dst_reg->var_off, src_reg.var_off);\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tret = sanitize_val_alu(env, insn);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to sub from different pointers or scalars\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\tif (signed_sub_overflows(dst_reg->smin_value, smax_val) ||\n\t\t    signed_sub_overflows(dst_reg->smax_value, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value -= smax_val;\n\t\t\tdst_reg->smax_value -= smin_val;\n\t\t}\n\t\tif (dst_reg->umin_value < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value -= umax_val;\n\t\t\tdst_reg->umax_value -= umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(dst_reg->var_off, src_reg.var_off);\n\t\tbreak;\n\tcase BPF_MUL:\n\t\tdst_reg->var_off = tnum_mul(dst_reg->var_off, src_reg.var_off);\n\t\tif (smin_val < 0 || dst_reg->smin_value < 0) {\n\t\t\t/* Ain't nobody got time to multiply that sign */\n\t\t\t__mark_reg_unbounded(dst_reg);\n\t\t\t__update_reg_bounds(dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\t/* Both values are positive, so we can work with unsigned and\n\t\t * copy the result to signed (unless it exceeds S64_MAX).\n\t\t */\n\t\tif (umax_val > U32_MAX || dst_reg->umax_value > U32_MAX) {\n\t\t\t/* Potential overflow, we know nothing */\n\t\t\t__mark_reg_unbounded(dst_reg);\n\t\t\t/* (except what we can learn from the var_off) */\n\t\t\t__update_reg_bounds(dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\tdst_reg->umin_value *= umin_val;\n\t\tdst_reg->umax_value *= umax_val;\n\t\tif (dst_reg->umax_value > S64_MAX) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = dst_reg->umin_value;\n\t\t\tdst_reg->smax_value = dst_reg->umax_value;\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\t\tif (src_known && dst_known) {\n\t\t\t__mark_reg_known(dst_reg, dst_reg->var_off.value &\n\t\t\t\t\t\t  src_reg.var_off.value);\n\t\t\tbreak;\n\t\t}\n\t\t/* We get our minimum from the var_off, since that's inherently\n\t\t * bitwise.  Our maximum is the minimum of the operands' maxima.\n\t\t */\n\t\tdst_reg->var_off = tnum_and(dst_reg->var_off, src_reg.var_off);\n\t\tdst_reg->umin_value = dst_reg->var_off.value;\n\t\tdst_reg->umax_value = min(dst_reg->umax_value, umax_val);\n\t\tif (dst_reg->smin_value < 0 || smin_val < 0) {\n\t\t\t/* Lose signed bounds when ANDing negative numbers,\n\t\t\t * ain't nobody got time for that.\n\t\t\t */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\t/* ANDing two positives gives a positive, so safe to\n\t\t\t * cast result into s64.\n\t\t\t */\n\t\t\tdst_reg->smin_value = dst_reg->umin_value;\n\t\t\tdst_reg->smax_value = dst_reg->umax_value;\n\t\t}\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tcase BPF_OR:\n\t\tif (src_known && dst_known) {\n\t\t\t__mark_reg_known(dst_reg, dst_reg->var_off.value |\n\t\t\t\t\t\t  src_reg.var_off.value);\n\t\t\tbreak;\n\t\t}\n\t\t/* We get our maximum from the var_off, and our minimum is the\n\t\t * maximum of the operands' minima\n\t\t */\n\t\tdst_reg->var_off = tnum_or(dst_reg->var_off, src_reg.var_off);\n\t\tdst_reg->umin_value = max(dst_reg->umin_value, umin_val);\n\t\tdst_reg->umax_value = dst_reg->var_off.value |\n\t\t\t\t      dst_reg->var_off.mask;\n\t\tif (dst_reg->smin_value < 0 || smin_val < 0) {\n\t\t\t/* Lose signed bounds when ORing negative numbers,\n\t\t\t * ain't nobody got time for that.\n\t\t\t */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\t/* ORing two positives gives a positive, so safe to\n\t\t\t * cast result into s64.\n\t\t\t */\n\t\t\tdst_reg->smin_value = dst_reg->umin_value;\n\t\t\tdst_reg->smax_value = dst_reg->umax_value;\n\t\t}\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tcase BPF_LSH:\n\t\tif (umax_val >= insn_bitness) {\n\t\t\t/* Shifts greater than 31 or 63 are undefined.\n\t\t\t * This includes shifts by a negative number.\n\t\t\t */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\t/* We lose all sign bit information (except what we can pick\n\t\t * up from var_off)\n\t\t */\n\t\tdst_reg->smin_value = S64_MIN;\n\t\tdst_reg->smax_value = S64_MAX;\n\t\t/* If we might shift our top bit out, then we know nothing */\n\t\tif (dst_reg->umax_value > 1ULL << (63 - umax_val)) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value <<= umin_val;\n\t\t\tdst_reg->umax_value <<= umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_lshift(dst_reg->var_off, umin_val);\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tcase BPF_RSH:\n\t\tif (umax_val >= insn_bitness) {\n\t\t\t/* Shifts greater than 31 or 63 are undefined.\n\t\t\t * This includes shifts by a negative number.\n\t\t\t */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\t/* BPF_RSH is an unsigned shift.  If the value in dst_reg might\n\t\t * be negative, then either:\n\t\t * 1) src_reg might be zero, so the sign bit of the result is\n\t\t *    unknown, so we lose our signed bounds\n\t\t * 2) it's known negative, thus the unsigned bounds capture the\n\t\t *    signed bounds\n\t\t * 3) the signed bounds cross zero, so they tell us nothing\n\t\t *    about the result\n\t\t * If the value in dst_reg is known nonnegative, then again the\n\t\t * unsigned bounts capture the signed bounds.\n\t\t * Thus, in all cases it suffices to blow away our signed bounds\n\t\t * and rely on inferring new ones from the unsigned bounds and\n\t\t * var_off of the result.\n\t\t */\n\t\tdst_reg->smin_value = S64_MIN;\n\t\tdst_reg->smax_value = S64_MAX;\n\t\tdst_reg->var_off = tnum_rshift(dst_reg->var_off, umin_val);\n\t\tdst_reg->umin_value >>= umax_val;\n\t\tdst_reg->umax_value >>= umin_val;\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tcase BPF_ARSH:\n\t\tif (umax_val >= insn_bitness) {\n\t\t\t/* Shifts greater than 31 or 63 are undefined.\n\t\t\t * This includes shifts by a negative number.\n\t\t\t */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Upon reaching here, src_known is true and\n\t\t * umax_val is equal to umin_val.\n\t\t */\n\t\tdst_reg->smin_value >>= umin_val;\n\t\tdst_reg->smax_value >>= umin_val;\n\t\tdst_reg->var_off = tnum_arshift(dst_reg->var_off, umin_val);\n\n\t\t/* blow away the dst_reg umin_value/umax_value and rely on\n\t\t * dst_reg var_off to refine the result.\n\t\t */\n\t\tdst_reg->umin_value = 0;\n\t\tdst_reg->umax_value = U64_MAX;\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tdefault:\n\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\tbreak;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops are (32,32)->32 */\n\t\tcoerce_reg_to_size(dst_reg, 4);\n\t}\n\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\treturn 0;\n}\n\n/* Handles ALU ops other than BPF_END, BPF_NEG and BPF_MOV: computes new min/max\n * and var_off.\n */\nstatic int adjust_reg_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg, *src_reg;\n\tstruct bpf_reg_state *ptr_reg = NULL, off_reg = {0};\n\tu8 opcode = BPF_OP(insn->code);\n\n\tdst_reg = &regs[insn->dst_reg];\n\tsrc_reg = NULL;\n\tif (dst_reg->type != SCALAR_VALUE)\n\t\tptr_reg = dst_reg;\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tsrc_reg = &regs[insn->src_reg];\n\t\tif (src_reg->type != SCALAR_VALUE) {\n\t\t\tif (dst_reg->type != SCALAR_VALUE) {\n\t\t\t\t/* Combining two pointers by any ALU op yields\n\t\t\t\t * an arbitrary scalar. Disallow all math except\n\t\t\t\t * pointer subtraction\n\t\t\t\t */\n\t\t\t\tif (opcode == BPF_SUB && env->allow_ptr_leaks) {\n\t\t\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t\tverbose(env, \"R%d pointer %s pointer prohibited\\n\",\n\t\t\t\t\tinsn->dst_reg,\n\t\t\t\t\tbpf_alu_string[opcode >> 4]);\n\t\t\t\treturn -EACCES;\n\t\t\t} else {\n\t\t\t\t/* scalar += pointer\n\t\t\t\t * This is legal, but we have to reverse our\n\t\t\t\t * src/dest handling in computing the range\n\t\t\t\t */\n\t\t\t\treturn adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t\t       src_reg, dst_reg);\n\t\t\t}\n\t\t} else if (ptr_reg) {\n\t\t\t/* pointer += scalar */\n\t\t\treturn adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t       dst_reg, src_reg);\n\t\t}\n\t} else {\n\t\t/* Pretend the src is a reg with a known value, since we only\n\t\t * need to be able to read from this state.\n\t\t */\n\t\toff_reg.type = SCALAR_VALUE;\n\t\t__mark_reg_known(&off_reg, insn->imm);\n\t\tsrc_reg = &off_reg;\n\t\tif (ptr_reg) /* pointer += K */\n\t\t\treturn adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t       ptr_reg, src_reg);\n\t}\n\n\t/* Got here implies adding two SCALAR_VALUEs */\n\tif (WARN_ON_ONCE(ptr_reg)) {\n\t\tprint_verifier_state(env, state);\n\t\tverbose(env, \"verifier internal error: unexpected ptr_reg\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (WARN_ON(!src_reg)) {\n\t\tprint_verifier_state(env, state);\n\t\tverbose(env, \"verifier internal error: no src_reg\\n\");\n\t\treturn -EINVAL;\n\t}\n\treturn adjust_scalar_min_max_vals(env, insn, dst_reg, *src_reg);\n}\n\n/* check validity of 32-bit and 64-bit arithmetic operations */\nstatic int check_alu_op(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 opcode = BPF_OP(insn->code);\n\tint err;\n\n\tif (opcode == BPF_END || opcode == BPF_NEG) {\n\t\tif (opcode == BPF_NEG) {\n\t\t\tif (BPF_SRC(insn->code) != 0 ||\n\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t    insn->off != 0 || insn->imm != 0) {\n\t\t\t\tverbose(env, \"BPF_NEG uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else {\n\t\t\tif (insn->src_reg != BPF_REG_0 || insn->off != 0 ||\n\t\t\t    (insn->imm != 16 && insn->imm != 32 && insn->imm != 64) ||\n\t\t\t    BPF_CLASS(insn->code) == BPF_ALU64) {\n\t\t\t\tverbose(env, \"BPF_END uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check src operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (is_pointer_value(env, insn->dst_reg)) {\n\t\t\tverbose(env, \"R%d pointer arithmetic prohibited\\n\",\n\t\t\t\tinsn->dst_reg);\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\t/* check dest operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t} else if (opcode == BPF_MOV) {\n\n\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\tif (insn->imm != 0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_MOV uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\t/* check src operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t} else {\n\t\t\tif (insn->src_reg != BPF_REG_0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_MOV uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check dest operand, mark as required later */\n\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP_NO_MARK);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\tstruct bpf_reg_state *src_reg = regs + insn->src_reg;\n\t\t\tstruct bpf_reg_state *dst_reg = regs + insn->dst_reg;\n\n\t\t\tif (BPF_CLASS(insn->code) == BPF_ALU64) {\n\t\t\t\t/* case: R1 = R2\n\t\t\t\t * copy register state to dest reg\n\t\t\t\t */\n\t\t\t\t*dst_reg = *src_reg;\n\t\t\t\tdst_reg->live |= REG_LIVE_WRITTEN;\n\t\t\t} else {\n\t\t\t\t/* R1 = (u32) R2 */\n\t\t\t\tif (is_pointer_value(env, insn->src_reg)) {\n\t\t\t\t\tverbose(env,\n\t\t\t\t\t\t\"R%d partial copy of pointer\\n\",\n\t\t\t\t\t\tinsn->src_reg);\n\t\t\t\t\treturn -EACCES;\n\t\t\t\t} else if (src_reg->type == SCALAR_VALUE) {\n\t\t\t\t\t*dst_reg = *src_reg;\n\t\t\t\t\tdst_reg->live |= REG_LIVE_WRITTEN;\n\t\t\t\t} else {\n\t\t\t\t\tmark_reg_unknown(env, regs,\n\t\t\t\t\t\t\t insn->dst_reg);\n\t\t\t\t}\n\t\t\t\tcoerce_reg_to_size(dst_reg, 4);\n\t\t\t}\n\t\t} else {\n\t\t\t/* case: R = imm\n\t\t\t * remember the value we stored into this reg\n\t\t\t */\n\t\t\t/* clear any state __mark_reg_known doesn't set */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tregs[insn->dst_reg].type = SCALAR_VALUE;\n\t\t\tif (BPF_CLASS(insn->code) == BPF_ALU64) {\n\t\t\t\t__mark_reg_known(regs + insn->dst_reg,\n\t\t\t\t\t\t insn->imm);\n\t\t\t} else {\n\t\t\t\t__mark_reg_known(regs + insn->dst_reg,\n\t\t\t\t\t\t (u32)insn->imm);\n\t\t\t}\n\t\t}\n\n\t} else if (opcode > BPF_END) {\n\t\tverbose(env, \"invalid BPF_ALU opcode %x\\n\", opcode);\n\t\treturn -EINVAL;\n\n\t} else {\t/* all other ALU ops: and, sub, xor, add, ... */\n\n\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\tif (insn->imm != 0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_ALU uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\t/* check src1 operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t} else {\n\t\t\tif (insn->src_reg != BPF_REG_0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_ALU uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check src2 operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif ((opcode == BPF_MOD || opcode == BPF_DIV) &&\n\t\t    BPF_SRC(insn->code) == BPF_K && insn->imm == 0) {\n\t\t\tverbose(env, \"div by zero\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif ((opcode == BPF_LSH || opcode == BPF_RSH ||\n\t\t     opcode == BPF_ARSH) && BPF_SRC(insn->code) == BPF_K) {\n\t\t\tint size = BPF_CLASS(insn->code) == BPF_ALU64 ? 64 : 32;\n\n\t\t\tif (insn->imm < 0 || insn->imm >= size) {\n\t\t\t\tverbose(env, \"invalid shift %d\\n\", insn->imm);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check dest operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP_NO_MARK);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\treturn adjust_reg_min_max_vals(env, insn);\n\t}\n\n\treturn 0;\n}\n\nstatic void find_good_pkt_pointers(struct bpf_verifier_state *vstate,\n\t\t\t\t   struct bpf_reg_state *dst_reg,\n\t\t\t\t   enum bpf_reg_type type,\n\t\t\t\t   bool range_right_open)\n{\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *reg;\n\tu16 new_range;\n\tint i, j;\n\n\tif (dst_reg->off < 0 ||\n\t    (dst_reg->off == 0 && range_right_open))\n\t\t/* This doesn't give us any range */\n\t\treturn;\n\n\tif (dst_reg->umax_value > MAX_PACKET_OFF ||\n\t    dst_reg->umax_value + dst_reg->off > MAX_PACKET_OFF)\n\t\t/* Risk of overflow.  For instance, ptr + (1<<63) may be less\n\t\t * than pkt_end, but that's because it's also less than pkt.\n\t\t */\n\t\treturn;\n\n\tnew_range = dst_reg->off;\n\tif (range_right_open)\n\t\tnew_range--;\n\n\t/* Examples for register markings:\n\t *\n\t * pkt_data in dst register:\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (r2 > pkt_end) goto <handle exception>\n\t *   <access okay>\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (r2 < pkt_end) goto <access okay>\n\t *   <handle exception>\n\t *\n\t *   Where:\n\t *     r2 == dst_reg, pkt_end == src_reg\n\t *     r2=pkt(id=n,off=8,r=0)\n\t *     r3=pkt(id=n,off=0,r=0)\n\t *\n\t * pkt_data in src register:\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (pkt_end >= r2) goto <access okay>\n\t *   <handle exception>\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (pkt_end <= r2) goto <handle exception>\n\t *   <access okay>\n\t *\n\t *   Where:\n\t *     pkt_end == dst_reg, r2 == src_reg\n\t *     r2=pkt(id=n,off=8,r=0)\n\t *     r3=pkt(id=n,off=0,r=0)\n\t *\n\t * Find register r3 and mark its range as r3=pkt(id=n,off=0,r=8)\n\t * or r3=pkt(id=n,off=0,r=8-1), so that range of bytes [r3, r3 + 8)\n\t * and [r3, r3 + 8-1) respectively is safe to access depending on\n\t * the check.\n\t */\n\n\t/* If our ids match, then we must have the same max_value.  And we\n\t * don't care about the other reg's fixed offset, since if it's too big\n\t * the range won't allow anything.\n\t * dst_reg->off is known < MAX_PACKET_OFF, therefore it fits in a u16.\n\t */\n\tfor (i = 0; i < MAX_BPF_REG; i++)\n\t\tif (regs[i].type == type && regs[i].id == dst_reg->id)\n\t\t\t/* keep the maximum range already checked */\n\t\t\tregs[i].range = max(regs[i].range, new_range);\n\n\tfor (j = 0; j <= vstate->curframe; j++) {\n\t\tstate = vstate->frame[j];\n\t\tbpf_for_each_spilled_reg(i, state, reg) {\n\t\t\tif (!reg)\n\t\t\t\tcontinue;\n\t\t\tif (reg->type == type && reg->id == dst_reg->id)\n\t\t\t\treg->range = max(reg->range, new_range);\n\t\t}\n\t}\n}\n\n/* compute branch direction of the expression \"if (reg opcode val) goto target;\"\n * and return:\n *  1 - branch will be taken and \"goto target\" will be executed\n *  0 - branch will not be taken and fall-through to next insn\n * -1 - unknown. Example: \"if (reg < 5)\" is unknown when register value range [0,10]\n */\nstatic int is_branch_taken(struct bpf_reg_state *reg, u64 val, u8 opcode)\n{\n\tif (__is_pointer_value(false, reg))\n\t\treturn -1;\n\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\tif (tnum_is_const(reg->var_off))\n\t\t\treturn !!tnum_equals_const(reg->var_off, val);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\tif (tnum_is_const(reg->var_off))\n\t\t\treturn !tnum_equals_const(reg->var_off, val);\n\t\tbreak;\n\tcase BPF_JSET:\n\t\tif ((~reg->var_off.mask & reg->var_off.value) & val)\n\t\t\treturn 1;\n\t\tif (!((reg->var_off.mask | reg->var_off.value) & val))\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JGT:\n\t\tif (reg->umin_value > val)\n\t\t\treturn 1;\n\t\telse if (reg->umax_value <= val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSGT:\n\t\tif (reg->smin_value > (s64)val)\n\t\t\treturn 1;\n\t\telse if (reg->smax_value < (s64)val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JLT:\n\t\tif (reg->umax_value < val)\n\t\t\treturn 1;\n\t\telse if (reg->umin_value >= val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSLT:\n\t\tif (reg->smax_value < (s64)val)\n\t\t\treturn 1;\n\t\telse if (reg->smin_value >= (s64)val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JGE:\n\t\tif (reg->umin_value >= val)\n\t\t\treturn 1;\n\t\telse if (reg->umax_value < val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSGE:\n\t\tif (reg->smin_value >= (s64)val)\n\t\t\treturn 1;\n\t\telse if (reg->smax_value < (s64)val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JLE:\n\t\tif (reg->umax_value <= val)\n\t\t\treturn 1;\n\t\telse if (reg->umin_value > val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSLE:\n\t\tif (reg->smax_value <= (s64)val)\n\t\t\treturn 1;\n\t\telse if (reg->smin_value > (s64)val)\n\t\t\treturn 0;\n\t\tbreak;\n\t}\n\n\treturn -1;\n}\n\n/* Adjusts the register min/max values in the case that the dst_reg is the\n * variable register that we are working on, and src_reg is a constant or we're\n * simply doing a BPF_K check.\n * In JEQ/JNE cases we also adjust the var_off values.\n */\nstatic void reg_set_min_max(struct bpf_reg_state *true_reg,\n\t\t\t    struct bpf_reg_state *false_reg, u64 val,\n\t\t\t    u8 opcode)\n{\n\t/* If the dst_reg is a pointer, we can't learn anything about its\n\t * variable offset from the compare (unless src_reg were a pointer into\n\t * the same object, but we don't bother with that.\n\t * Since false_reg and true_reg have the same type by construction, we\n\t * only need to check one of them for pointerness.\n\t */\n\tif (__is_pointer_value(false, false_reg))\n\t\treturn;\n\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\t/* If this is false then we know nothing Jon Snow, but if it is\n\t\t * true then we know for sure.\n\t\t */\n\t\t__mark_reg_known(true_reg, val);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\t/* If this is true we know nothing Jon Snow, but if it is false\n\t\t * we know the value for sure;\n\t\t */\n\t\t__mark_reg_known(false_reg, val);\n\t\tbreak;\n\tcase BPF_JSET:\n\t\tfalse_reg->var_off = tnum_and(false_reg->var_off,\n\t\t\t\t\t      tnum_const(~val));\n\t\tif (is_power_of_2(val))\n\t\t\ttrue_reg->var_off = tnum_or(true_reg->var_off,\n\t\t\t\t\t\t    tnum_const(val));\n\t\tbreak;\n\tcase BPF_JGT:\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val);\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JSGT:\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val);\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JLT:\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val);\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val - 1);\n\t\tbreak;\n\tcase BPF_JSLT:\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val);\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val - 1);\n\t\tbreak;\n\tcase BPF_JGE:\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val - 1);\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val);\n\t\tbreak;\n\tcase BPF_JSGE:\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val - 1);\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val);\n\t\tbreak;\n\tcase BPF_JLE:\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val + 1);\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val);\n\t\tbreak;\n\tcase BPF_JSLE:\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val + 1);\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t__reg_deduce_bounds(false_reg);\n\t__reg_deduce_bounds(true_reg);\n\t/* We might have learned some bits from the bounds. */\n\t__reg_bound_offset(false_reg);\n\t__reg_bound_offset(true_reg);\n\t/* Intersecting with the old var_off might have improved our bounds\n\t * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),\n\t * then new var_off is (0; 0x7f...fc) which improves our umax.\n\t */\n\t__update_reg_bounds(false_reg);\n\t__update_reg_bounds(true_reg);\n}\n\n/* Same as above, but for the case that dst_reg holds a constant and src_reg is\n * the variable reg.\n */\nstatic void reg_set_min_max_inv(struct bpf_reg_state *true_reg,\n\t\t\t\tstruct bpf_reg_state *false_reg, u64 val,\n\t\t\t\tu8 opcode)\n{\n\tif (__is_pointer_value(false, false_reg))\n\t\treturn;\n\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\t/* If this is false then we know nothing Jon Snow, but if it is\n\t\t * true then we know for sure.\n\t\t */\n\t\t__mark_reg_known(true_reg, val);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\t/* If this is true we know nothing Jon Snow, but if it is false\n\t\t * we know the value for sure;\n\t\t */\n\t\t__mark_reg_known(false_reg, val);\n\t\tbreak;\n\tcase BPF_JSET:\n\t\tfalse_reg->var_off = tnum_and(false_reg->var_off,\n\t\t\t\t\t      tnum_const(~val));\n\t\tif (is_power_of_2(val))\n\t\t\ttrue_reg->var_off = tnum_or(true_reg->var_off,\n\t\t\t\t\t\t    tnum_const(val));\n\t\tbreak;\n\tcase BPF_JGT:\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val - 1);\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val);\n\t\tbreak;\n\tcase BPF_JSGT:\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val - 1);\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val);\n\t\tbreak;\n\tcase BPF_JLT:\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val + 1);\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val);\n\t\tbreak;\n\tcase BPF_JSLT:\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val + 1);\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val);\n\t\tbreak;\n\tcase BPF_JGE:\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val);\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JSGE:\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val);\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JLE:\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val);\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val - 1);\n\t\tbreak;\n\tcase BPF_JSLE:\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val);\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val - 1);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t__reg_deduce_bounds(false_reg);\n\t__reg_deduce_bounds(true_reg);\n\t/* We might have learned some bits from the bounds. */\n\t__reg_bound_offset(false_reg);\n\t__reg_bound_offset(true_reg);\n\t/* Intersecting with the old var_off might have improved our bounds\n\t * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),\n\t * then new var_off is (0; 0x7f...fc) which improves our umax.\n\t */\n\t__update_reg_bounds(false_reg);\n\t__update_reg_bounds(true_reg);\n}\n\n/* Regs are known to be equal, so intersect their min/max/var_off */\nstatic void __reg_combine_min_max(struct bpf_reg_state *src_reg,\n\t\t\t\t  struct bpf_reg_state *dst_reg)\n{\n\tsrc_reg->umin_value = dst_reg->umin_value = max(src_reg->umin_value,\n\t\t\t\t\t\t\tdst_reg->umin_value);\n\tsrc_reg->umax_value = dst_reg->umax_value = min(src_reg->umax_value,\n\t\t\t\t\t\t\tdst_reg->umax_value);\n\tsrc_reg->smin_value = dst_reg->smin_value = max(src_reg->smin_value,\n\t\t\t\t\t\t\tdst_reg->smin_value);\n\tsrc_reg->smax_value = dst_reg->smax_value = min(src_reg->smax_value,\n\t\t\t\t\t\t\tdst_reg->smax_value);\n\tsrc_reg->var_off = dst_reg->var_off = tnum_intersect(src_reg->var_off,\n\t\t\t\t\t\t\t     dst_reg->var_off);\n\t/* We might have learned new bounds from the var_off. */\n\t__update_reg_bounds(src_reg);\n\t__update_reg_bounds(dst_reg);\n\t/* We might have learned something about the sign bit. */\n\t__reg_deduce_bounds(src_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t/* We might have learned some bits from the bounds. */\n\t__reg_bound_offset(src_reg);\n\t__reg_bound_offset(dst_reg);\n\t/* Intersecting with the old var_off might have improved our bounds\n\t * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),\n\t * then new var_off is (0; 0x7f...fc) which improves our umax.\n\t */\n\t__update_reg_bounds(src_reg);\n\t__update_reg_bounds(dst_reg);\n}\n\nstatic void reg_combine_min_max(struct bpf_reg_state *true_src,\n\t\t\t\tstruct bpf_reg_state *true_dst,\n\t\t\t\tstruct bpf_reg_state *false_src,\n\t\t\t\tstruct bpf_reg_state *false_dst,\n\t\t\t\tu8 opcode)\n{\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\t__reg_combine_min_max(true_src, true_dst);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\t__reg_combine_min_max(false_src, false_dst);\n\t\tbreak;\n\t}\n}\n\nstatic void mark_ptr_or_null_reg(struct bpf_func_state *state,\n\t\t\t\t struct bpf_reg_state *reg, u32 id,\n\t\t\t\t bool is_null)\n{\n\tif (reg_type_may_be_null(reg->type) && reg->id == id) {\n\t\t/* Old offset (both fixed and variable parts) should\n\t\t * have been known-zero, because we don't allow pointer\n\t\t * arithmetic on pointers that might be NULL.\n\t\t */\n\t\tif (WARN_ON_ONCE(reg->smin_value || reg->smax_value ||\n\t\t\t\t !tnum_equals_const(reg->var_off, 0) ||\n\t\t\t\t reg->off)) {\n\t\t\t__mark_reg_known_zero(reg);\n\t\t\treg->off = 0;\n\t\t}\n\t\tif (is_null) {\n\t\t\treg->type = SCALAR_VALUE;\n\t\t} else if (reg->type == PTR_TO_MAP_VALUE_OR_NULL) {\n\t\t\tif (reg->map_ptr->inner_map_meta) {\n\t\t\t\treg->type = CONST_PTR_TO_MAP;\n\t\t\t\treg->map_ptr = reg->map_ptr->inner_map_meta;\n\t\t\t} else {\n\t\t\t\treg->type = PTR_TO_MAP_VALUE;\n\t\t\t}\n\t\t} else if (reg->type == PTR_TO_SOCKET_OR_NULL) {\n\t\t\treg->type = PTR_TO_SOCKET;\n\t\t}\n\t\tif (is_null || !reg_is_refcounted(reg)) {\n\t\t\t/* We don't need id from this point onwards anymore,\n\t\t\t * thus we should better reset it, so that state\n\t\t\t * pruning has chances to take effect.\n\t\t\t */\n\t\t\treg->id = 0;\n\t\t}\n\t}\n}\n\n/* The logic is similar to find_good_pkt_pointers(), both could eventually\n * be folded together at some point.\n */\nstatic void mark_ptr_or_null_regs(struct bpf_verifier_state *vstate, u32 regno,\n\t\t\t\t  bool is_null)\n{\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *reg, *regs = state->regs;\n\tu32 id = regs[regno].id;\n\tint i, j;\n\n\tif (reg_is_refcounted_or_null(&regs[regno]) && is_null)\n\t\t__release_reference_state(state, id);\n\n\tfor (i = 0; i < MAX_BPF_REG; i++)\n\t\tmark_ptr_or_null_reg(state, &regs[i], id, is_null);\n\n\tfor (j = 0; j <= vstate->curframe; j++) {\n\t\tstate = vstate->frame[j];\n\t\tbpf_for_each_spilled_reg(i, state, reg) {\n\t\t\tif (!reg)\n\t\t\t\tcontinue;\n\t\t\tmark_ptr_or_null_reg(state, reg, id, is_null);\n\t\t}\n\t}\n}\n\nstatic bool try_match_pkt_pointers(const struct bpf_insn *insn,\n\t\t\t\t   struct bpf_reg_state *dst_reg,\n\t\t\t\t   struct bpf_reg_state *src_reg,\n\t\t\t\t   struct bpf_verifier_state *this_branch,\n\t\t\t\t   struct bpf_verifier_state *other_branch)\n{\n\tif (BPF_SRC(insn->code) != BPF_X)\n\t\treturn false;\n\n\tswitch (BPF_OP(insn->code)) {\n\tcase BPF_JGT:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' > pkt_end, pkt_meta' > pkt_data */\n\t\t\tfind_good_pkt_pointers(this_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, false);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end > pkt_data', pkt_data > pkt_meta' */\n\t\t\tfind_good_pkt_pointers(other_branch, src_reg,\n\t\t\t\t\t       src_reg->type, true);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase BPF_JLT:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' < pkt_end, pkt_meta' < pkt_data */\n\t\t\tfind_good_pkt_pointers(other_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, true);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end < pkt_data', pkt_data > pkt_meta' */\n\t\t\tfind_good_pkt_pointers(this_branch, src_reg,\n\t\t\t\t\t       src_reg->type, false);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase BPF_JGE:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' >= pkt_end, pkt_meta' >= pkt_data */\n\t\t\tfind_good_pkt_pointers(this_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, true);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end >= pkt_data', pkt_data >= pkt_meta' */\n\t\t\tfind_good_pkt_pointers(other_branch, src_reg,\n\t\t\t\t\t       src_reg->type, false);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase BPF_JLE:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' <= pkt_end, pkt_meta' <= pkt_data */\n\t\t\tfind_good_pkt_pointers(other_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, false);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end <= pkt_data', pkt_data <= pkt_meta' */\n\t\t\tfind_good_pkt_pointers(this_branch, src_reg,\n\t\t\t\t\t       src_reg->type, true);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic int check_cond_jmp_op(struct bpf_verifier_env *env,\n\t\t\t     struct bpf_insn *insn, int *insn_idx)\n{\n\tstruct bpf_verifier_state *this_branch = env->cur_state;\n\tstruct bpf_verifier_state *other_branch;\n\tstruct bpf_reg_state *regs = this_branch->frame[this_branch->curframe]->regs;\n\tstruct bpf_reg_state *dst_reg, *other_branch_regs;\n\tu8 opcode = BPF_OP(insn->code);\n\tint err;\n\n\tif (opcode > BPF_JSLE) {\n\t\tverbose(env, \"invalid BPF_JMP opcode %x\\n\", opcode);\n\t\treturn -EINVAL;\n\t}\n\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tif (insn->imm != 0) {\n\t\t\tverbose(env, \"BPF_JMP uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/* check src1 operand */\n\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (is_pointer_value(env, insn->src_reg)) {\n\t\t\tverbose(env, \"R%d pointer comparison prohibited\\n\",\n\t\t\t\tinsn->src_reg);\n\t\t\treturn -EACCES;\n\t\t}\n\t} else {\n\t\tif (insn->src_reg != BPF_REG_0) {\n\t\t\tverbose(env, \"BPF_JMP uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t/* check src2 operand */\n\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tdst_reg = &regs[insn->dst_reg];\n\n\tif (BPF_SRC(insn->code) == BPF_K) {\n\t\tint pred = is_branch_taken(dst_reg, insn->imm, opcode);\n\n\t\tif (pred == 1) {\n\t\t\t /* only follow the goto, ignore fall-through */\n\t\t\t*insn_idx += insn->off;\n\t\t\treturn 0;\n\t\t} else if (pred == 0) {\n\t\t\t/* only follow fall-through branch, since\n\t\t\t * that's where the program will go\n\t\t\t */\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tother_branch = push_stack(env, *insn_idx + insn->off + 1, *insn_idx,\n\t\t\t\t  false);\n\tif (!other_branch)\n\t\treturn -EFAULT;\n\tother_branch_regs = other_branch->frame[other_branch->curframe]->regs;\n\n\t/* detect if we are comparing against a constant value so we can adjust\n\t * our min/max values for our dst register.\n\t * this is only legit if both are scalars (or pointers to the same\n\t * object, I suppose, but we don't support that right now), because\n\t * otherwise the different base pointers mean the offsets aren't\n\t * comparable.\n\t */\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tif (dst_reg->type == SCALAR_VALUE &&\n\t\t    regs[insn->src_reg].type == SCALAR_VALUE) {\n\t\t\tif (tnum_is_const(regs[insn->src_reg].var_off))\n\t\t\t\treg_set_min_max(&other_branch_regs[insn->dst_reg],\n\t\t\t\t\t\tdst_reg, regs[insn->src_reg].var_off.value,\n\t\t\t\t\t\topcode);\n\t\t\telse if (tnum_is_const(dst_reg->var_off))\n\t\t\t\treg_set_min_max_inv(&other_branch_regs[insn->src_reg],\n\t\t\t\t\t\t    &regs[insn->src_reg],\n\t\t\t\t\t\t    dst_reg->var_off.value, opcode);\n\t\t\telse if (opcode == BPF_JEQ || opcode == BPF_JNE)\n\t\t\t\t/* Comparing for equality, we can combine knowledge */\n\t\t\t\treg_combine_min_max(&other_branch_regs[insn->src_reg],\n\t\t\t\t\t\t    &other_branch_regs[insn->dst_reg],\n\t\t\t\t\t\t    &regs[insn->src_reg],\n\t\t\t\t\t\t    &regs[insn->dst_reg], opcode);\n\t\t}\n\t} else if (dst_reg->type == SCALAR_VALUE) {\n\t\treg_set_min_max(&other_branch_regs[insn->dst_reg],\n\t\t\t\t\tdst_reg, insn->imm, opcode);\n\t}\n\n\t/* detect if R == 0 where R is returned from bpf_map_lookup_elem() */\n\tif (BPF_SRC(insn->code) == BPF_K &&\n\t    insn->imm == 0 && (opcode == BPF_JEQ || opcode == BPF_JNE) &&\n\t    reg_type_may_be_null(dst_reg->type)) {\n\t\t/* Mark all identical registers in each branch as either\n\t\t * safe or unknown depending R == 0 or R != 0 conditional.\n\t\t */\n\t\tmark_ptr_or_null_regs(this_branch, insn->dst_reg,\n\t\t\t\t      opcode == BPF_JNE);\n\t\tmark_ptr_or_null_regs(other_branch, insn->dst_reg,\n\t\t\t\t      opcode == BPF_JEQ);\n\t} else if (!try_match_pkt_pointers(insn, dst_reg, &regs[insn->src_reg],\n\t\t\t\t\t   this_branch, other_branch) &&\n\t\t   is_pointer_value(env, insn->dst_reg)) {\n\t\tverbose(env, \"R%d pointer comparison prohibited\\n\",\n\t\t\tinsn->dst_reg);\n\t\treturn -EACCES;\n\t}\n\tif (env->log.level)\n\t\tprint_verifier_state(env, this_branch->frame[this_branch->curframe]);\n\treturn 0;\n}\n\n/* return the map pointer stored inside BPF_LD_IMM64 instruction */\nstatic struct bpf_map *ld_imm64_to_map_ptr(struct bpf_insn *insn)\n{\n\tu64 imm64 = ((u64) (u32) insn[0].imm) | ((u64) (u32) insn[1].imm) << 32;\n\n\treturn (struct bpf_map *) (unsigned long) imm64;\n}\n\n/* verify BPF_LD_IMM64 instruction */\nstatic int check_ld_imm(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tint err;\n\n\tif (BPF_SIZE(insn->code) != BPF_DW) {\n\t\tverbose(env, \"invalid BPF_LD_IMM insn\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (insn->off != 0) {\n\t\tverbose(env, \"BPF_LD_IMM64 uses reserved fields\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\terr = check_reg_arg(env, insn->dst_reg, DST_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (insn->src_reg == 0) {\n\t\tu64 imm = ((u64)(insn + 1)->imm << 32) | (u32)insn->imm;\n\n\t\tregs[insn->dst_reg].type = SCALAR_VALUE;\n\t\t__mark_reg_known(&regs[insn->dst_reg], imm);\n\t\treturn 0;\n\t}\n\n\t/* replace_map_fd_with_map_ptr() should have caught bad ld_imm64 */\n\tBUG_ON(insn->src_reg != BPF_PSEUDO_MAP_FD);\n\n\tregs[insn->dst_reg].type = CONST_PTR_TO_MAP;\n\tregs[insn->dst_reg].map_ptr = ld_imm64_to_map_ptr(insn);\n\treturn 0;\n}\n\nstatic bool may_access_skb(enum bpf_prog_type type)\n{\n\tswitch (type) {\n\tcase BPF_PROG_TYPE_SOCKET_FILTER:\n\tcase BPF_PROG_TYPE_SCHED_CLS:\n\tcase BPF_PROG_TYPE_SCHED_ACT:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n/* verify safety of LD_ABS|LD_IND instructions:\n * - they can only appear in the programs where ctx == skb\n * - since they are wrappers of function calls, they scratch R1-R5 registers,\n *   preserve R6-R9, and store return value into R0\n *\n * Implicit input:\n *   ctx == skb == R6 == CTX\n *\n * Explicit input:\n *   SRC == any register\n *   IMM == 32-bit immediate\n *\n * Output:\n *   R0 - 8/16/32-bit skb data converted to cpu endianness\n */\nstatic int check_ld_abs(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 mode = BPF_MODE(insn->code);\n\tint i, err;\n\n\tif (!may_access_skb(env->prog->type)) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] instructions not allowed for this program type\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!env->ops->gen_ld_abs) {\n\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (env->subprog_cnt > 1) {\n\t\t/* when program has LD_ABS insn JITs and interpreter assume\n\t\t * that r1 == ctx == skb which is not the case for callees\n\t\t * that can have arbitrary arguments. It's problematic\n\t\t * for main prog as well since JITs would need to analyze\n\t\t * all functions in order to make proper register save/restore\n\t\t * decisions in the main prog. Hence disallow LD_ABS with calls\n\t\t */\n\t\tverbose(env, \"BPF_LD_[ABS|IND] instructions cannot be mixed with bpf-to-bpf calls\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (insn->dst_reg != BPF_REG_0 || insn->off != 0 ||\n\t    BPF_SIZE(insn->code) == BPF_DW ||\n\t    (mode == BPF_ABS && insn->src_reg != BPF_REG_0)) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] uses reserved fields\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* check whether implicit source operand (register R6) is readable */\n\terr = check_reg_arg(env, BPF_REG_6, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\t/* Disallow usage of BPF_LD_[ABS|IND] with reference tracking, as\n\t * gen_ld_abs() may terminate the program at runtime, leading to\n\t * reference leak.\n\t */\n\terr = check_reference_leak(env);\n\tif (err) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] cannot be mixed with socket references\\n\");\n\t\treturn err;\n\t}\n\n\tif (regs[BPF_REG_6].type != PTR_TO_CTX) {\n\t\tverbose(env,\n\t\t\t\"at the time of BPF_LD_ABS|IND R6 != pointer to skb\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (mode == BPF_IND) {\n\t\t/* check explicit source operand */\n\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\t/* reset caller saved regs to unreadable */\n\tfor (i = 0; i < CALLER_SAVED_REGS; i++) {\n\t\tmark_reg_not_init(env, regs, caller_saved[i]);\n\t\tcheck_reg_arg(env, caller_saved[i], DST_OP_NO_MARK);\n\t}\n\n\t/* mark destination R0 register as readable, since it contains\n\t * the value fetched from the packet.\n\t * Already marked as written above.\n\t */\n\tmark_reg_unknown(env, regs, BPF_REG_0);\n\treturn 0;\n}\n\nstatic int check_return_code(struct bpf_verifier_env *env)\n{\n\tstruct bpf_reg_state *reg;\n\tstruct tnum range = tnum_range(0, 1);\n\n\tswitch (env->prog->type) {\n\tcase BPF_PROG_TYPE_CGROUP_SKB:\n\tcase BPF_PROG_TYPE_CGROUP_SOCK:\n\tcase BPF_PROG_TYPE_CGROUP_SOCK_ADDR:\n\tcase BPF_PROG_TYPE_SOCK_OPS:\n\tcase BPF_PROG_TYPE_CGROUP_DEVICE:\n\t\tbreak;\n\tdefault:\n\t\treturn 0;\n\t}\n\n\treg = cur_regs(env) + BPF_REG_0;\n\tif (reg->type != SCALAR_VALUE) {\n\t\tverbose(env, \"At program exit the register R0 is not a known value (%s)\\n\",\n\t\t\treg_type_str[reg->type]);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!tnum_in(range, reg->var_off)) {\n\t\tverbose(env, \"At program exit the register R0 \");\n\t\tif (!tnum_is_unknown(reg->var_off)) {\n\t\t\tchar tn_buf[48];\n\n\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\tverbose(env, \"has value %s\", tn_buf);\n\t\t} else {\n\t\t\tverbose(env, \"has unknown scalar value\");\n\t\t}\n\t\tverbose(env, \" should have been 0 or 1\\n\");\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\n/* non-recursive DFS pseudo code\n * 1  procedure DFS-iterative(G,v):\n * 2      label v as discovered\n * 3      let S be a stack\n * 4      S.push(v)\n * 5      while S is not empty\n * 6            t <- S.pop()\n * 7            if t is what we're looking for:\n * 8                return t\n * 9            for all edges e in G.adjacentEdges(t) do\n * 10               if edge e is already labelled\n * 11                   continue with the next edge\n * 12               w <- G.adjacentVertex(t,e)\n * 13               if vertex w is not discovered and not explored\n * 14                   label e as tree-edge\n * 15                   label w as discovered\n * 16                   S.push(w)\n * 17                   continue at 5\n * 18               else if vertex w is discovered\n * 19                   label e as back-edge\n * 20               else\n * 21                   // vertex w is explored\n * 22                   label e as forward- or cross-edge\n * 23           label t as explored\n * 24           S.pop()\n *\n * convention:\n * 0x10 - discovered\n * 0x11 - discovered and fall-through edge labelled\n * 0x12 - discovered and fall-through and branch edges labelled\n * 0x20 - explored\n */\n\nenum {\n\tDISCOVERED = 0x10,\n\tEXPLORED = 0x20,\n\tFALLTHROUGH = 1,\n\tBRANCH = 2,\n};\n\n#define STATE_LIST_MARK ((struct bpf_verifier_state_list *) -1L)\n\nstatic int *insn_stack;\t/* stack of insns to process */\nstatic int cur_stack;\t/* current stack index */\nstatic int *insn_state;\n\n/* t, w, e - match pseudo-code above:\n * t - index of current instruction\n * w - next instruction\n * e - edge\n */\nstatic int push_insn(int t, int w, int e, struct bpf_verifier_env *env)\n{\n\tif (e == FALLTHROUGH && insn_state[t] >= (DISCOVERED | FALLTHROUGH))\n\t\treturn 0;\n\n\tif (e == BRANCH && insn_state[t] >= (DISCOVERED | BRANCH))\n\t\treturn 0;\n\n\tif (w < 0 || w >= env->prog->len) {\n\t\tverbose_linfo(env, t, \"%d: \", t);\n\t\tverbose(env, \"jump out of range from insn %d to %d\\n\", t, w);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e == BRANCH)\n\t\t/* mark branch target for state pruning */\n\t\tenv->explored_states[w] = STATE_LIST_MARK;\n\n\tif (insn_state[w] == 0) {\n\t\t/* tree-edge */\n\t\tinsn_state[t] = DISCOVERED | e;\n\t\tinsn_state[w] = DISCOVERED;\n\t\tif (cur_stack >= env->prog->len)\n\t\t\treturn -E2BIG;\n\t\tinsn_stack[cur_stack++] = w;\n\t\treturn 1;\n\t} else if ((insn_state[w] & 0xF0) == DISCOVERED) {\n\t\tverbose_linfo(env, t, \"%d: \", t);\n\t\tverbose_linfo(env, w, \"%d: \", w);\n\t\tverbose(env, \"back-edge from insn %d to %d\\n\", t, w);\n\t\treturn -EINVAL;\n\t} else if (insn_state[w] == EXPLORED) {\n\t\t/* forward- or cross-edge */\n\t\tinsn_state[t] = DISCOVERED | e;\n\t} else {\n\t\tverbose(env, \"insn state internal bug\\n\");\n\t\treturn -EFAULT;\n\t}\n\treturn 0;\n}\n\n/* non-recursive depth-first-search to detect loops in BPF program\n * loop == back-edge in directed graph\n */\nstatic int check_cfg(struct bpf_verifier_env *env)\n{\n\tstruct bpf_insn *insns = env->prog->insnsi;\n\tint insn_cnt = env->prog->len;\n\tint ret = 0;\n\tint i, t;\n\n\tinsn_state = kcalloc(insn_cnt, sizeof(int), GFP_KERNEL);\n\tif (!insn_state)\n\t\treturn -ENOMEM;\n\n\tinsn_stack = kcalloc(insn_cnt, sizeof(int), GFP_KERNEL);\n\tif (!insn_stack) {\n\t\tkfree(insn_state);\n\t\treturn -ENOMEM;\n\t}\n\n\tinsn_state[0] = DISCOVERED; /* mark 1st insn as discovered */\n\tinsn_stack[0] = 0; /* 0 is the first instruction */\n\tcur_stack = 1;\n\npeek_stack:\n\tif (cur_stack == 0)\n\t\tgoto check_state;\n\tt = insn_stack[cur_stack - 1];\n\n\tif (BPF_CLASS(insns[t].code) == BPF_JMP) {\n\t\tu8 opcode = BPF_OP(insns[t].code);\n\n\t\tif (opcode == BPF_EXIT) {\n\t\t\tgoto mark_explored;\n\t\t} else if (opcode == BPF_CALL) {\n\t\t\tret = push_insn(t, t + 1, FALLTHROUGH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t\tif (t + 1 < insn_cnt)\n\t\t\t\tenv->explored_states[t + 1] = STATE_LIST_MARK;\n\t\t\tif (insns[t].src_reg == BPF_PSEUDO_CALL) {\n\t\t\t\tenv->explored_states[t] = STATE_LIST_MARK;\n\t\t\t\tret = push_insn(t, t + insns[t].imm + 1, BRANCH, env);\n\t\t\t\tif (ret == 1)\n\t\t\t\t\tgoto peek_stack;\n\t\t\t\telse if (ret < 0)\n\t\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t} else if (opcode == BPF_JA) {\n\t\t\tif (BPF_SRC(insns[t].code) != BPF_K) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t\t/* unconditional jump with single edge */\n\t\t\tret = push_insn(t, t + insns[t].off + 1,\n\t\t\t\t\tFALLTHROUGH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t\t/* tell verifier to check for equivalent states\n\t\t\t * after every call and jump\n\t\t\t */\n\t\t\tif (t + 1 < insn_cnt)\n\t\t\t\tenv->explored_states[t + 1] = STATE_LIST_MARK;\n\t\t} else {\n\t\t\t/* conditional jump with two edges */\n\t\t\tenv->explored_states[t] = STATE_LIST_MARK;\n\t\t\tret = push_insn(t, t + 1, FALLTHROUGH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\n\t\t\tret = push_insn(t, t + insns[t].off + 1, BRANCH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t}\n\t} else {\n\t\t/* all other non-branch instructions with single\n\t\t * fall-through edge\n\t\t */\n\t\tret = push_insn(t, t + 1, FALLTHROUGH, env);\n\t\tif (ret == 1)\n\t\t\tgoto peek_stack;\n\t\telse if (ret < 0)\n\t\t\tgoto err_free;\n\t}\n\nmark_explored:\n\tinsn_state[t] = EXPLORED;\n\tif (cur_stack-- <= 0) {\n\t\tverbose(env, \"pop stack internal bug\\n\");\n\t\tret = -EFAULT;\n\t\tgoto err_free;\n\t}\n\tgoto peek_stack;\n\ncheck_state:\n\tfor (i = 0; i < insn_cnt; i++) {\n\t\tif (insn_state[i] != EXPLORED) {\n\t\t\tverbose(env, \"unreachable insn %d\\n\", i);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\t}\n\tret = 0; /* cfg looks good */\n\nerr_free:\n\tkfree(insn_state);\n\tkfree(insn_stack);\n\treturn ret;\n}\n\n/* The minimum supported BTF func info size */\n#define MIN_BPF_FUNCINFO_SIZE\t8\n#define MAX_FUNCINFO_REC_SIZE\t252\n\nstatic int check_btf_func(struct bpf_verifier_env *env,\n\t\t\t  const union bpf_attr *attr,\n\t\t\t  union bpf_attr __user *uattr)\n{\n\tu32 i, nfuncs, urec_size, min_size, prev_offset;\n\tu32 krec_size = sizeof(struct bpf_func_info);\n\tstruct bpf_func_info *krecord;\n\tconst struct btf_type *type;\n\tstruct bpf_prog *prog;\n\tconst struct btf *btf;\n\tvoid __user *urecord;\n\tint ret = 0;\n\n\tnfuncs = attr->func_info_cnt;\n\tif (!nfuncs)\n\t\treturn 0;\n\n\tif (nfuncs != env->subprog_cnt) {\n\t\tverbose(env, \"number of funcs in func_info doesn't match number of subprogs\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\turec_size = attr->func_info_rec_size;\n\tif (urec_size < MIN_BPF_FUNCINFO_SIZE ||\n\t    urec_size > MAX_FUNCINFO_REC_SIZE ||\n\t    urec_size % sizeof(u32)) {\n\t\tverbose(env, \"invalid func info rec size %u\\n\", urec_size);\n\t\treturn -EINVAL;\n\t}\n\n\tprog = env->prog;\n\tbtf = prog->aux->btf;\n\n\turecord = u64_to_user_ptr(attr->func_info);\n\tmin_size = min_t(u32, krec_size, urec_size);\n\n\tkrecord = kvcalloc(nfuncs, krec_size, GFP_KERNEL | __GFP_NOWARN);\n\tif (!krecord)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < nfuncs; i++) {\n\t\tret = bpf_check_uarg_tail_zero(urecord, krec_size, urec_size);\n\t\tif (ret) {\n\t\t\tif (ret == -E2BIG) {\n\t\t\t\tverbose(env, \"nonzero tailing record in func info\");\n\t\t\t\t/* set the size kernel expects so loader can zero\n\t\t\t\t * out the rest of the record.\n\t\t\t\t */\n\t\t\t\tif (put_user(min_size, &uattr->func_info_rec_size))\n\t\t\t\t\tret = -EFAULT;\n\t\t\t}\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (copy_from_user(&krecord[i], urecord, min_size)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/* check insn_off */\n\t\tif (i == 0) {\n\t\t\tif (krecord[i].insn_off) {\n\t\t\t\tverbose(env,\n\t\t\t\t\t\"nonzero insn_off %u for the first func info record\",\n\t\t\t\t\tkrecord[i].insn_off);\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t} else if (krecord[i].insn_off <= prev_offset) {\n\t\t\tverbose(env,\n\t\t\t\t\"same or smaller insn offset (%u) than previous func info record (%u)\",\n\t\t\t\tkrecord[i].insn_off, prev_offset);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (env->subprog_info[i].start != krecord[i].insn_off) {\n\t\t\tverbose(env, \"func_info BTF section doesn't match subprog layout in BPF program\\n\");\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/* check type_id */\n\t\ttype = btf_type_by_id(btf, krecord[i].type_id);\n\t\tif (!type || BTF_INFO_KIND(type->info) != BTF_KIND_FUNC) {\n\t\t\tverbose(env, \"invalid type id %d in func info\",\n\t\t\t\tkrecord[i].type_id);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tprev_offset = krecord[i].insn_off;\n\t\turecord += urec_size;\n\t}\n\n\tprog->aux->func_info = krecord;\n\tprog->aux->func_info_cnt = nfuncs;\n\treturn 0;\n\nerr_free:\n\tkvfree(krecord);\n\treturn ret;\n}\n\nstatic void adjust_btf_func(struct bpf_verifier_env *env)\n{\n\tint i;\n\n\tif (!env->prog->aux->func_info)\n\t\treturn;\n\n\tfor (i = 0; i < env->subprog_cnt; i++)\n\t\tenv->prog->aux->func_info[i].insn_off = env->subprog_info[i].start;\n}\n\n#define MIN_BPF_LINEINFO_SIZE\t(offsetof(struct bpf_line_info, line_col) + \\\n\t\tsizeof(((struct bpf_line_info *)(0))->line_col))\n#define MAX_LINEINFO_REC_SIZE\tMAX_FUNCINFO_REC_SIZE\n\nstatic int check_btf_line(struct bpf_verifier_env *env,\n\t\t\t  const union bpf_attr *attr,\n\t\t\t  union bpf_attr __user *uattr)\n{\n\tu32 i, s, nr_linfo, ncopy, expected_size, rec_size, prev_offset = 0;\n\tstruct bpf_subprog_info *sub;\n\tstruct bpf_line_info *linfo;\n\tstruct bpf_prog *prog;\n\tconst struct btf *btf;\n\tvoid __user *ulinfo;\n\tint err;\n\n\tnr_linfo = attr->line_info_cnt;\n\tif (!nr_linfo)\n\t\treturn 0;\n\n\trec_size = attr->line_info_rec_size;\n\tif (rec_size < MIN_BPF_LINEINFO_SIZE ||\n\t    rec_size > MAX_LINEINFO_REC_SIZE ||\n\t    rec_size & (sizeof(u32) - 1))\n\t\treturn -EINVAL;\n\n\t/* Need to zero it in case the userspace may\n\t * pass in a smaller bpf_line_info object.\n\t */\n\tlinfo = kvcalloc(nr_linfo, sizeof(struct bpf_line_info),\n\t\t\t GFP_KERNEL | __GFP_NOWARN);\n\tif (!linfo)\n\t\treturn -ENOMEM;\n\n\tprog = env->prog;\n\tbtf = prog->aux->btf;\n\n\ts = 0;\n\tsub = env->subprog_info;\n\tulinfo = u64_to_user_ptr(attr->line_info);\n\texpected_size = sizeof(struct bpf_line_info);\n\tncopy = min_t(u32, expected_size, rec_size);\n\tfor (i = 0; i < nr_linfo; i++) {\n\t\terr = bpf_check_uarg_tail_zero(ulinfo, expected_size, rec_size);\n\t\tif (err) {\n\t\t\tif (err == -E2BIG) {\n\t\t\t\tverbose(env, \"nonzero tailing record in line_info\");\n\t\t\t\tif (put_user(expected_size,\n\t\t\t\t\t     &uattr->line_info_rec_size))\n\t\t\t\t\terr = -EFAULT;\n\t\t\t}\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (copy_from_user(&linfo[i], ulinfo, ncopy)) {\n\t\t\terr = -EFAULT;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/*\n\t\t * Check insn_off to ensure\n\t\t * 1) strictly increasing AND\n\t\t * 2) bounded by prog->len\n\t\t *\n\t\t * The linfo[0].insn_off == 0 check logically falls into\n\t\t * the later \"missing bpf_line_info for func...\" case\n\t\t * because the first linfo[0].insn_off must be the\n\t\t * first sub also and the first sub must have\n\t\t * subprog_info[0].start == 0.\n\t\t */\n\t\tif ((i && linfo[i].insn_off <= prev_offset) ||\n\t\t    linfo[i].insn_off >= prog->len) {\n\t\t\tverbose(env, \"Invalid line_info[%u].insn_off:%u (prev_offset:%u prog->len:%u)\\n\",\n\t\t\t\ti, linfo[i].insn_off, prev_offset,\n\t\t\t\tprog->len);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (!prog->insnsi[linfo[i].insn_off].code) {\n\t\t\tverbose(env,\n\t\t\t\t\"Invalid insn code at line_info[%u].insn_off\\n\",\n\t\t\t\ti);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (!btf_name_by_offset(btf, linfo[i].line_off) ||\n\t\t    !btf_name_by_offset(btf, linfo[i].file_name_off)) {\n\t\t\tverbose(env, \"Invalid line_info[%u].line_off or .file_name_off\\n\", i);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (s != env->subprog_cnt) {\n\t\t\tif (linfo[i].insn_off == sub[s].start) {\n\t\t\t\tsub[s].linfo_idx = i;\n\t\t\t\ts++;\n\t\t\t} else if (sub[s].start < linfo[i].insn_off) {\n\t\t\t\tverbose(env, \"missing bpf_line_info for func#%u\\n\", s);\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t}\n\n\t\tprev_offset = linfo[i].insn_off;\n\t\tulinfo += rec_size;\n\t}\n\n\tif (s != env->subprog_cnt) {\n\t\tverbose(env, \"missing bpf_line_info for %u funcs starting from func#%u\\n\",\n\t\t\tenv->subprog_cnt - s, s);\n\t\terr = -EINVAL;\n\t\tgoto err_free;\n\t}\n\n\tprog->aux->linfo = linfo;\n\tprog->aux->nr_linfo = nr_linfo;\n\n\treturn 0;\n\nerr_free:\n\tkvfree(linfo);\n\treturn err;\n}\n\nstatic int check_btf_info(struct bpf_verifier_env *env,\n\t\t\t  const union bpf_attr *attr,\n\t\t\t  union bpf_attr __user *uattr)\n{\n\tstruct btf *btf;\n\tint err;\n\n\tif (!attr->func_info_cnt && !attr->line_info_cnt)\n\t\treturn 0;\n\n\tbtf = btf_get_by_fd(attr->prog_btf_fd);\n\tif (IS_ERR(btf))\n\t\treturn PTR_ERR(btf);\n\tenv->prog->aux->btf = btf;\n\n\terr = check_btf_func(env, attr, uattr);\n\tif (err)\n\t\treturn err;\n\n\terr = check_btf_line(env, attr, uattr);\n\tif (err)\n\t\treturn err;\n\n\treturn 0;\n}\n\n/* check %cur's range satisfies %old's */\nstatic bool range_within(struct bpf_reg_state *old,\n\t\t\t struct bpf_reg_state *cur)\n{\n\treturn old->umin_value <= cur->umin_value &&\n\t       old->umax_value >= cur->umax_value &&\n\t       old->smin_value <= cur->smin_value &&\n\t       old->smax_value >= cur->smax_value;\n}\n\n/* Maximum number of register states that can exist at once */\n#define ID_MAP_SIZE\t(MAX_BPF_REG + MAX_BPF_STACK / BPF_REG_SIZE)\nstruct idpair {\n\tu32 old;\n\tu32 cur;\n};\n\n/* If in the old state two registers had the same id, then they need to have\n * the same id in the new state as well.  But that id could be different from\n * the old state, so we need to track the mapping from old to new ids.\n * Once we have seen that, say, a reg with old id 5 had new id 9, any subsequent\n * regs with old id 5 must also have new id 9 for the new state to be safe.  But\n * regs with a different old id could still have new id 9, we don't care about\n * that.\n * So we look through our idmap to see if this old id has been seen before.  If\n * so, we require the new id to match; otherwise, we add the id pair to the map.\n */\nstatic bool check_ids(u32 old_id, u32 cur_id, struct idpair *idmap)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < ID_MAP_SIZE; i++) {\n\t\tif (!idmap[i].old) {\n\t\t\t/* Reached an empty slot; haven't seen this id before */\n\t\t\tidmap[i].old = old_id;\n\t\t\tidmap[i].cur = cur_id;\n\t\t\treturn true;\n\t\t}\n\t\tif (idmap[i].old == old_id)\n\t\t\treturn idmap[i].cur == cur_id;\n\t}\n\t/* We ran out of idmap slots, which should be impossible */\n\tWARN_ON_ONCE(1);\n\treturn false;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 3480,
        "critical_vars": [
            "ret"
        ],
        "function": "adjust_scalar_min_max_vals",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_d3bd7413e0ca40b60cf60d4003246d067cafdeda_verifier.c.diff",
        "label": "True",
        "function_code": "static int adjust_scalar_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t      struct bpf_insn *insn,\n\t\t\t\t      struct bpf_reg_state *dst_reg,\n\t\t\t\t      struct bpf_reg_state src_reg)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 opcode = BPF_OP(insn->code);\n\tbool src_known, dst_known;\n\ts64 smin_val, smax_val;\n\tu64 umin_val, umax_val;\n\tu64 insn_bitness = (BPF_CLASS(insn->code) == BPF_ALU64) ? 64 : 32;\n\tu32 dst = insn->dst_reg;\n\tint ret;\n\n\tif (insn_bitness == 32) {\n\t\t/* Relevant for 32-bit RSH: Information can propagate towards\n\t\t * LSB, so it isn't sufficient to only truncate the output to\n\t\t * 32 bits.\n\t\t */\n\t\tcoerce_reg_to_size(dst_reg, 4);\n\t\tcoerce_reg_to_size(&src_reg, 4);\n\t}\n\n\tsmin_val = src_reg.smin_value;\n\tsmax_val = src_reg.smax_value;\n\tumin_val = src_reg.umin_value;\n\tumax_val = src_reg.umax_value;\n\tsrc_known = tnum_is_const(src_reg.var_off);\n\tdst_known = tnum_is_const(dst_reg->var_off);\n\n\tif ((src_known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (!src_known &&\n\t    opcode != BPF_ADD && opcode != BPF_SUB && opcode != BPF_AND) {\n\t\t__mark_reg_unknown(dst_reg);\n\t\treturn 0;\n\t}\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\tret = sanitize_val_alu(env, insn);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to add from different pointers or scalars\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\tif (signed_add_overflows(dst_reg->smin_value, smin_val) ||\n\t\t    signed_add_overflows(dst_reg->smax_value, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value += smin_val;\n\t\t\tdst_reg->smax_value += smax_val;\n\t\t}\n\t\tif (dst_reg->umin_value + umin_val < umin_val ||\n\t\t    dst_reg->umax_value + umax_val < umax_val) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value += umin_val;\n\t\t\tdst_reg->umax_value += umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(dst_reg->var_off, src_reg.var_off);\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tret = sanitize_val_alu(env, insn);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to sub from different pointers or scalars\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\tif (signed_sub_overflows(dst_reg->smin_value, smax_val) ||\n\t\t    signed_sub_overflows(dst_reg->smax_value, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value -= smax_val;\n\t\t\tdst_reg->smax_value -= smin_val;\n\t\t}\n\t\tif (dst_reg->umin_value < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value -= umax_val;\n\t\t\tdst_reg->umax_value -= umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(dst_reg->var_off, src_reg.var_off);\n\t\tbreak;\n\tcase BPF_MUL:\n\t\tdst_reg->var_off = tnum_mul(dst_reg->var_off, src_reg.var_off);\n\t\tif (smin_val < 0 || dst_reg->smin_value < 0) {\n\t\t\t/* Ain't nobody got time to multiply that sign */\n\t\t\t__mark_reg_unbounded(dst_reg);\n\t\t\t__update_reg_bounds(dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\t/* Both values are positive, so we can work with unsigned and\n\t\t * copy the result to signed (unless it exceeds S64_MAX).\n\t\t */\n\t\tif (umax_val > U32_MAX || dst_reg->umax_value > U32_MAX) {\n\t\t\t/* Potential overflow, we know nothing */\n\t\t\t__mark_reg_unbounded(dst_reg);\n\t\t\t/* (except what we can learn from the var_off) */\n\t\t\t__update_reg_bounds(dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\tdst_reg->umin_value *= umin_val;\n\t\tdst_reg->umax_value *= umax_val;\n\t\tif (dst_reg->umax_value > S64_MAX) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = dst_reg->umin_value;\n\t\t\tdst_reg->smax_value = dst_reg->umax_value;\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\t\tif (src_known && dst_known) {\n\t\t\t__mark_reg_known(dst_reg, dst_reg->var_off.value &\n\t\t\t\t\t\t  src_reg.var_off.value);\n\t\t\tbreak;\n\t\t}\n\t\t/* We get our minimum from the var_off, since that's inherently\n\t\t * bitwise.  Our maximum is the minimum of the operands' maxima.\n\t\t */\n\t\tdst_reg->var_off = tnum_and(dst_reg->var_off, src_reg.var_off);\n\t\tdst_reg->umin_value = dst_reg->var_off.value;\n\t\tdst_reg->umax_value = min(dst_reg->umax_value, umax_val);\n\t\tif (dst_reg->smin_value < 0 || smin_val < 0) {\n\t\t\t/* Lose signed bounds when ANDing negative numbers,\n\t\t\t * ain't nobody got time for that.\n\t\t\t */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\t/* ANDing two positives gives a positive, so safe to\n\t\t\t * cast result into s64.\n\t\t\t */\n\t\t\tdst_reg->smin_value = dst_reg->umin_value;\n\t\t\tdst_reg->smax_value = dst_reg->umax_value;\n\t\t}\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tcase BPF_OR:\n\t\tif (src_known && dst_known) {\n\t\t\t__mark_reg_known(dst_reg, dst_reg->var_off.value |\n\t\t\t\t\t\t  src_reg.var_off.value);\n\t\t\tbreak;\n\t\t}\n\t\t/* We get our maximum from the var_off, and our minimum is the\n\t\t * maximum of the operands' minima\n\t\t */\n\t\tdst_reg->var_off = tnum_or(dst_reg->var_off, src_reg.var_off);\n\t\tdst_reg->umin_value = max(dst_reg->umin_value, umin_val);\n\t\tdst_reg->umax_value = dst_reg->var_off.value |\n\t\t\t\t      dst_reg->var_off.mask;\n\t\tif (dst_reg->smin_value < 0 || smin_val < 0) {\n\t\t\t/* Lose signed bounds when ORing negative numbers,\n\t\t\t * ain't nobody got time for that.\n\t\t\t */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\t/* ORing two positives gives a positive, so safe to\n\t\t\t * cast result into s64.\n\t\t\t */\n\t\t\tdst_reg->smin_value = dst_reg->umin_value;\n\t\t\tdst_reg->smax_value = dst_reg->umax_value;\n\t\t}\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tcase BPF_LSH:\n\t\tif (umax_val >= insn_bitness) {\n\t\t\t/* Shifts greater than 31 or 63 are undefined.\n\t\t\t * This includes shifts by a negative number.\n\t\t\t */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\t/* We lose all sign bit information (except what we can pick\n\t\t * up from var_off)\n\t\t */\n\t\tdst_reg->smin_value = S64_MIN;\n\t\tdst_reg->smax_value = S64_MAX;\n\t\t/* If we might shift our top bit out, then we know nothing */\n\t\tif (dst_reg->umax_value > 1ULL << (63 - umax_val)) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value <<= umin_val;\n\t\t\tdst_reg->umax_value <<= umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_lshift(dst_reg->var_off, umin_val);\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tcase BPF_RSH:\n\t\tif (umax_val >= insn_bitness) {\n\t\t\t/* Shifts greater than 31 or 63 are undefined.\n\t\t\t * This includes shifts by a negative number.\n\t\t\t */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\t/* BPF_RSH is an unsigned shift.  If the value in dst_reg might\n\t\t * be negative, then either:\n\t\t * 1) src_reg might be zero, so the sign bit of the result is\n\t\t *    unknown, so we lose our signed bounds\n\t\t * 2) it's known negative, thus the unsigned bounds capture the\n\t\t *    signed bounds\n\t\t * 3) the signed bounds cross zero, so they tell us nothing\n\t\t *    about the result\n\t\t * If the value in dst_reg is known nonnegative, then again the\n\t\t * unsigned bounts capture the signed bounds.\n\t\t * Thus, in all cases it suffices to blow away our signed bounds\n\t\t * and rely on inferring new ones from the unsigned bounds and\n\t\t * var_off of the result.\n\t\t */\n\t\tdst_reg->smin_value = S64_MIN;\n\t\tdst_reg->smax_value = S64_MAX;\n\t\tdst_reg->var_off = tnum_rshift(dst_reg->var_off, umin_val);\n\t\tdst_reg->umin_value >>= umax_val;\n\t\tdst_reg->umax_value >>= umin_val;\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tcase BPF_ARSH:\n\t\tif (umax_val >= insn_bitness) {\n\t\t\t/* Shifts greater than 31 or 63 are undefined.\n\t\t\t * This includes shifts by a negative number.\n\t\t\t */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Upon reaching here, src_known is true and\n\t\t * umax_val is equal to umin_val.\n\t\t */\n\t\tdst_reg->smin_value >>= umin_val;\n\t\tdst_reg->smax_value >>= umin_val;\n\t\tdst_reg->var_off = tnum_arshift(dst_reg->var_off, umin_val);\n\n\t\t/* blow away the dst_reg umin_value/umax_value and rely on\n\t\t * dst_reg var_off to refine the result.\n\t\t */\n\t\tdst_reg->umin_value = 0;\n\t\tdst_reg->umax_value = U64_MAX;\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tdefault:\n\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\tbreak;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops are (32,32)->32 */\n\t\tcoerce_reg_to_size(dst_reg, 4);\n\t}\n\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\treturn 0;\n}\n\n/* Handles ALU ops other than BPF_END, BPF_NEG and BPF_MOV: computes new min/max\n * and var_off.\n */\nstatic int adjust_reg_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg, *src_reg;\n\tstruct bpf_reg_state *ptr_reg = NULL, off_reg = {0};\n\tu8 opcode = BPF_OP(insn->code);\n\n\tdst_reg = &regs[insn->dst_reg];\n\tsrc_reg = NULL;\n\tif (dst_reg->type != SCALAR_VALUE)\n\t\tptr_reg = dst_reg;\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tsrc_reg = &regs[insn->src_reg];\n\t\tif (src_reg->type != SCALAR_VALUE) {\n\t\t\tif (dst_reg->type != SCALAR_VALUE) {\n\t\t\t\t/* Combining two pointers by any ALU op yields\n\t\t\t\t * an arbitrary scalar. Disallow all math except\n\t\t\t\t * pointer subtraction\n\t\t\t\t */\n\t\t\t\tif (opcode == BPF_SUB && env->allow_ptr_leaks) {\n\t\t\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t\tverbose(env, \"R%d pointer %s pointer prohibited\\n\",\n\t\t\t\t\tinsn->dst_reg,\n\t\t\t\t\tbpf_alu_string[opcode >> 4]);\n\t\t\t\treturn -EACCES;\n\t\t\t} else {\n\t\t\t\t/* scalar += pointer\n\t\t\t\t * This is legal, but we have to reverse our\n\t\t\t\t * src/dest handling in computing the range\n\t\t\t\t */\n\t\t\t\treturn adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t\t       src_reg, dst_reg);\n\t\t\t}\n\t\t} else if (ptr_reg) {\n\t\t\t/* pointer += scalar */\n\t\t\treturn adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t       dst_reg, src_reg);\n\t\t}\n\t} else {\n\t\t/* Pretend the src is a reg with a known value, since we only\n\t\t * need to be able to read from this state.\n\t\t */\n\t\toff_reg.type = SCALAR_VALUE;\n\t\t__mark_reg_known(&off_reg, insn->imm);\n\t\tsrc_reg = &off_reg;\n\t\tif (ptr_reg) /* pointer += K */\n\t\t\treturn adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t       ptr_reg, src_reg);\n\t}\n\n\t/* Got here implies adding two SCALAR_VALUEs */\n\tif (WARN_ON_ONCE(ptr_reg)) {\n\t\tprint_verifier_state(env, state);\n\t\tverbose(env, \"verifier internal error: unexpected ptr_reg\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (WARN_ON(!src_reg)) {\n\t\tprint_verifier_state(env, state);\n\t\tverbose(env, \"verifier internal error: no src_reg\\n\");\n\t\treturn -EINVAL;\n\t}\n\treturn adjust_scalar_min_max_vals(env, insn, dst_reg, *src_reg);\n}\n\n/* check validity of 32-bit and 64-bit arithmetic operations */\nstatic int check_alu_op(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 opcode = BPF_OP(insn->code);\n\tint err;\n\n\tif (opcode == BPF_END || opcode == BPF_NEG) {\n\t\tif (opcode == BPF_NEG) {\n\t\t\tif (BPF_SRC(insn->code) != 0 ||\n\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t    insn->off != 0 || insn->imm != 0) {\n\t\t\t\tverbose(env, \"BPF_NEG uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else {\n\t\t\tif (insn->src_reg != BPF_REG_0 || insn->off != 0 ||\n\t\t\t    (insn->imm != 16 && insn->imm != 32 && insn->imm != 64) ||\n\t\t\t    BPF_CLASS(insn->code) == BPF_ALU64) {\n\t\t\t\tverbose(env, \"BPF_END uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check src operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (is_pointer_value(env, insn->dst_reg)) {\n\t\t\tverbose(env, \"R%d pointer arithmetic prohibited\\n\",\n\t\t\t\tinsn->dst_reg);\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\t/* check dest operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t} else if (opcode == BPF_MOV) {\n\n\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\tif (insn->imm != 0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_MOV uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\t/* check src operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t} else {\n\t\t\tif (insn->src_reg != BPF_REG_0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_MOV uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check dest operand, mark as required later */\n\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP_NO_MARK);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\tstruct bpf_reg_state *src_reg = regs + insn->src_reg;\n\t\t\tstruct bpf_reg_state *dst_reg = regs + insn->dst_reg;\n\n\t\t\tif (BPF_CLASS(insn->code) == BPF_ALU64) {\n\t\t\t\t/* case: R1 = R2\n\t\t\t\t * copy register state to dest reg\n\t\t\t\t */\n\t\t\t\t*dst_reg = *src_reg;\n\t\t\t\tdst_reg->live |= REG_LIVE_WRITTEN;\n\t\t\t} else {\n\t\t\t\t/* R1 = (u32) R2 */\n\t\t\t\tif (is_pointer_value(env, insn->src_reg)) {\n\t\t\t\t\tverbose(env,\n\t\t\t\t\t\t\"R%d partial copy of pointer\\n\",\n\t\t\t\t\t\tinsn->src_reg);\n\t\t\t\t\treturn -EACCES;\n\t\t\t\t} else if (src_reg->type == SCALAR_VALUE) {\n\t\t\t\t\t*dst_reg = *src_reg;\n\t\t\t\t\tdst_reg->live |= REG_LIVE_WRITTEN;\n\t\t\t\t} else {\n\t\t\t\t\tmark_reg_unknown(env, regs,\n\t\t\t\t\t\t\t insn->dst_reg);\n\t\t\t\t}\n\t\t\t\tcoerce_reg_to_size(dst_reg, 4);\n\t\t\t}\n\t\t} else {\n\t\t\t/* case: R = imm\n\t\t\t * remember the value we stored into this reg\n\t\t\t */\n\t\t\t/* clear any state __mark_reg_known doesn't set */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tregs[insn->dst_reg].type = SCALAR_VALUE;\n\t\t\tif (BPF_CLASS(insn->code) == BPF_ALU64) {\n\t\t\t\t__mark_reg_known(regs + insn->dst_reg,\n\t\t\t\t\t\t insn->imm);\n\t\t\t} else {\n\t\t\t\t__mark_reg_known(regs + insn->dst_reg,\n\t\t\t\t\t\t (u32)insn->imm);\n\t\t\t}\n\t\t}\n\n\t} else if (opcode > BPF_END) {\n\t\tverbose(env, \"invalid BPF_ALU opcode %x\\n\", opcode);\n\t\treturn -EINVAL;\n\n\t} else {\t/* all other ALU ops: and, sub, xor, add, ... */\n\n\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\tif (insn->imm != 0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_ALU uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\t/* check src1 operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t} else {\n\t\t\tif (insn->src_reg != BPF_REG_0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_ALU uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check src2 operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif ((opcode == BPF_MOD || opcode == BPF_DIV) &&\n\t\t    BPF_SRC(insn->code) == BPF_K && insn->imm == 0) {\n\t\t\tverbose(env, \"div by zero\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif ((opcode == BPF_LSH || opcode == BPF_RSH ||\n\t\t     opcode == BPF_ARSH) && BPF_SRC(insn->code) == BPF_K) {\n\t\t\tint size = BPF_CLASS(insn->code) == BPF_ALU64 ? 64 : 32;\n\n\t\t\tif (insn->imm < 0 || insn->imm >= size) {\n\t\t\t\tverbose(env, \"invalid shift %d\\n\", insn->imm);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check dest operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP_NO_MARK);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\treturn adjust_reg_min_max_vals(env, insn);\n\t}\n\n\treturn 0;\n}\n\nstatic void find_good_pkt_pointers(struct bpf_verifier_state *vstate,\n\t\t\t\t   struct bpf_reg_state *dst_reg,\n\t\t\t\t   enum bpf_reg_type type,\n\t\t\t\t   bool range_right_open)\n{\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *reg;\n\tu16 new_range;\n\tint i, j;\n\n\tif (dst_reg->off < 0 ||\n\t    (dst_reg->off == 0 && range_right_open))\n\t\t/* This doesn't give us any range */\n\t\treturn;\n\n\tif (dst_reg->umax_value > MAX_PACKET_OFF ||\n\t    dst_reg->umax_value + dst_reg->off > MAX_PACKET_OFF)\n\t\t/* Risk of overflow.  For instance, ptr + (1<<63) may be less\n\t\t * than pkt_end, but that's because it's also less than pkt.\n\t\t */\n\t\treturn;\n\n\tnew_range = dst_reg->off;\n\tif (range_right_open)\n\t\tnew_range--;\n\n\t/* Examples for register markings:\n\t *\n\t * pkt_data in dst register:\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (r2 > pkt_end) goto <handle exception>\n\t *   <access okay>\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (r2 < pkt_end) goto <access okay>\n\t *   <handle exception>\n\t *\n\t *   Where:\n\t *     r2 == dst_reg, pkt_end == src_reg\n\t *     r2=pkt(id=n,off=8,r=0)\n\t *     r3=pkt(id=n,off=0,r=0)\n\t *\n\t * pkt_data in src register:\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (pkt_end >= r2) goto <access okay>\n\t *   <handle exception>\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (pkt_end <= r2) goto <handle exception>\n\t *   <access okay>\n\t *\n\t *   Where:\n\t *     pkt_end == dst_reg, r2 == src_reg\n\t *     r2=pkt(id=n,off=8,r=0)\n\t *     r3=pkt(id=n,off=0,r=0)\n\t *\n\t * Find register r3 and mark its range as r3=pkt(id=n,off=0,r=8)\n\t * or r3=pkt(id=n,off=0,r=8-1), so that range of bytes [r3, r3 + 8)\n\t * and [r3, r3 + 8-1) respectively is safe to access depending on\n\t * the check.\n\t */\n\n\t/* If our ids match, then we must have the same max_value.  And we\n\t * don't care about the other reg's fixed offset, since if it's too big\n\t * the range won't allow anything.\n\t * dst_reg->off is known < MAX_PACKET_OFF, therefore it fits in a u16.\n\t */\n\tfor (i = 0; i < MAX_BPF_REG; i++)\n\t\tif (regs[i].type == type && regs[i].id == dst_reg->id)\n\t\t\t/* keep the maximum range already checked */\n\t\t\tregs[i].range = max(regs[i].range, new_range);\n\n\tfor (j = 0; j <= vstate->curframe; j++) {\n\t\tstate = vstate->frame[j];\n\t\tbpf_for_each_spilled_reg(i, state, reg) {\n\t\t\tif (!reg)\n\t\t\t\tcontinue;\n\t\t\tif (reg->type == type && reg->id == dst_reg->id)\n\t\t\t\treg->range = max(reg->range, new_range);\n\t\t}\n\t}\n}\n\n/* compute branch direction of the expression \"if (reg opcode val) goto target;\"\n * and return:\n *  1 - branch will be taken and \"goto target\" will be executed\n *  0 - branch will not be taken and fall-through to next insn\n * -1 - unknown. Example: \"if (reg < 5)\" is unknown when register value range [0,10]\n */\nstatic int is_branch_taken(struct bpf_reg_state *reg, u64 val, u8 opcode)\n{\n\tif (__is_pointer_value(false, reg))\n\t\treturn -1;\n\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\tif (tnum_is_const(reg->var_off))\n\t\t\treturn !!tnum_equals_const(reg->var_off, val);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\tif (tnum_is_const(reg->var_off))\n\t\t\treturn !tnum_equals_const(reg->var_off, val);\n\t\tbreak;\n\tcase BPF_JSET:\n\t\tif ((~reg->var_off.mask & reg->var_off.value) & val)\n\t\t\treturn 1;\n\t\tif (!((reg->var_off.mask | reg->var_off.value) & val))\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JGT:\n\t\tif (reg->umin_value > val)\n\t\t\treturn 1;\n\t\telse if (reg->umax_value <= val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSGT:\n\t\tif (reg->smin_value > (s64)val)\n\t\t\treturn 1;\n\t\telse if (reg->smax_value < (s64)val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JLT:\n\t\tif (reg->umax_value < val)\n\t\t\treturn 1;\n\t\telse if (reg->umin_value >= val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSLT:\n\t\tif (reg->smax_value < (s64)val)\n\t\t\treturn 1;\n\t\telse if (reg->smin_value >= (s64)val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JGE:\n\t\tif (reg->umin_value >= val)\n\t\t\treturn 1;\n\t\telse if (reg->umax_value < val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSGE:\n\t\tif (reg->smin_value >= (s64)val)\n\t\t\treturn 1;\n\t\telse if (reg->smax_value < (s64)val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JLE:\n\t\tif (reg->umax_value <= val)\n\t\t\treturn 1;\n\t\telse if (reg->umin_value > val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSLE:\n\t\tif (reg->smax_value <= (s64)val)\n\t\t\treturn 1;\n\t\telse if (reg->smin_value > (s64)val)\n\t\t\treturn 0;\n\t\tbreak;\n\t}\n\n\treturn -1;\n}\n\n/* Adjusts the register min/max values in the case that the dst_reg is the\n * variable register that we are working on, and src_reg is a constant or we're\n * simply doing a BPF_K check.\n * In JEQ/JNE cases we also adjust the var_off values.\n */\nstatic void reg_set_min_max(struct bpf_reg_state *true_reg,\n\t\t\t    struct bpf_reg_state *false_reg, u64 val,\n\t\t\t    u8 opcode)\n{\n\t/* If the dst_reg is a pointer, we can't learn anything about its\n\t * variable offset from the compare (unless src_reg were a pointer into\n\t * the same object, but we don't bother with that.\n\t * Since false_reg and true_reg have the same type by construction, we\n\t * only need to check one of them for pointerness.\n\t */\n\tif (__is_pointer_value(false, false_reg))\n\t\treturn;\n\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\t/* If this is false then we know nothing Jon Snow, but if it is\n\t\t * true then we know for sure.\n\t\t */\n\t\t__mark_reg_known(true_reg, val);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\t/* If this is true we know nothing Jon Snow, but if it is false\n\t\t * we know the value for sure;\n\t\t */\n\t\t__mark_reg_known(false_reg, val);\n\t\tbreak;\n\tcase BPF_JSET:\n\t\tfalse_reg->var_off = tnum_and(false_reg->var_off,\n\t\t\t\t\t      tnum_const(~val));\n\t\tif (is_power_of_2(val))\n\t\t\ttrue_reg->var_off = tnum_or(true_reg->var_off,\n\t\t\t\t\t\t    tnum_const(val));\n\t\tbreak;\n\tcase BPF_JGT:\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val);\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JSGT:\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val);\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JLT:\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val);\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val - 1);\n\t\tbreak;\n\tcase BPF_JSLT:\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val);\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val - 1);\n\t\tbreak;\n\tcase BPF_JGE:\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val - 1);\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val);\n\t\tbreak;\n\tcase BPF_JSGE:\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val - 1);\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val);\n\t\tbreak;\n\tcase BPF_JLE:\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val + 1);\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val);\n\t\tbreak;\n\tcase BPF_JSLE:\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val + 1);\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t__reg_deduce_bounds(false_reg);\n\t__reg_deduce_bounds(true_reg);\n\t/* We might have learned some bits from the bounds. */\n\t__reg_bound_offset(false_reg);\n\t__reg_bound_offset(true_reg);\n\t/* Intersecting with the old var_off might have improved our bounds\n\t * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),\n\t * then new var_off is (0; 0x7f...fc) which improves our umax.\n\t */\n\t__update_reg_bounds(false_reg);\n\t__update_reg_bounds(true_reg);\n}\n\n/* Same as above, but for the case that dst_reg holds a constant and src_reg is\n * the variable reg.\n */\nstatic void reg_set_min_max_inv(struct bpf_reg_state *true_reg,\n\t\t\t\tstruct bpf_reg_state *false_reg, u64 val,\n\t\t\t\tu8 opcode)\n{\n\tif (__is_pointer_value(false, false_reg))\n\t\treturn;\n\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\t/* If this is false then we know nothing Jon Snow, but if it is\n\t\t * true then we know for sure.\n\t\t */\n\t\t__mark_reg_known(true_reg, val);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\t/* If this is true we know nothing Jon Snow, but if it is false\n\t\t * we know the value for sure;\n\t\t */\n\t\t__mark_reg_known(false_reg, val);\n\t\tbreak;\n\tcase BPF_JSET:\n\t\tfalse_reg->var_off = tnum_and(false_reg->var_off,\n\t\t\t\t\t      tnum_const(~val));\n\t\tif (is_power_of_2(val))\n\t\t\ttrue_reg->var_off = tnum_or(true_reg->var_off,\n\t\t\t\t\t\t    tnum_const(val));\n\t\tbreak;\n\tcase BPF_JGT:\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val - 1);\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val);\n\t\tbreak;\n\tcase BPF_JSGT:\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val - 1);\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val);\n\t\tbreak;\n\tcase BPF_JLT:\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val + 1);\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val);\n\t\tbreak;\n\tcase BPF_JSLT:\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val + 1);\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val);\n\t\tbreak;\n\tcase BPF_JGE:\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val);\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JSGE:\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val);\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JLE:\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val);\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val - 1);\n\t\tbreak;\n\tcase BPF_JSLE:\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val);\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val - 1);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t__reg_deduce_bounds(false_reg);\n\t__reg_deduce_bounds(true_reg);\n\t/* We might have learned some bits from the bounds. */\n\t__reg_bound_offset(false_reg);\n\t__reg_bound_offset(true_reg);\n\t/* Intersecting with the old var_off might have improved our bounds\n\t * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),\n\t * then new var_off is (0; 0x7f...fc) which improves our umax.\n\t */\n\t__update_reg_bounds(false_reg);\n\t__update_reg_bounds(true_reg);\n}\n\n/* Regs are known to be equal, so intersect their min/max/var_off */\nstatic void __reg_combine_min_max(struct bpf_reg_state *src_reg,\n\t\t\t\t  struct bpf_reg_state *dst_reg)\n{\n\tsrc_reg->umin_value = dst_reg->umin_value = max(src_reg->umin_value,\n\t\t\t\t\t\t\tdst_reg->umin_value);\n\tsrc_reg->umax_value = dst_reg->umax_value = min(src_reg->umax_value,\n\t\t\t\t\t\t\tdst_reg->umax_value);\n\tsrc_reg->smin_value = dst_reg->smin_value = max(src_reg->smin_value,\n\t\t\t\t\t\t\tdst_reg->smin_value);\n\tsrc_reg->smax_value = dst_reg->smax_value = min(src_reg->smax_value,\n\t\t\t\t\t\t\tdst_reg->smax_value);\n\tsrc_reg->var_off = dst_reg->var_off = tnum_intersect(src_reg->var_off,\n\t\t\t\t\t\t\t     dst_reg->var_off);\n\t/* We might have learned new bounds from the var_off. */\n\t__update_reg_bounds(src_reg);\n\t__update_reg_bounds(dst_reg);\n\t/* We might have learned something about the sign bit. */\n\t__reg_deduce_bounds(src_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t/* We might have learned some bits from the bounds. */\n\t__reg_bound_offset(src_reg);\n\t__reg_bound_offset(dst_reg);\n\t/* Intersecting with the old var_off might have improved our bounds\n\t * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),\n\t * then new var_off is (0; 0x7f...fc) which improves our umax.\n\t */\n\t__update_reg_bounds(src_reg);\n\t__update_reg_bounds(dst_reg);\n}\n\nstatic void reg_combine_min_max(struct bpf_reg_state *true_src,\n\t\t\t\tstruct bpf_reg_state *true_dst,\n\t\t\t\tstruct bpf_reg_state *false_src,\n\t\t\t\tstruct bpf_reg_state *false_dst,\n\t\t\t\tu8 opcode)\n{\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\t__reg_combine_min_max(true_src, true_dst);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\t__reg_combine_min_max(false_src, false_dst);\n\t\tbreak;\n\t}\n}\n\nstatic void mark_ptr_or_null_reg(struct bpf_func_state *state,\n\t\t\t\t struct bpf_reg_state *reg, u32 id,\n\t\t\t\t bool is_null)\n{\n\tif (reg_type_may_be_null(reg->type) && reg->id == id) {\n\t\t/* Old offset (both fixed and variable parts) should\n\t\t * have been known-zero, because we don't allow pointer\n\t\t * arithmetic on pointers that might be NULL.\n\t\t */\n\t\tif (WARN_ON_ONCE(reg->smin_value || reg->smax_value ||\n\t\t\t\t !tnum_equals_const(reg->var_off, 0) ||\n\t\t\t\t reg->off)) {\n\t\t\t__mark_reg_known_zero(reg);\n\t\t\treg->off = 0;\n\t\t}\n\t\tif (is_null) {\n\t\t\treg->type = SCALAR_VALUE;\n\t\t} else if (reg->type == PTR_TO_MAP_VALUE_OR_NULL) {\n\t\t\tif (reg->map_ptr->inner_map_meta) {\n\t\t\t\treg->type = CONST_PTR_TO_MAP;\n\t\t\t\treg->map_ptr = reg->map_ptr->inner_map_meta;\n\t\t\t} else {\n\t\t\t\treg->type = PTR_TO_MAP_VALUE;\n\t\t\t}\n\t\t} else if (reg->type == PTR_TO_SOCKET_OR_NULL) {\n\t\t\treg->type = PTR_TO_SOCKET;\n\t\t}\n\t\tif (is_null || !reg_is_refcounted(reg)) {\n\t\t\t/* We don't need id from this point onwards anymore,\n\t\t\t * thus we should better reset it, so that state\n\t\t\t * pruning has chances to take effect.\n\t\t\t */\n\t\t\treg->id = 0;\n\t\t}\n\t}\n}\n\n/* The logic is similar to find_good_pkt_pointers(), both could eventually\n * be folded together at some point.\n */\nstatic void mark_ptr_or_null_regs(struct bpf_verifier_state *vstate, u32 regno,\n\t\t\t\t  bool is_null)\n{\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *reg, *regs = state->regs;\n\tu32 id = regs[regno].id;\n\tint i, j;\n\n\tif (reg_is_refcounted_or_null(&regs[regno]) && is_null)\n\t\t__release_reference_state(state, id);\n\n\tfor (i = 0; i < MAX_BPF_REG; i++)\n\t\tmark_ptr_or_null_reg(state, &regs[i], id, is_null);\n\n\tfor (j = 0; j <= vstate->curframe; j++) {\n\t\tstate = vstate->frame[j];\n\t\tbpf_for_each_spilled_reg(i, state, reg) {\n\t\t\tif (!reg)\n\t\t\t\tcontinue;\n\t\t\tmark_ptr_or_null_reg(state, reg, id, is_null);\n\t\t}\n\t}\n}\n\nstatic bool try_match_pkt_pointers(const struct bpf_insn *insn,\n\t\t\t\t   struct bpf_reg_state *dst_reg,\n\t\t\t\t   struct bpf_reg_state *src_reg,\n\t\t\t\t   struct bpf_verifier_state *this_branch,\n\t\t\t\t   struct bpf_verifier_state *other_branch)\n{\n\tif (BPF_SRC(insn->code) != BPF_X)\n\t\treturn false;\n\n\tswitch (BPF_OP(insn->code)) {\n\tcase BPF_JGT:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' > pkt_end, pkt_meta' > pkt_data */\n\t\t\tfind_good_pkt_pointers(this_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, false);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end > pkt_data', pkt_data > pkt_meta' */\n\t\t\tfind_good_pkt_pointers(other_branch, src_reg,\n\t\t\t\t\t       src_reg->type, true);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase BPF_JLT:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' < pkt_end, pkt_meta' < pkt_data */\n\t\t\tfind_good_pkt_pointers(other_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, true);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end < pkt_data', pkt_data > pkt_meta' */\n\t\t\tfind_good_pkt_pointers(this_branch, src_reg,\n\t\t\t\t\t       src_reg->type, false);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase BPF_JGE:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' >= pkt_end, pkt_meta' >= pkt_data */\n\t\t\tfind_good_pkt_pointers(this_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, true);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end >= pkt_data', pkt_data >= pkt_meta' */\n\t\t\tfind_good_pkt_pointers(other_branch, src_reg,\n\t\t\t\t\t       src_reg->type, false);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase BPF_JLE:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' <= pkt_end, pkt_meta' <= pkt_data */\n\t\t\tfind_good_pkt_pointers(other_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, false);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end <= pkt_data', pkt_data <= pkt_meta' */\n\t\t\tfind_good_pkt_pointers(this_branch, src_reg,\n\t\t\t\t\t       src_reg->type, true);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic int check_cond_jmp_op(struct bpf_verifier_env *env,\n\t\t\t     struct bpf_insn *insn, int *insn_idx)\n{\n\tstruct bpf_verifier_state *this_branch = env->cur_state;\n\tstruct bpf_verifier_state *other_branch;\n\tstruct bpf_reg_state *regs = this_branch->frame[this_branch->curframe]->regs;\n\tstruct bpf_reg_state *dst_reg, *other_branch_regs;\n\tu8 opcode = BPF_OP(insn->code);\n\tint err;\n\n\tif (opcode > BPF_JSLE) {\n\t\tverbose(env, \"invalid BPF_JMP opcode %x\\n\", opcode);\n\t\treturn -EINVAL;\n\t}\n\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tif (insn->imm != 0) {\n\t\t\tverbose(env, \"BPF_JMP uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/* check src1 operand */\n\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (is_pointer_value(env, insn->src_reg)) {\n\t\t\tverbose(env, \"R%d pointer comparison prohibited\\n\",\n\t\t\t\tinsn->src_reg);\n\t\t\treturn -EACCES;\n\t\t}\n\t} else {\n\t\tif (insn->src_reg != BPF_REG_0) {\n\t\t\tverbose(env, \"BPF_JMP uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t/* check src2 operand */\n\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tdst_reg = &regs[insn->dst_reg];\n\n\tif (BPF_SRC(insn->code) == BPF_K) {\n\t\tint pred = is_branch_taken(dst_reg, insn->imm, opcode);\n\n\t\tif (pred == 1) {\n\t\t\t /* only follow the goto, ignore fall-through */\n\t\t\t*insn_idx += insn->off;\n\t\t\treturn 0;\n\t\t} else if (pred == 0) {\n\t\t\t/* only follow fall-through branch, since\n\t\t\t * that's where the program will go\n\t\t\t */\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tother_branch = push_stack(env, *insn_idx + insn->off + 1, *insn_idx,\n\t\t\t\t  false);\n\tif (!other_branch)\n\t\treturn -EFAULT;\n\tother_branch_regs = other_branch->frame[other_branch->curframe]->regs;\n\n\t/* detect if we are comparing against a constant value so we can adjust\n\t * our min/max values for our dst register.\n\t * this is only legit if both are scalars (or pointers to the same\n\t * object, I suppose, but we don't support that right now), because\n\t * otherwise the different base pointers mean the offsets aren't\n\t * comparable.\n\t */\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tif (dst_reg->type == SCALAR_VALUE &&\n\t\t    regs[insn->src_reg].type == SCALAR_VALUE) {\n\t\t\tif (tnum_is_const(regs[insn->src_reg].var_off))\n\t\t\t\treg_set_min_max(&other_branch_regs[insn->dst_reg],\n\t\t\t\t\t\tdst_reg, regs[insn->src_reg].var_off.value,\n\t\t\t\t\t\topcode);\n\t\t\telse if (tnum_is_const(dst_reg->var_off))\n\t\t\t\treg_set_min_max_inv(&other_branch_regs[insn->src_reg],\n\t\t\t\t\t\t    &regs[insn->src_reg],\n\t\t\t\t\t\t    dst_reg->var_off.value, opcode);\n\t\t\telse if (opcode == BPF_JEQ || opcode == BPF_JNE)\n\t\t\t\t/* Comparing for equality, we can combine knowledge */\n\t\t\t\treg_combine_min_max(&other_branch_regs[insn->src_reg],\n\t\t\t\t\t\t    &other_branch_regs[insn->dst_reg],\n\t\t\t\t\t\t    &regs[insn->src_reg],\n\t\t\t\t\t\t    &regs[insn->dst_reg], opcode);\n\t\t}\n\t} else if (dst_reg->type == SCALAR_VALUE) {\n\t\treg_set_min_max(&other_branch_regs[insn->dst_reg],\n\t\t\t\t\tdst_reg, insn->imm, opcode);\n\t}\n\n\t/* detect if R == 0 where R is returned from bpf_map_lookup_elem() */\n\tif (BPF_SRC(insn->code) == BPF_K &&\n\t    insn->imm == 0 && (opcode == BPF_JEQ || opcode == BPF_JNE) &&\n\t    reg_type_may_be_null(dst_reg->type)) {\n\t\t/* Mark all identical registers in each branch as either\n\t\t * safe or unknown depending R == 0 or R != 0 conditional.\n\t\t */\n\t\tmark_ptr_or_null_regs(this_branch, insn->dst_reg,\n\t\t\t\t      opcode == BPF_JNE);\n\t\tmark_ptr_or_null_regs(other_branch, insn->dst_reg,\n\t\t\t\t      opcode == BPF_JEQ);\n\t} else if (!try_match_pkt_pointers(insn, dst_reg, &regs[insn->src_reg],\n\t\t\t\t\t   this_branch, other_branch) &&\n\t\t   is_pointer_value(env, insn->dst_reg)) {\n\t\tverbose(env, \"R%d pointer comparison prohibited\\n\",\n\t\t\tinsn->dst_reg);\n\t\treturn -EACCES;\n\t}\n\tif (env->log.level)\n\t\tprint_verifier_state(env, this_branch->frame[this_branch->curframe]);\n\treturn 0;\n}\n\n/* return the map pointer stored inside BPF_LD_IMM64 instruction */\nstatic struct bpf_map *ld_imm64_to_map_ptr(struct bpf_insn *insn)\n{\n\tu64 imm64 = ((u64) (u32) insn[0].imm) | ((u64) (u32) insn[1].imm) << 32;\n\n\treturn (struct bpf_map *) (unsigned long) imm64;\n}\n\n/* verify BPF_LD_IMM64 instruction */\nstatic int check_ld_imm(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tint err;\n\n\tif (BPF_SIZE(insn->code) != BPF_DW) {\n\t\tverbose(env, \"invalid BPF_LD_IMM insn\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (insn->off != 0) {\n\t\tverbose(env, \"BPF_LD_IMM64 uses reserved fields\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\terr = check_reg_arg(env, insn->dst_reg, DST_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (insn->src_reg == 0) {\n\t\tu64 imm = ((u64)(insn + 1)->imm << 32) | (u32)insn->imm;\n\n\t\tregs[insn->dst_reg].type = SCALAR_VALUE;\n\t\t__mark_reg_known(&regs[insn->dst_reg], imm);\n\t\treturn 0;\n\t}\n\n\t/* replace_map_fd_with_map_ptr() should have caught bad ld_imm64 */\n\tBUG_ON(insn->src_reg != BPF_PSEUDO_MAP_FD);\n\n\tregs[insn->dst_reg].type = CONST_PTR_TO_MAP;\n\tregs[insn->dst_reg].map_ptr = ld_imm64_to_map_ptr(insn);\n\treturn 0;\n}\n\nstatic bool may_access_skb(enum bpf_prog_type type)\n{\n\tswitch (type) {\n\tcase BPF_PROG_TYPE_SOCKET_FILTER:\n\tcase BPF_PROG_TYPE_SCHED_CLS:\n\tcase BPF_PROG_TYPE_SCHED_ACT:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n/* verify safety of LD_ABS|LD_IND instructions:\n * - they can only appear in the programs where ctx == skb\n * - since they are wrappers of function calls, they scratch R1-R5 registers,\n *   preserve R6-R9, and store return value into R0\n *\n * Implicit input:\n *   ctx == skb == R6 == CTX\n *\n * Explicit input:\n *   SRC == any register\n *   IMM == 32-bit immediate\n *\n * Output:\n *   R0 - 8/16/32-bit skb data converted to cpu endianness\n */\nstatic int check_ld_abs(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 mode = BPF_MODE(insn->code);\n\tint i, err;\n\n\tif (!may_access_skb(env->prog->type)) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] instructions not allowed for this program type\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!env->ops->gen_ld_abs) {\n\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (env->subprog_cnt > 1) {\n\t\t/* when program has LD_ABS insn JITs and interpreter assume\n\t\t * that r1 == ctx == skb which is not the case for callees\n\t\t * that can have arbitrary arguments. It's problematic\n\t\t * for main prog as well since JITs would need to analyze\n\t\t * all functions in order to make proper register save/restore\n\t\t * decisions in the main prog. Hence disallow LD_ABS with calls\n\t\t */\n\t\tverbose(env, \"BPF_LD_[ABS|IND] instructions cannot be mixed with bpf-to-bpf calls\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (insn->dst_reg != BPF_REG_0 || insn->off != 0 ||\n\t    BPF_SIZE(insn->code) == BPF_DW ||\n\t    (mode == BPF_ABS && insn->src_reg != BPF_REG_0)) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] uses reserved fields\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* check whether implicit source operand (register R6) is readable */\n\terr = check_reg_arg(env, BPF_REG_6, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\t/* Disallow usage of BPF_LD_[ABS|IND] with reference tracking, as\n\t * gen_ld_abs() may terminate the program at runtime, leading to\n\t * reference leak.\n\t */\n\terr = check_reference_leak(env);\n\tif (err) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] cannot be mixed with socket references\\n\");\n\t\treturn err;\n\t}\n\n\tif (regs[BPF_REG_6].type != PTR_TO_CTX) {\n\t\tverbose(env,\n\t\t\t\"at the time of BPF_LD_ABS|IND R6 != pointer to skb\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (mode == BPF_IND) {\n\t\t/* check explicit source operand */\n\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\t/* reset caller saved regs to unreadable */\n\tfor (i = 0; i < CALLER_SAVED_REGS; i++) {\n\t\tmark_reg_not_init(env, regs, caller_saved[i]);\n\t\tcheck_reg_arg(env, caller_saved[i], DST_OP_NO_MARK);\n\t}\n\n\t/* mark destination R0 register as readable, since it contains\n\t * the value fetched from the packet.\n\t * Already marked as written above.\n\t */\n\tmark_reg_unknown(env, regs, BPF_REG_0);\n\treturn 0;\n}\n\nstatic int check_return_code(struct bpf_verifier_env *env)\n{\n\tstruct bpf_reg_state *reg;\n\tstruct tnum range = tnum_range(0, 1);\n\n\tswitch (env->prog->type) {\n\tcase BPF_PROG_TYPE_CGROUP_SKB:\n\tcase BPF_PROG_TYPE_CGROUP_SOCK:\n\tcase BPF_PROG_TYPE_CGROUP_SOCK_ADDR:\n\tcase BPF_PROG_TYPE_SOCK_OPS:\n\tcase BPF_PROG_TYPE_CGROUP_DEVICE:\n\t\tbreak;\n\tdefault:\n\t\treturn 0;\n\t}\n\n\treg = cur_regs(env) + BPF_REG_0;\n\tif (reg->type != SCALAR_VALUE) {\n\t\tverbose(env, \"At program exit the register R0 is not a known value (%s)\\n\",\n\t\t\treg_type_str[reg->type]);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!tnum_in(range, reg->var_off)) {\n\t\tverbose(env, \"At program exit the register R0 \");\n\t\tif (!tnum_is_unknown(reg->var_off)) {\n\t\t\tchar tn_buf[48];\n\n\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\tverbose(env, \"has value %s\", tn_buf);\n\t\t} else {\n\t\t\tverbose(env, \"has unknown scalar value\");\n\t\t}\n\t\tverbose(env, \" should have been 0 or 1\\n\");\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\n/* non-recursive DFS pseudo code\n * 1  procedure DFS-iterative(G,v):\n * 2      label v as discovered\n * 3      let S be a stack\n * 4      S.push(v)\n * 5      while S is not empty\n * 6            t <- S.pop()\n * 7            if t is what we're looking for:\n * 8                return t\n * 9            for all edges e in G.adjacentEdges(t) do\n * 10               if edge e is already labelled\n * 11                   continue with the next edge\n * 12               w <- G.adjacentVertex(t,e)\n * 13               if vertex w is not discovered and not explored\n * 14                   label e as tree-edge\n * 15                   label w as discovered\n * 16                   S.push(w)\n * 17                   continue at 5\n * 18               else if vertex w is discovered\n * 19                   label e as back-edge\n * 20               else\n * 21                   // vertex w is explored\n * 22                   label e as forward- or cross-edge\n * 23           label t as explored\n * 24           S.pop()\n *\n * convention:\n * 0x10 - discovered\n * 0x11 - discovered and fall-through edge labelled\n * 0x12 - discovered and fall-through and branch edges labelled\n * 0x20 - explored\n */\n\nenum {\n\tDISCOVERED = 0x10,\n\tEXPLORED = 0x20,\n\tFALLTHROUGH = 1,\n\tBRANCH = 2,\n};\n\n#define STATE_LIST_MARK ((struct bpf_verifier_state_list *) -1L)\n\nstatic int *insn_stack;\t/* stack of insns to process */\nstatic int cur_stack;\t/* current stack index */\nstatic int *insn_state;\n\n/* t, w, e - match pseudo-code above:\n * t - index of current instruction\n * w - next instruction\n * e - edge\n */\nstatic int push_insn(int t, int w, int e, struct bpf_verifier_env *env)\n{\n\tif (e == FALLTHROUGH && insn_state[t] >= (DISCOVERED | FALLTHROUGH))\n\t\treturn 0;\n\n\tif (e == BRANCH && insn_state[t] >= (DISCOVERED | BRANCH))\n\t\treturn 0;\n\n\tif (w < 0 || w >= env->prog->len) {\n\t\tverbose_linfo(env, t, \"%d: \", t);\n\t\tverbose(env, \"jump out of range from insn %d to %d\\n\", t, w);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e == BRANCH)\n\t\t/* mark branch target for state pruning */\n\t\tenv->explored_states[w] = STATE_LIST_MARK;\n\n\tif (insn_state[w] == 0) {\n\t\t/* tree-edge */\n\t\tinsn_state[t] = DISCOVERED | e;\n\t\tinsn_state[w] = DISCOVERED;\n\t\tif (cur_stack >= env->prog->len)\n\t\t\treturn -E2BIG;\n\t\tinsn_stack[cur_stack++] = w;\n\t\treturn 1;\n\t} else if ((insn_state[w] & 0xF0) == DISCOVERED) {\n\t\tverbose_linfo(env, t, \"%d: \", t);\n\t\tverbose_linfo(env, w, \"%d: \", w);\n\t\tverbose(env, \"back-edge from insn %d to %d\\n\", t, w);\n\t\treturn -EINVAL;\n\t} else if (insn_state[w] == EXPLORED) {\n\t\t/* forward- or cross-edge */\n\t\tinsn_state[t] = DISCOVERED | e;\n\t} else {\n\t\tverbose(env, \"insn state internal bug\\n\");\n\t\treturn -EFAULT;\n\t}\n\treturn 0;\n}\n\n/* non-recursive depth-first-search to detect loops in BPF program\n * loop == back-edge in directed graph\n */\nstatic int check_cfg(struct bpf_verifier_env *env)\n{\n\tstruct bpf_insn *insns = env->prog->insnsi;\n\tint insn_cnt = env->prog->len;\n\tint ret = 0;\n\tint i, t;\n\n\tinsn_state = kcalloc(insn_cnt, sizeof(int), GFP_KERNEL);\n\tif (!insn_state)\n\t\treturn -ENOMEM;\n\n\tinsn_stack = kcalloc(insn_cnt, sizeof(int), GFP_KERNEL);\n\tif (!insn_stack) {\n\t\tkfree(insn_state);\n\t\treturn -ENOMEM;\n\t}\n\n\tinsn_state[0] = DISCOVERED; /* mark 1st insn as discovered */\n\tinsn_stack[0] = 0; /* 0 is the first instruction */\n\tcur_stack = 1;\n\npeek_stack:\n\tif (cur_stack == 0)\n\t\tgoto check_state;\n\tt = insn_stack[cur_stack - 1];\n\n\tif (BPF_CLASS(insns[t].code) == BPF_JMP) {\n\t\tu8 opcode = BPF_OP(insns[t].code);\n\n\t\tif (opcode == BPF_EXIT) {\n\t\t\tgoto mark_explored;\n\t\t} else if (opcode == BPF_CALL) {\n\t\t\tret = push_insn(t, t + 1, FALLTHROUGH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t\tif (t + 1 < insn_cnt)\n\t\t\t\tenv->explored_states[t + 1] = STATE_LIST_MARK;\n\t\t\tif (insns[t].src_reg == BPF_PSEUDO_CALL) {\n\t\t\t\tenv->explored_states[t] = STATE_LIST_MARK;\n\t\t\t\tret = push_insn(t, t + insns[t].imm + 1, BRANCH, env);\n\t\t\t\tif (ret == 1)\n\t\t\t\t\tgoto peek_stack;\n\t\t\t\telse if (ret < 0)\n\t\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t} else if (opcode == BPF_JA) {\n\t\t\tif (BPF_SRC(insns[t].code) != BPF_K) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t\t/* unconditional jump with single edge */\n\t\t\tret = push_insn(t, t + insns[t].off + 1,\n\t\t\t\t\tFALLTHROUGH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t\t/* tell verifier to check for equivalent states\n\t\t\t * after every call and jump\n\t\t\t */\n\t\t\tif (t + 1 < insn_cnt)\n\t\t\t\tenv->explored_states[t + 1] = STATE_LIST_MARK;\n\t\t} else {\n\t\t\t/* conditional jump with two edges */\n\t\t\tenv->explored_states[t] = STATE_LIST_MARK;\n\t\t\tret = push_insn(t, t + 1, FALLTHROUGH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\n\t\t\tret = push_insn(t, t + insns[t].off + 1, BRANCH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t}\n\t} else {\n\t\t/* all other non-branch instructions with single\n\t\t * fall-through edge\n\t\t */\n\t\tret = push_insn(t, t + 1, FALLTHROUGH, env);\n\t\tif (ret == 1)\n\t\t\tgoto peek_stack;\n\t\telse if (ret < 0)\n\t\t\tgoto err_free;\n\t}\n\nmark_explored:\n\tinsn_state[t] = EXPLORED;\n\tif (cur_stack-- <= 0) {\n\t\tverbose(env, \"pop stack internal bug\\n\");\n\t\tret = -EFAULT;\n\t\tgoto err_free;\n\t}\n\tgoto peek_stack;\n\ncheck_state:\n\tfor (i = 0; i < insn_cnt; i++) {\n\t\tif (insn_state[i] != EXPLORED) {\n\t\t\tverbose(env, \"unreachable insn %d\\n\", i);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\t}\n\tret = 0; /* cfg looks good */\n\nerr_free:\n\tkfree(insn_state);\n\tkfree(insn_stack);\n\treturn ret;\n}\n\n/* The minimum supported BTF func info size */\n#define MIN_BPF_FUNCINFO_SIZE\t8\n#define MAX_FUNCINFO_REC_SIZE\t252\n\nstatic int check_btf_func(struct bpf_verifier_env *env,\n\t\t\t  const union bpf_attr *attr,\n\t\t\t  union bpf_attr __user *uattr)\n{\n\tu32 i, nfuncs, urec_size, min_size, prev_offset;\n\tu32 krec_size = sizeof(struct bpf_func_info);\n\tstruct bpf_func_info *krecord;\n\tconst struct btf_type *type;\n\tstruct bpf_prog *prog;\n\tconst struct btf *btf;\n\tvoid __user *urecord;\n\tint ret = 0;\n\n\tnfuncs = attr->func_info_cnt;\n\tif (!nfuncs)\n\t\treturn 0;\n\n\tif (nfuncs != env->subprog_cnt) {\n\t\tverbose(env, \"number of funcs in func_info doesn't match number of subprogs\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\turec_size = attr->func_info_rec_size;\n\tif (urec_size < MIN_BPF_FUNCINFO_SIZE ||\n\t    urec_size > MAX_FUNCINFO_REC_SIZE ||\n\t    urec_size % sizeof(u32)) {\n\t\tverbose(env, \"invalid func info rec size %u\\n\", urec_size);\n\t\treturn -EINVAL;\n\t}\n\n\tprog = env->prog;\n\tbtf = prog->aux->btf;\n\n\turecord = u64_to_user_ptr(attr->func_info);\n\tmin_size = min_t(u32, krec_size, urec_size);\n\n\tkrecord = kvcalloc(nfuncs, krec_size, GFP_KERNEL | __GFP_NOWARN);\n\tif (!krecord)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < nfuncs; i++) {\n\t\tret = bpf_check_uarg_tail_zero(urecord, krec_size, urec_size);\n\t\tif (ret) {\n\t\t\tif (ret == -E2BIG) {\n\t\t\t\tverbose(env, \"nonzero tailing record in func info\");\n\t\t\t\t/* set the size kernel expects so loader can zero\n\t\t\t\t * out the rest of the record.\n\t\t\t\t */\n\t\t\t\tif (put_user(min_size, &uattr->func_info_rec_size))\n\t\t\t\t\tret = -EFAULT;\n\t\t\t}\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (copy_from_user(&krecord[i], urecord, min_size)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/* check insn_off */\n\t\tif (i == 0) {\n\t\t\tif (krecord[i].insn_off) {\n\t\t\t\tverbose(env,\n\t\t\t\t\t\"nonzero insn_off %u for the first func info record\",\n\t\t\t\t\tkrecord[i].insn_off);\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t} else if (krecord[i].insn_off <= prev_offset) {\n\t\t\tverbose(env,\n\t\t\t\t\"same or smaller insn offset (%u) than previous func info record (%u)\",\n\t\t\t\tkrecord[i].insn_off, prev_offset);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (env->subprog_info[i].start != krecord[i].insn_off) {\n\t\t\tverbose(env, \"func_info BTF section doesn't match subprog layout in BPF program\\n\");\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/* check type_id */\n\t\ttype = btf_type_by_id(btf, krecord[i].type_id);\n\t\tif (!type || BTF_INFO_KIND(type->info) != BTF_KIND_FUNC) {\n\t\t\tverbose(env, \"invalid type id %d in func info\",\n\t\t\t\tkrecord[i].type_id);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tprev_offset = krecord[i].insn_off;\n\t\turecord += urec_size;\n\t}\n\n\tprog->aux->func_info = krecord;\n\tprog->aux->func_info_cnt = nfuncs;\n\treturn 0;\n\nerr_free:\n\tkvfree(krecord);\n\treturn ret;\n}\n\nstatic void adjust_btf_func(struct bpf_verifier_env *env)\n{\n\tint i;\n\n\tif (!env->prog->aux->func_info)\n\t\treturn;\n\n\tfor (i = 0; i < env->subprog_cnt; i++)\n\t\tenv->prog->aux->func_info[i].insn_off = env->subprog_info[i].start;\n}\n\n#define MIN_BPF_LINEINFO_SIZE\t(offsetof(struct bpf_line_info, line_col) + \\\n\t\tsizeof(((struct bpf_line_info *)(0))->line_col))\n#define MAX_LINEINFO_REC_SIZE\tMAX_FUNCINFO_REC_SIZE\n\nstatic int check_btf_line(struct bpf_verifier_env *env,\n\t\t\t  const union bpf_attr *attr,\n\t\t\t  union bpf_attr __user *uattr)\n{\n\tu32 i, s, nr_linfo, ncopy, expected_size, rec_size, prev_offset = 0;\n\tstruct bpf_subprog_info *sub;\n\tstruct bpf_line_info *linfo;\n\tstruct bpf_prog *prog;\n\tconst struct btf *btf;\n\tvoid __user *ulinfo;\n\tint err;\n\n\tnr_linfo = attr->line_info_cnt;\n\tif (!nr_linfo)\n\t\treturn 0;\n\n\trec_size = attr->line_info_rec_size;\n\tif (rec_size < MIN_BPF_LINEINFO_SIZE ||\n\t    rec_size > MAX_LINEINFO_REC_SIZE ||\n\t    rec_size & (sizeof(u32) - 1))\n\t\treturn -EINVAL;\n\n\t/* Need to zero it in case the userspace may\n\t * pass in a smaller bpf_line_info object.\n\t */\n\tlinfo = kvcalloc(nr_linfo, sizeof(struct bpf_line_info),\n\t\t\t GFP_KERNEL | __GFP_NOWARN);\n\tif (!linfo)\n\t\treturn -ENOMEM;\n\n\tprog = env->prog;\n\tbtf = prog->aux->btf;\n\n\ts = 0;\n\tsub = env->subprog_info;\n\tulinfo = u64_to_user_ptr(attr->line_info);\n\texpected_size = sizeof(struct bpf_line_info);\n\tncopy = min_t(u32, expected_size, rec_size);\n\tfor (i = 0; i < nr_linfo; i++) {\n\t\terr = bpf_check_uarg_tail_zero(ulinfo, expected_size, rec_size);\n\t\tif (err) {\n\t\t\tif (err == -E2BIG) {\n\t\t\t\tverbose(env, \"nonzero tailing record in line_info\");\n\t\t\t\tif (put_user(expected_size,\n\t\t\t\t\t     &uattr->line_info_rec_size))\n\t\t\t\t\terr = -EFAULT;\n\t\t\t}\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (copy_from_user(&linfo[i], ulinfo, ncopy)) {\n\t\t\terr = -EFAULT;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/*\n\t\t * Check insn_off to ensure\n\t\t * 1) strictly increasing AND\n\t\t * 2) bounded by prog->len\n\t\t *\n\t\t * The linfo[0].insn_off == 0 check logically falls into\n\t\t * the later \"missing bpf_line_info for func...\" case\n\t\t * because the first linfo[0].insn_off must be the\n\t\t * first sub also and the first sub must have\n\t\t * subprog_info[0].start == 0.\n\t\t */\n\t\tif ((i && linfo[i].insn_off <= prev_offset) ||\n\t\t    linfo[i].insn_off >= prog->len) {\n\t\t\tverbose(env, \"Invalid line_info[%u].insn_off:%u (prev_offset:%u prog->len:%u)\\n\",\n\t\t\t\ti, linfo[i].insn_off, prev_offset,\n\t\t\t\tprog->len);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (!prog->insnsi[linfo[i].insn_off].code) {\n\t\t\tverbose(env,\n\t\t\t\t\"Invalid insn code at line_info[%u].insn_off\\n\",\n\t\t\t\ti);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (!btf_name_by_offset(btf, linfo[i].line_off) ||\n\t\t    !btf_name_by_offset(btf, linfo[i].file_name_off)) {\n\t\t\tverbose(env, \"Invalid line_info[%u].line_off or .file_name_off\\n\", i);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (s != env->subprog_cnt) {\n\t\t\tif (linfo[i].insn_off == sub[s].start) {\n\t\t\t\tsub[s].linfo_idx = i;\n\t\t\t\ts++;\n\t\t\t} else if (sub[s].start < linfo[i].insn_off) {\n\t\t\t\tverbose(env, \"missing bpf_line_info for func#%u\\n\", s);\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t}\n\n\t\tprev_offset = linfo[i].insn_off;\n\t\tulinfo += rec_size;\n\t}\n\n\tif (s != env->subprog_cnt) {\n\t\tverbose(env, \"missing bpf_line_info for %u funcs starting from func#%u\\n\",\n\t\t\tenv->subprog_cnt - s, s);\n\t\terr = -EINVAL;\n\t\tgoto err_free;\n\t}\n\n\tprog->aux->linfo = linfo;\n\tprog->aux->nr_linfo = nr_linfo;\n\n\treturn 0;\n\nerr_free:\n\tkvfree(linfo);\n\treturn err;\n}\n\nstatic int check_btf_info(struct bpf_verifier_env *env,\n\t\t\t  const union bpf_attr *attr,\n\t\t\t  union bpf_attr __user *uattr)\n{\n\tstruct btf *btf;\n\tint err;\n\n\tif (!attr->func_info_cnt && !attr->line_info_cnt)\n\t\treturn 0;\n\n\tbtf = btf_get_by_fd(attr->prog_btf_fd);\n\tif (IS_ERR(btf))\n\t\treturn PTR_ERR(btf);\n\tenv->prog->aux->btf = btf;\n\n\terr = check_btf_func(env, attr, uattr);\n\tif (err)\n\t\treturn err;\n\n\terr = check_btf_line(env, attr, uattr);\n\tif (err)\n\t\treturn err;\n\n\treturn 0;\n}\n\n/* check %cur's range satisfies %old's */\nstatic bool range_within(struct bpf_reg_state *old,\n\t\t\t struct bpf_reg_state *cur)\n{\n\treturn old->umin_value <= cur->umin_value &&\n\t       old->umax_value >= cur->umax_value &&\n\t       old->smin_value <= cur->smin_value &&\n\t       old->smax_value >= cur->smax_value;\n}\n\n/* Maximum number of register states that can exist at once */\n#define ID_MAP_SIZE\t(MAX_BPF_REG + MAX_BPF_STACK / BPF_REG_SIZE)\nstruct idpair {\n\tu32 old;\n\tu32 cur;\n};\n\n/* If in the old state two registers had the same id, then they need to have\n * the same id in the new state as well.  But that id could be different from\n * the old state, so we need to track the mapping from old to new ids.\n * Once we have seen that, say, a reg with old id 5 had new id 9, any subsequent\n * regs with old id 5 must also have new id 9 for the new state to be safe.  But\n * regs with a different old id could still have new id 9, we don't care about\n * that.\n * So we look through our idmap to see if this old id has been seen before.  If\n * so, we require the new id to match; otherwise, we add the id pair to the map.\n */\nstatic bool check_ids(u32 old_id, u32 cur_id, struct idpair *idmap)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < ID_MAP_SIZE; i++) {\n\t\tif (!idmap[i].old) {\n\t\t\t/* Reached an empty slot; haven't seen this id before */\n\t\t\tidmap[i].old = old_id;\n\t\t\tidmap[i].cur = cur_id;\n\t\t\treturn true;\n\t\t}\n\t\tif (idmap[i].old == old_id)\n\t\t\treturn idmap[i].cur == cur_id;\n\t}\n\t/* We ran out of idmap slots, which should be impossible */\n\tWARN_ON_ONCE(1);\n\treturn false;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 3481,
        "critical_vars": [
            "ret"
        ],
        "function": "adjust_scalar_min_max_vals",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_d3bd7413e0ca40b60cf60d4003246d067cafdeda_verifier.c.diff",
        "label": "True",
        "function_code": "static int adjust_scalar_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t      struct bpf_insn *insn,\n\t\t\t\t      struct bpf_reg_state *dst_reg,\n\t\t\t\t      struct bpf_reg_state src_reg)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 opcode = BPF_OP(insn->code);\n\tbool src_known, dst_known;\n\ts64 smin_val, smax_val;\n\tu64 umin_val, umax_val;\n\tu64 insn_bitness = (BPF_CLASS(insn->code) == BPF_ALU64) ? 64 : 32;\n\tu32 dst = insn->dst_reg;\n\tint ret;\n\n\tif (insn_bitness == 32) {\n\t\t/* Relevant for 32-bit RSH: Information can propagate towards\n\t\t * LSB, so it isn't sufficient to only truncate the output to\n\t\t * 32 bits.\n\t\t */\n\t\tcoerce_reg_to_size(dst_reg, 4);\n\t\tcoerce_reg_to_size(&src_reg, 4);\n\t}\n\n\tsmin_val = src_reg.smin_value;\n\tsmax_val = src_reg.smax_value;\n\tumin_val = src_reg.umin_value;\n\tumax_val = src_reg.umax_value;\n\tsrc_known = tnum_is_const(src_reg.var_off);\n\tdst_known = tnum_is_const(dst_reg->var_off);\n\n\tif ((src_known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (!src_known &&\n\t    opcode != BPF_ADD && opcode != BPF_SUB && opcode != BPF_AND) {\n\t\t__mark_reg_unknown(dst_reg);\n\t\treturn 0;\n\t}\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\tret = sanitize_val_alu(env, insn);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to add from different pointers or scalars\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\tif (signed_add_overflows(dst_reg->smin_value, smin_val) ||\n\t\t    signed_add_overflows(dst_reg->smax_value, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value += smin_val;\n\t\t\tdst_reg->smax_value += smax_val;\n\t\t}\n\t\tif (dst_reg->umin_value + umin_val < umin_val ||\n\t\t    dst_reg->umax_value + umax_val < umax_val) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value += umin_val;\n\t\t\tdst_reg->umax_value += umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(dst_reg->var_off, src_reg.var_off);\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tret = sanitize_val_alu(env, insn);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to sub from different pointers or scalars\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\tif (signed_sub_overflows(dst_reg->smin_value, smax_val) ||\n\t\t    signed_sub_overflows(dst_reg->smax_value, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value -= smax_val;\n\t\t\tdst_reg->smax_value -= smin_val;\n\t\t}\n\t\tif (dst_reg->umin_value < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value -= umax_val;\n\t\t\tdst_reg->umax_value -= umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(dst_reg->var_off, src_reg.var_off);\n\t\tbreak;\n\tcase BPF_MUL:\n\t\tdst_reg->var_off = tnum_mul(dst_reg->var_off, src_reg.var_off);\n\t\tif (smin_val < 0 || dst_reg->smin_value < 0) {\n\t\t\t/* Ain't nobody got time to multiply that sign */\n\t\t\t__mark_reg_unbounded(dst_reg);\n\t\t\t__update_reg_bounds(dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\t/* Both values are positive, so we can work with unsigned and\n\t\t * copy the result to signed (unless it exceeds S64_MAX).\n\t\t */\n\t\tif (umax_val > U32_MAX || dst_reg->umax_value > U32_MAX) {\n\t\t\t/* Potential overflow, we know nothing */\n\t\t\t__mark_reg_unbounded(dst_reg);\n\t\t\t/* (except what we can learn from the var_off) */\n\t\t\t__update_reg_bounds(dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\tdst_reg->umin_value *= umin_val;\n\t\tdst_reg->umax_value *= umax_val;\n\t\tif (dst_reg->umax_value > S64_MAX) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = dst_reg->umin_value;\n\t\t\tdst_reg->smax_value = dst_reg->umax_value;\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\t\tif (src_known && dst_known) {\n\t\t\t__mark_reg_known(dst_reg, dst_reg->var_off.value &\n\t\t\t\t\t\t  src_reg.var_off.value);\n\t\t\tbreak;\n\t\t}\n\t\t/* We get our minimum from the var_off, since that's inherently\n\t\t * bitwise.  Our maximum is the minimum of the operands' maxima.\n\t\t */\n\t\tdst_reg->var_off = tnum_and(dst_reg->var_off, src_reg.var_off);\n\t\tdst_reg->umin_value = dst_reg->var_off.value;\n\t\tdst_reg->umax_value = min(dst_reg->umax_value, umax_val);\n\t\tif (dst_reg->smin_value < 0 || smin_val < 0) {\n\t\t\t/* Lose signed bounds when ANDing negative numbers,\n\t\t\t * ain't nobody got time for that.\n\t\t\t */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\t/* ANDing two positives gives a positive, so safe to\n\t\t\t * cast result into s64.\n\t\t\t */\n\t\t\tdst_reg->smin_value = dst_reg->umin_value;\n\t\t\tdst_reg->smax_value = dst_reg->umax_value;\n\t\t}\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tcase BPF_OR:\n\t\tif (src_known && dst_known) {\n\t\t\t__mark_reg_known(dst_reg, dst_reg->var_off.value |\n\t\t\t\t\t\t  src_reg.var_off.value);\n\t\t\tbreak;\n\t\t}\n\t\t/* We get our maximum from the var_off, and our minimum is the\n\t\t * maximum of the operands' minima\n\t\t */\n\t\tdst_reg->var_off = tnum_or(dst_reg->var_off, src_reg.var_off);\n\t\tdst_reg->umin_value = max(dst_reg->umin_value, umin_val);\n\t\tdst_reg->umax_value = dst_reg->var_off.value |\n\t\t\t\t      dst_reg->var_off.mask;\n\t\tif (dst_reg->smin_value < 0 || smin_val < 0) {\n\t\t\t/* Lose signed bounds when ORing negative numbers,\n\t\t\t * ain't nobody got time for that.\n\t\t\t */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\t/* ORing two positives gives a positive, so safe to\n\t\t\t * cast result into s64.\n\t\t\t */\n\t\t\tdst_reg->smin_value = dst_reg->umin_value;\n\t\t\tdst_reg->smax_value = dst_reg->umax_value;\n\t\t}\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tcase BPF_LSH:\n\t\tif (umax_val >= insn_bitness) {\n\t\t\t/* Shifts greater than 31 or 63 are undefined.\n\t\t\t * This includes shifts by a negative number.\n\t\t\t */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\t/* We lose all sign bit information (except what we can pick\n\t\t * up from var_off)\n\t\t */\n\t\tdst_reg->smin_value = S64_MIN;\n\t\tdst_reg->smax_value = S64_MAX;\n\t\t/* If we might shift our top bit out, then we know nothing */\n\t\tif (dst_reg->umax_value > 1ULL << (63 - umax_val)) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value <<= umin_val;\n\t\t\tdst_reg->umax_value <<= umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_lshift(dst_reg->var_off, umin_val);\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tcase BPF_RSH:\n\t\tif (umax_val >= insn_bitness) {\n\t\t\t/* Shifts greater than 31 or 63 are undefined.\n\t\t\t * This includes shifts by a negative number.\n\t\t\t */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\t/* BPF_RSH is an unsigned shift.  If the value in dst_reg might\n\t\t * be negative, then either:\n\t\t * 1) src_reg might be zero, so the sign bit of the result is\n\t\t *    unknown, so we lose our signed bounds\n\t\t * 2) it's known negative, thus the unsigned bounds capture the\n\t\t *    signed bounds\n\t\t * 3) the signed bounds cross zero, so they tell us nothing\n\t\t *    about the result\n\t\t * If the value in dst_reg is known nonnegative, then again the\n\t\t * unsigned bounts capture the signed bounds.\n\t\t * Thus, in all cases it suffices to blow away our signed bounds\n\t\t * and rely on inferring new ones from the unsigned bounds and\n\t\t * var_off of the result.\n\t\t */\n\t\tdst_reg->smin_value = S64_MIN;\n\t\tdst_reg->smax_value = S64_MAX;\n\t\tdst_reg->var_off = tnum_rshift(dst_reg->var_off, umin_val);\n\t\tdst_reg->umin_value >>= umax_val;\n\t\tdst_reg->umax_value >>= umin_val;\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tcase BPF_ARSH:\n\t\tif (umax_val >= insn_bitness) {\n\t\t\t/* Shifts greater than 31 or 63 are undefined.\n\t\t\t * This includes shifts by a negative number.\n\t\t\t */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Upon reaching here, src_known is true and\n\t\t * umax_val is equal to umin_val.\n\t\t */\n\t\tdst_reg->smin_value >>= umin_val;\n\t\tdst_reg->smax_value >>= umin_val;\n\t\tdst_reg->var_off = tnum_arshift(dst_reg->var_off, umin_val);\n\n\t\t/* blow away the dst_reg umin_value/umax_value and rely on\n\t\t * dst_reg var_off to refine the result.\n\t\t */\n\t\tdst_reg->umin_value = 0;\n\t\tdst_reg->umax_value = U64_MAX;\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tdefault:\n\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\tbreak;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops are (32,32)->32 */\n\t\tcoerce_reg_to_size(dst_reg, 4);\n\t}\n\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\treturn 0;\n}\n\n/* Handles ALU ops other than BPF_END, BPF_NEG and BPF_MOV: computes new min/max\n * and var_off.\n */\nstatic int adjust_reg_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg, *src_reg;\n\tstruct bpf_reg_state *ptr_reg = NULL, off_reg = {0};\n\tu8 opcode = BPF_OP(insn->code);\n\n\tdst_reg = &regs[insn->dst_reg];\n\tsrc_reg = NULL;\n\tif (dst_reg->type != SCALAR_VALUE)\n\t\tptr_reg = dst_reg;\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tsrc_reg = &regs[insn->src_reg];\n\t\tif (src_reg->type != SCALAR_VALUE) {\n\t\t\tif (dst_reg->type != SCALAR_VALUE) {\n\t\t\t\t/* Combining two pointers by any ALU op yields\n\t\t\t\t * an arbitrary scalar. Disallow all math except\n\t\t\t\t * pointer subtraction\n\t\t\t\t */\n\t\t\t\tif (opcode == BPF_SUB && env->allow_ptr_leaks) {\n\t\t\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t\tverbose(env, \"R%d pointer %s pointer prohibited\\n\",\n\t\t\t\t\tinsn->dst_reg,\n\t\t\t\t\tbpf_alu_string[opcode >> 4]);\n\t\t\t\treturn -EACCES;\n\t\t\t} else {\n\t\t\t\t/* scalar += pointer\n\t\t\t\t * This is legal, but we have to reverse our\n\t\t\t\t * src/dest handling in computing the range\n\t\t\t\t */\n\t\t\t\treturn adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t\t       src_reg, dst_reg);\n\t\t\t}\n\t\t} else if (ptr_reg) {\n\t\t\t/* pointer += scalar */\n\t\t\treturn adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t       dst_reg, src_reg);\n\t\t}\n\t} else {\n\t\t/* Pretend the src is a reg with a known value, since we only\n\t\t * need to be able to read from this state.\n\t\t */\n\t\toff_reg.type = SCALAR_VALUE;\n\t\t__mark_reg_known(&off_reg, insn->imm);\n\t\tsrc_reg = &off_reg;\n\t\tif (ptr_reg) /* pointer += K */\n\t\t\treturn adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t       ptr_reg, src_reg);\n\t}\n\n\t/* Got here implies adding two SCALAR_VALUEs */\n\tif (WARN_ON_ONCE(ptr_reg)) {\n\t\tprint_verifier_state(env, state);\n\t\tverbose(env, \"verifier internal error: unexpected ptr_reg\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (WARN_ON(!src_reg)) {\n\t\tprint_verifier_state(env, state);\n\t\tverbose(env, \"verifier internal error: no src_reg\\n\");\n\t\treturn -EINVAL;\n\t}\n\treturn adjust_scalar_min_max_vals(env, insn, dst_reg, *src_reg);\n}\n\n/* check validity of 32-bit and 64-bit arithmetic operations */\nstatic int check_alu_op(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 opcode = BPF_OP(insn->code);\n\tint err;\n\n\tif (opcode == BPF_END || opcode == BPF_NEG) {\n\t\tif (opcode == BPF_NEG) {\n\t\t\tif (BPF_SRC(insn->code) != 0 ||\n\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t    insn->off != 0 || insn->imm != 0) {\n\t\t\t\tverbose(env, \"BPF_NEG uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else {\n\t\t\tif (insn->src_reg != BPF_REG_0 || insn->off != 0 ||\n\t\t\t    (insn->imm != 16 && insn->imm != 32 && insn->imm != 64) ||\n\t\t\t    BPF_CLASS(insn->code) == BPF_ALU64) {\n\t\t\t\tverbose(env, \"BPF_END uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check src operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (is_pointer_value(env, insn->dst_reg)) {\n\t\t\tverbose(env, \"R%d pointer arithmetic prohibited\\n\",\n\t\t\t\tinsn->dst_reg);\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\t/* check dest operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t} else if (opcode == BPF_MOV) {\n\n\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\tif (insn->imm != 0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_MOV uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\t/* check src operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t} else {\n\t\t\tif (insn->src_reg != BPF_REG_0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_MOV uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check dest operand, mark as required later */\n\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP_NO_MARK);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\tstruct bpf_reg_state *src_reg = regs + insn->src_reg;\n\t\t\tstruct bpf_reg_state *dst_reg = regs + insn->dst_reg;\n\n\t\t\tif (BPF_CLASS(insn->code) == BPF_ALU64) {\n\t\t\t\t/* case: R1 = R2\n\t\t\t\t * copy register state to dest reg\n\t\t\t\t */\n\t\t\t\t*dst_reg = *src_reg;\n\t\t\t\tdst_reg->live |= REG_LIVE_WRITTEN;\n\t\t\t} else {\n\t\t\t\t/* R1 = (u32) R2 */\n\t\t\t\tif (is_pointer_value(env, insn->src_reg)) {\n\t\t\t\t\tverbose(env,\n\t\t\t\t\t\t\"R%d partial copy of pointer\\n\",\n\t\t\t\t\t\tinsn->src_reg);\n\t\t\t\t\treturn -EACCES;\n\t\t\t\t} else if (src_reg->type == SCALAR_VALUE) {\n\t\t\t\t\t*dst_reg = *src_reg;\n\t\t\t\t\tdst_reg->live |= REG_LIVE_WRITTEN;\n\t\t\t\t} else {\n\t\t\t\t\tmark_reg_unknown(env, regs,\n\t\t\t\t\t\t\t insn->dst_reg);\n\t\t\t\t}\n\t\t\t\tcoerce_reg_to_size(dst_reg, 4);\n\t\t\t}\n\t\t} else {\n\t\t\t/* case: R = imm\n\t\t\t * remember the value we stored into this reg\n\t\t\t */\n\t\t\t/* clear any state __mark_reg_known doesn't set */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tregs[insn->dst_reg].type = SCALAR_VALUE;\n\t\t\tif (BPF_CLASS(insn->code) == BPF_ALU64) {\n\t\t\t\t__mark_reg_known(regs + insn->dst_reg,\n\t\t\t\t\t\t insn->imm);\n\t\t\t} else {\n\t\t\t\t__mark_reg_known(regs + insn->dst_reg,\n\t\t\t\t\t\t (u32)insn->imm);\n\t\t\t}\n\t\t}\n\n\t} else if (opcode > BPF_END) {\n\t\tverbose(env, \"invalid BPF_ALU opcode %x\\n\", opcode);\n\t\treturn -EINVAL;\n\n\t} else {\t/* all other ALU ops: and, sub, xor, add, ... */\n\n\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\tif (insn->imm != 0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_ALU uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\t/* check src1 operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t} else {\n\t\t\tif (insn->src_reg != BPF_REG_0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_ALU uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check src2 operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif ((opcode == BPF_MOD || opcode == BPF_DIV) &&\n\t\t    BPF_SRC(insn->code) == BPF_K && insn->imm == 0) {\n\t\t\tverbose(env, \"div by zero\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif ((opcode == BPF_LSH || opcode == BPF_RSH ||\n\t\t     opcode == BPF_ARSH) && BPF_SRC(insn->code) == BPF_K) {\n\t\t\tint size = BPF_CLASS(insn->code) == BPF_ALU64 ? 64 : 32;\n\n\t\t\tif (insn->imm < 0 || insn->imm >= size) {\n\t\t\t\tverbose(env, \"invalid shift %d\\n\", insn->imm);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check dest operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP_NO_MARK);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\treturn adjust_reg_min_max_vals(env, insn);\n\t}\n\n\treturn 0;\n}\n\nstatic void find_good_pkt_pointers(struct bpf_verifier_state *vstate,\n\t\t\t\t   struct bpf_reg_state *dst_reg,\n\t\t\t\t   enum bpf_reg_type type,\n\t\t\t\t   bool range_right_open)\n{\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *reg;\n\tu16 new_range;\n\tint i, j;\n\n\tif (dst_reg->off < 0 ||\n\t    (dst_reg->off == 0 && range_right_open))\n\t\t/* This doesn't give us any range */\n\t\treturn;\n\n\tif (dst_reg->umax_value > MAX_PACKET_OFF ||\n\t    dst_reg->umax_value + dst_reg->off > MAX_PACKET_OFF)\n\t\t/* Risk of overflow.  For instance, ptr + (1<<63) may be less\n\t\t * than pkt_end, but that's because it's also less than pkt.\n\t\t */\n\t\treturn;\n\n\tnew_range = dst_reg->off;\n\tif (range_right_open)\n\t\tnew_range--;\n\n\t/* Examples for register markings:\n\t *\n\t * pkt_data in dst register:\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (r2 > pkt_end) goto <handle exception>\n\t *   <access okay>\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (r2 < pkt_end) goto <access okay>\n\t *   <handle exception>\n\t *\n\t *   Where:\n\t *     r2 == dst_reg, pkt_end == src_reg\n\t *     r2=pkt(id=n,off=8,r=0)\n\t *     r3=pkt(id=n,off=0,r=0)\n\t *\n\t * pkt_data in src register:\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (pkt_end >= r2) goto <access okay>\n\t *   <handle exception>\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (pkt_end <= r2) goto <handle exception>\n\t *   <access okay>\n\t *\n\t *   Where:\n\t *     pkt_end == dst_reg, r2 == src_reg\n\t *     r2=pkt(id=n,off=8,r=0)\n\t *     r3=pkt(id=n,off=0,r=0)\n\t *\n\t * Find register r3 and mark its range as r3=pkt(id=n,off=0,r=8)\n\t * or r3=pkt(id=n,off=0,r=8-1), so that range of bytes [r3, r3 + 8)\n\t * and [r3, r3 + 8-1) respectively is safe to access depending on\n\t * the check.\n\t */\n\n\t/* If our ids match, then we must have the same max_value.  And we\n\t * don't care about the other reg's fixed offset, since if it's too big\n\t * the range won't allow anything.\n\t * dst_reg->off is known < MAX_PACKET_OFF, therefore it fits in a u16.\n\t */\n\tfor (i = 0; i < MAX_BPF_REG; i++)\n\t\tif (regs[i].type == type && regs[i].id == dst_reg->id)\n\t\t\t/* keep the maximum range already checked */\n\t\t\tregs[i].range = max(regs[i].range, new_range);\n\n\tfor (j = 0; j <= vstate->curframe; j++) {\n\t\tstate = vstate->frame[j];\n\t\tbpf_for_each_spilled_reg(i, state, reg) {\n\t\t\tif (!reg)\n\t\t\t\tcontinue;\n\t\t\tif (reg->type == type && reg->id == dst_reg->id)\n\t\t\t\treg->range = max(reg->range, new_range);\n\t\t}\n\t}\n}\n\n/* compute branch direction of the expression \"if (reg opcode val) goto target;\"\n * and return:\n *  1 - branch will be taken and \"goto target\" will be executed\n *  0 - branch will not be taken and fall-through to next insn\n * -1 - unknown. Example: \"if (reg < 5)\" is unknown when register value range [0,10]\n */\nstatic int is_branch_taken(struct bpf_reg_state *reg, u64 val, u8 opcode)\n{\n\tif (__is_pointer_value(false, reg))\n\t\treturn -1;\n\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\tif (tnum_is_const(reg->var_off))\n\t\t\treturn !!tnum_equals_const(reg->var_off, val);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\tif (tnum_is_const(reg->var_off))\n\t\t\treturn !tnum_equals_const(reg->var_off, val);\n\t\tbreak;\n\tcase BPF_JSET:\n\t\tif ((~reg->var_off.mask & reg->var_off.value) & val)\n\t\t\treturn 1;\n\t\tif (!((reg->var_off.mask | reg->var_off.value) & val))\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JGT:\n\t\tif (reg->umin_value > val)\n\t\t\treturn 1;\n\t\telse if (reg->umax_value <= val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSGT:\n\t\tif (reg->smin_value > (s64)val)\n\t\t\treturn 1;\n\t\telse if (reg->smax_value < (s64)val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JLT:\n\t\tif (reg->umax_value < val)\n\t\t\treturn 1;\n\t\telse if (reg->umin_value >= val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSLT:\n\t\tif (reg->smax_value < (s64)val)\n\t\t\treturn 1;\n\t\telse if (reg->smin_value >= (s64)val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JGE:\n\t\tif (reg->umin_value >= val)\n\t\t\treturn 1;\n\t\telse if (reg->umax_value < val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSGE:\n\t\tif (reg->smin_value >= (s64)val)\n\t\t\treturn 1;\n\t\telse if (reg->smax_value < (s64)val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JLE:\n\t\tif (reg->umax_value <= val)\n\t\t\treturn 1;\n\t\telse if (reg->umin_value > val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSLE:\n\t\tif (reg->smax_value <= (s64)val)\n\t\t\treturn 1;\n\t\telse if (reg->smin_value > (s64)val)\n\t\t\treturn 0;\n\t\tbreak;\n\t}\n\n\treturn -1;\n}\n\n/* Adjusts the register min/max values in the case that the dst_reg is the\n * variable register that we are working on, and src_reg is a constant or we're\n * simply doing a BPF_K check.\n * In JEQ/JNE cases we also adjust the var_off values.\n */\nstatic void reg_set_min_max(struct bpf_reg_state *true_reg,\n\t\t\t    struct bpf_reg_state *false_reg, u64 val,\n\t\t\t    u8 opcode)\n{\n\t/* If the dst_reg is a pointer, we can't learn anything about its\n\t * variable offset from the compare (unless src_reg were a pointer into\n\t * the same object, but we don't bother with that.\n\t * Since false_reg and true_reg have the same type by construction, we\n\t * only need to check one of them for pointerness.\n\t */\n\tif (__is_pointer_value(false, false_reg))\n\t\treturn;\n\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\t/* If this is false then we know nothing Jon Snow, but if it is\n\t\t * true then we know for sure.\n\t\t */\n\t\t__mark_reg_known(true_reg, val);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\t/* If this is true we know nothing Jon Snow, but if it is false\n\t\t * we know the value for sure;\n\t\t */\n\t\t__mark_reg_known(false_reg, val);\n\t\tbreak;\n\tcase BPF_JSET:\n\t\tfalse_reg->var_off = tnum_and(false_reg->var_off,\n\t\t\t\t\t      tnum_const(~val));\n\t\tif (is_power_of_2(val))\n\t\t\ttrue_reg->var_off = tnum_or(true_reg->var_off,\n\t\t\t\t\t\t    tnum_const(val));\n\t\tbreak;\n\tcase BPF_JGT:\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val);\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JSGT:\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val);\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JLT:\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val);\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val - 1);\n\t\tbreak;\n\tcase BPF_JSLT:\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val);\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val - 1);\n\t\tbreak;\n\tcase BPF_JGE:\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val - 1);\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val);\n\t\tbreak;\n\tcase BPF_JSGE:\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val - 1);\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val);\n\t\tbreak;\n\tcase BPF_JLE:\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val + 1);\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val);\n\t\tbreak;\n\tcase BPF_JSLE:\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val + 1);\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t__reg_deduce_bounds(false_reg);\n\t__reg_deduce_bounds(true_reg);\n\t/* We might have learned some bits from the bounds. */\n\t__reg_bound_offset(false_reg);\n\t__reg_bound_offset(true_reg);\n\t/* Intersecting with the old var_off might have improved our bounds\n\t * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),\n\t * then new var_off is (0; 0x7f...fc) which improves our umax.\n\t */\n\t__update_reg_bounds(false_reg);\n\t__update_reg_bounds(true_reg);\n}\n\n/* Same as above, but for the case that dst_reg holds a constant and src_reg is\n * the variable reg.\n */\nstatic void reg_set_min_max_inv(struct bpf_reg_state *true_reg,\n\t\t\t\tstruct bpf_reg_state *false_reg, u64 val,\n\t\t\t\tu8 opcode)\n{\n\tif (__is_pointer_value(false, false_reg))\n\t\treturn;\n\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\t/* If this is false then we know nothing Jon Snow, but if it is\n\t\t * true then we know for sure.\n\t\t */\n\t\t__mark_reg_known(true_reg, val);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\t/* If this is true we know nothing Jon Snow, but if it is false\n\t\t * we know the value for sure;\n\t\t */\n\t\t__mark_reg_known(false_reg, val);\n\t\tbreak;\n\tcase BPF_JSET:\n\t\tfalse_reg->var_off = tnum_and(false_reg->var_off,\n\t\t\t\t\t      tnum_const(~val));\n\t\tif (is_power_of_2(val))\n\t\t\ttrue_reg->var_off = tnum_or(true_reg->var_off,\n\t\t\t\t\t\t    tnum_const(val));\n\t\tbreak;\n\tcase BPF_JGT:\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val - 1);\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val);\n\t\tbreak;\n\tcase BPF_JSGT:\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val - 1);\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val);\n\t\tbreak;\n\tcase BPF_JLT:\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val + 1);\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val);\n\t\tbreak;\n\tcase BPF_JSLT:\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val + 1);\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val);\n\t\tbreak;\n\tcase BPF_JGE:\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val);\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JSGE:\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val);\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JLE:\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val);\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val - 1);\n\t\tbreak;\n\tcase BPF_JSLE:\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val);\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val - 1);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t__reg_deduce_bounds(false_reg);\n\t__reg_deduce_bounds(true_reg);\n\t/* We might have learned some bits from the bounds. */\n\t__reg_bound_offset(false_reg);\n\t__reg_bound_offset(true_reg);\n\t/* Intersecting with the old var_off might have improved our bounds\n\t * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),\n\t * then new var_off is (0; 0x7f...fc) which improves our umax.\n\t */\n\t__update_reg_bounds(false_reg);\n\t__update_reg_bounds(true_reg);\n}\n\n/* Regs are known to be equal, so intersect their min/max/var_off */\nstatic void __reg_combine_min_max(struct bpf_reg_state *src_reg,\n\t\t\t\t  struct bpf_reg_state *dst_reg)\n{\n\tsrc_reg->umin_value = dst_reg->umin_value = max(src_reg->umin_value,\n\t\t\t\t\t\t\tdst_reg->umin_value);\n\tsrc_reg->umax_value = dst_reg->umax_value = min(src_reg->umax_value,\n\t\t\t\t\t\t\tdst_reg->umax_value);\n\tsrc_reg->smin_value = dst_reg->smin_value = max(src_reg->smin_value,\n\t\t\t\t\t\t\tdst_reg->smin_value);\n\tsrc_reg->smax_value = dst_reg->smax_value = min(src_reg->smax_value,\n\t\t\t\t\t\t\tdst_reg->smax_value);\n\tsrc_reg->var_off = dst_reg->var_off = tnum_intersect(src_reg->var_off,\n\t\t\t\t\t\t\t     dst_reg->var_off);\n\t/* We might have learned new bounds from the var_off. */\n\t__update_reg_bounds(src_reg);\n\t__update_reg_bounds(dst_reg);\n\t/* We might have learned something about the sign bit. */\n\t__reg_deduce_bounds(src_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t/* We might have learned some bits from the bounds. */\n\t__reg_bound_offset(src_reg);\n\t__reg_bound_offset(dst_reg);\n\t/* Intersecting with the old var_off might have improved our bounds\n\t * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),\n\t * then new var_off is (0; 0x7f...fc) which improves our umax.\n\t */\n\t__update_reg_bounds(src_reg);\n\t__update_reg_bounds(dst_reg);\n}\n\nstatic void reg_combine_min_max(struct bpf_reg_state *true_src,\n\t\t\t\tstruct bpf_reg_state *true_dst,\n\t\t\t\tstruct bpf_reg_state *false_src,\n\t\t\t\tstruct bpf_reg_state *false_dst,\n\t\t\t\tu8 opcode)\n{\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\t__reg_combine_min_max(true_src, true_dst);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\t__reg_combine_min_max(false_src, false_dst);\n\t\tbreak;\n\t}\n}\n\nstatic void mark_ptr_or_null_reg(struct bpf_func_state *state,\n\t\t\t\t struct bpf_reg_state *reg, u32 id,\n\t\t\t\t bool is_null)\n{\n\tif (reg_type_may_be_null(reg->type) && reg->id == id) {\n\t\t/* Old offset (both fixed and variable parts) should\n\t\t * have been known-zero, because we don't allow pointer\n\t\t * arithmetic on pointers that might be NULL.\n\t\t */\n\t\tif (WARN_ON_ONCE(reg->smin_value || reg->smax_value ||\n\t\t\t\t !tnum_equals_const(reg->var_off, 0) ||\n\t\t\t\t reg->off)) {\n\t\t\t__mark_reg_known_zero(reg);\n\t\t\treg->off = 0;\n\t\t}\n\t\tif (is_null) {\n\t\t\treg->type = SCALAR_VALUE;\n\t\t} else if (reg->type == PTR_TO_MAP_VALUE_OR_NULL) {\n\t\t\tif (reg->map_ptr->inner_map_meta) {\n\t\t\t\treg->type = CONST_PTR_TO_MAP;\n\t\t\t\treg->map_ptr = reg->map_ptr->inner_map_meta;\n\t\t\t} else {\n\t\t\t\treg->type = PTR_TO_MAP_VALUE;\n\t\t\t}\n\t\t} else if (reg->type == PTR_TO_SOCKET_OR_NULL) {\n\t\t\treg->type = PTR_TO_SOCKET;\n\t\t}\n\t\tif (is_null || !reg_is_refcounted(reg)) {\n\t\t\t/* We don't need id from this point onwards anymore,\n\t\t\t * thus we should better reset it, so that state\n\t\t\t * pruning has chances to take effect.\n\t\t\t */\n\t\t\treg->id = 0;\n\t\t}\n\t}\n}\n\n/* The logic is similar to find_good_pkt_pointers(), both could eventually\n * be folded together at some point.\n */\nstatic void mark_ptr_or_null_regs(struct bpf_verifier_state *vstate, u32 regno,\n\t\t\t\t  bool is_null)\n{\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *reg, *regs = state->regs;\n\tu32 id = regs[regno].id;\n\tint i, j;\n\n\tif (reg_is_refcounted_or_null(&regs[regno]) && is_null)\n\t\t__release_reference_state(state, id);\n\n\tfor (i = 0; i < MAX_BPF_REG; i++)\n\t\tmark_ptr_or_null_reg(state, &regs[i], id, is_null);\n\n\tfor (j = 0; j <= vstate->curframe; j++) {\n\t\tstate = vstate->frame[j];\n\t\tbpf_for_each_spilled_reg(i, state, reg) {\n\t\t\tif (!reg)\n\t\t\t\tcontinue;\n\t\t\tmark_ptr_or_null_reg(state, reg, id, is_null);\n\t\t}\n\t}\n}\n\nstatic bool try_match_pkt_pointers(const struct bpf_insn *insn,\n\t\t\t\t   struct bpf_reg_state *dst_reg,\n\t\t\t\t   struct bpf_reg_state *src_reg,\n\t\t\t\t   struct bpf_verifier_state *this_branch,\n\t\t\t\t   struct bpf_verifier_state *other_branch)\n{\n\tif (BPF_SRC(insn->code) != BPF_X)\n\t\treturn false;\n\n\tswitch (BPF_OP(insn->code)) {\n\tcase BPF_JGT:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' > pkt_end, pkt_meta' > pkt_data */\n\t\t\tfind_good_pkt_pointers(this_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, false);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end > pkt_data', pkt_data > pkt_meta' */\n\t\t\tfind_good_pkt_pointers(other_branch, src_reg,\n\t\t\t\t\t       src_reg->type, true);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase BPF_JLT:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' < pkt_end, pkt_meta' < pkt_data */\n\t\t\tfind_good_pkt_pointers(other_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, true);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end < pkt_data', pkt_data > pkt_meta' */\n\t\t\tfind_good_pkt_pointers(this_branch, src_reg,\n\t\t\t\t\t       src_reg->type, false);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase BPF_JGE:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' >= pkt_end, pkt_meta' >= pkt_data */\n\t\t\tfind_good_pkt_pointers(this_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, true);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end >= pkt_data', pkt_data >= pkt_meta' */\n\t\t\tfind_good_pkt_pointers(other_branch, src_reg,\n\t\t\t\t\t       src_reg->type, false);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase BPF_JLE:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' <= pkt_end, pkt_meta' <= pkt_data */\n\t\t\tfind_good_pkt_pointers(other_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, false);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end <= pkt_data', pkt_data <= pkt_meta' */\n\t\t\tfind_good_pkt_pointers(this_branch, src_reg,\n\t\t\t\t\t       src_reg->type, true);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic int check_cond_jmp_op(struct bpf_verifier_env *env,\n\t\t\t     struct bpf_insn *insn, int *insn_idx)\n{\n\tstruct bpf_verifier_state *this_branch = env->cur_state;\n\tstruct bpf_verifier_state *other_branch;\n\tstruct bpf_reg_state *regs = this_branch->frame[this_branch->curframe]->regs;\n\tstruct bpf_reg_state *dst_reg, *other_branch_regs;\n\tu8 opcode = BPF_OP(insn->code);\n\tint err;\n\n\tif (opcode > BPF_JSLE) {\n\t\tverbose(env, \"invalid BPF_JMP opcode %x\\n\", opcode);\n\t\treturn -EINVAL;\n\t}\n\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tif (insn->imm != 0) {\n\t\t\tverbose(env, \"BPF_JMP uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/* check src1 operand */\n\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (is_pointer_value(env, insn->src_reg)) {\n\t\t\tverbose(env, \"R%d pointer comparison prohibited\\n\",\n\t\t\t\tinsn->src_reg);\n\t\t\treturn -EACCES;\n\t\t}\n\t} else {\n\t\tif (insn->src_reg != BPF_REG_0) {\n\t\t\tverbose(env, \"BPF_JMP uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t/* check src2 operand */\n\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tdst_reg = &regs[insn->dst_reg];\n\n\tif (BPF_SRC(insn->code) == BPF_K) {\n\t\tint pred = is_branch_taken(dst_reg, insn->imm, opcode);\n\n\t\tif (pred == 1) {\n\t\t\t /* only follow the goto, ignore fall-through */\n\t\t\t*insn_idx += insn->off;\n\t\t\treturn 0;\n\t\t} else if (pred == 0) {\n\t\t\t/* only follow fall-through branch, since\n\t\t\t * that's where the program will go\n\t\t\t */\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tother_branch = push_stack(env, *insn_idx + insn->off + 1, *insn_idx,\n\t\t\t\t  false);\n\tif (!other_branch)\n\t\treturn -EFAULT;\n\tother_branch_regs = other_branch->frame[other_branch->curframe]->regs;\n\n\t/* detect if we are comparing against a constant value so we can adjust\n\t * our min/max values for our dst register.\n\t * this is only legit if both are scalars (or pointers to the same\n\t * object, I suppose, but we don't support that right now), because\n\t * otherwise the different base pointers mean the offsets aren't\n\t * comparable.\n\t */\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tif (dst_reg->type == SCALAR_VALUE &&\n\t\t    regs[insn->src_reg].type == SCALAR_VALUE) {\n\t\t\tif (tnum_is_const(regs[insn->src_reg].var_off))\n\t\t\t\treg_set_min_max(&other_branch_regs[insn->dst_reg],\n\t\t\t\t\t\tdst_reg, regs[insn->src_reg].var_off.value,\n\t\t\t\t\t\topcode);\n\t\t\telse if (tnum_is_const(dst_reg->var_off))\n\t\t\t\treg_set_min_max_inv(&other_branch_regs[insn->src_reg],\n\t\t\t\t\t\t    &regs[insn->src_reg],\n\t\t\t\t\t\t    dst_reg->var_off.value, opcode);\n\t\t\telse if (opcode == BPF_JEQ || opcode == BPF_JNE)\n\t\t\t\t/* Comparing for equality, we can combine knowledge */\n\t\t\t\treg_combine_min_max(&other_branch_regs[insn->src_reg],\n\t\t\t\t\t\t    &other_branch_regs[insn->dst_reg],\n\t\t\t\t\t\t    &regs[insn->src_reg],\n\t\t\t\t\t\t    &regs[insn->dst_reg], opcode);\n\t\t}\n\t} else if (dst_reg->type == SCALAR_VALUE) {\n\t\treg_set_min_max(&other_branch_regs[insn->dst_reg],\n\t\t\t\t\tdst_reg, insn->imm, opcode);\n\t}\n\n\t/* detect if R == 0 where R is returned from bpf_map_lookup_elem() */\n\tif (BPF_SRC(insn->code) == BPF_K &&\n\t    insn->imm == 0 && (opcode == BPF_JEQ || opcode == BPF_JNE) &&\n\t    reg_type_may_be_null(dst_reg->type)) {\n\t\t/* Mark all identical registers in each branch as either\n\t\t * safe or unknown depending R == 0 or R != 0 conditional.\n\t\t */\n\t\tmark_ptr_or_null_regs(this_branch, insn->dst_reg,\n\t\t\t\t      opcode == BPF_JNE);\n\t\tmark_ptr_or_null_regs(other_branch, insn->dst_reg,\n\t\t\t\t      opcode == BPF_JEQ);\n\t} else if (!try_match_pkt_pointers(insn, dst_reg, &regs[insn->src_reg],\n\t\t\t\t\t   this_branch, other_branch) &&\n\t\t   is_pointer_value(env, insn->dst_reg)) {\n\t\tverbose(env, \"R%d pointer comparison prohibited\\n\",\n\t\t\tinsn->dst_reg);\n\t\treturn -EACCES;\n\t}\n\tif (env->log.level)\n\t\tprint_verifier_state(env, this_branch->frame[this_branch->curframe]);\n\treturn 0;\n}\n\n/* return the map pointer stored inside BPF_LD_IMM64 instruction */\nstatic struct bpf_map *ld_imm64_to_map_ptr(struct bpf_insn *insn)\n{\n\tu64 imm64 = ((u64) (u32) insn[0].imm) | ((u64) (u32) insn[1].imm) << 32;\n\n\treturn (struct bpf_map *) (unsigned long) imm64;\n}\n\n/* verify BPF_LD_IMM64 instruction */\nstatic int check_ld_imm(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tint err;\n\n\tif (BPF_SIZE(insn->code) != BPF_DW) {\n\t\tverbose(env, \"invalid BPF_LD_IMM insn\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (insn->off != 0) {\n\t\tverbose(env, \"BPF_LD_IMM64 uses reserved fields\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\terr = check_reg_arg(env, insn->dst_reg, DST_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (insn->src_reg == 0) {\n\t\tu64 imm = ((u64)(insn + 1)->imm << 32) | (u32)insn->imm;\n\n\t\tregs[insn->dst_reg].type = SCALAR_VALUE;\n\t\t__mark_reg_known(&regs[insn->dst_reg], imm);\n\t\treturn 0;\n\t}\n\n\t/* replace_map_fd_with_map_ptr() should have caught bad ld_imm64 */\n\tBUG_ON(insn->src_reg != BPF_PSEUDO_MAP_FD);\n\n\tregs[insn->dst_reg].type = CONST_PTR_TO_MAP;\n\tregs[insn->dst_reg].map_ptr = ld_imm64_to_map_ptr(insn);\n\treturn 0;\n}\n\nstatic bool may_access_skb(enum bpf_prog_type type)\n{\n\tswitch (type) {\n\tcase BPF_PROG_TYPE_SOCKET_FILTER:\n\tcase BPF_PROG_TYPE_SCHED_CLS:\n\tcase BPF_PROG_TYPE_SCHED_ACT:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n/* verify safety of LD_ABS|LD_IND instructions:\n * - they can only appear in the programs where ctx == skb\n * - since they are wrappers of function calls, they scratch R1-R5 registers,\n *   preserve R6-R9, and store return value into R0\n *\n * Implicit input:\n *   ctx == skb == R6 == CTX\n *\n * Explicit input:\n *   SRC == any register\n *   IMM == 32-bit immediate\n *\n * Output:\n *   R0 - 8/16/32-bit skb data converted to cpu endianness\n */\nstatic int check_ld_abs(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 mode = BPF_MODE(insn->code);\n\tint i, err;\n\n\tif (!may_access_skb(env->prog->type)) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] instructions not allowed for this program type\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!env->ops->gen_ld_abs) {\n\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (env->subprog_cnt > 1) {\n\t\t/* when program has LD_ABS insn JITs and interpreter assume\n\t\t * that r1 == ctx == skb which is not the case for callees\n\t\t * that can have arbitrary arguments. It's problematic\n\t\t * for main prog as well since JITs would need to analyze\n\t\t * all functions in order to make proper register save/restore\n\t\t * decisions in the main prog. Hence disallow LD_ABS with calls\n\t\t */\n\t\tverbose(env, \"BPF_LD_[ABS|IND] instructions cannot be mixed with bpf-to-bpf calls\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (insn->dst_reg != BPF_REG_0 || insn->off != 0 ||\n\t    BPF_SIZE(insn->code) == BPF_DW ||\n\t    (mode == BPF_ABS && insn->src_reg != BPF_REG_0)) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] uses reserved fields\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* check whether implicit source operand (register R6) is readable */\n\terr = check_reg_arg(env, BPF_REG_6, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\t/* Disallow usage of BPF_LD_[ABS|IND] with reference tracking, as\n\t * gen_ld_abs() may terminate the program at runtime, leading to\n\t * reference leak.\n\t */\n\terr = check_reference_leak(env);\n\tif (err) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] cannot be mixed with socket references\\n\");\n\t\treturn err;\n\t}\n\n\tif (regs[BPF_REG_6].type != PTR_TO_CTX) {\n\t\tverbose(env,\n\t\t\t\"at the time of BPF_LD_ABS|IND R6 != pointer to skb\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (mode == BPF_IND) {\n\t\t/* check explicit source operand */\n\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\t/* reset caller saved regs to unreadable */\n\tfor (i = 0; i < CALLER_SAVED_REGS; i++) {\n\t\tmark_reg_not_init(env, regs, caller_saved[i]);\n\t\tcheck_reg_arg(env, caller_saved[i], DST_OP_NO_MARK);\n\t}\n\n\t/* mark destination R0 register as readable, since it contains\n\t * the value fetched from the packet.\n\t * Already marked as written above.\n\t */\n\tmark_reg_unknown(env, regs, BPF_REG_0);\n\treturn 0;\n}\n\nstatic int check_return_code(struct bpf_verifier_env *env)\n{\n\tstruct bpf_reg_state *reg;\n\tstruct tnum range = tnum_range(0, 1);\n\n\tswitch (env->prog->type) {\n\tcase BPF_PROG_TYPE_CGROUP_SKB:\n\tcase BPF_PROG_TYPE_CGROUP_SOCK:\n\tcase BPF_PROG_TYPE_CGROUP_SOCK_ADDR:\n\tcase BPF_PROG_TYPE_SOCK_OPS:\n\tcase BPF_PROG_TYPE_CGROUP_DEVICE:\n\t\tbreak;\n\tdefault:\n\t\treturn 0;\n\t}\n\n\treg = cur_regs(env) + BPF_REG_0;\n\tif (reg->type != SCALAR_VALUE) {\n\t\tverbose(env, \"At program exit the register R0 is not a known value (%s)\\n\",\n\t\t\treg_type_str[reg->type]);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!tnum_in(range, reg->var_off)) {\n\t\tverbose(env, \"At program exit the register R0 \");\n\t\tif (!tnum_is_unknown(reg->var_off)) {\n\t\t\tchar tn_buf[48];\n\n\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\tverbose(env, \"has value %s\", tn_buf);\n\t\t} else {\n\t\t\tverbose(env, \"has unknown scalar value\");\n\t\t}\n\t\tverbose(env, \" should have been 0 or 1\\n\");\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\n/* non-recursive DFS pseudo code\n * 1  procedure DFS-iterative(G,v):\n * 2      label v as discovered\n * 3      let S be a stack\n * 4      S.push(v)\n * 5      while S is not empty\n * 6            t <- S.pop()\n * 7            if t is what we're looking for:\n * 8                return t\n * 9            for all edges e in G.adjacentEdges(t) do\n * 10               if edge e is already labelled\n * 11                   continue with the next edge\n * 12               w <- G.adjacentVertex(t,e)\n * 13               if vertex w is not discovered and not explored\n * 14                   label e as tree-edge\n * 15                   label w as discovered\n * 16                   S.push(w)\n * 17                   continue at 5\n * 18               else if vertex w is discovered\n * 19                   label e as back-edge\n * 20               else\n * 21                   // vertex w is explored\n * 22                   label e as forward- or cross-edge\n * 23           label t as explored\n * 24           S.pop()\n *\n * convention:\n * 0x10 - discovered\n * 0x11 - discovered and fall-through edge labelled\n * 0x12 - discovered and fall-through and branch edges labelled\n * 0x20 - explored\n */\n\nenum {\n\tDISCOVERED = 0x10,\n\tEXPLORED = 0x20,\n\tFALLTHROUGH = 1,\n\tBRANCH = 2,\n};\n\n#define STATE_LIST_MARK ((struct bpf_verifier_state_list *) -1L)\n\nstatic int *insn_stack;\t/* stack of insns to process */\nstatic int cur_stack;\t/* current stack index */\nstatic int *insn_state;\n\n/* t, w, e - match pseudo-code above:\n * t - index of current instruction\n * w - next instruction\n * e - edge\n */\nstatic int push_insn(int t, int w, int e, struct bpf_verifier_env *env)\n{\n\tif (e == FALLTHROUGH && insn_state[t] >= (DISCOVERED | FALLTHROUGH))\n\t\treturn 0;\n\n\tif (e == BRANCH && insn_state[t] >= (DISCOVERED | BRANCH))\n\t\treturn 0;\n\n\tif (w < 0 || w >= env->prog->len) {\n\t\tverbose_linfo(env, t, \"%d: \", t);\n\t\tverbose(env, \"jump out of range from insn %d to %d\\n\", t, w);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e == BRANCH)\n\t\t/* mark branch target for state pruning */\n\t\tenv->explored_states[w] = STATE_LIST_MARK;\n\n\tif (insn_state[w] == 0) {\n\t\t/* tree-edge */\n\t\tinsn_state[t] = DISCOVERED | e;\n\t\tinsn_state[w] = DISCOVERED;\n\t\tif (cur_stack >= env->prog->len)\n\t\t\treturn -E2BIG;\n\t\tinsn_stack[cur_stack++] = w;\n\t\treturn 1;\n\t} else if ((insn_state[w] & 0xF0) == DISCOVERED) {\n\t\tverbose_linfo(env, t, \"%d: \", t);\n\t\tverbose_linfo(env, w, \"%d: \", w);\n\t\tverbose(env, \"back-edge from insn %d to %d\\n\", t, w);\n\t\treturn -EINVAL;\n\t} else if (insn_state[w] == EXPLORED) {\n\t\t/* forward- or cross-edge */\n\t\tinsn_state[t] = DISCOVERED | e;\n\t} else {\n\t\tverbose(env, \"insn state internal bug\\n\");\n\t\treturn -EFAULT;\n\t}\n\treturn 0;\n}\n\n/* non-recursive depth-first-search to detect loops in BPF program\n * loop == back-edge in directed graph\n */\nstatic int check_cfg(struct bpf_verifier_env *env)\n{\n\tstruct bpf_insn *insns = env->prog->insnsi;\n\tint insn_cnt = env->prog->len;\n\tint ret = 0;\n\tint i, t;\n\n\tinsn_state = kcalloc(insn_cnt, sizeof(int), GFP_KERNEL);\n\tif (!insn_state)\n\t\treturn -ENOMEM;\n\n\tinsn_stack = kcalloc(insn_cnt, sizeof(int), GFP_KERNEL);\n\tif (!insn_stack) {\n\t\tkfree(insn_state);\n\t\treturn -ENOMEM;\n\t}\n\n\tinsn_state[0] = DISCOVERED; /* mark 1st insn as discovered */\n\tinsn_stack[0] = 0; /* 0 is the first instruction */\n\tcur_stack = 1;\n\npeek_stack:\n\tif (cur_stack == 0)\n\t\tgoto check_state;\n\tt = insn_stack[cur_stack - 1];\n\n\tif (BPF_CLASS(insns[t].code) == BPF_JMP) {\n\t\tu8 opcode = BPF_OP(insns[t].code);\n\n\t\tif (opcode == BPF_EXIT) {\n\t\t\tgoto mark_explored;\n\t\t} else if (opcode == BPF_CALL) {\n\t\t\tret = push_insn(t, t + 1, FALLTHROUGH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t\tif (t + 1 < insn_cnt)\n\t\t\t\tenv->explored_states[t + 1] = STATE_LIST_MARK;\n\t\t\tif (insns[t].src_reg == BPF_PSEUDO_CALL) {\n\t\t\t\tenv->explored_states[t] = STATE_LIST_MARK;\n\t\t\t\tret = push_insn(t, t + insns[t].imm + 1, BRANCH, env);\n\t\t\t\tif (ret == 1)\n\t\t\t\t\tgoto peek_stack;\n\t\t\t\telse if (ret < 0)\n\t\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t} else if (opcode == BPF_JA) {\n\t\t\tif (BPF_SRC(insns[t].code) != BPF_K) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t\t/* unconditional jump with single edge */\n\t\t\tret = push_insn(t, t + insns[t].off + 1,\n\t\t\t\t\tFALLTHROUGH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t\t/* tell verifier to check for equivalent states\n\t\t\t * after every call and jump\n\t\t\t */\n\t\t\tif (t + 1 < insn_cnt)\n\t\t\t\tenv->explored_states[t + 1] = STATE_LIST_MARK;\n\t\t} else {\n\t\t\t/* conditional jump with two edges */\n\t\t\tenv->explored_states[t] = STATE_LIST_MARK;\n\t\t\tret = push_insn(t, t + 1, FALLTHROUGH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\n\t\t\tret = push_insn(t, t + insns[t].off + 1, BRANCH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t}\n\t} else {\n\t\t/* all other non-branch instructions with single\n\t\t * fall-through edge\n\t\t */\n\t\tret = push_insn(t, t + 1, FALLTHROUGH, env);\n\t\tif (ret == 1)\n\t\t\tgoto peek_stack;\n\t\telse if (ret < 0)\n\t\t\tgoto err_free;\n\t}\n\nmark_explored:\n\tinsn_state[t] = EXPLORED;\n\tif (cur_stack-- <= 0) {\n\t\tverbose(env, \"pop stack internal bug\\n\");\n\t\tret = -EFAULT;\n\t\tgoto err_free;\n\t}\n\tgoto peek_stack;\n\ncheck_state:\n\tfor (i = 0; i < insn_cnt; i++) {\n\t\tif (insn_state[i] != EXPLORED) {\n\t\t\tverbose(env, \"unreachable insn %d\\n\", i);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\t}\n\tret = 0; /* cfg looks good */\n\nerr_free:\n\tkfree(insn_state);\n\tkfree(insn_stack);\n\treturn ret;\n}\n\n/* The minimum supported BTF func info size */\n#define MIN_BPF_FUNCINFO_SIZE\t8\n#define MAX_FUNCINFO_REC_SIZE\t252\n\nstatic int check_btf_func(struct bpf_verifier_env *env,\n\t\t\t  const union bpf_attr *attr,\n\t\t\t  union bpf_attr __user *uattr)\n{\n\tu32 i, nfuncs, urec_size, min_size, prev_offset;\n\tu32 krec_size = sizeof(struct bpf_func_info);\n\tstruct bpf_func_info *krecord;\n\tconst struct btf_type *type;\n\tstruct bpf_prog *prog;\n\tconst struct btf *btf;\n\tvoid __user *urecord;\n\tint ret = 0;\n\n\tnfuncs = attr->func_info_cnt;\n\tif (!nfuncs)\n\t\treturn 0;\n\n\tif (nfuncs != env->subprog_cnt) {\n\t\tverbose(env, \"number of funcs in func_info doesn't match number of subprogs\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\turec_size = attr->func_info_rec_size;\n\tif (urec_size < MIN_BPF_FUNCINFO_SIZE ||\n\t    urec_size > MAX_FUNCINFO_REC_SIZE ||\n\t    urec_size % sizeof(u32)) {\n\t\tverbose(env, \"invalid func info rec size %u\\n\", urec_size);\n\t\treturn -EINVAL;\n\t}\n\n\tprog = env->prog;\n\tbtf = prog->aux->btf;\n\n\turecord = u64_to_user_ptr(attr->func_info);\n\tmin_size = min_t(u32, krec_size, urec_size);\n\n\tkrecord = kvcalloc(nfuncs, krec_size, GFP_KERNEL | __GFP_NOWARN);\n\tif (!krecord)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < nfuncs; i++) {\n\t\tret = bpf_check_uarg_tail_zero(urecord, krec_size, urec_size);\n\t\tif (ret) {\n\t\t\tif (ret == -E2BIG) {\n\t\t\t\tverbose(env, \"nonzero tailing record in func info\");\n\t\t\t\t/* set the size kernel expects so loader can zero\n\t\t\t\t * out the rest of the record.\n\t\t\t\t */\n\t\t\t\tif (put_user(min_size, &uattr->func_info_rec_size))\n\t\t\t\t\tret = -EFAULT;\n\t\t\t}\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (copy_from_user(&krecord[i], urecord, min_size)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/* check insn_off */\n\t\tif (i == 0) {\n\t\t\tif (krecord[i].insn_off) {\n\t\t\t\tverbose(env,\n\t\t\t\t\t\"nonzero insn_off %u for the first func info record\",\n\t\t\t\t\tkrecord[i].insn_off);\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t} else if (krecord[i].insn_off <= prev_offset) {\n\t\t\tverbose(env,\n\t\t\t\t\"same or smaller insn offset (%u) than previous func info record (%u)\",\n\t\t\t\tkrecord[i].insn_off, prev_offset);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (env->subprog_info[i].start != krecord[i].insn_off) {\n\t\t\tverbose(env, \"func_info BTF section doesn't match subprog layout in BPF program\\n\");\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/* check type_id */\n\t\ttype = btf_type_by_id(btf, krecord[i].type_id);\n\t\tif (!type || BTF_INFO_KIND(type->info) != BTF_KIND_FUNC) {\n\t\t\tverbose(env, \"invalid type id %d in func info\",\n\t\t\t\tkrecord[i].type_id);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tprev_offset = krecord[i].insn_off;\n\t\turecord += urec_size;\n\t}\n\n\tprog->aux->func_info = krecord;\n\tprog->aux->func_info_cnt = nfuncs;\n\treturn 0;\n\nerr_free:\n\tkvfree(krecord);\n\treturn ret;\n}\n\nstatic void adjust_btf_func(struct bpf_verifier_env *env)\n{\n\tint i;\n\n\tif (!env->prog->aux->func_info)\n\t\treturn;\n\n\tfor (i = 0; i < env->subprog_cnt; i++)\n\t\tenv->prog->aux->func_info[i].insn_off = env->subprog_info[i].start;\n}\n\n#define MIN_BPF_LINEINFO_SIZE\t(offsetof(struct bpf_line_info, line_col) + \\\n\t\tsizeof(((struct bpf_line_info *)(0))->line_col))\n#define MAX_LINEINFO_REC_SIZE\tMAX_FUNCINFO_REC_SIZE\n\nstatic int check_btf_line(struct bpf_verifier_env *env,\n\t\t\t  const union bpf_attr *attr,\n\t\t\t  union bpf_attr __user *uattr)\n{\n\tu32 i, s, nr_linfo, ncopy, expected_size, rec_size, prev_offset = 0;\n\tstruct bpf_subprog_info *sub;\n\tstruct bpf_line_info *linfo;\n\tstruct bpf_prog *prog;\n\tconst struct btf *btf;\n\tvoid __user *ulinfo;\n\tint err;\n\n\tnr_linfo = attr->line_info_cnt;\n\tif (!nr_linfo)\n\t\treturn 0;\n\n\trec_size = attr->line_info_rec_size;\n\tif (rec_size < MIN_BPF_LINEINFO_SIZE ||\n\t    rec_size > MAX_LINEINFO_REC_SIZE ||\n\t    rec_size & (sizeof(u32) - 1))\n\t\treturn -EINVAL;\n\n\t/* Need to zero it in case the userspace may\n\t * pass in a smaller bpf_line_info object.\n\t */\n\tlinfo = kvcalloc(nr_linfo, sizeof(struct bpf_line_info),\n\t\t\t GFP_KERNEL | __GFP_NOWARN);\n\tif (!linfo)\n\t\treturn -ENOMEM;\n\n\tprog = env->prog;\n\tbtf = prog->aux->btf;\n\n\ts = 0;\n\tsub = env->subprog_info;\n\tulinfo = u64_to_user_ptr(attr->line_info);\n\texpected_size = sizeof(struct bpf_line_info);\n\tncopy = min_t(u32, expected_size, rec_size);\n\tfor (i = 0; i < nr_linfo; i++) {\n\t\terr = bpf_check_uarg_tail_zero(ulinfo, expected_size, rec_size);\n\t\tif (err) {\n\t\t\tif (err == -E2BIG) {\n\t\t\t\tverbose(env, \"nonzero tailing record in line_info\");\n\t\t\t\tif (put_user(expected_size,\n\t\t\t\t\t     &uattr->line_info_rec_size))\n\t\t\t\t\terr = -EFAULT;\n\t\t\t}\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (copy_from_user(&linfo[i], ulinfo, ncopy)) {\n\t\t\terr = -EFAULT;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/*\n\t\t * Check insn_off to ensure\n\t\t * 1) strictly increasing AND\n\t\t * 2) bounded by prog->len\n\t\t *\n\t\t * The linfo[0].insn_off == 0 check logically falls into\n\t\t * the later \"missing bpf_line_info for func...\" case\n\t\t * because the first linfo[0].insn_off must be the\n\t\t * first sub also and the first sub must have\n\t\t * subprog_info[0].start == 0.\n\t\t */\n\t\tif ((i && linfo[i].insn_off <= prev_offset) ||\n\t\t    linfo[i].insn_off >= prog->len) {\n\t\t\tverbose(env, \"Invalid line_info[%u].insn_off:%u (prev_offset:%u prog->len:%u)\\n\",\n\t\t\t\ti, linfo[i].insn_off, prev_offset,\n\t\t\t\tprog->len);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (!prog->insnsi[linfo[i].insn_off].code) {\n\t\t\tverbose(env,\n\t\t\t\t\"Invalid insn code at line_info[%u].insn_off\\n\",\n\t\t\t\ti);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (!btf_name_by_offset(btf, linfo[i].line_off) ||\n\t\t    !btf_name_by_offset(btf, linfo[i].file_name_off)) {\n\t\t\tverbose(env, \"Invalid line_info[%u].line_off or .file_name_off\\n\", i);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (s != env->subprog_cnt) {\n\t\t\tif (linfo[i].insn_off == sub[s].start) {\n\t\t\t\tsub[s].linfo_idx = i;\n\t\t\t\ts++;\n\t\t\t} else if (sub[s].start < linfo[i].insn_off) {\n\t\t\t\tverbose(env, \"missing bpf_line_info for func#%u\\n\", s);\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t}\n\n\t\tprev_offset = linfo[i].insn_off;\n\t\tulinfo += rec_size;\n\t}\n\n\tif (s != env->subprog_cnt) {\n\t\tverbose(env, \"missing bpf_line_info for %u funcs starting from func#%u\\n\",\n\t\t\tenv->subprog_cnt - s, s);\n\t\terr = -EINVAL;\n\t\tgoto err_free;\n\t}\n\n\tprog->aux->linfo = linfo;\n\tprog->aux->nr_linfo = nr_linfo;\n\n\treturn 0;\n\nerr_free:\n\tkvfree(linfo);\n\treturn err;\n}\n\nstatic int check_btf_info(struct bpf_verifier_env *env,\n\t\t\t  const union bpf_attr *attr,\n\t\t\t  union bpf_attr __user *uattr)\n{\n\tstruct btf *btf;\n\tint err;\n\n\tif (!attr->func_info_cnt && !attr->line_info_cnt)\n\t\treturn 0;\n\n\tbtf = btf_get_by_fd(attr->prog_btf_fd);\n\tif (IS_ERR(btf))\n\t\treturn PTR_ERR(btf);\n\tenv->prog->aux->btf = btf;\n\n\terr = check_btf_func(env, attr, uattr);\n\tif (err)\n\t\treturn err;\n\n\terr = check_btf_line(env, attr, uattr);\n\tif (err)\n\t\treturn err;\n\n\treturn 0;\n}\n\n/* check %cur's range satisfies %old's */\nstatic bool range_within(struct bpf_reg_state *old,\n\t\t\t struct bpf_reg_state *cur)\n{\n\treturn old->umin_value <= cur->umin_value &&\n\t       old->umax_value >= cur->umax_value &&\n\t       old->smin_value <= cur->smin_value &&\n\t       old->smax_value >= cur->smax_value;\n}\n\n/* Maximum number of register states that can exist at once */\n#define ID_MAP_SIZE\t(MAX_BPF_REG + MAX_BPF_STACK / BPF_REG_SIZE)\nstruct idpair {\n\tu32 old;\n\tu32 cur;\n};\n\n/* If in the old state two registers had the same id, then they need to have\n * the same id in the new state as well.  But that id could be different from\n * the old state, so we need to track the mapping from old to new ids.\n * Once we have seen that, say, a reg with old id 5 had new id 9, any subsequent\n * regs with old id 5 must also have new id 9 for the new state to be safe.  But\n * regs with a different old id could still have new id 9, we don't care about\n * that.\n * So we look through our idmap to see if this old id has been seen before.  If\n * so, we require the new id to match; otherwise, we add the id pair to the map.\n */\nstatic bool check_ids(u32 old_id, u32 cur_id, struct idpair *idmap)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < ID_MAP_SIZE; i++) {\n\t\tif (!idmap[i].old) {\n\t\t\t/* Reached an empty slot; haven't seen this id before */\n\t\t\tidmap[i].old = old_id;\n\t\t\tidmap[i].cur = cur_id;\n\t\t\treturn true;\n\t\t}\n\t\tif (idmap[i].old == old_id)\n\t\t\treturn idmap[i].cur == cur_id;\n\t}\n\t/* We ran out of idmap slots, which should be impossible */\n\tWARN_ON_ONCE(1);\n\treturn false;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 3482,
        "critical_vars": [
            "dst",
            "env"
        ],
        "function": "adjust_scalar_min_max_vals",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_d3bd7413e0ca40b60cf60d4003246d067cafdeda_verifier.c.diff",
        "label": "True",
        "function_code": "static int adjust_scalar_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t      struct bpf_insn *insn,\n\t\t\t\t      struct bpf_reg_state *dst_reg,\n\t\t\t\t      struct bpf_reg_state src_reg)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 opcode = BPF_OP(insn->code);\n\tbool src_known, dst_known;\n\ts64 smin_val, smax_val;\n\tu64 umin_val, umax_val;\n\tu64 insn_bitness = (BPF_CLASS(insn->code) == BPF_ALU64) ? 64 : 32;\n\tu32 dst = insn->dst_reg;\n\tint ret;\n\n\tif (insn_bitness == 32) {\n\t\t/* Relevant for 32-bit RSH: Information can propagate towards\n\t\t * LSB, so it isn't sufficient to only truncate the output to\n\t\t * 32 bits.\n\t\t */\n\t\tcoerce_reg_to_size(dst_reg, 4);\n\t\tcoerce_reg_to_size(&src_reg, 4);\n\t}\n\n\tsmin_val = src_reg.smin_value;\n\tsmax_val = src_reg.smax_value;\n\tumin_val = src_reg.umin_value;\n\tumax_val = src_reg.umax_value;\n\tsrc_known = tnum_is_const(src_reg.var_off);\n\tdst_known = tnum_is_const(dst_reg->var_off);\n\n\tif ((src_known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (!src_known &&\n\t    opcode != BPF_ADD && opcode != BPF_SUB && opcode != BPF_AND) {\n\t\t__mark_reg_unknown(dst_reg);\n\t\treturn 0;\n\t}\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\tret = sanitize_val_alu(env, insn);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to add from different pointers or scalars\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\tif (signed_add_overflows(dst_reg->smin_value, smin_val) ||\n\t\t    signed_add_overflows(dst_reg->smax_value, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value += smin_val;\n\t\t\tdst_reg->smax_value += smax_val;\n\t\t}\n\t\tif (dst_reg->umin_value + umin_val < umin_val ||\n\t\t    dst_reg->umax_value + umax_val < umax_val) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value += umin_val;\n\t\t\tdst_reg->umax_value += umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(dst_reg->var_off, src_reg.var_off);\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tret = sanitize_val_alu(env, insn);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to sub from different pointers or scalars\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\tif (signed_sub_overflows(dst_reg->smin_value, smax_val) ||\n\t\t    signed_sub_overflows(dst_reg->smax_value, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value -= smax_val;\n\t\t\tdst_reg->smax_value -= smin_val;\n\t\t}\n\t\tif (dst_reg->umin_value < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value -= umax_val;\n\t\t\tdst_reg->umax_value -= umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(dst_reg->var_off, src_reg.var_off);\n\t\tbreak;\n\tcase BPF_MUL:\n\t\tdst_reg->var_off = tnum_mul(dst_reg->var_off, src_reg.var_off);\n\t\tif (smin_val < 0 || dst_reg->smin_value < 0) {\n\t\t\t/* Ain't nobody got time to multiply that sign */\n\t\t\t__mark_reg_unbounded(dst_reg);\n\t\t\t__update_reg_bounds(dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\t/* Both values are positive, so we can work with unsigned and\n\t\t * copy the result to signed (unless it exceeds S64_MAX).\n\t\t */\n\t\tif (umax_val > U32_MAX || dst_reg->umax_value > U32_MAX) {\n\t\t\t/* Potential overflow, we know nothing */\n\t\t\t__mark_reg_unbounded(dst_reg);\n\t\t\t/* (except what we can learn from the var_off) */\n\t\t\t__update_reg_bounds(dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\tdst_reg->umin_value *= umin_val;\n\t\tdst_reg->umax_value *= umax_val;\n\t\tif (dst_reg->umax_value > S64_MAX) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = dst_reg->umin_value;\n\t\t\tdst_reg->smax_value = dst_reg->umax_value;\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\t\tif (src_known && dst_known) {\n\t\t\t__mark_reg_known(dst_reg, dst_reg->var_off.value &\n\t\t\t\t\t\t  src_reg.var_off.value);\n\t\t\tbreak;\n\t\t}\n\t\t/* We get our minimum from the var_off, since that's inherently\n\t\t * bitwise.  Our maximum is the minimum of the operands' maxima.\n\t\t */\n\t\tdst_reg->var_off = tnum_and(dst_reg->var_off, src_reg.var_off);\n\t\tdst_reg->umin_value = dst_reg->var_off.value;\n\t\tdst_reg->umax_value = min(dst_reg->umax_value, umax_val);\n\t\tif (dst_reg->smin_value < 0 || smin_val < 0) {\n\t\t\t/* Lose signed bounds when ANDing negative numbers,\n\t\t\t * ain't nobody got time for that.\n\t\t\t */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\t/* ANDing two positives gives a positive, so safe to\n\t\t\t * cast result into s64.\n\t\t\t */\n\t\t\tdst_reg->smin_value = dst_reg->umin_value;\n\t\t\tdst_reg->smax_value = dst_reg->umax_value;\n\t\t}\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tcase BPF_OR:\n\t\tif (src_known && dst_known) {\n\t\t\t__mark_reg_known(dst_reg, dst_reg->var_off.value |\n\t\t\t\t\t\t  src_reg.var_off.value);\n\t\t\tbreak;\n\t\t}\n\t\t/* We get our maximum from the var_off, and our minimum is the\n\t\t * maximum of the operands' minima\n\t\t */\n\t\tdst_reg->var_off = tnum_or(dst_reg->var_off, src_reg.var_off);\n\t\tdst_reg->umin_value = max(dst_reg->umin_value, umin_val);\n\t\tdst_reg->umax_value = dst_reg->var_off.value |\n\t\t\t\t      dst_reg->var_off.mask;\n\t\tif (dst_reg->smin_value < 0 || smin_val < 0) {\n\t\t\t/* Lose signed bounds when ORing negative numbers,\n\t\t\t * ain't nobody got time for that.\n\t\t\t */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\t/* ORing two positives gives a positive, so safe to\n\t\t\t * cast result into s64.\n\t\t\t */\n\t\t\tdst_reg->smin_value = dst_reg->umin_value;\n\t\t\tdst_reg->smax_value = dst_reg->umax_value;\n\t\t}\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tcase BPF_LSH:\n\t\tif (umax_val >= insn_bitness) {\n\t\t\t/* Shifts greater than 31 or 63 are undefined.\n\t\t\t * This includes shifts by a negative number.\n\t\t\t */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\t/* We lose all sign bit information (except what we can pick\n\t\t * up from var_off)\n\t\t */\n\t\tdst_reg->smin_value = S64_MIN;\n\t\tdst_reg->smax_value = S64_MAX;\n\t\t/* If we might shift our top bit out, then we know nothing */\n\t\tif (dst_reg->umax_value > 1ULL << (63 - umax_val)) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value <<= umin_val;\n\t\t\tdst_reg->umax_value <<= umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_lshift(dst_reg->var_off, umin_val);\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tcase BPF_RSH:\n\t\tif (umax_val >= insn_bitness) {\n\t\t\t/* Shifts greater than 31 or 63 are undefined.\n\t\t\t * This includes shifts by a negative number.\n\t\t\t */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\t/* BPF_RSH is an unsigned shift.  If the value in dst_reg might\n\t\t * be negative, then either:\n\t\t * 1) src_reg might be zero, so the sign bit of the result is\n\t\t *    unknown, so we lose our signed bounds\n\t\t * 2) it's known negative, thus the unsigned bounds capture the\n\t\t *    signed bounds\n\t\t * 3) the signed bounds cross zero, so they tell us nothing\n\t\t *    about the result\n\t\t * If the value in dst_reg is known nonnegative, then again the\n\t\t * unsigned bounts capture the signed bounds.\n\t\t * Thus, in all cases it suffices to blow away our signed bounds\n\t\t * and rely on inferring new ones from the unsigned bounds and\n\t\t * var_off of the result.\n\t\t */\n\t\tdst_reg->smin_value = S64_MIN;\n\t\tdst_reg->smax_value = S64_MAX;\n\t\tdst_reg->var_off = tnum_rshift(dst_reg->var_off, umin_val);\n\t\tdst_reg->umin_value >>= umax_val;\n\t\tdst_reg->umax_value >>= umin_val;\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tcase BPF_ARSH:\n\t\tif (umax_val >= insn_bitness) {\n\t\t\t/* Shifts greater than 31 or 63 are undefined.\n\t\t\t * This includes shifts by a negative number.\n\t\t\t */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Upon reaching here, src_known is true and\n\t\t * umax_val is equal to umin_val.\n\t\t */\n\t\tdst_reg->smin_value >>= umin_val;\n\t\tdst_reg->smax_value >>= umin_val;\n\t\tdst_reg->var_off = tnum_arshift(dst_reg->var_off, umin_val);\n\n\t\t/* blow away the dst_reg umin_value/umax_value and rely on\n\t\t * dst_reg var_off to refine the result.\n\t\t */\n\t\tdst_reg->umin_value = 0;\n\t\tdst_reg->umax_value = U64_MAX;\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tdefault:\n\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\tbreak;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops are (32,32)->32 */\n\t\tcoerce_reg_to_size(dst_reg, 4);\n\t}\n\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\treturn 0;\n}\n\n/* Handles ALU ops other than BPF_END, BPF_NEG and BPF_MOV: computes new min/max\n * and var_off.\n */\nstatic int adjust_reg_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg, *src_reg;\n\tstruct bpf_reg_state *ptr_reg = NULL, off_reg = {0};\n\tu8 opcode = BPF_OP(insn->code);\n\n\tdst_reg = &regs[insn->dst_reg];\n\tsrc_reg = NULL;\n\tif (dst_reg->type != SCALAR_VALUE)\n\t\tptr_reg = dst_reg;\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tsrc_reg = &regs[insn->src_reg];\n\t\tif (src_reg->type != SCALAR_VALUE) {\n\t\t\tif (dst_reg->type != SCALAR_VALUE) {\n\t\t\t\t/* Combining two pointers by any ALU op yields\n\t\t\t\t * an arbitrary scalar. Disallow all math except\n\t\t\t\t * pointer subtraction\n\t\t\t\t */\n\t\t\t\tif (opcode == BPF_SUB && env->allow_ptr_leaks) {\n\t\t\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t\tverbose(env, \"R%d pointer %s pointer prohibited\\n\",\n\t\t\t\t\tinsn->dst_reg,\n\t\t\t\t\tbpf_alu_string[opcode >> 4]);\n\t\t\t\treturn -EACCES;\n\t\t\t} else {\n\t\t\t\t/* scalar += pointer\n\t\t\t\t * This is legal, but we have to reverse our\n\t\t\t\t * src/dest handling in computing the range\n\t\t\t\t */\n\t\t\t\treturn adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t\t       src_reg, dst_reg);\n\t\t\t}\n\t\t} else if (ptr_reg) {\n\t\t\t/* pointer += scalar */\n\t\t\treturn adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t       dst_reg, src_reg);\n\t\t}\n\t} else {\n\t\t/* Pretend the src is a reg with a known value, since we only\n\t\t * need to be able to read from this state.\n\t\t */\n\t\toff_reg.type = SCALAR_VALUE;\n\t\t__mark_reg_known(&off_reg, insn->imm);\n\t\tsrc_reg = &off_reg;\n\t\tif (ptr_reg) /* pointer += K */\n\t\t\treturn adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t       ptr_reg, src_reg);\n\t}\n\n\t/* Got here implies adding two SCALAR_VALUEs */\n\tif (WARN_ON_ONCE(ptr_reg)) {\n\t\tprint_verifier_state(env, state);\n\t\tverbose(env, \"verifier internal error: unexpected ptr_reg\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (WARN_ON(!src_reg)) {\n\t\tprint_verifier_state(env, state);\n\t\tverbose(env, \"verifier internal error: no src_reg\\n\");\n\t\treturn -EINVAL;\n\t}\n\treturn adjust_scalar_min_max_vals(env, insn, dst_reg, *src_reg);\n}\n\n/* check validity of 32-bit and 64-bit arithmetic operations */\nstatic int check_alu_op(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 opcode = BPF_OP(insn->code);\n\tint err;\n\n\tif (opcode == BPF_END || opcode == BPF_NEG) {\n\t\tif (opcode == BPF_NEG) {\n\t\t\tif (BPF_SRC(insn->code) != 0 ||\n\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t    insn->off != 0 || insn->imm != 0) {\n\t\t\t\tverbose(env, \"BPF_NEG uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else {\n\t\t\tif (insn->src_reg != BPF_REG_0 || insn->off != 0 ||\n\t\t\t    (insn->imm != 16 && insn->imm != 32 && insn->imm != 64) ||\n\t\t\t    BPF_CLASS(insn->code) == BPF_ALU64) {\n\t\t\t\tverbose(env, \"BPF_END uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check src operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (is_pointer_value(env, insn->dst_reg)) {\n\t\t\tverbose(env, \"R%d pointer arithmetic prohibited\\n\",\n\t\t\t\tinsn->dst_reg);\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\t/* check dest operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t} else if (opcode == BPF_MOV) {\n\n\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\tif (insn->imm != 0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_MOV uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\t/* check src operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t} else {\n\t\t\tif (insn->src_reg != BPF_REG_0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_MOV uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check dest operand, mark as required later */\n\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP_NO_MARK);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\tstruct bpf_reg_state *src_reg = regs + insn->src_reg;\n\t\t\tstruct bpf_reg_state *dst_reg = regs + insn->dst_reg;\n\n\t\t\tif (BPF_CLASS(insn->code) == BPF_ALU64) {\n\t\t\t\t/* case: R1 = R2\n\t\t\t\t * copy register state to dest reg\n\t\t\t\t */\n\t\t\t\t*dst_reg = *src_reg;\n\t\t\t\tdst_reg->live |= REG_LIVE_WRITTEN;\n\t\t\t} else {\n\t\t\t\t/* R1 = (u32) R2 */\n\t\t\t\tif (is_pointer_value(env, insn->src_reg)) {\n\t\t\t\t\tverbose(env,\n\t\t\t\t\t\t\"R%d partial copy of pointer\\n\",\n\t\t\t\t\t\tinsn->src_reg);\n\t\t\t\t\treturn -EACCES;\n\t\t\t\t} else if (src_reg->type == SCALAR_VALUE) {\n\t\t\t\t\t*dst_reg = *src_reg;\n\t\t\t\t\tdst_reg->live |= REG_LIVE_WRITTEN;\n\t\t\t\t} else {\n\t\t\t\t\tmark_reg_unknown(env, regs,\n\t\t\t\t\t\t\t insn->dst_reg);\n\t\t\t\t}\n\t\t\t\tcoerce_reg_to_size(dst_reg, 4);\n\t\t\t}\n\t\t} else {\n\t\t\t/* case: R = imm\n\t\t\t * remember the value we stored into this reg\n\t\t\t */\n\t\t\t/* clear any state __mark_reg_known doesn't set */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tregs[insn->dst_reg].type = SCALAR_VALUE;\n\t\t\tif (BPF_CLASS(insn->code) == BPF_ALU64) {\n\t\t\t\t__mark_reg_known(regs + insn->dst_reg,\n\t\t\t\t\t\t insn->imm);\n\t\t\t} else {\n\t\t\t\t__mark_reg_known(regs + insn->dst_reg,\n\t\t\t\t\t\t (u32)insn->imm);\n\t\t\t}\n\t\t}\n\n\t} else if (opcode > BPF_END) {\n\t\tverbose(env, \"invalid BPF_ALU opcode %x\\n\", opcode);\n\t\treturn -EINVAL;\n\n\t} else {\t/* all other ALU ops: and, sub, xor, add, ... */\n\n\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\tif (insn->imm != 0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_ALU uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\t/* check src1 operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t} else {\n\t\t\tif (insn->src_reg != BPF_REG_0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_ALU uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check src2 operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif ((opcode == BPF_MOD || opcode == BPF_DIV) &&\n\t\t    BPF_SRC(insn->code) == BPF_K && insn->imm == 0) {\n\t\t\tverbose(env, \"div by zero\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif ((opcode == BPF_LSH || opcode == BPF_RSH ||\n\t\t     opcode == BPF_ARSH) && BPF_SRC(insn->code) == BPF_K) {\n\t\t\tint size = BPF_CLASS(insn->code) == BPF_ALU64 ? 64 : 32;\n\n\t\t\tif (insn->imm < 0 || insn->imm >= size) {\n\t\t\t\tverbose(env, \"invalid shift %d\\n\", insn->imm);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check dest operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP_NO_MARK);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\treturn adjust_reg_min_max_vals(env, insn);\n\t}\n\n\treturn 0;\n}\n\nstatic void find_good_pkt_pointers(struct bpf_verifier_state *vstate,\n\t\t\t\t   struct bpf_reg_state *dst_reg,\n\t\t\t\t   enum bpf_reg_type type,\n\t\t\t\t   bool range_right_open)\n{\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *reg;\n\tu16 new_range;\n\tint i, j;\n\n\tif (dst_reg->off < 0 ||\n\t    (dst_reg->off == 0 && range_right_open))\n\t\t/* This doesn't give us any range */\n\t\treturn;\n\n\tif (dst_reg->umax_value > MAX_PACKET_OFF ||\n\t    dst_reg->umax_value + dst_reg->off > MAX_PACKET_OFF)\n\t\t/* Risk of overflow.  For instance, ptr + (1<<63) may be less\n\t\t * than pkt_end, but that's because it's also less than pkt.\n\t\t */\n\t\treturn;\n\n\tnew_range = dst_reg->off;\n\tif (range_right_open)\n\t\tnew_range--;\n\n\t/* Examples for register markings:\n\t *\n\t * pkt_data in dst register:\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (r2 > pkt_end) goto <handle exception>\n\t *   <access okay>\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (r2 < pkt_end) goto <access okay>\n\t *   <handle exception>\n\t *\n\t *   Where:\n\t *     r2 == dst_reg, pkt_end == src_reg\n\t *     r2=pkt(id=n,off=8,r=0)\n\t *     r3=pkt(id=n,off=0,r=0)\n\t *\n\t * pkt_data in src register:\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (pkt_end >= r2) goto <access okay>\n\t *   <handle exception>\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (pkt_end <= r2) goto <handle exception>\n\t *   <access okay>\n\t *\n\t *   Where:\n\t *     pkt_end == dst_reg, r2 == src_reg\n\t *     r2=pkt(id=n,off=8,r=0)\n\t *     r3=pkt(id=n,off=0,r=0)\n\t *\n\t * Find register r3 and mark its range as r3=pkt(id=n,off=0,r=8)\n\t * or r3=pkt(id=n,off=0,r=8-1), so that range of bytes [r3, r3 + 8)\n\t * and [r3, r3 + 8-1) respectively is safe to access depending on\n\t * the check.\n\t */\n\n\t/* If our ids match, then we must have the same max_value.  And we\n\t * don't care about the other reg's fixed offset, since if it's too big\n\t * the range won't allow anything.\n\t * dst_reg->off is known < MAX_PACKET_OFF, therefore it fits in a u16.\n\t */\n\tfor (i = 0; i < MAX_BPF_REG; i++)\n\t\tif (regs[i].type == type && regs[i].id == dst_reg->id)\n\t\t\t/* keep the maximum range already checked */\n\t\t\tregs[i].range = max(regs[i].range, new_range);\n\n\tfor (j = 0; j <= vstate->curframe; j++) {\n\t\tstate = vstate->frame[j];\n\t\tbpf_for_each_spilled_reg(i, state, reg) {\n\t\t\tif (!reg)\n\t\t\t\tcontinue;\n\t\t\tif (reg->type == type && reg->id == dst_reg->id)\n\t\t\t\treg->range = max(reg->range, new_range);\n\t\t}\n\t}\n}\n\n/* compute branch direction of the expression \"if (reg opcode val) goto target;\"\n * and return:\n *  1 - branch will be taken and \"goto target\" will be executed\n *  0 - branch will not be taken and fall-through to next insn\n * -1 - unknown. Example: \"if (reg < 5)\" is unknown when register value range [0,10]\n */\nstatic int is_branch_taken(struct bpf_reg_state *reg, u64 val, u8 opcode)\n{\n\tif (__is_pointer_value(false, reg))\n\t\treturn -1;\n\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\tif (tnum_is_const(reg->var_off))\n\t\t\treturn !!tnum_equals_const(reg->var_off, val);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\tif (tnum_is_const(reg->var_off))\n\t\t\treturn !tnum_equals_const(reg->var_off, val);\n\t\tbreak;\n\tcase BPF_JSET:\n\t\tif ((~reg->var_off.mask & reg->var_off.value) & val)\n\t\t\treturn 1;\n\t\tif (!((reg->var_off.mask | reg->var_off.value) & val))\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JGT:\n\t\tif (reg->umin_value > val)\n\t\t\treturn 1;\n\t\telse if (reg->umax_value <= val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSGT:\n\t\tif (reg->smin_value > (s64)val)\n\t\t\treturn 1;\n\t\telse if (reg->smax_value < (s64)val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JLT:\n\t\tif (reg->umax_value < val)\n\t\t\treturn 1;\n\t\telse if (reg->umin_value >= val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSLT:\n\t\tif (reg->smax_value < (s64)val)\n\t\t\treturn 1;\n\t\telse if (reg->smin_value >= (s64)val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JGE:\n\t\tif (reg->umin_value >= val)\n\t\t\treturn 1;\n\t\telse if (reg->umax_value < val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSGE:\n\t\tif (reg->smin_value >= (s64)val)\n\t\t\treturn 1;\n\t\telse if (reg->smax_value < (s64)val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JLE:\n\t\tif (reg->umax_value <= val)\n\t\t\treturn 1;\n\t\telse if (reg->umin_value > val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSLE:\n\t\tif (reg->smax_value <= (s64)val)\n\t\t\treturn 1;\n\t\telse if (reg->smin_value > (s64)val)\n\t\t\treturn 0;\n\t\tbreak;\n\t}\n\n\treturn -1;\n}\n\n/* Adjusts the register min/max values in the case that the dst_reg is the\n * variable register that we are working on, and src_reg is a constant or we're\n * simply doing a BPF_K check.\n * In JEQ/JNE cases we also adjust the var_off values.\n */\nstatic void reg_set_min_max(struct bpf_reg_state *true_reg,\n\t\t\t    struct bpf_reg_state *false_reg, u64 val,\n\t\t\t    u8 opcode)\n{\n\t/* If the dst_reg is a pointer, we can't learn anything about its\n\t * variable offset from the compare (unless src_reg were a pointer into\n\t * the same object, but we don't bother with that.\n\t * Since false_reg and true_reg have the same type by construction, we\n\t * only need to check one of them for pointerness.\n\t */\n\tif (__is_pointer_value(false, false_reg))\n\t\treturn;\n\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\t/* If this is false then we know nothing Jon Snow, but if it is\n\t\t * true then we know for sure.\n\t\t */\n\t\t__mark_reg_known(true_reg, val);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\t/* If this is true we know nothing Jon Snow, but if it is false\n\t\t * we know the value for sure;\n\t\t */\n\t\t__mark_reg_known(false_reg, val);\n\t\tbreak;\n\tcase BPF_JSET:\n\t\tfalse_reg->var_off = tnum_and(false_reg->var_off,\n\t\t\t\t\t      tnum_const(~val));\n\t\tif (is_power_of_2(val))\n\t\t\ttrue_reg->var_off = tnum_or(true_reg->var_off,\n\t\t\t\t\t\t    tnum_const(val));\n\t\tbreak;\n\tcase BPF_JGT:\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val);\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JSGT:\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val);\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JLT:\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val);\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val - 1);\n\t\tbreak;\n\tcase BPF_JSLT:\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val);\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val - 1);\n\t\tbreak;\n\tcase BPF_JGE:\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val - 1);\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val);\n\t\tbreak;\n\tcase BPF_JSGE:\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val - 1);\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val);\n\t\tbreak;\n\tcase BPF_JLE:\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val + 1);\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val);\n\t\tbreak;\n\tcase BPF_JSLE:\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val + 1);\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t__reg_deduce_bounds(false_reg);\n\t__reg_deduce_bounds(true_reg);\n\t/* We might have learned some bits from the bounds. */\n\t__reg_bound_offset(false_reg);\n\t__reg_bound_offset(true_reg);\n\t/* Intersecting with the old var_off might have improved our bounds\n\t * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),\n\t * then new var_off is (0; 0x7f...fc) which improves our umax.\n\t */\n\t__update_reg_bounds(false_reg);\n\t__update_reg_bounds(true_reg);\n}\n\n/* Same as above, but for the case that dst_reg holds a constant and src_reg is\n * the variable reg.\n */\nstatic void reg_set_min_max_inv(struct bpf_reg_state *true_reg,\n\t\t\t\tstruct bpf_reg_state *false_reg, u64 val,\n\t\t\t\tu8 opcode)\n{\n\tif (__is_pointer_value(false, false_reg))\n\t\treturn;\n\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\t/* If this is false then we know nothing Jon Snow, but if it is\n\t\t * true then we know for sure.\n\t\t */\n\t\t__mark_reg_known(true_reg, val);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\t/* If this is true we know nothing Jon Snow, but if it is false\n\t\t * we know the value for sure;\n\t\t */\n\t\t__mark_reg_known(false_reg, val);\n\t\tbreak;\n\tcase BPF_JSET:\n\t\tfalse_reg->var_off = tnum_and(false_reg->var_off,\n\t\t\t\t\t      tnum_const(~val));\n\t\tif (is_power_of_2(val))\n\t\t\ttrue_reg->var_off = tnum_or(true_reg->var_off,\n\t\t\t\t\t\t    tnum_const(val));\n\t\tbreak;\n\tcase BPF_JGT:\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val - 1);\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val);\n\t\tbreak;\n\tcase BPF_JSGT:\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val - 1);\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val);\n\t\tbreak;\n\tcase BPF_JLT:\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val + 1);\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val);\n\t\tbreak;\n\tcase BPF_JSLT:\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val + 1);\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val);\n\t\tbreak;\n\tcase BPF_JGE:\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val);\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JSGE:\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val);\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JLE:\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val);\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val - 1);\n\t\tbreak;\n\tcase BPF_JSLE:\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val);\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val - 1);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t__reg_deduce_bounds(false_reg);\n\t__reg_deduce_bounds(true_reg);\n\t/* We might have learned some bits from the bounds. */\n\t__reg_bound_offset(false_reg);\n\t__reg_bound_offset(true_reg);\n\t/* Intersecting with the old var_off might have improved our bounds\n\t * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),\n\t * then new var_off is (0; 0x7f...fc) which improves our umax.\n\t */\n\t__update_reg_bounds(false_reg);\n\t__update_reg_bounds(true_reg);\n}\n\n/* Regs are known to be equal, so intersect their min/max/var_off */\nstatic void __reg_combine_min_max(struct bpf_reg_state *src_reg,\n\t\t\t\t  struct bpf_reg_state *dst_reg)\n{\n\tsrc_reg->umin_value = dst_reg->umin_value = max(src_reg->umin_value,\n\t\t\t\t\t\t\tdst_reg->umin_value);\n\tsrc_reg->umax_value = dst_reg->umax_value = min(src_reg->umax_value,\n\t\t\t\t\t\t\tdst_reg->umax_value);\n\tsrc_reg->smin_value = dst_reg->smin_value = max(src_reg->smin_value,\n\t\t\t\t\t\t\tdst_reg->smin_value);\n\tsrc_reg->smax_value = dst_reg->smax_value = min(src_reg->smax_value,\n\t\t\t\t\t\t\tdst_reg->smax_value);\n\tsrc_reg->var_off = dst_reg->var_off = tnum_intersect(src_reg->var_off,\n\t\t\t\t\t\t\t     dst_reg->var_off);\n\t/* We might have learned new bounds from the var_off. */\n\t__update_reg_bounds(src_reg);\n\t__update_reg_bounds(dst_reg);\n\t/* We might have learned something about the sign bit. */\n\t__reg_deduce_bounds(src_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t/* We might have learned some bits from the bounds. */\n\t__reg_bound_offset(src_reg);\n\t__reg_bound_offset(dst_reg);\n\t/* Intersecting with the old var_off might have improved our bounds\n\t * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),\n\t * then new var_off is (0; 0x7f...fc) which improves our umax.\n\t */\n\t__update_reg_bounds(src_reg);\n\t__update_reg_bounds(dst_reg);\n}\n\nstatic void reg_combine_min_max(struct bpf_reg_state *true_src,\n\t\t\t\tstruct bpf_reg_state *true_dst,\n\t\t\t\tstruct bpf_reg_state *false_src,\n\t\t\t\tstruct bpf_reg_state *false_dst,\n\t\t\t\tu8 opcode)\n{\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\t__reg_combine_min_max(true_src, true_dst);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\t__reg_combine_min_max(false_src, false_dst);\n\t\tbreak;\n\t}\n}\n\nstatic void mark_ptr_or_null_reg(struct bpf_func_state *state,\n\t\t\t\t struct bpf_reg_state *reg, u32 id,\n\t\t\t\t bool is_null)\n{\n\tif (reg_type_may_be_null(reg->type) && reg->id == id) {\n\t\t/* Old offset (both fixed and variable parts) should\n\t\t * have been known-zero, because we don't allow pointer\n\t\t * arithmetic on pointers that might be NULL.\n\t\t */\n\t\tif (WARN_ON_ONCE(reg->smin_value || reg->smax_value ||\n\t\t\t\t !tnum_equals_const(reg->var_off, 0) ||\n\t\t\t\t reg->off)) {\n\t\t\t__mark_reg_known_zero(reg);\n\t\t\treg->off = 0;\n\t\t}\n\t\tif (is_null) {\n\t\t\treg->type = SCALAR_VALUE;\n\t\t} else if (reg->type == PTR_TO_MAP_VALUE_OR_NULL) {\n\t\t\tif (reg->map_ptr->inner_map_meta) {\n\t\t\t\treg->type = CONST_PTR_TO_MAP;\n\t\t\t\treg->map_ptr = reg->map_ptr->inner_map_meta;\n\t\t\t} else {\n\t\t\t\treg->type = PTR_TO_MAP_VALUE;\n\t\t\t}\n\t\t} else if (reg->type == PTR_TO_SOCKET_OR_NULL) {\n\t\t\treg->type = PTR_TO_SOCKET;\n\t\t}\n\t\tif (is_null || !reg_is_refcounted(reg)) {\n\t\t\t/* We don't need id from this point onwards anymore,\n\t\t\t * thus we should better reset it, so that state\n\t\t\t * pruning has chances to take effect.\n\t\t\t */\n\t\t\treg->id = 0;\n\t\t}\n\t}\n}\n\n/* The logic is similar to find_good_pkt_pointers(), both could eventually\n * be folded together at some point.\n */\nstatic void mark_ptr_or_null_regs(struct bpf_verifier_state *vstate, u32 regno,\n\t\t\t\t  bool is_null)\n{\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *reg, *regs = state->regs;\n\tu32 id = regs[regno].id;\n\tint i, j;\n\n\tif (reg_is_refcounted_or_null(&regs[regno]) && is_null)\n\t\t__release_reference_state(state, id);\n\n\tfor (i = 0; i < MAX_BPF_REG; i++)\n\t\tmark_ptr_or_null_reg(state, &regs[i], id, is_null);\n\n\tfor (j = 0; j <= vstate->curframe; j++) {\n\t\tstate = vstate->frame[j];\n\t\tbpf_for_each_spilled_reg(i, state, reg) {\n\t\t\tif (!reg)\n\t\t\t\tcontinue;\n\t\t\tmark_ptr_or_null_reg(state, reg, id, is_null);\n\t\t}\n\t}\n}\n\nstatic bool try_match_pkt_pointers(const struct bpf_insn *insn,\n\t\t\t\t   struct bpf_reg_state *dst_reg,\n\t\t\t\t   struct bpf_reg_state *src_reg,\n\t\t\t\t   struct bpf_verifier_state *this_branch,\n\t\t\t\t   struct bpf_verifier_state *other_branch)\n{\n\tif (BPF_SRC(insn->code) != BPF_X)\n\t\treturn false;\n\n\tswitch (BPF_OP(insn->code)) {\n\tcase BPF_JGT:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' > pkt_end, pkt_meta' > pkt_data */\n\t\t\tfind_good_pkt_pointers(this_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, false);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end > pkt_data', pkt_data > pkt_meta' */\n\t\t\tfind_good_pkt_pointers(other_branch, src_reg,\n\t\t\t\t\t       src_reg->type, true);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase BPF_JLT:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' < pkt_end, pkt_meta' < pkt_data */\n\t\t\tfind_good_pkt_pointers(other_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, true);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end < pkt_data', pkt_data > pkt_meta' */\n\t\t\tfind_good_pkt_pointers(this_branch, src_reg,\n\t\t\t\t\t       src_reg->type, false);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase BPF_JGE:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' >= pkt_end, pkt_meta' >= pkt_data */\n\t\t\tfind_good_pkt_pointers(this_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, true);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end >= pkt_data', pkt_data >= pkt_meta' */\n\t\t\tfind_good_pkt_pointers(other_branch, src_reg,\n\t\t\t\t\t       src_reg->type, false);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase BPF_JLE:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' <= pkt_end, pkt_meta' <= pkt_data */\n\t\t\tfind_good_pkt_pointers(other_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, false);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end <= pkt_data', pkt_data <= pkt_meta' */\n\t\t\tfind_good_pkt_pointers(this_branch, src_reg,\n\t\t\t\t\t       src_reg->type, true);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic int check_cond_jmp_op(struct bpf_verifier_env *env,\n\t\t\t     struct bpf_insn *insn, int *insn_idx)\n{\n\tstruct bpf_verifier_state *this_branch = env->cur_state;\n\tstruct bpf_verifier_state *other_branch;\n\tstruct bpf_reg_state *regs = this_branch->frame[this_branch->curframe]->regs;\n\tstruct bpf_reg_state *dst_reg, *other_branch_regs;\n\tu8 opcode = BPF_OP(insn->code);\n\tint err;\n\n\tif (opcode > BPF_JSLE) {\n\t\tverbose(env, \"invalid BPF_JMP opcode %x\\n\", opcode);\n\t\treturn -EINVAL;\n\t}\n\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tif (insn->imm != 0) {\n\t\t\tverbose(env, \"BPF_JMP uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/* check src1 operand */\n\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (is_pointer_value(env, insn->src_reg)) {\n\t\t\tverbose(env, \"R%d pointer comparison prohibited\\n\",\n\t\t\t\tinsn->src_reg);\n\t\t\treturn -EACCES;\n\t\t}\n\t} else {\n\t\tif (insn->src_reg != BPF_REG_0) {\n\t\t\tverbose(env, \"BPF_JMP uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t/* check src2 operand */\n\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tdst_reg = &regs[insn->dst_reg];\n\n\tif (BPF_SRC(insn->code) == BPF_K) {\n\t\tint pred = is_branch_taken(dst_reg, insn->imm, opcode);\n\n\t\tif (pred == 1) {\n\t\t\t /* only follow the goto, ignore fall-through */\n\t\t\t*insn_idx += insn->off;\n\t\t\treturn 0;\n\t\t} else if (pred == 0) {\n\t\t\t/* only follow fall-through branch, since\n\t\t\t * that's where the program will go\n\t\t\t */\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tother_branch = push_stack(env, *insn_idx + insn->off + 1, *insn_idx,\n\t\t\t\t  false);\n\tif (!other_branch)\n\t\treturn -EFAULT;\n\tother_branch_regs = other_branch->frame[other_branch->curframe]->regs;\n\n\t/* detect if we are comparing against a constant value so we can adjust\n\t * our min/max values for our dst register.\n\t * this is only legit if both are scalars (or pointers to the same\n\t * object, I suppose, but we don't support that right now), because\n\t * otherwise the different base pointers mean the offsets aren't\n\t * comparable.\n\t */\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tif (dst_reg->type == SCALAR_VALUE &&\n\t\t    regs[insn->src_reg].type == SCALAR_VALUE) {\n\t\t\tif (tnum_is_const(regs[insn->src_reg].var_off))\n\t\t\t\treg_set_min_max(&other_branch_regs[insn->dst_reg],\n\t\t\t\t\t\tdst_reg, regs[insn->src_reg].var_off.value,\n\t\t\t\t\t\topcode);\n\t\t\telse if (tnum_is_const(dst_reg->var_off))\n\t\t\t\treg_set_min_max_inv(&other_branch_regs[insn->src_reg],\n\t\t\t\t\t\t    &regs[insn->src_reg],\n\t\t\t\t\t\t    dst_reg->var_off.value, opcode);\n\t\t\telse if (opcode == BPF_JEQ || opcode == BPF_JNE)\n\t\t\t\t/* Comparing for equality, we can combine knowledge */\n\t\t\t\treg_combine_min_max(&other_branch_regs[insn->src_reg],\n\t\t\t\t\t\t    &other_branch_regs[insn->dst_reg],\n\t\t\t\t\t\t    &regs[insn->src_reg],\n\t\t\t\t\t\t    &regs[insn->dst_reg], opcode);\n\t\t}\n\t} else if (dst_reg->type == SCALAR_VALUE) {\n\t\treg_set_min_max(&other_branch_regs[insn->dst_reg],\n\t\t\t\t\tdst_reg, insn->imm, opcode);\n\t}\n\n\t/* detect if R == 0 where R is returned from bpf_map_lookup_elem() */\n\tif (BPF_SRC(insn->code) == BPF_K &&\n\t    insn->imm == 0 && (opcode == BPF_JEQ || opcode == BPF_JNE) &&\n\t    reg_type_may_be_null(dst_reg->type)) {\n\t\t/* Mark all identical registers in each branch as either\n\t\t * safe or unknown depending R == 0 or R != 0 conditional.\n\t\t */\n\t\tmark_ptr_or_null_regs(this_branch, insn->dst_reg,\n\t\t\t\t      opcode == BPF_JNE);\n\t\tmark_ptr_or_null_regs(other_branch, insn->dst_reg,\n\t\t\t\t      opcode == BPF_JEQ);\n\t} else if (!try_match_pkt_pointers(insn, dst_reg, &regs[insn->src_reg],\n\t\t\t\t\t   this_branch, other_branch) &&\n\t\t   is_pointer_value(env, insn->dst_reg)) {\n\t\tverbose(env, \"R%d pointer comparison prohibited\\n\",\n\t\t\tinsn->dst_reg);\n\t\treturn -EACCES;\n\t}\n\tif (env->log.level)\n\t\tprint_verifier_state(env, this_branch->frame[this_branch->curframe]);\n\treturn 0;\n}\n\n/* return the map pointer stored inside BPF_LD_IMM64 instruction */\nstatic struct bpf_map *ld_imm64_to_map_ptr(struct bpf_insn *insn)\n{\n\tu64 imm64 = ((u64) (u32) insn[0].imm) | ((u64) (u32) insn[1].imm) << 32;\n\n\treturn (struct bpf_map *) (unsigned long) imm64;\n}\n\n/* verify BPF_LD_IMM64 instruction */\nstatic int check_ld_imm(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tint err;\n\n\tif (BPF_SIZE(insn->code) != BPF_DW) {\n\t\tverbose(env, \"invalid BPF_LD_IMM insn\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (insn->off != 0) {\n\t\tverbose(env, \"BPF_LD_IMM64 uses reserved fields\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\terr = check_reg_arg(env, insn->dst_reg, DST_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (insn->src_reg == 0) {\n\t\tu64 imm = ((u64)(insn + 1)->imm << 32) | (u32)insn->imm;\n\n\t\tregs[insn->dst_reg].type = SCALAR_VALUE;\n\t\t__mark_reg_known(&regs[insn->dst_reg], imm);\n\t\treturn 0;\n\t}\n\n\t/* replace_map_fd_with_map_ptr() should have caught bad ld_imm64 */\n\tBUG_ON(insn->src_reg != BPF_PSEUDO_MAP_FD);\n\n\tregs[insn->dst_reg].type = CONST_PTR_TO_MAP;\n\tregs[insn->dst_reg].map_ptr = ld_imm64_to_map_ptr(insn);\n\treturn 0;\n}\n\nstatic bool may_access_skb(enum bpf_prog_type type)\n{\n\tswitch (type) {\n\tcase BPF_PROG_TYPE_SOCKET_FILTER:\n\tcase BPF_PROG_TYPE_SCHED_CLS:\n\tcase BPF_PROG_TYPE_SCHED_ACT:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n/* verify safety of LD_ABS|LD_IND instructions:\n * - they can only appear in the programs where ctx == skb\n * - since they are wrappers of function calls, they scratch R1-R5 registers,\n *   preserve R6-R9, and store return value into R0\n *\n * Implicit input:\n *   ctx == skb == R6 == CTX\n *\n * Explicit input:\n *   SRC == any register\n *   IMM == 32-bit immediate\n *\n * Output:\n *   R0 - 8/16/32-bit skb data converted to cpu endianness\n */\nstatic int check_ld_abs(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 mode = BPF_MODE(insn->code);\n\tint i, err;\n\n\tif (!may_access_skb(env->prog->type)) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] instructions not allowed for this program type\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!env->ops->gen_ld_abs) {\n\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (env->subprog_cnt > 1) {\n\t\t/* when program has LD_ABS insn JITs and interpreter assume\n\t\t * that r1 == ctx == skb which is not the case for callees\n\t\t * that can have arbitrary arguments. It's problematic\n\t\t * for main prog as well since JITs would need to analyze\n\t\t * all functions in order to make proper register save/restore\n\t\t * decisions in the main prog. Hence disallow LD_ABS with calls\n\t\t */\n\t\tverbose(env, \"BPF_LD_[ABS|IND] instructions cannot be mixed with bpf-to-bpf calls\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (insn->dst_reg != BPF_REG_0 || insn->off != 0 ||\n\t    BPF_SIZE(insn->code) == BPF_DW ||\n\t    (mode == BPF_ABS && insn->src_reg != BPF_REG_0)) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] uses reserved fields\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* check whether implicit source operand (register R6) is readable */\n\terr = check_reg_arg(env, BPF_REG_6, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\t/* Disallow usage of BPF_LD_[ABS|IND] with reference tracking, as\n\t * gen_ld_abs() may terminate the program at runtime, leading to\n\t * reference leak.\n\t */\n\terr = check_reference_leak(env);\n\tif (err) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] cannot be mixed with socket references\\n\");\n\t\treturn err;\n\t}\n\n\tif (regs[BPF_REG_6].type != PTR_TO_CTX) {\n\t\tverbose(env,\n\t\t\t\"at the time of BPF_LD_ABS|IND R6 != pointer to skb\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (mode == BPF_IND) {\n\t\t/* check explicit source operand */\n\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\t/* reset caller saved regs to unreadable */\n\tfor (i = 0; i < CALLER_SAVED_REGS; i++) {\n\t\tmark_reg_not_init(env, regs, caller_saved[i]);\n\t\tcheck_reg_arg(env, caller_saved[i], DST_OP_NO_MARK);\n\t}\n\n\t/* mark destination R0 register as readable, since it contains\n\t * the value fetched from the packet.\n\t * Already marked as written above.\n\t */\n\tmark_reg_unknown(env, regs, BPF_REG_0);\n\treturn 0;\n}\n\nstatic int check_return_code(struct bpf_verifier_env *env)\n{\n\tstruct bpf_reg_state *reg;\n\tstruct tnum range = tnum_range(0, 1);\n\n\tswitch (env->prog->type) {\n\tcase BPF_PROG_TYPE_CGROUP_SKB:\n\tcase BPF_PROG_TYPE_CGROUP_SOCK:\n\tcase BPF_PROG_TYPE_CGROUP_SOCK_ADDR:\n\tcase BPF_PROG_TYPE_SOCK_OPS:\n\tcase BPF_PROG_TYPE_CGROUP_DEVICE:\n\t\tbreak;\n\tdefault:\n\t\treturn 0;\n\t}\n\n\treg = cur_regs(env) + BPF_REG_0;\n\tif (reg->type != SCALAR_VALUE) {\n\t\tverbose(env, \"At program exit the register R0 is not a known value (%s)\\n\",\n\t\t\treg_type_str[reg->type]);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!tnum_in(range, reg->var_off)) {\n\t\tverbose(env, \"At program exit the register R0 \");\n\t\tif (!tnum_is_unknown(reg->var_off)) {\n\t\t\tchar tn_buf[48];\n\n\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\tverbose(env, \"has value %s\", tn_buf);\n\t\t} else {\n\t\t\tverbose(env, \"has unknown scalar value\");\n\t\t}\n\t\tverbose(env, \" should have been 0 or 1\\n\");\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\n/* non-recursive DFS pseudo code\n * 1  procedure DFS-iterative(G,v):\n * 2      label v as discovered\n * 3      let S be a stack\n * 4      S.push(v)\n * 5      while S is not empty\n * 6            t <- S.pop()\n * 7            if t is what we're looking for:\n * 8                return t\n * 9            for all edges e in G.adjacentEdges(t) do\n * 10               if edge e is already labelled\n * 11                   continue with the next edge\n * 12               w <- G.adjacentVertex(t,e)\n * 13               if vertex w is not discovered and not explored\n * 14                   label e as tree-edge\n * 15                   label w as discovered\n * 16                   S.push(w)\n * 17                   continue at 5\n * 18               else if vertex w is discovered\n * 19                   label e as back-edge\n * 20               else\n * 21                   // vertex w is explored\n * 22                   label e as forward- or cross-edge\n * 23           label t as explored\n * 24           S.pop()\n *\n * convention:\n * 0x10 - discovered\n * 0x11 - discovered and fall-through edge labelled\n * 0x12 - discovered and fall-through and branch edges labelled\n * 0x20 - explored\n */\n\nenum {\n\tDISCOVERED = 0x10,\n\tEXPLORED = 0x20,\n\tFALLTHROUGH = 1,\n\tBRANCH = 2,\n};\n\n#define STATE_LIST_MARK ((struct bpf_verifier_state_list *) -1L)\n\nstatic int *insn_stack;\t/* stack of insns to process */\nstatic int cur_stack;\t/* current stack index */\nstatic int *insn_state;\n\n/* t, w, e - match pseudo-code above:\n * t - index of current instruction\n * w - next instruction\n * e - edge\n */\nstatic int push_insn(int t, int w, int e, struct bpf_verifier_env *env)\n{\n\tif (e == FALLTHROUGH && insn_state[t] >= (DISCOVERED | FALLTHROUGH))\n\t\treturn 0;\n\n\tif (e == BRANCH && insn_state[t] >= (DISCOVERED | BRANCH))\n\t\treturn 0;\n\n\tif (w < 0 || w >= env->prog->len) {\n\t\tverbose_linfo(env, t, \"%d: \", t);\n\t\tverbose(env, \"jump out of range from insn %d to %d\\n\", t, w);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e == BRANCH)\n\t\t/* mark branch target for state pruning */\n\t\tenv->explored_states[w] = STATE_LIST_MARK;\n\n\tif (insn_state[w] == 0) {\n\t\t/* tree-edge */\n\t\tinsn_state[t] = DISCOVERED | e;\n\t\tinsn_state[w] = DISCOVERED;\n\t\tif (cur_stack >= env->prog->len)\n\t\t\treturn -E2BIG;\n\t\tinsn_stack[cur_stack++] = w;\n\t\treturn 1;\n\t} else if ((insn_state[w] & 0xF0) == DISCOVERED) {\n\t\tverbose_linfo(env, t, \"%d: \", t);\n\t\tverbose_linfo(env, w, \"%d: \", w);\n\t\tverbose(env, \"back-edge from insn %d to %d\\n\", t, w);\n\t\treturn -EINVAL;\n\t} else if (insn_state[w] == EXPLORED) {\n\t\t/* forward- or cross-edge */\n\t\tinsn_state[t] = DISCOVERED | e;\n\t} else {\n\t\tverbose(env, \"insn state internal bug\\n\");\n\t\treturn -EFAULT;\n\t}\n\treturn 0;\n}\n\n/* non-recursive depth-first-search to detect loops in BPF program\n * loop == back-edge in directed graph\n */\nstatic int check_cfg(struct bpf_verifier_env *env)\n{\n\tstruct bpf_insn *insns = env->prog->insnsi;\n\tint insn_cnt = env->prog->len;\n\tint ret = 0;\n\tint i, t;\n\n\tinsn_state = kcalloc(insn_cnt, sizeof(int), GFP_KERNEL);\n\tif (!insn_state)\n\t\treturn -ENOMEM;\n\n\tinsn_stack = kcalloc(insn_cnt, sizeof(int), GFP_KERNEL);\n\tif (!insn_stack) {\n\t\tkfree(insn_state);\n\t\treturn -ENOMEM;\n\t}\n\n\tinsn_state[0] = DISCOVERED; /* mark 1st insn as discovered */\n\tinsn_stack[0] = 0; /* 0 is the first instruction */\n\tcur_stack = 1;\n\npeek_stack:\n\tif (cur_stack == 0)\n\t\tgoto check_state;\n\tt = insn_stack[cur_stack - 1];\n\n\tif (BPF_CLASS(insns[t].code) == BPF_JMP) {\n\t\tu8 opcode = BPF_OP(insns[t].code);\n\n\t\tif (opcode == BPF_EXIT) {\n\t\t\tgoto mark_explored;\n\t\t} else if (opcode == BPF_CALL) {\n\t\t\tret = push_insn(t, t + 1, FALLTHROUGH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t\tif (t + 1 < insn_cnt)\n\t\t\t\tenv->explored_states[t + 1] = STATE_LIST_MARK;\n\t\t\tif (insns[t].src_reg == BPF_PSEUDO_CALL) {\n\t\t\t\tenv->explored_states[t] = STATE_LIST_MARK;\n\t\t\t\tret = push_insn(t, t + insns[t].imm + 1, BRANCH, env);\n\t\t\t\tif (ret == 1)\n\t\t\t\t\tgoto peek_stack;\n\t\t\t\telse if (ret < 0)\n\t\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t} else if (opcode == BPF_JA) {\n\t\t\tif (BPF_SRC(insns[t].code) != BPF_K) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t\t/* unconditional jump with single edge */\n\t\t\tret = push_insn(t, t + insns[t].off + 1,\n\t\t\t\t\tFALLTHROUGH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t\t/* tell verifier to check for equivalent states\n\t\t\t * after every call and jump\n\t\t\t */\n\t\t\tif (t + 1 < insn_cnt)\n\t\t\t\tenv->explored_states[t + 1] = STATE_LIST_MARK;\n\t\t} else {\n\t\t\t/* conditional jump with two edges */\n\t\t\tenv->explored_states[t] = STATE_LIST_MARK;\n\t\t\tret = push_insn(t, t + 1, FALLTHROUGH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\n\t\t\tret = push_insn(t, t + insns[t].off + 1, BRANCH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t}\n\t} else {\n\t\t/* all other non-branch instructions with single\n\t\t * fall-through edge\n\t\t */\n\t\tret = push_insn(t, t + 1, FALLTHROUGH, env);\n\t\tif (ret == 1)\n\t\t\tgoto peek_stack;\n\t\telse if (ret < 0)\n\t\t\tgoto err_free;\n\t}\n\nmark_explored:\n\tinsn_state[t] = EXPLORED;\n\tif (cur_stack-- <= 0) {\n\t\tverbose(env, \"pop stack internal bug\\n\");\n\t\tret = -EFAULT;\n\t\tgoto err_free;\n\t}\n\tgoto peek_stack;\n\ncheck_state:\n\tfor (i = 0; i < insn_cnt; i++) {\n\t\tif (insn_state[i] != EXPLORED) {\n\t\t\tverbose(env, \"unreachable insn %d\\n\", i);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\t}\n\tret = 0; /* cfg looks good */\n\nerr_free:\n\tkfree(insn_state);\n\tkfree(insn_stack);\n\treturn ret;\n}\n\n/* The minimum supported BTF func info size */\n#define MIN_BPF_FUNCINFO_SIZE\t8\n#define MAX_FUNCINFO_REC_SIZE\t252\n\nstatic int check_btf_func(struct bpf_verifier_env *env,\n\t\t\t  const union bpf_attr *attr,\n\t\t\t  union bpf_attr __user *uattr)\n{\n\tu32 i, nfuncs, urec_size, min_size, prev_offset;\n\tu32 krec_size = sizeof(struct bpf_func_info);\n\tstruct bpf_func_info *krecord;\n\tconst struct btf_type *type;\n\tstruct bpf_prog *prog;\n\tconst struct btf *btf;\n\tvoid __user *urecord;\n\tint ret = 0;\n\n\tnfuncs = attr->func_info_cnt;\n\tif (!nfuncs)\n\t\treturn 0;\n\n\tif (nfuncs != env->subprog_cnt) {\n\t\tverbose(env, \"number of funcs in func_info doesn't match number of subprogs\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\turec_size = attr->func_info_rec_size;\n\tif (urec_size < MIN_BPF_FUNCINFO_SIZE ||\n\t    urec_size > MAX_FUNCINFO_REC_SIZE ||\n\t    urec_size % sizeof(u32)) {\n\t\tverbose(env, \"invalid func info rec size %u\\n\", urec_size);\n\t\treturn -EINVAL;\n\t}\n\n\tprog = env->prog;\n\tbtf = prog->aux->btf;\n\n\turecord = u64_to_user_ptr(attr->func_info);\n\tmin_size = min_t(u32, krec_size, urec_size);\n\n\tkrecord = kvcalloc(nfuncs, krec_size, GFP_KERNEL | __GFP_NOWARN);\n\tif (!krecord)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < nfuncs; i++) {\n\t\tret = bpf_check_uarg_tail_zero(urecord, krec_size, urec_size);\n\t\tif (ret) {\n\t\t\tif (ret == -E2BIG) {\n\t\t\t\tverbose(env, \"nonzero tailing record in func info\");\n\t\t\t\t/* set the size kernel expects so loader can zero\n\t\t\t\t * out the rest of the record.\n\t\t\t\t */\n\t\t\t\tif (put_user(min_size, &uattr->func_info_rec_size))\n\t\t\t\t\tret = -EFAULT;\n\t\t\t}\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (copy_from_user(&krecord[i], urecord, min_size)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/* check insn_off */\n\t\tif (i == 0) {\n\t\t\tif (krecord[i].insn_off) {\n\t\t\t\tverbose(env,\n\t\t\t\t\t\"nonzero insn_off %u for the first func info record\",\n\t\t\t\t\tkrecord[i].insn_off);\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t} else if (krecord[i].insn_off <= prev_offset) {\n\t\t\tverbose(env,\n\t\t\t\t\"same or smaller insn offset (%u) than previous func info record (%u)\",\n\t\t\t\tkrecord[i].insn_off, prev_offset);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (env->subprog_info[i].start != krecord[i].insn_off) {\n\t\t\tverbose(env, \"func_info BTF section doesn't match subprog layout in BPF program\\n\");\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/* check type_id */\n\t\ttype = btf_type_by_id(btf, krecord[i].type_id);\n\t\tif (!type || BTF_INFO_KIND(type->info) != BTF_KIND_FUNC) {\n\t\t\tverbose(env, \"invalid type id %d in func info\",\n\t\t\t\tkrecord[i].type_id);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tprev_offset = krecord[i].insn_off;\n\t\turecord += urec_size;\n\t}\n\n\tprog->aux->func_info = krecord;\n\tprog->aux->func_info_cnt = nfuncs;\n\treturn 0;\n\nerr_free:\n\tkvfree(krecord);\n\treturn ret;\n}\n\nstatic void adjust_btf_func(struct bpf_verifier_env *env)\n{\n\tint i;\n\n\tif (!env->prog->aux->func_info)\n\t\treturn;\n\n\tfor (i = 0; i < env->subprog_cnt; i++)\n\t\tenv->prog->aux->func_info[i].insn_off = env->subprog_info[i].start;\n}\n\n#define MIN_BPF_LINEINFO_SIZE\t(offsetof(struct bpf_line_info, line_col) + \\\n\t\tsizeof(((struct bpf_line_info *)(0))->line_col))\n#define MAX_LINEINFO_REC_SIZE\tMAX_FUNCINFO_REC_SIZE\n\nstatic int check_btf_line(struct bpf_verifier_env *env,\n\t\t\t  const union bpf_attr *attr,\n\t\t\t  union bpf_attr __user *uattr)\n{\n\tu32 i, s, nr_linfo, ncopy, expected_size, rec_size, prev_offset = 0;\n\tstruct bpf_subprog_info *sub;\n\tstruct bpf_line_info *linfo;\n\tstruct bpf_prog *prog;\n\tconst struct btf *btf;\n\tvoid __user *ulinfo;\n\tint err;\n\n\tnr_linfo = attr->line_info_cnt;\n\tif (!nr_linfo)\n\t\treturn 0;\n\n\trec_size = attr->line_info_rec_size;\n\tif (rec_size < MIN_BPF_LINEINFO_SIZE ||\n\t    rec_size > MAX_LINEINFO_REC_SIZE ||\n\t    rec_size & (sizeof(u32) - 1))\n\t\treturn -EINVAL;\n\n\t/* Need to zero it in case the userspace may\n\t * pass in a smaller bpf_line_info object.\n\t */\n\tlinfo = kvcalloc(nr_linfo, sizeof(struct bpf_line_info),\n\t\t\t GFP_KERNEL | __GFP_NOWARN);\n\tif (!linfo)\n\t\treturn -ENOMEM;\n\n\tprog = env->prog;\n\tbtf = prog->aux->btf;\n\n\ts = 0;\n\tsub = env->subprog_info;\n\tulinfo = u64_to_user_ptr(attr->line_info);\n\texpected_size = sizeof(struct bpf_line_info);\n\tncopy = min_t(u32, expected_size, rec_size);\n\tfor (i = 0; i < nr_linfo; i++) {\n\t\terr = bpf_check_uarg_tail_zero(ulinfo, expected_size, rec_size);\n\t\tif (err) {\n\t\t\tif (err == -E2BIG) {\n\t\t\t\tverbose(env, \"nonzero tailing record in line_info\");\n\t\t\t\tif (put_user(expected_size,\n\t\t\t\t\t     &uattr->line_info_rec_size))\n\t\t\t\t\terr = -EFAULT;\n\t\t\t}\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (copy_from_user(&linfo[i], ulinfo, ncopy)) {\n\t\t\terr = -EFAULT;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/*\n\t\t * Check insn_off to ensure\n\t\t * 1) strictly increasing AND\n\t\t * 2) bounded by prog->len\n\t\t *\n\t\t * The linfo[0].insn_off == 0 check logically falls into\n\t\t * the later \"missing bpf_line_info for func...\" case\n\t\t * because the first linfo[0].insn_off must be the\n\t\t * first sub also and the first sub must have\n\t\t * subprog_info[0].start == 0.\n\t\t */\n\t\tif ((i && linfo[i].insn_off <= prev_offset) ||\n\t\t    linfo[i].insn_off >= prog->len) {\n\t\t\tverbose(env, \"Invalid line_info[%u].insn_off:%u (prev_offset:%u prog->len:%u)\\n\",\n\t\t\t\ti, linfo[i].insn_off, prev_offset,\n\t\t\t\tprog->len);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (!prog->insnsi[linfo[i].insn_off].code) {\n\t\t\tverbose(env,\n\t\t\t\t\"Invalid insn code at line_info[%u].insn_off\\n\",\n\t\t\t\ti);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (!btf_name_by_offset(btf, linfo[i].line_off) ||\n\t\t    !btf_name_by_offset(btf, linfo[i].file_name_off)) {\n\t\t\tverbose(env, \"Invalid line_info[%u].line_off or .file_name_off\\n\", i);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (s != env->subprog_cnt) {\n\t\t\tif (linfo[i].insn_off == sub[s].start) {\n\t\t\t\tsub[s].linfo_idx = i;\n\t\t\t\ts++;\n\t\t\t} else if (sub[s].start < linfo[i].insn_off) {\n\t\t\t\tverbose(env, \"missing bpf_line_info for func#%u\\n\", s);\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t}\n\n\t\tprev_offset = linfo[i].insn_off;\n\t\tulinfo += rec_size;\n\t}\n\n\tif (s != env->subprog_cnt) {\n\t\tverbose(env, \"missing bpf_line_info for %u funcs starting from func#%u\\n\",\n\t\t\tenv->subprog_cnt - s, s);\n\t\terr = -EINVAL;\n\t\tgoto err_free;\n\t}\n\n\tprog->aux->linfo = linfo;\n\tprog->aux->nr_linfo = nr_linfo;\n\n\treturn 0;\n\nerr_free:\n\tkvfree(linfo);\n\treturn err;\n}\n\nstatic int check_btf_info(struct bpf_verifier_env *env,\n\t\t\t  const union bpf_attr *attr,\n\t\t\t  union bpf_attr __user *uattr)\n{\n\tstruct btf *btf;\n\tint err;\n\n\tif (!attr->func_info_cnt && !attr->line_info_cnt)\n\t\treturn 0;\n\n\tbtf = btf_get_by_fd(attr->prog_btf_fd);\n\tif (IS_ERR(btf))\n\t\treturn PTR_ERR(btf);\n\tenv->prog->aux->btf = btf;\n\n\terr = check_btf_func(env, attr, uattr);\n\tif (err)\n\t\treturn err;\n\n\terr = check_btf_line(env, attr, uattr);\n\tif (err)\n\t\treturn err;\n\n\treturn 0;\n}\n\n/* check %cur's range satisfies %old's */\nstatic bool range_within(struct bpf_reg_state *old,\n\t\t\t struct bpf_reg_state *cur)\n{\n\treturn old->umin_value <= cur->umin_value &&\n\t       old->umax_value >= cur->umax_value &&\n\t       old->smin_value <= cur->smin_value &&\n\t       old->smax_value >= cur->smax_value;\n}\n\n/* Maximum number of register states that can exist at once */\n#define ID_MAP_SIZE\t(MAX_BPF_REG + MAX_BPF_STACK / BPF_REG_SIZE)\nstruct idpair {\n\tu32 old;\n\tu32 cur;\n};\n\n/* If in the old state two registers had the same id, then they need to have\n * the same id in the new state as well.  But that id could be different from\n * the old state, so we need to track the mapping from old to new ids.\n * Once we have seen that, say, a reg with old id 5 had new id 9, any subsequent\n * regs with old id 5 must also have new id 9 for the new state to be safe.  But\n * regs with a different old id could still have new id 9, we don't care about\n * that.\n * So we look through our idmap to see if this old id has been seen before.  If\n * so, we require the new id to match; otherwise, we add the id pair to the map.\n */\nstatic bool check_ids(u32 old_id, u32 cur_id, struct idpair *idmap)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < ID_MAP_SIZE; i++) {\n\t\tif (!idmap[i].old) {\n\t\t\t/* Reached an empty slot; haven't seen this id before */\n\t\t\tidmap[i].old = old_id;\n\t\t\tidmap[i].cur = cur_id;\n\t\t\treturn true;\n\t\t}\n\t\tif (idmap[i].old == old_id)\n\t\t\treturn idmap[i].cur == cur_id;\n\t}\n\t/* We ran out of idmap slots, which should be impossible */\n\tWARN_ON_ONCE(1);\n\treturn false;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 3504,
        "critical_vars": [
            "ret"
        ],
        "function": "adjust_scalar_min_max_vals",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_d3bd7413e0ca40b60cf60d4003246d067cafdeda_verifier.c.diff",
        "label": "True",
        "function_code": "static int adjust_scalar_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t      struct bpf_insn *insn,\n\t\t\t\t      struct bpf_reg_state *dst_reg,\n\t\t\t\t      struct bpf_reg_state src_reg)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 opcode = BPF_OP(insn->code);\n\tbool src_known, dst_known;\n\ts64 smin_val, smax_val;\n\tu64 umin_val, umax_val;\n\tu64 insn_bitness = (BPF_CLASS(insn->code) == BPF_ALU64) ? 64 : 32;\n\tu32 dst = insn->dst_reg;\n\tint ret;\n\n\tif (insn_bitness == 32) {\n\t\t/* Relevant for 32-bit RSH: Information can propagate towards\n\t\t * LSB, so it isn't sufficient to only truncate the output to\n\t\t * 32 bits.\n\t\t */\n\t\tcoerce_reg_to_size(dst_reg, 4);\n\t\tcoerce_reg_to_size(&src_reg, 4);\n\t}\n\n\tsmin_val = src_reg.smin_value;\n\tsmax_val = src_reg.smax_value;\n\tumin_val = src_reg.umin_value;\n\tumax_val = src_reg.umax_value;\n\tsrc_known = tnum_is_const(src_reg.var_off);\n\tdst_known = tnum_is_const(dst_reg->var_off);\n\n\tif ((src_known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (!src_known &&\n\t    opcode != BPF_ADD && opcode != BPF_SUB && opcode != BPF_AND) {\n\t\t__mark_reg_unknown(dst_reg);\n\t\treturn 0;\n\t}\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\tret = sanitize_val_alu(env, insn);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to add from different pointers or scalars\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\tif (signed_add_overflows(dst_reg->smin_value, smin_val) ||\n\t\t    signed_add_overflows(dst_reg->smax_value, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value += smin_val;\n\t\t\tdst_reg->smax_value += smax_val;\n\t\t}\n\t\tif (dst_reg->umin_value + umin_val < umin_val ||\n\t\t    dst_reg->umax_value + umax_val < umax_val) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value += umin_val;\n\t\t\tdst_reg->umax_value += umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(dst_reg->var_off, src_reg.var_off);\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tret = sanitize_val_alu(env, insn);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to sub from different pointers or scalars\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\tif (signed_sub_overflows(dst_reg->smin_value, smax_val) ||\n\t\t    signed_sub_overflows(dst_reg->smax_value, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value -= smax_val;\n\t\t\tdst_reg->smax_value -= smin_val;\n\t\t}\n\t\tif (dst_reg->umin_value < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value -= umax_val;\n\t\t\tdst_reg->umax_value -= umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(dst_reg->var_off, src_reg.var_off);\n\t\tbreak;\n\tcase BPF_MUL:\n\t\tdst_reg->var_off = tnum_mul(dst_reg->var_off, src_reg.var_off);\n\t\tif (smin_val < 0 || dst_reg->smin_value < 0) {\n\t\t\t/* Ain't nobody got time to multiply that sign */\n\t\t\t__mark_reg_unbounded(dst_reg);\n\t\t\t__update_reg_bounds(dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\t/* Both values are positive, so we can work with unsigned and\n\t\t * copy the result to signed (unless it exceeds S64_MAX).\n\t\t */\n\t\tif (umax_val > U32_MAX || dst_reg->umax_value > U32_MAX) {\n\t\t\t/* Potential overflow, we know nothing */\n\t\t\t__mark_reg_unbounded(dst_reg);\n\t\t\t/* (except what we can learn from the var_off) */\n\t\t\t__update_reg_bounds(dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\tdst_reg->umin_value *= umin_val;\n\t\tdst_reg->umax_value *= umax_val;\n\t\tif (dst_reg->umax_value > S64_MAX) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = dst_reg->umin_value;\n\t\t\tdst_reg->smax_value = dst_reg->umax_value;\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\t\tif (src_known && dst_known) {\n\t\t\t__mark_reg_known(dst_reg, dst_reg->var_off.value &\n\t\t\t\t\t\t  src_reg.var_off.value);\n\t\t\tbreak;\n\t\t}\n\t\t/* We get our minimum from the var_off, since that's inherently\n\t\t * bitwise.  Our maximum is the minimum of the operands' maxima.\n\t\t */\n\t\tdst_reg->var_off = tnum_and(dst_reg->var_off, src_reg.var_off);\n\t\tdst_reg->umin_value = dst_reg->var_off.value;\n\t\tdst_reg->umax_value = min(dst_reg->umax_value, umax_val);\n\t\tif (dst_reg->smin_value < 0 || smin_val < 0) {\n\t\t\t/* Lose signed bounds when ANDing negative numbers,\n\t\t\t * ain't nobody got time for that.\n\t\t\t */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\t/* ANDing two positives gives a positive, so safe to\n\t\t\t * cast result into s64.\n\t\t\t */\n\t\t\tdst_reg->smin_value = dst_reg->umin_value;\n\t\t\tdst_reg->smax_value = dst_reg->umax_value;\n\t\t}\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tcase BPF_OR:\n\t\tif (src_known && dst_known) {\n\t\t\t__mark_reg_known(dst_reg, dst_reg->var_off.value |\n\t\t\t\t\t\t  src_reg.var_off.value);\n\t\t\tbreak;\n\t\t}\n\t\t/* We get our maximum from the var_off, and our minimum is the\n\t\t * maximum of the operands' minima\n\t\t */\n\t\tdst_reg->var_off = tnum_or(dst_reg->var_off, src_reg.var_off);\n\t\tdst_reg->umin_value = max(dst_reg->umin_value, umin_val);\n\t\tdst_reg->umax_value = dst_reg->var_off.value |\n\t\t\t\t      dst_reg->var_off.mask;\n\t\tif (dst_reg->smin_value < 0 || smin_val < 0) {\n\t\t\t/* Lose signed bounds when ORing negative numbers,\n\t\t\t * ain't nobody got time for that.\n\t\t\t */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\t/* ORing two positives gives a positive, so safe to\n\t\t\t * cast result into s64.\n\t\t\t */\n\t\t\tdst_reg->smin_value = dst_reg->umin_value;\n\t\t\tdst_reg->smax_value = dst_reg->umax_value;\n\t\t}\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tcase BPF_LSH:\n\t\tif (umax_val >= insn_bitness) {\n\t\t\t/* Shifts greater than 31 or 63 are undefined.\n\t\t\t * This includes shifts by a negative number.\n\t\t\t */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\t/* We lose all sign bit information (except what we can pick\n\t\t * up from var_off)\n\t\t */\n\t\tdst_reg->smin_value = S64_MIN;\n\t\tdst_reg->smax_value = S64_MAX;\n\t\t/* If we might shift our top bit out, then we know nothing */\n\t\tif (dst_reg->umax_value > 1ULL << (63 - umax_val)) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value <<= umin_val;\n\t\t\tdst_reg->umax_value <<= umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_lshift(dst_reg->var_off, umin_val);\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tcase BPF_RSH:\n\t\tif (umax_val >= insn_bitness) {\n\t\t\t/* Shifts greater than 31 or 63 are undefined.\n\t\t\t * This includes shifts by a negative number.\n\t\t\t */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\t/* BPF_RSH is an unsigned shift.  If the value in dst_reg might\n\t\t * be negative, then either:\n\t\t * 1) src_reg might be zero, so the sign bit of the result is\n\t\t *    unknown, so we lose our signed bounds\n\t\t * 2) it's known negative, thus the unsigned bounds capture the\n\t\t *    signed bounds\n\t\t * 3) the signed bounds cross zero, so they tell us nothing\n\t\t *    about the result\n\t\t * If the value in dst_reg is known nonnegative, then again the\n\t\t * unsigned bounts capture the signed bounds.\n\t\t * Thus, in all cases it suffices to blow away our signed bounds\n\t\t * and rely on inferring new ones from the unsigned bounds and\n\t\t * var_off of the result.\n\t\t */\n\t\tdst_reg->smin_value = S64_MIN;\n\t\tdst_reg->smax_value = S64_MAX;\n\t\tdst_reg->var_off = tnum_rshift(dst_reg->var_off, umin_val);\n\t\tdst_reg->umin_value >>= umax_val;\n\t\tdst_reg->umax_value >>= umin_val;\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tcase BPF_ARSH:\n\t\tif (umax_val >= insn_bitness) {\n\t\t\t/* Shifts greater than 31 or 63 are undefined.\n\t\t\t * This includes shifts by a negative number.\n\t\t\t */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Upon reaching here, src_known is true and\n\t\t * umax_val is equal to umin_val.\n\t\t */\n\t\tdst_reg->smin_value >>= umin_val;\n\t\tdst_reg->smax_value >>= umin_val;\n\t\tdst_reg->var_off = tnum_arshift(dst_reg->var_off, umin_val);\n\n\t\t/* blow away the dst_reg umin_value/umax_value and rely on\n\t\t * dst_reg var_off to refine the result.\n\t\t */\n\t\tdst_reg->umin_value = 0;\n\t\tdst_reg->umax_value = U64_MAX;\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tdefault:\n\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\tbreak;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops are (32,32)->32 */\n\t\tcoerce_reg_to_size(dst_reg, 4);\n\t}\n\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\treturn 0;\n}\n\n/* Handles ALU ops other than BPF_END, BPF_NEG and BPF_MOV: computes new min/max\n * and var_off.\n */\nstatic int adjust_reg_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg, *src_reg;\n\tstruct bpf_reg_state *ptr_reg = NULL, off_reg = {0};\n\tu8 opcode = BPF_OP(insn->code);\n\n\tdst_reg = &regs[insn->dst_reg];\n\tsrc_reg = NULL;\n\tif (dst_reg->type != SCALAR_VALUE)\n\t\tptr_reg = dst_reg;\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tsrc_reg = &regs[insn->src_reg];\n\t\tif (src_reg->type != SCALAR_VALUE) {\n\t\t\tif (dst_reg->type != SCALAR_VALUE) {\n\t\t\t\t/* Combining two pointers by any ALU op yields\n\t\t\t\t * an arbitrary scalar. Disallow all math except\n\t\t\t\t * pointer subtraction\n\t\t\t\t */\n\t\t\t\tif (opcode == BPF_SUB && env->allow_ptr_leaks) {\n\t\t\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t\tverbose(env, \"R%d pointer %s pointer prohibited\\n\",\n\t\t\t\t\tinsn->dst_reg,\n\t\t\t\t\tbpf_alu_string[opcode >> 4]);\n\t\t\t\treturn -EACCES;\n\t\t\t} else {\n\t\t\t\t/* scalar += pointer\n\t\t\t\t * This is legal, but we have to reverse our\n\t\t\t\t * src/dest handling in computing the range\n\t\t\t\t */\n\t\t\t\treturn adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t\t       src_reg, dst_reg);\n\t\t\t}\n\t\t} else if (ptr_reg) {\n\t\t\t/* pointer += scalar */\n\t\t\treturn adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t       dst_reg, src_reg);\n\t\t}\n\t} else {\n\t\t/* Pretend the src is a reg with a known value, since we only\n\t\t * need to be able to read from this state.\n\t\t */\n\t\toff_reg.type = SCALAR_VALUE;\n\t\t__mark_reg_known(&off_reg, insn->imm);\n\t\tsrc_reg = &off_reg;\n\t\tif (ptr_reg) /* pointer += K */\n\t\t\treturn adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t       ptr_reg, src_reg);\n\t}\n\n\t/* Got here implies adding two SCALAR_VALUEs */\n\tif (WARN_ON_ONCE(ptr_reg)) {\n\t\tprint_verifier_state(env, state);\n\t\tverbose(env, \"verifier internal error: unexpected ptr_reg\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (WARN_ON(!src_reg)) {\n\t\tprint_verifier_state(env, state);\n\t\tverbose(env, \"verifier internal error: no src_reg\\n\");\n\t\treturn -EINVAL;\n\t}\n\treturn adjust_scalar_min_max_vals(env, insn, dst_reg, *src_reg);\n}\n\n/* check validity of 32-bit and 64-bit arithmetic operations */\nstatic int check_alu_op(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 opcode = BPF_OP(insn->code);\n\tint err;\n\n\tif (opcode == BPF_END || opcode == BPF_NEG) {\n\t\tif (opcode == BPF_NEG) {\n\t\t\tif (BPF_SRC(insn->code) != 0 ||\n\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t    insn->off != 0 || insn->imm != 0) {\n\t\t\t\tverbose(env, \"BPF_NEG uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else {\n\t\t\tif (insn->src_reg != BPF_REG_0 || insn->off != 0 ||\n\t\t\t    (insn->imm != 16 && insn->imm != 32 && insn->imm != 64) ||\n\t\t\t    BPF_CLASS(insn->code) == BPF_ALU64) {\n\t\t\t\tverbose(env, \"BPF_END uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check src operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (is_pointer_value(env, insn->dst_reg)) {\n\t\t\tverbose(env, \"R%d pointer arithmetic prohibited\\n\",\n\t\t\t\tinsn->dst_reg);\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\t/* check dest operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t} else if (opcode == BPF_MOV) {\n\n\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\tif (insn->imm != 0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_MOV uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\t/* check src operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t} else {\n\t\t\tif (insn->src_reg != BPF_REG_0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_MOV uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check dest operand, mark as required later */\n\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP_NO_MARK);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\tstruct bpf_reg_state *src_reg = regs + insn->src_reg;\n\t\t\tstruct bpf_reg_state *dst_reg = regs + insn->dst_reg;\n\n\t\t\tif (BPF_CLASS(insn->code) == BPF_ALU64) {\n\t\t\t\t/* case: R1 = R2\n\t\t\t\t * copy register state to dest reg\n\t\t\t\t */\n\t\t\t\t*dst_reg = *src_reg;\n\t\t\t\tdst_reg->live |= REG_LIVE_WRITTEN;\n\t\t\t} else {\n\t\t\t\t/* R1 = (u32) R2 */\n\t\t\t\tif (is_pointer_value(env, insn->src_reg)) {\n\t\t\t\t\tverbose(env,\n\t\t\t\t\t\t\"R%d partial copy of pointer\\n\",\n\t\t\t\t\t\tinsn->src_reg);\n\t\t\t\t\treturn -EACCES;\n\t\t\t\t} else if (src_reg->type == SCALAR_VALUE) {\n\t\t\t\t\t*dst_reg = *src_reg;\n\t\t\t\t\tdst_reg->live |= REG_LIVE_WRITTEN;\n\t\t\t\t} else {\n\t\t\t\t\tmark_reg_unknown(env, regs,\n\t\t\t\t\t\t\t insn->dst_reg);\n\t\t\t\t}\n\t\t\t\tcoerce_reg_to_size(dst_reg, 4);\n\t\t\t}\n\t\t} else {\n\t\t\t/* case: R = imm\n\t\t\t * remember the value we stored into this reg\n\t\t\t */\n\t\t\t/* clear any state __mark_reg_known doesn't set */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tregs[insn->dst_reg].type = SCALAR_VALUE;\n\t\t\tif (BPF_CLASS(insn->code) == BPF_ALU64) {\n\t\t\t\t__mark_reg_known(regs + insn->dst_reg,\n\t\t\t\t\t\t insn->imm);\n\t\t\t} else {\n\t\t\t\t__mark_reg_known(regs + insn->dst_reg,\n\t\t\t\t\t\t (u32)insn->imm);\n\t\t\t}\n\t\t}\n\n\t} else if (opcode > BPF_END) {\n\t\tverbose(env, \"invalid BPF_ALU opcode %x\\n\", opcode);\n\t\treturn -EINVAL;\n\n\t} else {\t/* all other ALU ops: and, sub, xor, add, ... */\n\n\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\tif (insn->imm != 0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_ALU uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\t/* check src1 operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t} else {\n\t\t\tif (insn->src_reg != BPF_REG_0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_ALU uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check src2 operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif ((opcode == BPF_MOD || opcode == BPF_DIV) &&\n\t\t    BPF_SRC(insn->code) == BPF_K && insn->imm == 0) {\n\t\t\tverbose(env, \"div by zero\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif ((opcode == BPF_LSH || opcode == BPF_RSH ||\n\t\t     opcode == BPF_ARSH) && BPF_SRC(insn->code) == BPF_K) {\n\t\t\tint size = BPF_CLASS(insn->code) == BPF_ALU64 ? 64 : 32;\n\n\t\t\tif (insn->imm < 0 || insn->imm >= size) {\n\t\t\t\tverbose(env, \"invalid shift %d\\n\", insn->imm);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check dest operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP_NO_MARK);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\treturn adjust_reg_min_max_vals(env, insn);\n\t}\n\n\treturn 0;\n}\n\nstatic void find_good_pkt_pointers(struct bpf_verifier_state *vstate,\n\t\t\t\t   struct bpf_reg_state *dst_reg,\n\t\t\t\t   enum bpf_reg_type type,\n\t\t\t\t   bool range_right_open)\n{\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *reg;\n\tu16 new_range;\n\tint i, j;\n\n\tif (dst_reg->off < 0 ||\n\t    (dst_reg->off == 0 && range_right_open))\n\t\t/* This doesn't give us any range */\n\t\treturn;\n\n\tif (dst_reg->umax_value > MAX_PACKET_OFF ||\n\t    dst_reg->umax_value + dst_reg->off > MAX_PACKET_OFF)\n\t\t/* Risk of overflow.  For instance, ptr + (1<<63) may be less\n\t\t * than pkt_end, but that's because it's also less than pkt.\n\t\t */\n\t\treturn;\n\n\tnew_range = dst_reg->off;\n\tif (range_right_open)\n\t\tnew_range--;\n\n\t/* Examples for register markings:\n\t *\n\t * pkt_data in dst register:\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (r2 > pkt_end) goto <handle exception>\n\t *   <access okay>\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (r2 < pkt_end) goto <access okay>\n\t *   <handle exception>\n\t *\n\t *   Where:\n\t *     r2 == dst_reg, pkt_end == src_reg\n\t *     r2=pkt(id=n,off=8,r=0)\n\t *     r3=pkt(id=n,off=0,r=0)\n\t *\n\t * pkt_data in src register:\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (pkt_end >= r2) goto <access okay>\n\t *   <handle exception>\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (pkt_end <= r2) goto <handle exception>\n\t *   <access okay>\n\t *\n\t *   Where:\n\t *     pkt_end == dst_reg, r2 == src_reg\n\t *     r2=pkt(id=n,off=8,r=0)\n\t *     r3=pkt(id=n,off=0,r=0)\n\t *\n\t * Find register r3 and mark its range as r3=pkt(id=n,off=0,r=8)\n\t * or r3=pkt(id=n,off=0,r=8-1), so that range of bytes [r3, r3 + 8)\n\t * and [r3, r3 + 8-1) respectively is safe to access depending on\n\t * the check.\n\t */\n\n\t/* If our ids match, then we must have the same max_value.  And we\n\t * don't care about the other reg's fixed offset, since if it's too big\n\t * the range won't allow anything.\n\t * dst_reg->off is known < MAX_PACKET_OFF, therefore it fits in a u16.\n\t */\n\tfor (i = 0; i < MAX_BPF_REG; i++)\n\t\tif (regs[i].type == type && regs[i].id == dst_reg->id)\n\t\t\t/* keep the maximum range already checked */\n\t\t\tregs[i].range = max(regs[i].range, new_range);\n\n\tfor (j = 0; j <= vstate->curframe; j++) {\n\t\tstate = vstate->frame[j];\n\t\tbpf_for_each_spilled_reg(i, state, reg) {\n\t\t\tif (!reg)\n\t\t\t\tcontinue;\n\t\t\tif (reg->type == type && reg->id == dst_reg->id)\n\t\t\t\treg->range = max(reg->range, new_range);\n\t\t}\n\t}\n}\n\n/* compute branch direction of the expression \"if (reg opcode val) goto target;\"\n * and return:\n *  1 - branch will be taken and \"goto target\" will be executed\n *  0 - branch will not be taken and fall-through to next insn\n * -1 - unknown. Example: \"if (reg < 5)\" is unknown when register value range [0,10]\n */\nstatic int is_branch_taken(struct bpf_reg_state *reg, u64 val, u8 opcode)\n{\n\tif (__is_pointer_value(false, reg))\n\t\treturn -1;\n\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\tif (tnum_is_const(reg->var_off))\n\t\t\treturn !!tnum_equals_const(reg->var_off, val);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\tif (tnum_is_const(reg->var_off))\n\t\t\treturn !tnum_equals_const(reg->var_off, val);\n\t\tbreak;\n\tcase BPF_JSET:\n\t\tif ((~reg->var_off.mask & reg->var_off.value) & val)\n\t\t\treturn 1;\n\t\tif (!((reg->var_off.mask | reg->var_off.value) & val))\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JGT:\n\t\tif (reg->umin_value > val)\n\t\t\treturn 1;\n\t\telse if (reg->umax_value <= val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSGT:\n\t\tif (reg->smin_value > (s64)val)\n\t\t\treturn 1;\n\t\telse if (reg->smax_value < (s64)val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JLT:\n\t\tif (reg->umax_value < val)\n\t\t\treturn 1;\n\t\telse if (reg->umin_value >= val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSLT:\n\t\tif (reg->smax_value < (s64)val)\n\t\t\treturn 1;\n\t\telse if (reg->smin_value >= (s64)val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JGE:\n\t\tif (reg->umin_value >= val)\n\t\t\treturn 1;\n\t\telse if (reg->umax_value < val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSGE:\n\t\tif (reg->smin_value >= (s64)val)\n\t\t\treturn 1;\n\t\telse if (reg->smax_value < (s64)val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JLE:\n\t\tif (reg->umax_value <= val)\n\t\t\treturn 1;\n\t\telse if (reg->umin_value > val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSLE:\n\t\tif (reg->smax_value <= (s64)val)\n\t\t\treturn 1;\n\t\telse if (reg->smin_value > (s64)val)\n\t\t\treturn 0;\n\t\tbreak;\n\t}\n\n\treturn -1;\n}\n\n/* Adjusts the register min/max values in the case that the dst_reg is the\n * variable register that we are working on, and src_reg is a constant or we're\n * simply doing a BPF_K check.\n * In JEQ/JNE cases we also adjust the var_off values.\n */\nstatic void reg_set_min_max(struct bpf_reg_state *true_reg,\n\t\t\t    struct bpf_reg_state *false_reg, u64 val,\n\t\t\t    u8 opcode)\n{\n\t/* If the dst_reg is a pointer, we can't learn anything about its\n\t * variable offset from the compare (unless src_reg were a pointer into\n\t * the same object, but we don't bother with that.\n\t * Since false_reg and true_reg have the same type by construction, we\n\t * only need to check one of them for pointerness.\n\t */\n\tif (__is_pointer_value(false, false_reg))\n\t\treturn;\n\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\t/* If this is false then we know nothing Jon Snow, but if it is\n\t\t * true then we know for sure.\n\t\t */\n\t\t__mark_reg_known(true_reg, val);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\t/* If this is true we know nothing Jon Snow, but if it is false\n\t\t * we know the value for sure;\n\t\t */\n\t\t__mark_reg_known(false_reg, val);\n\t\tbreak;\n\tcase BPF_JSET:\n\t\tfalse_reg->var_off = tnum_and(false_reg->var_off,\n\t\t\t\t\t      tnum_const(~val));\n\t\tif (is_power_of_2(val))\n\t\t\ttrue_reg->var_off = tnum_or(true_reg->var_off,\n\t\t\t\t\t\t    tnum_const(val));\n\t\tbreak;\n\tcase BPF_JGT:\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val);\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JSGT:\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val);\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JLT:\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val);\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val - 1);\n\t\tbreak;\n\tcase BPF_JSLT:\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val);\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val - 1);\n\t\tbreak;\n\tcase BPF_JGE:\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val - 1);\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val);\n\t\tbreak;\n\tcase BPF_JSGE:\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val - 1);\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val);\n\t\tbreak;\n\tcase BPF_JLE:\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val + 1);\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val);\n\t\tbreak;\n\tcase BPF_JSLE:\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val + 1);\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t__reg_deduce_bounds(false_reg);\n\t__reg_deduce_bounds(true_reg);\n\t/* We might have learned some bits from the bounds. */\n\t__reg_bound_offset(false_reg);\n\t__reg_bound_offset(true_reg);\n\t/* Intersecting with the old var_off might have improved our bounds\n\t * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),\n\t * then new var_off is (0; 0x7f...fc) which improves our umax.\n\t */\n\t__update_reg_bounds(false_reg);\n\t__update_reg_bounds(true_reg);\n}\n\n/* Same as above, but for the case that dst_reg holds a constant and src_reg is\n * the variable reg.\n */\nstatic void reg_set_min_max_inv(struct bpf_reg_state *true_reg,\n\t\t\t\tstruct bpf_reg_state *false_reg, u64 val,\n\t\t\t\tu8 opcode)\n{\n\tif (__is_pointer_value(false, false_reg))\n\t\treturn;\n\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\t/* If this is false then we know nothing Jon Snow, but if it is\n\t\t * true then we know for sure.\n\t\t */\n\t\t__mark_reg_known(true_reg, val);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\t/* If this is true we know nothing Jon Snow, but if it is false\n\t\t * we know the value for sure;\n\t\t */\n\t\t__mark_reg_known(false_reg, val);\n\t\tbreak;\n\tcase BPF_JSET:\n\t\tfalse_reg->var_off = tnum_and(false_reg->var_off,\n\t\t\t\t\t      tnum_const(~val));\n\t\tif (is_power_of_2(val))\n\t\t\ttrue_reg->var_off = tnum_or(true_reg->var_off,\n\t\t\t\t\t\t    tnum_const(val));\n\t\tbreak;\n\tcase BPF_JGT:\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val - 1);\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val);\n\t\tbreak;\n\tcase BPF_JSGT:\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val - 1);\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val);\n\t\tbreak;\n\tcase BPF_JLT:\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val + 1);\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val);\n\t\tbreak;\n\tcase BPF_JSLT:\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val + 1);\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val);\n\t\tbreak;\n\tcase BPF_JGE:\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val);\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JSGE:\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val);\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JLE:\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val);\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val - 1);\n\t\tbreak;\n\tcase BPF_JSLE:\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val);\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val - 1);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t__reg_deduce_bounds(false_reg);\n\t__reg_deduce_bounds(true_reg);\n\t/* We might have learned some bits from the bounds. */\n\t__reg_bound_offset(false_reg);\n\t__reg_bound_offset(true_reg);\n\t/* Intersecting with the old var_off might have improved our bounds\n\t * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),\n\t * then new var_off is (0; 0x7f...fc) which improves our umax.\n\t */\n\t__update_reg_bounds(false_reg);\n\t__update_reg_bounds(true_reg);\n}\n\n/* Regs are known to be equal, so intersect their min/max/var_off */\nstatic void __reg_combine_min_max(struct bpf_reg_state *src_reg,\n\t\t\t\t  struct bpf_reg_state *dst_reg)\n{\n\tsrc_reg->umin_value = dst_reg->umin_value = max(src_reg->umin_value,\n\t\t\t\t\t\t\tdst_reg->umin_value);\n\tsrc_reg->umax_value = dst_reg->umax_value = min(src_reg->umax_value,\n\t\t\t\t\t\t\tdst_reg->umax_value);\n\tsrc_reg->smin_value = dst_reg->smin_value = max(src_reg->smin_value,\n\t\t\t\t\t\t\tdst_reg->smin_value);\n\tsrc_reg->smax_value = dst_reg->smax_value = min(src_reg->smax_value,\n\t\t\t\t\t\t\tdst_reg->smax_value);\n\tsrc_reg->var_off = dst_reg->var_off = tnum_intersect(src_reg->var_off,\n\t\t\t\t\t\t\t     dst_reg->var_off);\n\t/* We might have learned new bounds from the var_off. */\n\t__update_reg_bounds(src_reg);\n\t__update_reg_bounds(dst_reg);\n\t/* We might have learned something about the sign bit. */\n\t__reg_deduce_bounds(src_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t/* We might have learned some bits from the bounds. */\n\t__reg_bound_offset(src_reg);\n\t__reg_bound_offset(dst_reg);\n\t/* Intersecting with the old var_off might have improved our bounds\n\t * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),\n\t * then new var_off is (0; 0x7f...fc) which improves our umax.\n\t */\n\t__update_reg_bounds(src_reg);\n\t__update_reg_bounds(dst_reg);\n}\n\nstatic void reg_combine_min_max(struct bpf_reg_state *true_src,\n\t\t\t\tstruct bpf_reg_state *true_dst,\n\t\t\t\tstruct bpf_reg_state *false_src,\n\t\t\t\tstruct bpf_reg_state *false_dst,\n\t\t\t\tu8 opcode)\n{\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\t__reg_combine_min_max(true_src, true_dst);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\t__reg_combine_min_max(false_src, false_dst);\n\t\tbreak;\n\t}\n}\n\nstatic void mark_ptr_or_null_reg(struct bpf_func_state *state,\n\t\t\t\t struct bpf_reg_state *reg, u32 id,\n\t\t\t\t bool is_null)\n{\n\tif (reg_type_may_be_null(reg->type) && reg->id == id) {\n\t\t/* Old offset (both fixed and variable parts) should\n\t\t * have been known-zero, because we don't allow pointer\n\t\t * arithmetic on pointers that might be NULL.\n\t\t */\n\t\tif (WARN_ON_ONCE(reg->smin_value || reg->smax_value ||\n\t\t\t\t !tnum_equals_const(reg->var_off, 0) ||\n\t\t\t\t reg->off)) {\n\t\t\t__mark_reg_known_zero(reg);\n\t\t\treg->off = 0;\n\t\t}\n\t\tif (is_null) {\n\t\t\treg->type = SCALAR_VALUE;\n\t\t} else if (reg->type == PTR_TO_MAP_VALUE_OR_NULL) {\n\t\t\tif (reg->map_ptr->inner_map_meta) {\n\t\t\t\treg->type = CONST_PTR_TO_MAP;\n\t\t\t\treg->map_ptr = reg->map_ptr->inner_map_meta;\n\t\t\t} else {\n\t\t\t\treg->type = PTR_TO_MAP_VALUE;\n\t\t\t}\n\t\t} else if (reg->type == PTR_TO_SOCKET_OR_NULL) {\n\t\t\treg->type = PTR_TO_SOCKET;\n\t\t}\n\t\tif (is_null || !reg_is_refcounted(reg)) {\n\t\t\t/* We don't need id from this point onwards anymore,\n\t\t\t * thus we should better reset it, so that state\n\t\t\t * pruning has chances to take effect.\n\t\t\t */\n\t\t\treg->id = 0;\n\t\t}\n\t}\n}\n\n/* The logic is similar to find_good_pkt_pointers(), both could eventually\n * be folded together at some point.\n */\nstatic void mark_ptr_or_null_regs(struct bpf_verifier_state *vstate, u32 regno,\n\t\t\t\t  bool is_null)\n{\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *reg, *regs = state->regs;\n\tu32 id = regs[regno].id;\n\tint i, j;\n\n\tif (reg_is_refcounted_or_null(&regs[regno]) && is_null)\n\t\t__release_reference_state(state, id);\n\n\tfor (i = 0; i < MAX_BPF_REG; i++)\n\t\tmark_ptr_or_null_reg(state, &regs[i], id, is_null);\n\n\tfor (j = 0; j <= vstate->curframe; j++) {\n\t\tstate = vstate->frame[j];\n\t\tbpf_for_each_spilled_reg(i, state, reg) {\n\t\t\tif (!reg)\n\t\t\t\tcontinue;\n\t\t\tmark_ptr_or_null_reg(state, reg, id, is_null);\n\t\t}\n\t}\n}\n\nstatic bool try_match_pkt_pointers(const struct bpf_insn *insn,\n\t\t\t\t   struct bpf_reg_state *dst_reg,\n\t\t\t\t   struct bpf_reg_state *src_reg,\n\t\t\t\t   struct bpf_verifier_state *this_branch,\n\t\t\t\t   struct bpf_verifier_state *other_branch)\n{\n\tif (BPF_SRC(insn->code) != BPF_X)\n\t\treturn false;\n\n\tswitch (BPF_OP(insn->code)) {\n\tcase BPF_JGT:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' > pkt_end, pkt_meta' > pkt_data */\n\t\t\tfind_good_pkt_pointers(this_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, false);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end > pkt_data', pkt_data > pkt_meta' */\n\t\t\tfind_good_pkt_pointers(other_branch, src_reg,\n\t\t\t\t\t       src_reg->type, true);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase BPF_JLT:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' < pkt_end, pkt_meta' < pkt_data */\n\t\t\tfind_good_pkt_pointers(other_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, true);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end < pkt_data', pkt_data > pkt_meta' */\n\t\t\tfind_good_pkt_pointers(this_branch, src_reg,\n\t\t\t\t\t       src_reg->type, false);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase BPF_JGE:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' >= pkt_end, pkt_meta' >= pkt_data */\n\t\t\tfind_good_pkt_pointers(this_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, true);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end >= pkt_data', pkt_data >= pkt_meta' */\n\t\t\tfind_good_pkt_pointers(other_branch, src_reg,\n\t\t\t\t\t       src_reg->type, false);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase BPF_JLE:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' <= pkt_end, pkt_meta' <= pkt_data */\n\t\t\tfind_good_pkt_pointers(other_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, false);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end <= pkt_data', pkt_data <= pkt_meta' */\n\t\t\tfind_good_pkt_pointers(this_branch, src_reg,\n\t\t\t\t\t       src_reg->type, true);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic int check_cond_jmp_op(struct bpf_verifier_env *env,\n\t\t\t     struct bpf_insn *insn, int *insn_idx)\n{\n\tstruct bpf_verifier_state *this_branch = env->cur_state;\n\tstruct bpf_verifier_state *other_branch;\n\tstruct bpf_reg_state *regs = this_branch->frame[this_branch->curframe]->regs;\n\tstruct bpf_reg_state *dst_reg, *other_branch_regs;\n\tu8 opcode = BPF_OP(insn->code);\n\tint err;\n\n\tif (opcode > BPF_JSLE) {\n\t\tverbose(env, \"invalid BPF_JMP opcode %x\\n\", opcode);\n\t\treturn -EINVAL;\n\t}\n\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tif (insn->imm != 0) {\n\t\t\tverbose(env, \"BPF_JMP uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/* check src1 operand */\n\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (is_pointer_value(env, insn->src_reg)) {\n\t\t\tverbose(env, \"R%d pointer comparison prohibited\\n\",\n\t\t\t\tinsn->src_reg);\n\t\t\treturn -EACCES;\n\t\t}\n\t} else {\n\t\tif (insn->src_reg != BPF_REG_0) {\n\t\t\tverbose(env, \"BPF_JMP uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t/* check src2 operand */\n\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tdst_reg = &regs[insn->dst_reg];\n\n\tif (BPF_SRC(insn->code) == BPF_K) {\n\t\tint pred = is_branch_taken(dst_reg, insn->imm, opcode);\n\n\t\tif (pred == 1) {\n\t\t\t /* only follow the goto, ignore fall-through */\n\t\t\t*insn_idx += insn->off;\n\t\t\treturn 0;\n\t\t} else if (pred == 0) {\n\t\t\t/* only follow fall-through branch, since\n\t\t\t * that's where the program will go\n\t\t\t */\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tother_branch = push_stack(env, *insn_idx + insn->off + 1, *insn_idx,\n\t\t\t\t  false);\n\tif (!other_branch)\n\t\treturn -EFAULT;\n\tother_branch_regs = other_branch->frame[other_branch->curframe]->regs;\n\n\t/* detect if we are comparing against a constant value so we can adjust\n\t * our min/max values for our dst register.\n\t * this is only legit if both are scalars (or pointers to the same\n\t * object, I suppose, but we don't support that right now), because\n\t * otherwise the different base pointers mean the offsets aren't\n\t * comparable.\n\t */\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tif (dst_reg->type == SCALAR_VALUE &&\n\t\t    regs[insn->src_reg].type == SCALAR_VALUE) {\n\t\t\tif (tnum_is_const(regs[insn->src_reg].var_off))\n\t\t\t\treg_set_min_max(&other_branch_regs[insn->dst_reg],\n\t\t\t\t\t\tdst_reg, regs[insn->src_reg].var_off.value,\n\t\t\t\t\t\topcode);\n\t\t\telse if (tnum_is_const(dst_reg->var_off))\n\t\t\t\treg_set_min_max_inv(&other_branch_regs[insn->src_reg],\n\t\t\t\t\t\t    &regs[insn->src_reg],\n\t\t\t\t\t\t    dst_reg->var_off.value, opcode);\n\t\t\telse if (opcode == BPF_JEQ || opcode == BPF_JNE)\n\t\t\t\t/* Comparing for equality, we can combine knowledge */\n\t\t\t\treg_combine_min_max(&other_branch_regs[insn->src_reg],\n\t\t\t\t\t\t    &other_branch_regs[insn->dst_reg],\n\t\t\t\t\t\t    &regs[insn->src_reg],\n\t\t\t\t\t\t    &regs[insn->dst_reg], opcode);\n\t\t}\n\t} else if (dst_reg->type == SCALAR_VALUE) {\n\t\treg_set_min_max(&other_branch_regs[insn->dst_reg],\n\t\t\t\t\tdst_reg, insn->imm, opcode);\n\t}\n\n\t/* detect if R == 0 where R is returned from bpf_map_lookup_elem() */\n\tif (BPF_SRC(insn->code) == BPF_K &&\n\t    insn->imm == 0 && (opcode == BPF_JEQ || opcode == BPF_JNE) &&\n\t    reg_type_may_be_null(dst_reg->type)) {\n\t\t/* Mark all identical registers in each branch as either\n\t\t * safe or unknown depending R == 0 or R != 0 conditional.\n\t\t */\n\t\tmark_ptr_or_null_regs(this_branch, insn->dst_reg,\n\t\t\t\t      opcode == BPF_JNE);\n\t\tmark_ptr_or_null_regs(other_branch, insn->dst_reg,\n\t\t\t\t      opcode == BPF_JEQ);\n\t} else if (!try_match_pkt_pointers(insn, dst_reg, &regs[insn->src_reg],\n\t\t\t\t\t   this_branch, other_branch) &&\n\t\t   is_pointer_value(env, insn->dst_reg)) {\n\t\tverbose(env, \"R%d pointer comparison prohibited\\n\",\n\t\t\tinsn->dst_reg);\n\t\treturn -EACCES;\n\t}\n\tif (env->log.level)\n\t\tprint_verifier_state(env, this_branch->frame[this_branch->curframe]);\n\treturn 0;\n}\n\n/* return the map pointer stored inside BPF_LD_IMM64 instruction */\nstatic struct bpf_map *ld_imm64_to_map_ptr(struct bpf_insn *insn)\n{\n\tu64 imm64 = ((u64) (u32) insn[0].imm) | ((u64) (u32) insn[1].imm) << 32;\n\n\treturn (struct bpf_map *) (unsigned long) imm64;\n}\n\n/* verify BPF_LD_IMM64 instruction */\nstatic int check_ld_imm(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tint err;\n\n\tif (BPF_SIZE(insn->code) != BPF_DW) {\n\t\tverbose(env, \"invalid BPF_LD_IMM insn\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (insn->off != 0) {\n\t\tverbose(env, \"BPF_LD_IMM64 uses reserved fields\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\terr = check_reg_arg(env, insn->dst_reg, DST_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (insn->src_reg == 0) {\n\t\tu64 imm = ((u64)(insn + 1)->imm << 32) | (u32)insn->imm;\n\n\t\tregs[insn->dst_reg].type = SCALAR_VALUE;\n\t\t__mark_reg_known(&regs[insn->dst_reg], imm);\n\t\treturn 0;\n\t}\n\n\t/* replace_map_fd_with_map_ptr() should have caught bad ld_imm64 */\n\tBUG_ON(insn->src_reg != BPF_PSEUDO_MAP_FD);\n\n\tregs[insn->dst_reg].type = CONST_PTR_TO_MAP;\n\tregs[insn->dst_reg].map_ptr = ld_imm64_to_map_ptr(insn);\n\treturn 0;\n}\n\nstatic bool may_access_skb(enum bpf_prog_type type)\n{\n\tswitch (type) {\n\tcase BPF_PROG_TYPE_SOCKET_FILTER:\n\tcase BPF_PROG_TYPE_SCHED_CLS:\n\tcase BPF_PROG_TYPE_SCHED_ACT:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n/* verify safety of LD_ABS|LD_IND instructions:\n * - they can only appear in the programs where ctx == skb\n * - since they are wrappers of function calls, they scratch R1-R5 registers,\n *   preserve R6-R9, and store return value into R0\n *\n * Implicit input:\n *   ctx == skb == R6 == CTX\n *\n * Explicit input:\n *   SRC == any register\n *   IMM == 32-bit immediate\n *\n * Output:\n *   R0 - 8/16/32-bit skb data converted to cpu endianness\n */\nstatic int check_ld_abs(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 mode = BPF_MODE(insn->code);\n\tint i, err;\n\n\tif (!may_access_skb(env->prog->type)) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] instructions not allowed for this program type\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!env->ops->gen_ld_abs) {\n\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (env->subprog_cnt > 1) {\n\t\t/* when program has LD_ABS insn JITs and interpreter assume\n\t\t * that r1 == ctx == skb which is not the case for callees\n\t\t * that can have arbitrary arguments. It's problematic\n\t\t * for main prog as well since JITs would need to analyze\n\t\t * all functions in order to make proper register save/restore\n\t\t * decisions in the main prog. Hence disallow LD_ABS with calls\n\t\t */\n\t\tverbose(env, \"BPF_LD_[ABS|IND] instructions cannot be mixed with bpf-to-bpf calls\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (insn->dst_reg != BPF_REG_0 || insn->off != 0 ||\n\t    BPF_SIZE(insn->code) == BPF_DW ||\n\t    (mode == BPF_ABS && insn->src_reg != BPF_REG_0)) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] uses reserved fields\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* check whether implicit source operand (register R6) is readable */\n\terr = check_reg_arg(env, BPF_REG_6, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\t/* Disallow usage of BPF_LD_[ABS|IND] with reference tracking, as\n\t * gen_ld_abs() may terminate the program at runtime, leading to\n\t * reference leak.\n\t */\n\terr = check_reference_leak(env);\n\tif (err) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] cannot be mixed with socket references\\n\");\n\t\treturn err;\n\t}\n\n\tif (regs[BPF_REG_6].type != PTR_TO_CTX) {\n\t\tverbose(env,\n\t\t\t\"at the time of BPF_LD_ABS|IND R6 != pointer to skb\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (mode == BPF_IND) {\n\t\t/* check explicit source operand */\n\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\t/* reset caller saved regs to unreadable */\n\tfor (i = 0; i < CALLER_SAVED_REGS; i++) {\n\t\tmark_reg_not_init(env, regs, caller_saved[i]);\n\t\tcheck_reg_arg(env, caller_saved[i], DST_OP_NO_MARK);\n\t}\n\n\t/* mark destination R0 register as readable, since it contains\n\t * the value fetched from the packet.\n\t * Already marked as written above.\n\t */\n\tmark_reg_unknown(env, regs, BPF_REG_0);\n\treturn 0;\n}\n\nstatic int check_return_code(struct bpf_verifier_env *env)\n{\n\tstruct bpf_reg_state *reg;\n\tstruct tnum range = tnum_range(0, 1);\n\n\tswitch (env->prog->type) {\n\tcase BPF_PROG_TYPE_CGROUP_SKB:\n\tcase BPF_PROG_TYPE_CGROUP_SOCK:\n\tcase BPF_PROG_TYPE_CGROUP_SOCK_ADDR:\n\tcase BPF_PROG_TYPE_SOCK_OPS:\n\tcase BPF_PROG_TYPE_CGROUP_DEVICE:\n\t\tbreak;\n\tdefault:\n\t\treturn 0;\n\t}\n\n\treg = cur_regs(env) + BPF_REG_0;\n\tif (reg->type != SCALAR_VALUE) {\n\t\tverbose(env, \"At program exit the register R0 is not a known value (%s)\\n\",\n\t\t\treg_type_str[reg->type]);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!tnum_in(range, reg->var_off)) {\n\t\tverbose(env, \"At program exit the register R0 \");\n\t\tif (!tnum_is_unknown(reg->var_off)) {\n\t\t\tchar tn_buf[48];\n\n\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\tverbose(env, \"has value %s\", tn_buf);\n\t\t} else {\n\t\t\tverbose(env, \"has unknown scalar value\");\n\t\t}\n\t\tverbose(env, \" should have been 0 or 1\\n\");\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\n/* non-recursive DFS pseudo code\n * 1  procedure DFS-iterative(G,v):\n * 2      label v as discovered\n * 3      let S be a stack\n * 4      S.push(v)\n * 5      while S is not empty\n * 6            t <- S.pop()\n * 7            if t is what we're looking for:\n * 8                return t\n * 9            for all edges e in G.adjacentEdges(t) do\n * 10               if edge e is already labelled\n * 11                   continue with the next edge\n * 12               w <- G.adjacentVertex(t,e)\n * 13               if vertex w is not discovered and not explored\n * 14                   label e as tree-edge\n * 15                   label w as discovered\n * 16                   S.push(w)\n * 17                   continue at 5\n * 18               else if vertex w is discovered\n * 19                   label e as back-edge\n * 20               else\n * 21                   // vertex w is explored\n * 22                   label e as forward- or cross-edge\n * 23           label t as explored\n * 24           S.pop()\n *\n * convention:\n * 0x10 - discovered\n * 0x11 - discovered and fall-through edge labelled\n * 0x12 - discovered and fall-through and branch edges labelled\n * 0x20 - explored\n */\n\nenum {\n\tDISCOVERED = 0x10,\n\tEXPLORED = 0x20,\n\tFALLTHROUGH = 1,\n\tBRANCH = 2,\n};\n\n#define STATE_LIST_MARK ((struct bpf_verifier_state_list *) -1L)\n\nstatic int *insn_stack;\t/* stack of insns to process */\nstatic int cur_stack;\t/* current stack index */\nstatic int *insn_state;\n\n/* t, w, e - match pseudo-code above:\n * t - index of current instruction\n * w - next instruction\n * e - edge\n */\nstatic int push_insn(int t, int w, int e, struct bpf_verifier_env *env)\n{\n\tif (e == FALLTHROUGH && insn_state[t] >= (DISCOVERED | FALLTHROUGH))\n\t\treturn 0;\n\n\tif (e == BRANCH && insn_state[t] >= (DISCOVERED | BRANCH))\n\t\treturn 0;\n\n\tif (w < 0 || w >= env->prog->len) {\n\t\tverbose_linfo(env, t, \"%d: \", t);\n\t\tverbose(env, \"jump out of range from insn %d to %d\\n\", t, w);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e == BRANCH)\n\t\t/* mark branch target for state pruning */\n\t\tenv->explored_states[w] = STATE_LIST_MARK;\n\n\tif (insn_state[w] == 0) {\n\t\t/* tree-edge */\n\t\tinsn_state[t] = DISCOVERED | e;\n\t\tinsn_state[w] = DISCOVERED;\n\t\tif (cur_stack >= env->prog->len)\n\t\t\treturn -E2BIG;\n\t\tinsn_stack[cur_stack++] = w;\n\t\treturn 1;\n\t} else if ((insn_state[w] & 0xF0) == DISCOVERED) {\n\t\tverbose_linfo(env, t, \"%d: \", t);\n\t\tverbose_linfo(env, w, \"%d: \", w);\n\t\tverbose(env, \"back-edge from insn %d to %d\\n\", t, w);\n\t\treturn -EINVAL;\n\t} else if (insn_state[w] == EXPLORED) {\n\t\t/* forward- or cross-edge */\n\t\tinsn_state[t] = DISCOVERED | e;\n\t} else {\n\t\tverbose(env, \"insn state internal bug\\n\");\n\t\treturn -EFAULT;\n\t}\n\treturn 0;\n}\n\n/* non-recursive depth-first-search to detect loops in BPF program\n * loop == back-edge in directed graph\n */\nstatic int check_cfg(struct bpf_verifier_env *env)\n{\n\tstruct bpf_insn *insns = env->prog->insnsi;\n\tint insn_cnt = env->prog->len;\n\tint ret = 0;\n\tint i, t;\n\n\tinsn_state = kcalloc(insn_cnt, sizeof(int), GFP_KERNEL);\n\tif (!insn_state)\n\t\treturn -ENOMEM;\n\n\tinsn_stack = kcalloc(insn_cnt, sizeof(int), GFP_KERNEL);\n\tif (!insn_stack) {\n\t\tkfree(insn_state);\n\t\treturn -ENOMEM;\n\t}\n\n\tinsn_state[0] = DISCOVERED; /* mark 1st insn as discovered */\n\tinsn_stack[0] = 0; /* 0 is the first instruction */\n\tcur_stack = 1;\n\npeek_stack:\n\tif (cur_stack == 0)\n\t\tgoto check_state;\n\tt = insn_stack[cur_stack - 1];\n\n\tif (BPF_CLASS(insns[t].code) == BPF_JMP) {\n\t\tu8 opcode = BPF_OP(insns[t].code);\n\n\t\tif (opcode == BPF_EXIT) {\n\t\t\tgoto mark_explored;\n\t\t} else if (opcode == BPF_CALL) {\n\t\t\tret = push_insn(t, t + 1, FALLTHROUGH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t\tif (t + 1 < insn_cnt)\n\t\t\t\tenv->explored_states[t + 1] = STATE_LIST_MARK;\n\t\t\tif (insns[t].src_reg == BPF_PSEUDO_CALL) {\n\t\t\t\tenv->explored_states[t] = STATE_LIST_MARK;\n\t\t\t\tret = push_insn(t, t + insns[t].imm + 1, BRANCH, env);\n\t\t\t\tif (ret == 1)\n\t\t\t\t\tgoto peek_stack;\n\t\t\t\telse if (ret < 0)\n\t\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t} else if (opcode == BPF_JA) {\n\t\t\tif (BPF_SRC(insns[t].code) != BPF_K) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t\t/* unconditional jump with single edge */\n\t\t\tret = push_insn(t, t + insns[t].off + 1,\n\t\t\t\t\tFALLTHROUGH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t\t/* tell verifier to check for equivalent states\n\t\t\t * after every call and jump\n\t\t\t */\n\t\t\tif (t + 1 < insn_cnt)\n\t\t\t\tenv->explored_states[t + 1] = STATE_LIST_MARK;\n\t\t} else {\n\t\t\t/* conditional jump with two edges */\n\t\t\tenv->explored_states[t] = STATE_LIST_MARK;\n\t\t\tret = push_insn(t, t + 1, FALLTHROUGH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\n\t\t\tret = push_insn(t, t + insns[t].off + 1, BRANCH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t}\n\t} else {\n\t\t/* all other non-branch instructions with single\n\t\t * fall-through edge\n\t\t */\n\t\tret = push_insn(t, t + 1, FALLTHROUGH, env);\n\t\tif (ret == 1)\n\t\t\tgoto peek_stack;\n\t\telse if (ret < 0)\n\t\t\tgoto err_free;\n\t}\n\nmark_explored:\n\tinsn_state[t] = EXPLORED;\n\tif (cur_stack-- <= 0) {\n\t\tverbose(env, \"pop stack internal bug\\n\");\n\t\tret = -EFAULT;\n\t\tgoto err_free;\n\t}\n\tgoto peek_stack;\n\ncheck_state:\n\tfor (i = 0; i < insn_cnt; i++) {\n\t\tif (insn_state[i] != EXPLORED) {\n\t\t\tverbose(env, \"unreachable insn %d\\n\", i);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\t}\n\tret = 0; /* cfg looks good */\n\nerr_free:\n\tkfree(insn_state);\n\tkfree(insn_stack);\n\treturn ret;\n}\n\n/* The minimum supported BTF func info size */\n#define MIN_BPF_FUNCINFO_SIZE\t8\n#define MAX_FUNCINFO_REC_SIZE\t252\n\nstatic int check_btf_func(struct bpf_verifier_env *env,\n\t\t\t  const union bpf_attr *attr,\n\t\t\t  union bpf_attr __user *uattr)\n{\n\tu32 i, nfuncs, urec_size, min_size, prev_offset;\n\tu32 krec_size = sizeof(struct bpf_func_info);\n\tstruct bpf_func_info *krecord;\n\tconst struct btf_type *type;\n\tstruct bpf_prog *prog;\n\tconst struct btf *btf;\n\tvoid __user *urecord;\n\tint ret = 0;\n\n\tnfuncs = attr->func_info_cnt;\n\tif (!nfuncs)\n\t\treturn 0;\n\n\tif (nfuncs != env->subprog_cnt) {\n\t\tverbose(env, \"number of funcs in func_info doesn't match number of subprogs\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\turec_size = attr->func_info_rec_size;\n\tif (urec_size < MIN_BPF_FUNCINFO_SIZE ||\n\t    urec_size > MAX_FUNCINFO_REC_SIZE ||\n\t    urec_size % sizeof(u32)) {\n\t\tverbose(env, \"invalid func info rec size %u\\n\", urec_size);\n\t\treturn -EINVAL;\n\t}\n\n\tprog = env->prog;\n\tbtf = prog->aux->btf;\n\n\turecord = u64_to_user_ptr(attr->func_info);\n\tmin_size = min_t(u32, krec_size, urec_size);\n\n\tkrecord = kvcalloc(nfuncs, krec_size, GFP_KERNEL | __GFP_NOWARN);\n\tif (!krecord)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < nfuncs; i++) {\n\t\tret = bpf_check_uarg_tail_zero(urecord, krec_size, urec_size);\n\t\tif (ret) {\n\t\t\tif (ret == -E2BIG) {\n\t\t\t\tverbose(env, \"nonzero tailing record in func info\");\n\t\t\t\t/* set the size kernel expects so loader can zero\n\t\t\t\t * out the rest of the record.\n\t\t\t\t */\n\t\t\t\tif (put_user(min_size, &uattr->func_info_rec_size))\n\t\t\t\t\tret = -EFAULT;\n\t\t\t}\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (copy_from_user(&krecord[i], urecord, min_size)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/* check insn_off */\n\t\tif (i == 0) {\n\t\t\tif (krecord[i].insn_off) {\n\t\t\t\tverbose(env,\n\t\t\t\t\t\"nonzero insn_off %u for the first func info record\",\n\t\t\t\t\tkrecord[i].insn_off);\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t} else if (krecord[i].insn_off <= prev_offset) {\n\t\t\tverbose(env,\n\t\t\t\t\"same or smaller insn offset (%u) than previous func info record (%u)\",\n\t\t\t\tkrecord[i].insn_off, prev_offset);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (env->subprog_info[i].start != krecord[i].insn_off) {\n\t\t\tverbose(env, \"func_info BTF section doesn't match subprog layout in BPF program\\n\");\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/* check type_id */\n\t\ttype = btf_type_by_id(btf, krecord[i].type_id);\n\t\tif (!type || BTF_INFO_KIND(type->info) != BTF_KIND_FUNC) {\n\t\t\tverbose(env, \"invalid type id %d in func info\",\n\t\t\t\tkrecord[i].type_id);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tprev_offset = krecord[i].insn_off;\n\t\turecord += urec_size;\n\t}\n\n\tprog->aux->func_info = krecord;\n\tprog->aux->func_info_cnt = nfuncs;\n\treturn 0;\n\nerr_free:\n\tkvfree(krecord);\n\treturn ret;\n}\n\nstatic void adjust_btf_func(struct bpf_verifier_env *env)\n{\n\tint i;\n\n\tif (!env->prog->aux->func_info)\n\t\treturn;\n\n\tfor (i = 0; i < env->subprog_cnt; i++)\n\t\tenv->prog->aux->func_info[i].insn_off = env->subprog_info[i].start;\n}\n\n#define MIN_BPF_LINEINFO_SIZE\t(offsetof(struct bpf_line_info, line_col) + \\\n\t\tsizeof(((struct bpf_line_info *)(0))->line_col))\n#define MAX_LINEINFO_REC_SIZE\tMAX_FUNCINFO_REC_SIZE\n\nstatic int check_btf_line(struct bpf_verifier_env *env,\n\t\t\t  const union bpf_attr *attr,\n\t\t\t  union bpf_attr __user *uattr)\n{\n\tu32 i, s, nr_linfo, ncopy, expected_size, rec_size, prev_offset = 0;\n\tstruct bpf_subprog_info *sub;\n\tstruct bpf_line_info *linfo;\n\tstruct bpf_prog *prog;\n\tconst struct btf *btf;\n\tvoid __user *ulinfo;\n\tint err;\n\n\tnr_linfo = attr->line_info_cnt;\n\tif (!nr_linfo)\n\t\treturn 0;\n\n\trec_size = attr->line_info_rec_size;\n\tif (rec_size < MIN_BPF_LINEINFO_SIZE ||\n\t    rec_size > MAX_LINEINFO_REC_SIZE ||\n\t    rec_size & (sizeof(u32) - 1))\n\t\treturn -EINVAL;\n\n\t/* Need to zero it in case the userspace may\n\t * pass in a smaller bpf_line_info object.\n\t */\n\tlinfo = kvcalloc(nr_linfo, sizeof(struct bpf_line_info),\n\t\t\t GFP_KERNEL | __GFP_NOWARN);\n\tif (!linfo)\n\t\treturn -ENOMEM;\n\n\tprog = env->prog;\n\tbtf = prog->aux->btf;\n\n\ts = 0;\n\tsub = env->subprog_info;\n\tulinfo = u64_to_user_ptr(attr->line_info);\n\texpected_size = sizeof(struct bpf_line_info);\n\tncopy = min_t(u32, expected_size, rec_size);\n\tfor (i = 0; i < nr_linfo; i++) {\n\t\terr = bpf_check_uarg_tail_zero(ulinfo, expected_size, rec_size);\n\t\tif (err) {\n\t\t\tif (err == -E2BIG) {\n\t\t\t\tverbose(env, \"nonzero tailing record in line_info\");\n\t\t\t\tif (put_user(expected_size,\n\t\t\t\t\t     &uattr->line_info_rec_size))\n\t\t\t\t\terr = -EFAULT;\n\t\t\t}\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (copy_from_user(&linfo[i], ulinfo, ncopy)) {\n\t\t\terr = -EFAULT;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/*\n\t\t * Check insn_off to ensure\n\t\t * 1) strictly increasing AND\n\t\t * 2) bounded by prog->len\n\t\t *\n\t\t * The linfo[0].insn_off == 0 check logically falls into\n\t\t * the later \"missing bpf_line_info for func...\" case\n\t\t * because the first linfo[0].insn_off must be the\n\t\t * first sub also and the first sub must have\n\t\t * subprog_info[0].start == 0.\n\t\t */\n\t\tif ((i && linfo[i].insn_off <= prev_offset) ||\n\t\t    linfo[i].insn_off >= prog->len) {\n\t\t\tverbose(env, \"Invalid line_info[%u].insn_off:%u (prev_offset:%u prog->len:%u)\\n\",\n\t\t\t\ti, linfo[i].insn_off, prev_offset,\n\t\t\t\tprog->len);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (!prog->insnsi[linfo[i].insn_off].code) {\n\t\t\tverbose(env,\n\t\t\t\t\"Invalid insn code at line_info[%u].insn_off\\n\",\n\t\t\t\ti);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (!btf_name_by_offset(btf, linfo[i].line_off) ||\n\t\t    !btf_name_by_offset(btf, linfo[i].file_name_off)) {\n\t\t\tverbose(env, \"Invalid line_info[%u].line_off or .file_name_off\\n\", i);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (s != env->subprog_cnt) {\n\t\t\tif (linfo[i].insn_off == sub[s].start) {\n\t\t\t\tsub[s].linfo_idx = i;\n\t\t\t\ts++;\n\t\t\t} else if (sub[s].start < linfo[i].insn_off) {\n\t\t\t\tverbose(env, \"missing bpf_line_info for func#%u\\n\", s);\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t}\n\n\t\tprev_offset = linfo[i].insn_off;\n\t\tulinfo += rec_size;\n\t}\n\n\tif (s != env->subprog_cnt) {\n\t\tverbose(env, \"missing bpf_line_info for %u funcs starting from func#%u\\n\",\n\t\t\tenv->subprog_cnt - s, s);\n\t\terr = -EINVAL;\n\t\tgoto err_free;\n\t}\n\n\tprog->aux->linfo = linfo;\n\tprog->aux->nr_linfo = nr_linfo;\n\n\treturn 0;\n\nerr_free:\n\tkvfree(linfo);\n\treturn err;\n}\n\nstatic int check_btf_info(struct bpf_verifier_env *env,\n\t\t\t  const union bpf_attr *attr,\n\t\t\t  union bpf_attr __user *uattr)\n{\n\tstruct btf *btf;\n\tint err;\n\n\tif (!attr->func_info_cnt && !attr->line_info_cnt)\n\t\treturn 0;\n\n\tbtf = btf_get_by_fd(attr->prog_btf_fd);\n\tif (IS_ERR(btf))\n\t\treturn PTR_ERR(btf);\n\tenv->prog->aux->btf = btf;\n\n\terr = check_btf_func(env, attr, uattr);\n\tif (err)\n\t\treturn err;\n\n\terr = check_btf_line(env, attr, uattr);\n\tif (err)\n\t\treturn err;\n\n\treturn 0;\n}\n\n/* check %cur's range satisfies %old's */\nstatic bool range_within(struct bpf_reg_state *old,\n\t\t\t struct bpf_reg_state *cur)\n{\n\treturn old->umin_value <= cur->umin_value &&\n\t       old->umax_value >= cur->umax_value &&\n\t       old->smin_value <= cur->smin_value &&\n\t       old->smax_value >= cur->smax_value;\n}\n\n/* Maximum number of register states that can exist at once */\n#define ID_MAP_SIZE\t(MAX_BPF_REG + MAX_BPF_STACK / BPF_REG_SIZE)\nstruct idpair {\n\tu32 old;\n\tu32 cur;\n};\n\n/* If in the old state two registers had the same id, then they need to have\n * the same id in the new state as well.  But that id could be different from\n * the old state, so we need to track the mapping from old to new ids.\n * Once we have seen that, say, a reg with old id 5 had new id 9, any subsequent\n * regs with old id 5 must also have new id 9 for the new state to be safe.  But\n * regs with a different old id could still have new id 9, we don't care about\n * that.\n * So we look through our idmap to see if this old id has been seen before.  If\n * so, we require the new id to match; otherwise, we add the id pair to the map.\n */\nstatic bool check_ids(u32 old_id, u32 cur_id, struct idpair *idmap)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < ID_MAP_SIZE; i++) {\n\t\tif (!idmap[i].old) {\n\t\t\t/* Reached an empty slot; haven't seen this id before */\n\t\t\tidmap[i].old = old_id;\n\t\t\tidmap[i].cur = cur_id;\n\t\t\treturn true;\n\t\t}\n\t\tif (idmap[i].old == old_id)\n\t\t\treturn idmap[i].cur == cur_id;\n\t}\n\t/* We ran out of idmap slots, which should be impossible */\n\tWARN_ON_ONCE(1);\n\treturn false;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 3505,
        "critical_vars": [
            "ret"
        ],
        "function": "adjust_scalar_min_max_vals",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_d3bd7413e0ca40b60cf60d4003246d067cafdeda_verifier.c.diff",
        "label": "True",
        "function_code": "static int adjust_scalar_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t      struct bpf_insn *insn,\n\t\t\t\t      struct bpf_reg_state *dst_reg,\n\t\t\t\t      struct bpf_reg_state src_reg)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 opcode = BPF_OP(insn->code);\n\tbool src_known, dst_known;\n\ts64 smin_val, smax_val;\n\tu64 umin_val, umax_val;\n\tu64 insn_bitness = (BPF_CLASS(insn->code) == BPF_ALU64) ? 64 : 32;\n\tu32 dst = insn->dst_reg;\n\tint ret;\n\n\tif (insn_bitness == 32) {\n\t\t/* Relevant for 32-bit RSH: Information can propagate towards\n\t\t * LSB, so it isn't sufficient to only truncate the output to\n\t\t * 32 bits.\n\t\t */\n\t\tcoerce_reg_to_size(dst_reg, 4);\n\t\tcoerce_reg_to_size(&src_reg, 4);\n\t}\n\n\tsmin_val = src_reg.smin_value;\n\tsmax_val = src_reg.smax_value;\n\tumin_val = src_reg.umin_value;\n\tumax_val = src_reg.umax_value;\n\tsrc_known = tnum_is_const(src_reg.var_off);\n\tdst_known = tnum_is_const(dst_reg->var_off);\n\n\tif ((src_known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (!src_known &&\n\t    opcode != BPF_ADD && opcode != BPF_SUB && opcode != BPF_AND) {\n\t\t__mark_reg_unknown(dst_reg);\n\t\treturn 0;\n\t}\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\tret = sanitize_val_alu(env, insn);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to add from different pointers or scalars\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\tif (signed_add_overflows(dst_reg->smin_value, smin_val) ||\n\t\t    signed_add_overflows(dst_reg->smax_value, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value += smin_val;\n\t\t\tdst_reg->smax_value += smax_val;\n\t\t}\n\t\tif (dst_reg->umin_value + umin_val < umin_val ||\n\t\t    dst_reg->umax_value + umax_val < umax_val) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value += umin_val;\n\t\t\tdst_reg->umax_value += umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(dst_reg->var_off, src_reg.var_off);\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tret = sanitize_val_alu(env, insn);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to sub from different pointers or scalars\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\tif (signed_sub_overflows(dst_reg->smin_value, smax_val) ||\n\t\t    signed_sub_overflows(dst_reg->smax_value, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value -= smax_val;\n\t\t\tdst_reg->smax_value -= smin_val;\n\t\t}\n\t\tif (dst_reg->umin_value < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value -= umax_val;\n\t\t\tdst_reg->umax_value -= umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(dst_reg->var_off, src_reg.var_off);\n\t\tbreak;\n\tcase BPF_MUL:\n\t\tdst_reg->var_off = tnum_mul(dst_reg->var_off, src_reg.var_off);\n\t\tif (smin_val < 0 || dst_reg->smin_value < 0) {\n\t\t\t/* Ain't nobody got time to multiply that sign */\n\t\t\t__mark_reg_unbounded(dst_reg);\n\t\t\t__update_reg_bounds(dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\t/* Both values are positive, so we can work with unsigned and\n\t\t * copy the result to signed (unless it exceeds S64_MAX).\n\t\t */\n\t\tif (umax_val > U32_MAX || dst_reg->umax_value > U32_MAX) {\n\t\t\t/* Potential overflow, we know nothing */\n\t\t\t__mark_reg_unbounded(dst_reg);\n\t\t\t/* (except what we can learn from the var_off) */\n\t\t\t__update_reg_bounds(dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\tdst_reg->umin_value *= umin_val;\n\t\tdst_reg->umax_value *= umax_val;\n\t\tif (dst_reg->umax_value > S64_MAX) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = dst_reg->umin_value;\n\t\t\tdst_reg->smax_value = dst_reg->umax_value;\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\t\tif (src_known && dst_known) {\n\t\t\t__mark_reg_known(dst_reg, dst_reg->var_off.value &\n\t\t\t\t\t\t  src_reg.var_off.value);\n\t\t\tbreak;\n\t\t}\n\t\t/* We get our minimum from the var_off, since that's inherently\n\t\t * bitwise.  Our maximum is the minimum of the operands' maxima.\n\t\t */\n\t\tdst_reg->var_off = tnum_and(dst_reg->var_off, src_reg.var_off);\n\t\tdst_reg->umin_value = dst_reg->var_off.value;\n\t\tdst_reg->umax_value = min(dst_reg->umax_value, umax_val);\n\t\tif (dst_reg->smin_value < 0 || smin_val < 0) {\n\t\t\t/* Lose signed bounds when ANDing negative numbers,\n\t\t\t * ain't nobody got time for that.\n\t\t\t */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\t/* ANDing two positives gives a positive, so safe to\n\t\t\t * cast result into s64.\n\t\t\t */\n\t\t\tdst_reg->smin_value = dst_reg->umin_value;\n\t\t\tdst_reg->smax_value = dst_reg->umax_value;\n\t\t}\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tcase BPF_OR:\n\t\tif (src_known && dst_known) {\n\t\t\t__mark_reg_known(dst_reg, dst_reg->var_off.value |\n\t\t\t\t\t\t  src_reg.var_off.value);\n\t\t\tbreak;\n\t\t}\n\t\t/* We get our maximum from the var_off, and our minimum is the\n\t\t * maximum of the operands' minima\n\t\t */\n\t\tdst_reg->var_off = tnum_or(dst_reg->var_off, src_reg.var_off);\n\t\tdst_reg->umin_value = max(dst_reg->umin_value, umin_val);\n\t\tdst_reg->umax_value = dst_reg->var_off.value |\n\t\t\t\t      dst_reg->var_off.mask;\n\t\tif (dst_reg->smin_value < 0 || smin_val < 0) {\n\t\t\t/* Lose signed bounds when ORing negative numbers,\n\t\t\t * ain't nobody got time for that.\n\t\t\t */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\t/* ORing two positives gives a positive, so safe to\n\t\t\t * cast result into s64.\n\t\t\t */\n\t\t\tdst_reg->smin_value = dst_reg->umin_value;\n\t\t\tdst_reg->smax_value = dst_reg->umax_value;\n\t\t}\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tcase BPF_LSH:\n\t\tif (umax_val >= insn_bitness) {\n\t\t\t/* Shifts greater than 31 or 63 are undefined.\n\t\t\t * This includes shifts by a negative number.\n\t\t\t */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\t/* We lose all sign bit information (except what we can pick\n\t\t * up from var_off)\n\t\t */\n\t\tdst_reg->smin_value = S64_MIN;\n\t\tdst_reg->smax_value = S64_MAX;\n\t\t/* If we might shift our top bit out, then we know nothing */\n\t\tif (dst_reg->umax_value > 1ULL << (63 - umax_val)) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value <<= umin_val;\n\t\t\tdst_reg->umax_value <<= umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_lshift(dst_reg->var_off, umin_val);\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tcase BPF_RSH:\n\t\tif (umax_val >= insn_bitness) {\n\t\t\t/* Shifts greater than 31 or 63 are undefined.\n\t\t\t * This includes shifts by a negative number.\n\t\t\t */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\t/* BPF_RSH is an unsigned shift.  If the value in dst_reg might\n\t\t * be negative, then either:\n\t\t * 1) src_reg might be zero, so the sign bit of the result is\n\t\t *    unknown, so we lose our signed bounds\n\t\t * 2) it's known negative, thus the unsigned bounds capture the\n\t\t *    signed bounds\n\t\t * 3) the signed bounds cross zero, so they tell us nothing\n\t\t *    about the result\n\t\t * If the value in dst_reg is known nonnegative, then again the\n\t\t * unsigned bounts capture the signed bounds.\n\t\t * Thus, in all cases it suffices to blow away our signed bounds\n\t\t * and rely on inferring new ones from the unsigned bounds and\n\t\t * var_off of the result.\n\t\t */\n\t\tdst_reg->smin_value = S64_MIN;\n\t\tdst_reg->smax_value = S64_MAX;\n\t\tdst_reg->var_off = tnum_rshift(dst_reg->var_off, umin_val);\n\t\tdst_reg->umin_value >>= umax_val;\n\t\tdst_reg->umax_value >>= umin_val;\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tcase BPF_ARSH:\n\t\tif (umax_val >= insn_bitness) {\n\t\t\t/* Shifts greater than 31 or 63 are undefined.\n\t\t\t * This includes shifts by a negative number.\n\t\t\t */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Upon reaching here, src_known is true and\n\t\t * umax_val is equal to umin_val.\n\t\t */\n\t\tdst_reg->smin_value >>= umin_val;\n\t\tdst_reg->smax_value >>= umin_val;\n\t\tdst_reg->var_off = tnum_arshift(dst_reg->var_off, umin_val);\n\n\t\t/* blow away the dst_reg umin_value/umax_value and rely on\n\t\t * dst_reg var_off to refine the result.\n\t\t */\n\t\tdst_reg->umin_value = 0;\n\t\tdst_reg->umax_value = U64_MAX;\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tdefault:\n\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\tbreak;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops are (32,32)->32 */\n\t\tcoerce_reg_to_size(dst_reg, 4);\n\t}\n\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\treturn 0;\n}\n\n/* Handles ALU ops other than BPF_END, BPF_NEG and BPF_MOV: computes new min/max\n * and var_off.\n */\nstatic int adjust_reg_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg, *src_reg;\n\tstruct bpf_reg_state *ptr_reg = NULL, off_reg = {0};\n\tu8 opcode = BPF_OP(insn->code);\n\n\tdst_reg = &regs[insn->dst_reg];\n\tsrc_reg = NULL;\n\tif (dst_reg->type != SCALAR_VALUE)\n\t\tptr_reg = dst_reg;\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tsrc_reg = &regs[insn->src_reg];\n\t\tif (src_reg->type != SCALAR_VALUE) {\n\t\t\tif (dst_reg->type != SCALAR_VALUE) {\n\t\t\t\t/* Combining two pointers by any ALU op yields\n\t\t\t\t * an arbitrary scalar. Disallow all math except\n\t\t\t\t * pointer subtraction\n\t\t\t\t */\n\t\t\t\tif (opcode == BPF_SUB && env->allow_ptr_leaks) {\n\t\t\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t\tverbose(env, \"R%d pointer %s pointer prohibited\\n\",\n\t\t\t\t\tinsn->dst_reg,\n\t\t\t\t\tbpf_alu_string[opcode >> 4]);\n\t\t\t\treturn -EACCES;\n\t\t\t} else {\n\t\t\t\t/* scalar += pointer\n\t\t\t\t * This is legal, but we have to reverse our\n\t\t\t\t * src/dest handling in computing the range\n\t\t\t\t */\n\t\t\t\treturn adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t\t       src_reg, dst_reg);\n\t\t\t}\n\t\t} else if (ptr_reg) {\n\t\t\t/* pointer += scalar */\n\t\t\treturn adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t       dst_reg, src_reg);\n\t\t}\n\t} else {\n\t\t/* Pretend the src is a reg with a known value, since we only\n\t\t * need to be able to read from this state.\n\t\t */\n\t\toff_reg.type = SCALAR_VALUE;\n\t\t__mark_reg_known(&off_reg, insn->imm);\n\t\tsrc_reg = &off_reg;\n\t\tif (ptr_reg) /* pointer += K */\n\t\t\treturn adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t       ptr_reg, src_reg);\n\t}\n\n\t/* Got here implies adding two SCALAR_VALUEs */\n\tif (WARN_ON_ONCE(ptr_reg)) {\n\t\tprint_verifier_state(env, state);\n\t\tverbose(env, \"verifier internal error: unexpected ptr_reg\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (WARN_ON(!src_reg)) {\n\t\tprint_verifier_state(env, state);\n\t\tverbose(env, \"verifier internal error: no src_reg\\n\");\n\t\treturn -EINVAL;\n\t}\n\treturn adjust_scalar_min_max_vals(env, insn, dst_reg, *src_reg);\n}\n\n/* check validity of 32-bit and 64-bit arithmetic operations */\nstatic int check_alu_op(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 opcode = BPF_OP(insn->code);\n\tint err;\n\n\tif (opcode == BPF_END || opcode == BPF_NEG) {\n\t\tif (opcode == BPF_NEG) {\n\t\t\tif (BPF_SRC(insn->code) != 0 ||\n\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t    insn->off != 0 || insn->imm != 0) {\n\t\t\t\tverbose(env, \"BPF_NEG uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else {\n\t\t\tif (insn->src_reg != BPF_REG_0 || insn->off != 0 ||\n\t\t\t    (insn->imm != 16 && insn->imm != 32 && insn->imm != 64) ||\n\t\t\t    BPF_CLASS(insn->code) == BPF_ALU64) {\n\t\t\t\tverbose(env, \"BPF_END uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check src operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (is_pointer_value(env, insn->dst_reg)) {\n\t\t\tverbose(env, \"R%d pointer arithmetic prohibited\\n\",\n\t\t\t\tinsn->dst_reg);\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\t/* check dest operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t} else if (opcode == BPF_MOV) {\n\n\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\tif (insn->imm != 0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_MOV uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\t/* check src operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t} else {\n\t\t\tif (insn->src_reg != BPF_REG_0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_MOV uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check dest operand, mark as required later */\n\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP_NO_MARK);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\tstruct bpf_reg_state *src_reg = regs + insn->src_reg;\n\t\t\tstruct bpf_reg_state *dst_reg = regs + insn->dst_reg;\n\n\t\t\tif (BPF_CLASS(insn->code) == BPF_ALU64) {\n\t\t\t\t/* case: R1 = R2\n\t\t\t\t * copy register state to dest reg\n\t\t\t\t */\n\t\t\t\t*dst_reg = *src_reg;\n\t\t\t\tdst_reg->live |= REG_LIVE_WRITTEN;\n\t\t\t} else {\n\t\t\t\t/* R1 = (u32) R2 */\n\t\t\t\tif (is_pointer_value(env, insn->src_reg)) {\n\t\t\t\t\tverbose(env,\n\t\t\t\t\t\t\"R%d partial copy of pointer\\n\",\n\t\t\t\t\t\tinsn->src_reg);\n\t\t\t\t\treturn -EACCES;\n\t\t\t\t} else if (src_reg->type == SCALAR_VALUE) {\n\t\t\t\t\t*dst_reg = *src_reg;\n\t\t\t\t\tdst_reg->live |= REG_LIVE_WRITTEN;\n\t\t\t\t} else {\n\t\t\t\t\tmark_reg_unknown(env, regs,\n\t\t\t\t\t\t\t insn->dst_reg);\n\t\t\t\t}\n\t\t\t\tcoerce_reg_to_size(dst_reg, 4);\n\t\t\t}\n\t\t} else {\n\t\t\t/* case: R = imm\n\t\t\t * remember the value we stored into this reg\n\t\t\t */\n\t\t\t/* clear any state __mark_reg_known doesn't set */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tregs[insn->dst_reg].type = SCALAR_VALUE;\n\t\t\tif (BPF_CLASS(insn->code) == BPF_ALU64) {\n\t\t\t\t__mark_reg_known(regs + insn->dst_reg,\n\t\t\t\t\t\t insn->imm);\n\t\t\t} else {\n\t\t\t\t__mark_reg_known(regs + insn->dst_reg,\n\t\t\t\t\t\t (u32)insn->imm);\n\t\t\t}\n\t\t}\n\n\t} else if (opcode > BPF_END) {\n\t\tverbose(env, \"invalid BPF_ALU opcode %x\\n\", opcode);\n\t\treturn -EINVAL;\n\n\t} else {\t/* all other ALU ops: and, sub, xor, add, ... */\n\n\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\tif (insn->imm != 0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_ALU uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\t/* check src1 operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t} else {\n\t\t\tif (insn->src_reg != BPF_REG_0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_ALU uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check src2 operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif ((opcode == BPF_MOD || opcode == BPF_DIV) &&\n\t\t    BPF_SRC(insn->code) == BPF_K && insn->imm == 0) {\n\t\t\tverbose(env, \"div by zero\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif ((opcode == BPF_LSH || opcode == BPF_RSH ||\n\t\t     opcode == BPF_ARSH) && BPF_SRC(insn->code) == BPF_K) {\n\t\t\tint size = BPF_CLASS(insn->code) == BPF_ALU64 ? 64 : 32;\n\n\t\t\tif (insn->imm < 0 || insn->imm >= size) {\n\t\t\t\tverbose(env, \"invalid shift %d\\n\", insn->imm);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check dest operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP_NO_MARK);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\treturn adjust_reg_min_max_vals(env, insn);\n\t}\n\n\treturn 0;\n}\n\nstatic void find_good_pkt_pointers(struct bpf_verifier_state *vstate,\n\t\t\t\t   struct bpf_reg_state *dst_reg,\n\t\t\t\t   enum bpf_reg_type type,\n\t\t\t\t   bool range_right_open)\n{\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *reg;\n\tu16 new_range;\n\tint i, j;\n\n\tif (dst_reg->off < 0 ||\n\t    (dst_reg->off == 0 && range_right_open))\n\t\t/* This doesn't give us any range */\n\t\treturn;\n\n\tif (dst_reg->umax_value > MAX_PACKET_OFF ||\n\t    dst_reg->umax_value + dst_reg->off > MAX_PACKET_OFF)\n\t\t/* Risk of overflow.  For instance, ptr + (1<<63) may be less\n\t\t * than pkt_end, but that's because it's also less than pkt.\n\t\t */\n\t\treturn;\n\n\tnew_range = dst_reg->off;\n\tif (range_right_open)\n\t\tnew_range--;\n\n\t/* Examples for register markings:\n\t *\n\t * pkt_data in dst register:\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (r2 > pkt_end) goto <handle exception>\n\t *   <access okay>\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (r2 < pkt_end) goto <access okay>\n\t *   <handle exception>\n\t *\n\t *   Where:\n\t *     r2 == dst_reg, pkt_end == src_reg\n\t *     r2=pkt(id=n,off=8,r=0)\n\t *     r3=pkt(id=n,off=0,r=0)\n\t *\n\t * pkt_data in src register:\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (pkt_end >= r2) goto <access okay>\n\t *   <handle exception>\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (pkt_end <= r2) goto <handle exception>\n\t *   <access okay>\n\t *\n\t *   Where:\n\t *     pkt_end == dst_reg, r2 == src_reg\n\t *     r2=pkt(id=n,off=8,r=0)\n\t *     r3=pkt(id=n,off=0,r=0)\n\t *\n\t * Find register r3 and mark its range as r3=pkt(id=n,off=0,r=8)\n\t * or r3=pkt(id=n,off=0,r=8-1), so that range of bytes [r3, r3 + 8)\n\t * and [r3, r3 + 8-1) respectively is safe to access depending on\n\t * the check.\n\t */\n\n\t/* If our ids match, then we must have the same max_value.  And we\n\t * don't care about the other reg's fixed offset, since if it's too big\n\t * the range won't allow anything.\n\t * dst_reg->off is known < MAX_PACKET_OFF, therefore it fits in a u16.\n\t */\n\tfor (i = 0; i < MAX_BPF_REG; i++)\n\t\tif (regs[i].type == type && regs[i].id == dst_reg->id)\n\t\t\t/* keep the maximum range already checked */\n\t\t\tregs[i].range = max(regs[i].range, new_range);\n\n\tfor (j = 0; j <= vstate->curframe; j++) {\n\t\tstate = vstate->frame[j];\n\t\tbpf_for_each_spilled_reg(i, state, reg) {\n\t\t\tif (!reg)\n\t\t\t\tcontinue;\n\t\t\tif (reg->type == type && reg->id == dst_reg->id)\n\t\t\t\treg->range = max(reg->range, new_range);\n\t\t}\n\t}\n}\n\n/* compute branch direction of the expression \"if (reg opcode val) goto target;\"\n * and return:\n *  1 - branch will be taken and \"goto target\" will be executed\n *  0 - branch will not be taken and fall-through to next insn\n * -1 - unknown. Example: \"if (reg < 5)\" is unknown when register value range [0,10]\n */\nstatic int is_branch_taken(struct bpf_reg_state *reg, u64 val, u8 opcode)\n{\n\tif (__is_pointer_value(false, reg))\n\t\treturn -1;\n\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\tif (tnum_is_const(reg->var_off))\n\t\t\treturn !!tnum_equals_const(reg->var_off, val);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\tif (tnum_is_const(reg->var_off))\n\t\t\treturn !tnum_equals_const(reg->var_off, val);\n\t\tbreak;\n\tcase BPF_JSET:\n\t\tif ((~reg->var_off.mask & reg->var_off.value) & val)\n\t\t\treturn 1;\n\t\tif (!((reg->var_off.mask | reg->var_off.value) & val))\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JGT:\n\t\tif (reg->umin_value > val)\n\t\t\treturn 1;\n\t\telse if (reg->umax_value <= val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSGT:\n\t\tif (reg->smin_value > (s64)val)\n\t\t\treturn 1;\n\t\telse if (reg->smax_value < (s64)val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JLT:\n\t\tif (reg->umax_value < val)\n\t\t\treturn 1;\n\t\telse if (reg->umin_value >= val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSLT:\n\t\tif (reg->smax_value < (s64)val)\n\t\t\treturn 1;\n\t\telse if (reg->smin_value >= (s64)val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JGE:\n\t\tif (reg->umin_value >= val)\n\t\t\treturn 1;\n\t\telse if (reg->umax_value < val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSGE:\n\t\tif (reg->smin_value >= (s64)val)\n\t\t\treturn 1;\n\t\telse if (reg->smax_value < (s64)val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JLE:\n\t\tif (reg->umax_value <= val)\n\t\t\treturn 1;\n\t\telse if (reg->umin_value > val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSLE:\n\t\tif (reg->smax_value <= (s64)val)\n\t\t\treturn 1;\n\t\telse if (reg->smin_value > (s64)val)\n\t\t\treturn 0;\n\t\tbreak;\n\t}\n\n\treturn -1;\n}\n\n/* Adjusts the register min/max values in the case that the dst_reg is the\n * variable register that we are working on, and src_reg is a constant or we're\n * simply doing a BPF_K check.\n * In JEQ/JNE cases we also adjust the var_off values.\n */\nstatic void reg_set_min_max(struct bpf_reg_state *true_reg,\n\t\t\t    struct bpf_reg_state *false_reg, u64 val,\n\t\t\t    u8 opcode)\n{\n\t/* If the dst_reg is a pointer, we can't learn anything about its\n\t * variable offset from the compare (unless src_reg were a pointer into\n\t * the same object, but we don't bother with that.\n\t * Since false_reg and true_reg have the same type by construction, we\n\t * only need to check one of them for pointerness.\n\t */\n\tif (__is_pointer_value(false, false_reg))\n\t\treturn;\n\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\t/* If this is false then we know nothing Jon Snow, but if it is\n\t\t * true then we know for sure.\n\t\t */\n\t\t__mark_reg_known(true_reg, val);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\t/* If this is true we know nothing Jon Snow, but if it is false\n\t\t * we know the value for sure;\n\t\t */\n\t\t__mark_reg_known(false_reg, val);\n\t\tbreak;\n\tcase BPF_JSET:\n\t\tfalse_reg->var_off = tnum_and(false_reg->var_off,\n\t\t\t\t\t      tnum_const(~val));\n\t\tif (is_power_of_2(val))\n\t\t\ttrue_reg->var_off = tnum_or(true_reg->var_off,\n\t\t\t\t\t\t    tnum_const(val));\n\t\tbreak;\n\tcase BPF_JGT:\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val);\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JSGT:\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val);\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JLT:\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val);\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val - 1);\n\t\tbreak;\n\tcase BPF_JSLT:\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val);\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val - 1);\n\t\tbreak;\n\tcase BPF_JGE:\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val - 1);\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val);\n\t\tbreak;\n\tcase BPF_JSGE:\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val - 1);\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val);\n\t\tbreak;\n\tcase BPF_JLE:\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val + 1);\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val);\n\t\tbreak;\n\tcase BPF_JSLE:\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val + 1);\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t__reg_deduce_bounds(false_reg);\n\t__reg_deduce_bounds(true_reg);\n\t/* We might have learned some bits from the bounds. */\n\t__reg_bound_offset(false_reg);\n\t__reg_bound_offset(true_reg);\n\t/* Intersecting with the old var_off might have improved our bounds\n\t * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),\n\t * then new var_off is (0; 0x7f...fc) which improves our umax.\n\t */\n\t__update_reg_bounds(false_reg);\n\t__update_reg_bounds(true_reg);\n}\n\n/* Same as above, but for the case that dst_reg holds a constant and src_reg is\n * the variable reg.\n */\nstatic void reg_set_min_max_inv(struct bpf_reg_state *true_reg,\n\t\t\t\tstruct bpf_reg_state *false_reg, u64 val,\n\t\t\t\tu8 opcode)\n{\n\tif (__is_pointer_value(false, false_reg))\n\t\treturn;\n\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\t/* If this is false then we know nothing Jon Snow, but if it is\n\t\t * true then we know for sure.\n\t\t */\n\t\t__mark_reg_known(true_reg, val);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\t/* If this is true we know nothing Jon Snow, but if it is false\n\t\t * we know the value for sure;\n\t\t */\n\t\t__mark_reg_known(false_reg, val);\n\t\tbreak;\n\tcase BPF_JSET:\n\t\tfalse_reg->var_off = tnum_and(false_reg->var_off,\n\t\t\t\t\t      tnum_const(~val));\n\t\tif (is_power_of_2(val))\n\t\t\ttrue_reg->var_off = tnum_or(true_reg->var_off,\n\t\t\t\t\t\t    tnum_const(val));\n\t\tbreak;\n\tcase BPF_JGT:\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val - 1);\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val);\n\t\tbreak;\n\tcase BPF_JSGT:\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val - 1);\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val);\n\t\tbreak;\n\tcase BPF_JLT:\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val + 1);\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val);\n\t\tbreak;\n\tcase BPF_JSLT:\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val + 1);\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val);\n\t\tbreak;\n\tcase BPF_JGE:\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val);\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JSGE:\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val);\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JLE:\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val);\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val - 1);\n\t\tbreak;\n\tcase BPF_JSLE:\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val);\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val - 1);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t__reg_deduce_bounds(false_reg);\n\t__reg_deduce_bounds(true_reg);\n\t/* We might have learned some bits from the bounds. */\n\t__reg_bound_offset(false_reg);\n\t__reg_bound_offset(true_reg);\n\t/* Intersecting with the old var_off might have improved our bounds\n\t * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),\n\t * then new var_off is (0; 0x7f...fc) which improves our umax.\n\t */\n\t__update_reg_bounds(false_reg);\n\t__update_reg_bounds(true_reg);\n}\n\n/* Regs are known to be equal, so intersect their min/max/var_off */\nstatic void __reg_combine_min_max(struct bpf_reg_state *src_reg,\n\t\t\t\t  struct bpf_reg_state *dst_reg)\n{\n\tsrc_reg->umin_value = dst_reg->umin_value = max(src_reg->umin_value,\n\t\t\t\t\t\t\tdst_reg->umin_value);\n\tsrc_reg->umax_value = dst_reg->umax_value = min(src_reg->umax_value,\n\t\t\t\t\t\t\tdst_reg->umax_value);\n\tsrc_reg->smin_value = dst_reg->smin_value = max(src_reg->smin_value,\n\t\t\t\t\t\t\tdst_reg->smin_value);\n\tsrc_reg->smax_value = dst_reg->smax_value = min(src_reg->smax_value,\n\t\t\t\t\t\t\tdst_reg->smax_value);\n\tsrc_reg->var_off = dst_reg->var_off = tnum_intersect(src_reg->var_off,\n\t\t\t\t\t\t\t     dst_reg->var_off);\n\t/* We might have learned new bounds from the var_off. */\n\t__update_reg_bounds(src_reg);\n\t__update_reg_bounds(dst_reg);\n\t/* We might have learned something about the sign bit. */\n\t__reg_deduce_bounds(src_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t/* We might have learned some bits from the bounds. */\n\t__reg_bound_offset(src_reg);\n\t__reg_bound_offset(dst_reg);\n\t/* Intersecting with the old var_off might have improved our bounds\n\t * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),\n\t * then new var_off is (0; 0x7f...fc) which improves our umax.\n\t */\n\t__update_reg_bounds(src_reg);\n\t__update_reg_bounds(dst_reg);\n}\n\nstatic void reg_combine_min_max(struct bpf_reg_state *true_src,\n\t\t\t\tstruct bpf_reg_state *true_dst,\n\t\t\t\tstruct bpf_reg_state *false_src,\n\t\t\t\tstruct bpf_reg_state *false_dst,\n\t\t\t\tu8 opcode)\n{\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\t__reg_combine_min_max(true_src, true_dst);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\t__reg_combine_min_max(false_src, false_dst);\n\t\tbreak;\n\t}\n}\n\nstatic void mark_ptr_or_null_reg(struct bpf_func_state *state,\n\t\t\t\t struct bpf_reg_state *reg, u32 id,\n\t\t\t\t bool is_null)\n{\n\tif (reg_type_may_be_null(reg->type) && reg->id == id) {\n\t\t/* Old offset (both fixed and variable parts) should\n\t\t * have been known-zero, because we don't allow pointer\n\t\t * arithmetic on pointers that might be NULL.\n\t\t */\n\t\tif (WARN_ON_ONCE(reg->smin_value || reg->smax_value ||\n\t\t\t\t !tnum_equals_const(reg->var_off, 0) ||\n\t\t\t\t reg->off)) {\n\t\t\t__mark_reg_known_zero(reg);\n\t\t\treg->off = 0;\n\t\t}\n\t\tif (is_null) {\n\t\t\treg->type = SCALAR_VALUE;\n\t\t} else if (reg->type == PTR_TO_MAP_VALUE_OR_NULL) {\n\t\t\tif (reg->map_ptr->inner_map_meta) {\n\t\t\t\treg->type = CONST_PTR_TO_MAP;\n\t\t\t\treg->map_ptr = reg->map_ptr->inner_map_meta;\n\t\t\t} else {\n\t\t\t\treg->type = PTR_TO_MAP_VALUE;\n\t\t\t}\n\t\t} else if (reg->type == PTR_TO_SOCKET_OR_NULL) {\n\t\t\treg->type = PTR_TO_SOCKET;\n\t\t}\n\t\tif (is_null || !reg_is_refcounted(reg)) {\n\t\t\t/* We don't need id from this point onwards anymore,\n\t\t\t * thus we should better reset it, so that state\n\t\t\t * pruning has chances to take effect.\n\t\t\t */\n\t\t\treg->id = 0;\n\t\t}\n\t}\n}\n\n/* The logic is similar to find_good_pkt_pointers(), both could eventually\n * be folded together at some point.\n */\nstatic void mark_ptr_or_null_regs(struct bpf_verifier_state *vstate, u32 regno,\n\t\t\t\t  bool is_null)\n{\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *reg, *regs = state->regs;\n\tu32 id = regs[regno].id;\n\tint i, j;\n\n\tif (reg_is_refcounted_or_null(&regs[regno]) && is_null)\n\t\t__release_reference_state(state, id);\n\n\tfor (i = 0; i < MAX_BPF_REG; i++)\n\t\tmark_ptr_or_null_reg(state, &regs[i], id, is_null);\n\n\tfor (j = 0; j <= vstate->curframe; j++) {\n\t\tstate = vstate->frame[j];\n\t\tbpf_for_each_spilled_reg(i, state, reg) {\n\t\t\tif (!reg)\n\t\t\t\tcontinue;\n\t\t\tmark_ptr_or_null_reg(state, reg, id, is_null);\n\t\t}\n\t}\n}\n\nstatic bool try_match_pkt_pointers(const struct bpf_insn *insn,\n\t\t\t\t   struct bpf_reg_state *dst_reg,\n\t\t\t\t   struct bpf_reg_state *src_reg,\n\t\t\t\t   struct bpf_verifier_state *this_branch,\n\t\t\t\t   struct bpf_verifier_state *other_branch)\n{\n\tif (BPF_SRC(insn->code) != BPF_X)\n\t\treturn false;\n\n\tswitch (BPF_OP(insn->code)) {\n\tcase BPF_JGT:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' > pkt_end, pkt_meta' > pkt_data */\n\t\t\tfind_good_pkt_pointers(this_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, false);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end > pkt_data', pkt_data > pkt_meta' */\n\t\t\tfind_good_pkt_pointers(other_branch, src_reg,\n\t\t\t\t\t       src_reg->type, true);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase BPF_JLT:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' < pkt_end, pkt_meta' < pkt_data */\n\t\t\tfind_good_pkt_pointers(other_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, true);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end < pkt_data', pkt_data > pkt_meta' */\n\t\t\tfind_good_pkt_pointers(this_branch, src_reg,\n\t\t\t\t\t       src_reg->type, false);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase BPF_JGE:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' >= pkt_end, pkt_meta' >= pkt_data */\n\t\t\tfind_good_pkt_pointers(this_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, true);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end >= pkt_data', pkt_data >= pkt_meta' */\n\t\t\tfind_good_pkt_pointers(other_branch, src_reg,\n\t\t\t\t\t       src_reg->type, false);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase BPF_JLE:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' <= pkt_end, pkt_meta' <= pkt_data */\n\t\t\tfind_good_pkt_pointers(other_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, false);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end <= pkt_data', pkt_data <= pkt_meta' */\n\t\t\tfind_good_pkt_pointers(this_branch, src_reg,\n\t\t\t\t\t       src_reg->type, true);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic int check_cond_jmp_op(struct bpf_verifier_env *env,\n\t\t\t     struct bpf_insn *insn, int *insn_idx)\n{\n\tstruct bpf_verifier_state *this_branch = env->cur_state;\n\tstruct bpf_verifier_state *other_branch;\n\tstruct bpf_reg_state *regs = this_branch->frame[this_branch->curframe]->regs;\n\tstruct bpf_reg_state *dst_reg, *other_branch_regs;\n\tu8 opcode = BPF_OP(insn->code);\n\tint err;\n\n\tif (opcode > BPF_JSLE) {\n\t\tverbose(env, \"invalid BPF_JMP opcode %x\\n\", opcode);\n\t\treturn -EINVAL;\n\t}\n\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tif (insn->imm != 0) {\n\t\t\tverbose(env, \"BPF_JMP uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/* check src1 operand */\n\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (is_pointer_value(env, insn->src_reg)) {\n\t\t\tverbose(env, \"R%d pointer comparison prohibited\\n\",\n\t\t\t\tinsn->src_reg);\n\t\t\treturn -EACCES;\n\t\t}\n\t} else {\n\t\tif (insn->src_reg != BPF_REG_0) {\n\t\t\tverbose(env, \"BPF_JMP uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t/* check src2 operand */\n\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tdst_reg = &regs[insn->dst_reg];\n\n\tif (BPF_SRC(insn->code) == BPF_K) {\n\t\tint pred = is_branch_taken(dst_reg, insn->imm, opcode);\n\n\t\tif (pred == 1) {\n\t\t\t /* only follow the goto, ignore fall-through */\n\t\t\t*insn_idx += insn->off;\n\t\t\treturn 0;\n\t\t} else if (pred == 0) {\n\t\t\t/* only follow fall-through branch, since\n\t\t\t * that's where the program will go\n\t\t\t */\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tother_branch = push_stack(env, *insn_idx + insn->off + 1, *insn_idx,\n\t\t\t\t  false);\n\tif (!other_branch)\n\t\treturn -EFAULT;\n\tother_branch_regs = other_branch->frame[other_branch->curframe]->regs;\n\n\t/* detect if we are comparing against a constant value so we can adjust\n\t * our min/max values for our dst register.\n\t * this is only legit if both are scalars (or pointers to the same\n\t * object, I suppose, but we don't support that right now), because\n\t * otherwise the different base pointers mean the offsets aren't\n\t * comparable.\n\t */\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tif (dst_reg->type == SCALAR_VALUE &&\n\t\t    regs[insn->src_reg].type == SCALAR_VALUE) {\n\t\t\tif (tnum_is_const(regs[insn->src_reg].var_off))\n\t\t\t\treg_set_min_max(&other_branch_regs[insn->dst_reg],\n\t\t\t\t\t\tdst_reg, regs[insn->src_reg].var_off.value,\n\t\t\t\t\t\topcode);\n\t\t\telse if (tnum_is_const(dst_reg->var_off))\n\t\t\t\treg_set_min_max_inv(&other_branch_regs[insn->src_reg],\n\t\t\t\t\t\t    &regs[insn->src_reg],\n\t\t\t\t\t\t    dst_reg->var_off.value, opcode);\n\t\t\telse if (opcode == BPF_JEQ || opcode == BPF_JNE)\n\t\t\t\t/* Comparing for equality, we can combine knowledge */\n\t\t\t\treg_combine_min_max(&other_branch_regs[insn->src_reg],\n\t\t\t\t\t\t    &other_branch_regs[insn->dst_reg],\n\t\t\t\t\t\t    &regs[insn->src_reg],\n\t\t\t\t\t\t    &regs[insn->dst_reg], opcode);\n\t\t}\n\t} else if (dst_reg->type == SCALAR_VALUE) {\n\t\treg_set_min_max(&other_branch_regs[insn->dst_reg],\n\t\t\t\t\tdst_reg, insn->imm, opcode);\n\t}\n\n\t/* detect if R == 0 where R is returned from bpf_map_lookup_elem() */\n\tif (BPF_SRC(insn->code) == BPF_K &&\n\t    insn->imm == 0 && (opcode == BPF_JEQ || opcode == BPF_JNE) &&\n\t    reg_type_may_be_null(dst_reg->type)) {\n\t\t/* Mark all identical registers in each branch as either\n\t\t * safe or unknown depending R == 0 or R != 0 conditional.\n\t\t */\n\t\tmark_ptr_or_null_regs(this_branch, insn->dst_reg,\n\t\t\t\t      opcode == BPF_JNE);\n\t\tmark_ptr_or_null_regs(other_branch, insn->dst_reg,\n\t\t\t\t      opcode == BPF_JEQ);\n\t} else if (!try_match_pkt_pointers(insn, dst_reg, &regs[insn->src_reg],\n\t\t\t\t\t   this_branch, other_branch) &&\n\t\t   is_pointer_value(env, insn->dst_reg)) {\n\t\tverbose(env, \"R%d pointer comparison prohibited\\n\",\n\t\t\tinsn->dst_reg);\n\t\treturn -EACCES;\n\t}\n\tif (env->log.level)\n\t\tprint_verifier_state(env, this_branch->frame[this_branch->curframe]);\n\treturn 0;\n}\n\n/* return the map pointer stored inside BPF_LD_IMM64 instruction */\nstatic struct bpf_map *ld_imm64_to_map_ptr(struct bpf_insn *insn)\n{\n\tu64 imm64 = ((u64) (u32) insn[0].imm) | ((u64) (u32) insn[1].imm) << 32;\n\n\treturn (struct bpf_map *) (unsigned long) imm64;\n}\n\n/* verify BPF_LD_IMM64 instruction */\nstatic int check_ld_imm(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tint err;\n\n\tif (BPF_SIZE(insn->code) != BPF_DW) {\n\t\tverbose(env, \"invalid BPF_LD_IMM insn\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (insn->off != 0) {\n\t\tverbose(env, \"BPF_LD_IMM64 uses reserved fields\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\terr = check_reg_arg(env, insn->dst_reg, DST_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (insn->src_reg == 0) {\n\t\tu64 imm = ((u64)(insn + 1)->imm << 32) | (u32)insn->imm;\n\n\t\tregs[insn->dst_reg].type = SCALAR_VALUE;\n\t\t__mark_reg_known(&regs[insn->dst_reg], imm);\n\t\treturn 0;\n\t}\n\n\t/* replace_map_fd_with_map_ptr() should have caught bad ld_imm64 */\n\tBUG_ON(insn->src_reg != BPF_PSEUDO_MAP_FD);\n\n\tregs[insn->dst_reg].type = CONST_PTR_TO_MAP;\n\tregs[insn->dst_reg].map_ptr = ld_imm64_to_map_ptr(insn);\n\treturn 0;\n}\n\nstatic bool may_access_skb(enum bpf_prog_type type)\n{\n\tswitch (type) {\n\tcase BPF_PROG_TYPE_SOCKET_FILTER:\n\tcase BPF_PROG_TYPE_SCHED_CLS:\n\tcase BPF_PROG_TYPE_SCHED_ACT:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n/* verify safety of LD_ABS|LD_IND instructions:\n * - they can only appear in the programs where ctx == skb\n * - since they are wrappers of function calls, they scratch R1-R5 registers,\n *   preserve R6-R9, and store return value into R0\n *\n * Implicit input:\n *   ctx == skb == R6 == CTX\n *\n * Explicit input:\n *   SRC == any register\n *   IMM == 32-bit immediate\n *\n * Output:\n *   R0 - 8/16/32-bit skb data converted to cpu endianness\n */\nstatic int check_ld_abs(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 mode = BPF_MODE(insn->code);\n\tint i, err;\n\n\tif (!may_access_skb(env->prog->type)) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] instructions not allowed for this program type\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!env->ops->gen_ld_abs) {\n\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (env->subprog_cnt > 1) {\n\t\t/* when program has LD_ABS insn JITs and interpreter assume\n\t\t * that r1 == ctx == skb which is not the case for callees\n\t\t * that can have arbitrary arguments. It's problematic\n\t\t * for main prog as well since JITs would need to analyze\n\t\t * all functions in order to make proper register save/restore\n\t\t * decisions in the main prog. Hence disallow LD_ABS with calls\n\t\t */\n\t\tverbose(env, \"BPF_LD_[ABS|IND] instructions cannot be mixed with bpf-to-bpf calls\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (insn->dst_reg != BPF_REG_0 || insn->off != 0 ||\n\t    BPF_SIZE(insn->code) == BPF_DW ||\n\t    (mode == BPF_ABS && insn->src_reg != BPF_REG_0)) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] uses reserved fields\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* check whether implicit source operand (register R6) is readable */\n\terr = check_reg_arg(env, BPF_REG_6, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\t/* Disallow usage of BPF_LD_[ABS|IND] with reference tracking, as\n\t * gen_ld_abs() may terminate the program at runtime, leading to\n\t * reference leak.\n\t */\n\terr = check_reference_leak(env);\n\tif (err) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] cannot be mixed with socket references\\n\");\n\t\treturn err;\n\t}\n\n\tif (regs[BPF_REG_6].type != PTR_TO_CTX) {\n\t\tverbose(env,\n\t\t\t\"at the time of BPF_LD_ABS|IND R6 != pointer to skb\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (mode == BPF_IND) {\n\t\t/* check explicit source operand */\n\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\t/* reset caller saved regs to unreadable */\n\tfor (i = 0; i < CALLER_SAVED_REGS; i++) {\n\t\tmark_reg_not_init(env, regs, caller_saved[i]);\n\t\tcheck_reg_arg(env, caller_saved[i], DST_OP_NO_MARK);\n\t}\n\n\t/* mark destination R0 register as readable, since it contains\n\t * the value fetched from the packet.\n\t * Already marked as written above.\n\t */\n\tmark_reg_unknown(env, regs, BPF_REG_0);\n\treturn 0;\n}\n\nstatic int check_return_code(struct bpf_verifier_env *env)\n{\n\tstruct bpf_reg_state *reg;\n\tstruct tnum range = tnum_range(0, 1);\n\n\tswitch (env->prog->type) {\n\tcase BPF_PROG_TYPE_CGROUP_SKB:\n\tcase BPF_PROG_TYPE_CGROUP_SOCK:\n\tcase BPF_PROG_TYPE_CGROUP_SOCK_ADDR:\n\tcase BPF_PROG_TYPE_SOCK_OPS:\n\tcase BPF_PROG_TYPE_CGROUP_DEVICE:\n\t\tbreak;\n\tdefault:\n\t\treturn 0;\n\t}\n\n\treg = cur_regs(env) + BPF_REG_0;\n\tif (reg->type != SCALAR_VALUE) {\n\t\tverbose(env, \"At program exit the register R0 is not a known value (%s)\\n\",\n\t\t\treg_type_str[reg->type]);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!tnum_in(range, reg->var_off)) {\n\t\tverbose(env, \"At program exit the register R0 \");\n\t\tif (!tnum_is_unknown(reg->var_off)) {\n\t\t\tchar tn_buf[48];\n\n\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\tverbose(env, \"has value %s\", tn_buf);\n\t\t} else {\n\t\t\tverbose(env, \"has unknown scalar value\");\n\t\t}\n\t\tverbose(env, \" should have been 0 or 1\\n\");\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\n/* non-recursive DFS pseudo code\n * 1  procedure DFS-iterative(G,v):\n * 2      label v as discovered\n * 3      let S be a stack\n * 4      S.push(v)\n * 5      while S is not empty\n * 6            t <- S.pop()\n * 7            if t is what we're looking for:\n * 8                return t\n * 9            for all edges e in G.adjacentEdges(t) do\n * 10               if edge e is already labelled\n * 11                   continue with the next edge\n * 12               w <- G.adjacentVertex(t,e)\n * 13               if vertex w is not discovered and not explored\n * 14                   label e as tree-edge\n * 15                   label w as discovered\n * 16                   S.push(w)\n * 17                   continue at 5\n * 18               else if vertex w is discovered\n * 19                   label e as back-edge\n * 20               else\n * 21                   // vertex w is explored\n * 22                   label e as forward- or cross-edge\n * 23           label t as explored\n * 24           S.pop()\n *\n * convention:\n * 0x10 - discovered\n * 0x11 - discovered and fall-through edge labelled\n * 0x12 - discovered and fall-through and branch edges labelled\n * 0x20 - explored\n */\n\nenum {\n\tDISCOVERED = 0x10,\n\tEXPLORED = 0x20,\n\tFALLTHROUGH = 1,\n\tBRANCH = 2,\n};\n\n#define STATE_LIST_MARK ((struct bpf_verifier_state_list *) -1L)\n\nstatic int *insn_stack;\t/* stack of insns to process */\nstatic int cur_stack;\t/* current stack index */\nstatic int *insn_state;\n\n/* t, w, e - match pseudo-code above:\n * t - index of current instruction\n * w - next instruction\n * e - edge\n */\nstatic int push_insn(int t, int w, int e, struct bpf_verifier_env *env)\n{\n\tif (e == FALLTHROUGH && insn_state[t] >= (DISCOVERED | FALLTHROUGH))\n\t\treturn 0;\n\n\tif (e == BRANCH && insn_state[t] >= (DISCOVERED | BRANCH))\n\t\treturn 0;\n\n\tif (w < 0 || w >= env->prog->len) {\n\t\tverbose_linfo(env, t, \"%d: \", t);\n\t\tverbose(env, \"jump out of range from insn %d to %d\\n\", t, w);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e == BRANCH)\n\t\t/* mark branch target for state pruning */\n\t\tenv->explored_states[w] = STATE_LIST_MARK;\n\n\tif (insn_state[w] == 0) {\n\t\t/* tree-edge */\n\t\tinsn_state[t] = DISCOVERED | e;\n\t\tinsn_state[w] = DISCOVERED;\n\t\tif (cur_stack >= env->prog->len)\n\t\t\treturn -E2BIG;\n\t\tinsn_stack[cur_stack++] = w;\n\t\treturn 1;\n\t} else if ((insn_state[w] & 0xF0) == DISCOVERED) {\n\t\tverbose_linfo(env, t, \"%d: \", t);\n\t\tverbose_linfo(env, w, \"%d: \", w);\n\t\tverbose(env, \"back-edge from insn %d to %d\\n\", t, w);\n\t\treturn -EINVAL;\n\t} else if (insn_state[w] == EXPLORED) {\n\t\t/* forward- or cross-edge */\n\t\tinsn_state[t] = DISCOVERED | e;\n\t} else {\n\t\tverbose(env, \"insn state internal bug\\n\");\n\t\treturn -EFAULT;\n\t}\n\treturn 0;\n}\n\n/* non-recursive depth-first-search to detect loops in BPF program\n * loop == back-edge in directed graph\n */\nstatic int check_cfg(struct bpf_verifier_env *env)\n{\n\tstruct bpf_insn *insns = env->prog->insnsi;\n\tint insn_cnt = env->prog->len;\n\tint ret = 0;\n\tint i, t;\n\n\tinsn_state = kcalloc(insn_cnt, sizeof(int), GFP_KERNEL);\n\tif (!insn_state)\n\t\treturn -ENOMEM;\n\n\tinsn_stack = kcalloc(insn_cnt, sizeof(int), GFP_KERNEL);\n\tif (!insn_stack) {\n\t\tkfree(insn_state);\n\t\treturn -ENOMEM;\n\t}\n\n\tinsn_state[0] = DISCOVERED; /* mark 1st insn as discovered */\n\tinsn_stack[0] = 0; /* 0 is the first instruction */\n\tcur_stack = 1;\n\npeek_stack:\n\tif (cur_stack == 0)\n\t\tgoto check_state;\n\tt = insn_stack[cur_stack - 1];\n\n\tif (BPF_CLASS(insns[t].code) == BPF_JMP) {\n\t\tu8 opcode = BPF_OP(insns[t].code);\n\n\t\tif (opcode == BPF_EXIT) {\n\t\t\tgoto mark_explored;\n\t\t} else if (opcode == BPF_CALL) {\n\t\t\tret = push_insn(t, t + 1, FALLTHROUGH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t\tif (t + 1 < insn_cnt)\n\t\t\t\tenv->explored_states[t + 1] = STATE_LIST_MARK;\n\t\t\tif (insns[t].src_reg == BPF_PSEUDO_CALL) {\n\t\t\t\tenv->explored_states[t] = STATE_LIST_MARK;\n\t\t\t\tret = push_insn(t, t + insns[t].imm + 1, BRANCH, env);\n\t\t\t\tif (ret == 1)\n\t\t\t\t\tgoto peek_stack;\n\t\t\t\telse if (ret < 0)\n\t\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t} else if (opcode == BPF_JA) {\n\t\t\tif (BPF_SRC(insns[t].code) != BPF_K) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t\t/* unconditional jump with single edge */\n\t\t\tret = push_insn(t, t + insns[t].off + 1,\n\t\t\t\t\tFALLTHROUGH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t\t/* tell verifier to check for equivalent states\n\t\t\t * after every call and jump\n\t\t\t */\n\t\t\tif (t + 1 < insn_cnt)\n\t\t\t\tenv->explored_states[t + 1] = STATE_LIST_MARK;\n\t\t} else {\n\t\t\t/* conditional jump with two edges */\n\t\t\tenv->explored_states[t] = STATE_LIST_MARK;\n\t\t\tret = push_insn(t, t + 1, FALLTHROUGH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\n\t\t\tret = push_insn(t, t + insns[t].off + 1, BRANCH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t}\n\t} else {\n\t\t/* all other non-branch instructions with single\n\t\t * fall-through edge\n\t\t */\n\t\tret = push_insn(t, t + 1, FALLTHROUGH, env);\n\t\tif (ret == 1)\n\t\t\tgoto peek_stack;\n\t\telse if (ret < 0)\n\t\t\tgoto err_free;\n\t}\n\nmark_explored:\n\tinsn_state[t] = EXPLORED;\n\tif (cur_stack-- <= 0) {\n\t\tverbose(env, \"pop stack internal bug\\n\");\n\t\tret = -EFAULT;\n\t\tgoto err_free;\n\t}\n\tgoto peek_stack;\n\ncheck_state:\n\tfor (i = 0; i < insn_cnt; i++) {\n\t\tif (insn_state[i] != EXPLORED) {\n\t\t\tverbose(env, \"unreachable insn %d\\n\", i);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\t}\n\tret = 0; /* cfg looks good */\n\nerr_free:\n\tkfree(insn_state);\n\tkfree(insn_stack);\n\treturn ret;\n}\n\n/* The minimum supported BTF func info size */\n#define MIN_BPF_FUNCINFO_SIZE\t8\n#define MAX_FUNCINFO_REC_SIZE\t252\n\nstatic int check_btf_func(struct bpf_verifier_env *env,\n\t\t\t  const union bpf_attr *attr,\n\t\t\t  union bpf_attr __user *uattr)\n{\n\tu32 i, nfuncs, urec_size, min_size, prev_offset;\n\tu32 krec_size = sizeof(struct bpf_func_info);\n\tstruct bpf_func_info *krecord;\n\tconst struct btf_type *type;\n\tstruct bpf_prog *prog;\n\tconst struct btf *btf;\n\tvoid __user *urecord;\n\tint ret = 0;\n\n\tnfuncs = attr->func_info_cnt;\n\tif (!nfuncs)\n\t\treturn 0;\n\n\tif (nfuncs != env->subprog_cnt) {\n\t\tverbose(env, \"number of funcs in func_info doesn't match number of subprogs\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\turec_size = attr->func_info_rec_size;\n\tif (urec_size < MIN_BPF_FUNCINFO_SIZE ||\n\t    urec_size > MAX_FUNCINFO_REC_SIZE ||\n\t    urec_size % sizeof(u32)) {\n\t\tverbose(env, \"invalid func info rec size %u\\n\", urec_size);\n\t\treturn -EINVAL;\n\t}\n\n\tprog = env->prog;\n\tbtf = prog->aux->btf;\n\n\turecord = u64_to_user_ptr(attr->func_info);\n\tmin_size = min_t(u32, krec_size, urec_size);\n\n\tkrecord = kvcalloc(nfuncs, krec_size, GFP_KERNEL | __GFP_NOWARN);\n\tif (!krecord)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < nfuncs; i++) {\n\t\tret = bpf_check_uarg_tail_zero(urecord, krec_size, urec_size);\n\t\tif (ret) {\n\t\t\tif (ret == -E2BIG) {\n\t\t\t\tverbose(env, \"nonzero tailing record in func info\");\n\t\t\t\t/* set the size kernel expects so loader can zero\n\t\t\t\t * out the rest of the record.\n\t\t\t\t */\n\t\t\t\tif (put_user(min_size, &uattr->func_info_rec_size))\n\t\t\t\t\tret = -EFAULT;\n\t\t\t}\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (copy_from_user(&krecord[i], urecord, min_size)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/* check insn_off */\n\t\tif (i == 0) {\n\t\t\tif (krecord[i].insn_off) {\n\t\t\t\tverbose(env,\n\t\t\t\t\t\"nonzero insn_off %u for the first func info record\",\n\t\t\t\t\tkrecord[i].insn_off);\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t} else if (krecord[i].insn_off <= prev_offset) {\n\t\t\tverbose(env,\n\t\t\t\t\"same or smaller insn offset (%u) than previous func info record (%u)\",\n\t\t\t\tkrecord[i].insn_off, prev_offset);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (env->subprog_info[i].start != krecord[i].insn_off) {\n\t\t\tverbose(env, \"func_info BTF section doesn't match subprog layout in BPF program\\n\");\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/* check type_id */\n\t\ttype = btf_type_by_id(btf, krecord[i].type_id);\n\t\tif (!type || BTF_INFO_KIND(type->info) != BTF_KIND_FUNC) {\n\t\t\tverbose(env, \"invalid type id %d in func info\",\n\t\t\t\tkrecord[i].type_id);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tprev_offset = krecord[i].insn_off;\n\t\turecord += urec_size;\n\t}\n\n\tprog->aux->func_info = krecord;\n\tprog->aux->func_info_cnt = nfuncs;\n\treturn 0;\n\nerr_free:\n\tkvfree(krecord);\n\treturn ret;\n}\n\nstatic void adjust_btf_func(struct bpf_verifier_env *env)\n{\n\tint i;\n\n\tif (!env->prog->aux->func_info)\n\t\treturn;\n\n\tfor (i = 0; i < env->subprog_cnt; i++)\n\t\tenv->prog->aux->func_info[i].insn_off = env->subprog_info[i].start;\n}\n\n#define MIN_BPF_LINEINFO_SIZE\t(offsetof(struct bpf_line_info, line_col) + \\\n\t\tsizeof(((struct bpf_line_info *)(0))->line_col))\n#define MAX_LINEINFO_REC_SIZE\tMAX_FUNCINFO_REC_SIZE\n\nstatic int check_btf_line(struct bpf_verifier_env *env,\n\t\t\t  const union bpf_attr *attr,\n\t\t\t  union bpf_attr __user *uattr)\n{\n\tu32 i, s, nr_linfo, ncopy, expected_size, rec_size, prev_offset = 0;\n\tstruct bpf_subprog_info *sub;\n\tstruct bpf_line_info *linfo;\n\tstruct bpf_prog *prog;\n\tconst struct btf *btf;\n\tvoid __user *ulinfo;\n\tint err;\n\n\tnr_linfo = attr->line_info_cnt;\n\tif (!nr_linfo)\n\t\treturn 0;\n\n\trec_size = attr->line_info_rec_size;\n\tif (rec_size < MIN_BPF_LINEINFO_SIZE ||\n\t    rec_size > MAX_LINEINFO_REC_SIZE ||\n\t    rec_size & (sizeof(u32) - 1))\n\t\treturn -EINVAL;\n\n\t/* Need to zero it in case the userspace may\n\t * pass in a smaller bpf_line_info object.\n\t */\n\tlinfo = kvcalloc(nr_linfo, sizeof(struct bpf_line_info),\n\t\t\t GFP_KERNEL | __GFP_NOWARN);\n\tif (!linfo)\n\t\treturn -ENOMEM;\n\n\tprog = env->prog;\n\tbtf = prog->aux->btf;\n\n\ts = 0;\n\tsub = env->subprog_info;\n\tulinfo = u64_to_user_ptr(attr->line_info);\n\texpected_size = sizeof(struct bpf_line_info);\n\tncopy = min_t(u32, expected_size, rec_size);\n\tfor (i = 0; i < nr_linfo; i++) {\n\t\terr = bpf_check_uarg_tail_zero(ulinfo, expected_size, rec_size);\n\t\tif (err) {\n\t\t\tif (err == -E2BIG) {\n\t\t\t\tverbose(env, \"nonzero tailing record in line_info\");\n\t\t\t\tif (put_user(expected_size,\n\t\t\t\t\t     &uattr->line_info_rec_size))\n\t\t\t\t\terr = -EFAULT;\n\t\t\t}\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (copy_from_user(&linfo[i], ulinfo, ncopy)) {\n\t\t\terr = -EFAULT;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/*\n\t\t * Check insn_off to ensure\n\t\t * 1) strictly increasing AND\n\t\t * 2) bounded by prog->len\n\t\t *\n\t\t * The linfo[0].insn_off == 0 check logically falls into\n\t\t * the later \"missing bpf_line_info for func...\" case\n\t\t * because the first linfo[0].insn_off must be the\n\t\t * first sub also and the first sub must have\n\t\t * subprog_info[0].start == 0.\n\t\t */\n\t\tif ((i && linfo[i].insn_off <= prev_offset) ||\n\t\t    linfo[i].insn_off >= prog->len) {\n\t\t\tverbose(env, \"Invalid line_info[%u].insn_off:%u (prev_offset:%u prog->len:%u)\\n\",\n\t\t\t\ti, linfo[i].insn_off, prev_offset,\n\t\t\t\tprog->len);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (!prog->insnsi[linfo[i].insn_off].code) {\n\t\t\tverbose(env,\n\t\t\t\t\"Invalid insn code at line_info[%u].insn_off\\n\",\n\t\t\t\ti);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (!btf_name_by_offset(btf, linfo[i].line_off) ||\n\t\t    !btf_name_by_offset(btf, linfo[i].file_name_off)) {\n\t\t\tverbose(env, \"Invalid line_info[%u].line_off or .file_name_off\\n\", i);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (s != env->subprog_cnt) {\n\t\t\tif (linfo[i].insn_off == sub[s].start) {\n\t\t\t\tsub[s].linfo_idx = i;\n\t\t\t\ts++;\n\t\t\t} else if (sub[s].start < linfo[i].insn_off) {\n\t\t\t\tverbose(env, \"missing bpf_line_info for func#%u\\n\", s);\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t}\n\n\t\tprev_offset = linfo[i].insn_off;\n\t\tulinfo += rec_size;\n\t}\n\n\tif (s != env->subprog_cnt) {\n\t\tverbose(env, \"missing bpf_line_info for %u funcs starting from func#%u\\n\",\n\t\t\tenv->subprog_cnt - s, s);\n\t\terr = -EINVAL;\n\t\tgoto err_free;\n\t}\n\n\tprog->aux->linfo = linfo;\n\tprog->aux->nr_linfo = nr_linfo;\n\n\treturn 0;\n\nerr_free:\n\tkvfree(linfo);\n\treturn err;\n}\n\nstatic int check_btf_info(struct bpf_verifier_env *env,\n\t\t\t  const union bpf_attr *attr,\n\t\t\t  union bpf_attr __user *uattr)\n{\n\tstruct btf *btf;\n\tint err;\n\n\tif (!attr->func_info_cnt && !attr->line_info_cnt)\n\t\treturn 0;\n\n\tbtf = btf_get_by_fd(attr->prog_btf_fd);\n\tif (IS_ERR(btf))\n\t\treturn PTR_ERR(btf);\n\tenv->prog->aux->btf = btf;\n\n\terr = check_btf_func(env, attr, uattr);\n\tif (err)\n\t\treturn err;\n\n\terr = check_btf_line(env, attr, uattr);\n\tif (err)\n\t\treturn err;\n\n\treturn 0;\n}\n\n/* check %cur's range satisfies %old's */\nstatic bool range_within(struct bpf_reg_state *old,\n\t\t\t struct bpf_reg_state *cur)\n{\n\treturn old->umin_value <= cur->umin_value &&\n\t       old->umax_value >= cur->umax_value &&\n\t       old->smin_value <= cur->smin_value &&\n\t       old->smax_value >= cur->smax_value;\n}\n\n/* Maximum number of register states that can exist at once */\n#define ID_MAP_SIZE\t(MAX_BPF_REG + MAX_BPF_STACK / BPF_REG_SIZE)\nstruct idpair {\n\tu32 old;\n\tu32 cur;\n};\n\n/* If in the old state two registers had the same id, then they need to have\n * the same id in the new state as well.  But that id could be different from\n * the old state, so we need to track the mapping from old to new ids.\n * Once we have seen that, say, a reg with old id 5 had new id 9, any subsequent\n * regs with old id 5 must also have new id 9 for the new state to be safe.  But\n * regs with a different old id could still have new id 9, we don't care about\n * that.\n * So we look through our idmap to see if this old id has been seen before.  If\n * so, we require the new id to match; otherwise, we add the id pair to the map.\n */\nstatic bool check_ids(u32 old_id, u32 cur_id, struct idpair *idmap)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < ID_MAP_SIZE; i++) {\n\t\tif (!idmap[i].old) {\n\t\t\t/* Reached an empty slot; haven't seen this id before */\n\t\t\tidmap[i].old = old_id;\n\t\t\tidmap[i].cur = cur_id;\n\t\t\treturn true;\n\t\t}\n\t\tif (idmap[i].old == old_id)\n\t\t\treturn idmap[i].cur == cur_id;\n\t}\n\t/* We ran out of idmap slots, which should be impossible */\n\tWARN_ON_ONCE(1);\n\treturn false;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 3506,
        "critical_vars": [
            "dst",
            "env"
        ],
        "function": "adjust_scalar_min_max_vals",
        "filename": "linux/CVE-2019-7308/CVE-2019-7308_CWE-189_d3bd7413e0ca40b60cf60d4003246d067cafdeda_verifier.c.diff",
        "label": "True",
        "function_code": "static int adjust_scalar_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t      struct bpf_insn *insn,\n\t\t\t\t      struct bpf_reg_state *dst_reg,\n\t\t\t\t      struct bpf_reg_state src_reg)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 opcode = BPF_OP(insn->code);\n\tbool src_known, dst_known;\n\ts64 smin_val, smax_val;\n\tu64 umin_val, umax_val;\n\tu64 insn_bitness = (BPF_CLASS(insn->code) == BPF_ALU64) ? 64 : 32;\n\tu32 dst = insn->dst_reg;\n\tint ret;\n\n\tif (insn_bitness == 32) {\n\t\t/* Relevant for 32-bit RSH: Information can propagate towards\n\t\t * LSB, so it isn't sufficient to only truncate the output to\n\t\t * 32 bits.\n\t\t */\n\t\tcoerce_reg_to_size(dst_reg, 4);\n\t\tcoerce_reg_to_size(&src_reg, 4);\n\t}\n\n\tsmin_val = src_reg.smin_value;\n\tsmax_val = src_reg.smax_value;\n\tumin_val = src_reg.umin_value;\n\tumax_val = src_reg.umax_value;\n\tsrc_known = tnum_is_const(src_reg.var_off);\n\tdst_known = tnum_is_const(dst_reg->var_off);\n\n\tif ((src_known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (!src_known &&\n\t    opcode != BPF_ADD && opcode != BPF_SUB && opcode != BPF_AND) {\n\t\t__mark_reg_unknown(dst_reg);\n\t\treturn 0;\n\t}\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\tret = sanitize_val_alu(env, insn);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to add from different pointers or scalars\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\tif (signed_add_overflows(dst_reg->smin_value, smin_val) ||\n\t\t    signed_add_overflows(dst_reg->smax_value, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value += smin_val;\n\t\t\tdst_reg->smax_value += smax_val;\n\t\t}\n\t\tif (dst_reg->umin_value + umin_val < umin_val ||\n\t\t    dst_reg->umax_value + umax_val < umax_val) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value += umin_val;\n\t\t\tdst_reg->umax_value += umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(dst_reg->var_off, src_reg.var_off);\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tret = sanitize_val_alu(env, insn);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to sub from different pointers or scalars\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\tif (signed_sub_overflows(dst_reg->smin_value, smax_val) ||\n\t\t    signed_sub_overflows(dst_reg->smax_value, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value -= smax_val;\n\t\t\tdst_reg->smax_value -= smin_val;\n\t\t}\n\t\tif (dst_reg->umin_value < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value -= umax_val;\n\t\t\tdst_reg->umax_value -= umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(dst_reg->var_off, src_reg.var_off);\n\t\tbreak;\n\tcase BPF_MUL:\n\t\tdst_reg->var_off = tnum_mul(dst_reg->var_off, src_reg.var_off);\n\t\tif (smin_val < 0 || dst_reg->smin_value < 0) {\n\t\t\t/* Ain't nobody got time to multiply that sign */\n\t\t\t__mark_reg_unbounded(dst_reg);\n\t\t\t__update_reg_bounds(dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\t/* Both values are positive, so we can work with unsigned and\n\t\t * copy the result to signed (unless it exceeds S64_MAX).\n\t\t */\n\t\tif (umax_val > U32_MAX || dst_reg->umax_value > U32_MAX) {\n\t\t\t/* Potential overflow, we know nothing */\n\t\t\t__mark_reg_unbounded(dst_reg);\n\t\t\t/* (except what we can learn from the var_off) */\n\t\t\t__update_reg_bounds(dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\tdst_reg->umin_value *= umin_val;\n\t\tdst_reg->umax_value *= umax_val;\n\t\tif (dst_reg->umax_value > S64_MAX) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = dst_reg->umin_value;\n\t\t\tdst_reg->smax_value = dst_reg->umax_value;\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\t\tif (src_known && dst_known) {\n\t\t\t__mark_reg_known(dst_reg, dst_reg->var_off.value &\n\t\t\t\t\t\t  src_reg.var_off.value);\n\t\t\tbreak;\n\t\t}\n\t\t/* We get our minimum from the var_off, since that's inherently\n\t\t * bitwise.  Our maximum is the minimum of the operands' maxima.\n\t\t */\n\t\tdst_reg->var_off = tnum_and(dst_reg->var_off, src_reg.var_off);\n\t\tdst_reg->umin_value = dst_reg->var_off.value;\n\t\tdst_reg->umax_value = min(dst_reg->umax_value, umax_val);\n\t\tif (dst_reg->smin_value < 0 || smin_val < 0) {\n\t\t\t/* Lose signed bounds when ANDing negative numbers,\n\t\t\t * ain't nobody got time for that.\n\t\t\t */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\t/* ANDing two positives gives a positive, so safe to\n\t\t\t * cast result into s64.\n\t\t\t */\n\t\t\tdst_reg->smin_value = dst_reg->umin_value;\n\t\t\tdst_reg->smax_value = dst_reg->umax_value;\n\t\t}\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tcase BPF_OR:\n\t\tif (src_known && dst_known) {\n\t\t\t__mark_reg_known(dst_reg, dst_reg->var_off.value |\n\t\t\t\t\t\t  src_reg.var_off.value);\n\t\t\tbreak;\n\t\t}\n\t\t/* We get our maximum from the var_off, and our minimum is the\n\t\t * maximum of the operands' minima\n\t\t */\n\t\tdst_reg->var_off = tnum_or(dst_reg->var_off, src_reg.var_off);\n\t\tdst_reg->umin_value = max(dst_reg->umin_value, umin_val);\n\t\tdst_reg->umax_value = dst_reg->var_off.value |\n\t\t\t\t      dst_reg->var_off.mask;\n\t\tif (dst_reg->smin_value < 0 || smin_val < 0) {\n\t\t\t/* Lose signed bounds when ORing negative numbers,\n\t\t\t * ain't nobody got time for that.\n\t\t\t */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\t/* ORing two positives gives a positive, so safe to\n\t\t\t * cast result into s64.\n\t\t\t */\n\t\t\tdst_reg->smin_value = dst_reg->umin_value;\n\t\t\tdst_reg->smax_value = dst_reg->umax_value;\n\t\t}\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tcase BPF_LSH:\n\t\tif (umax_val >= insn_bitness) {\n\t\t\t/* Shifts greater than 31 or 63 are undefined.\n\t\t\t * This includes shifts by a negative number.\n\t\t\t */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\t/* We lose all sign bit information (except what we can pick\n\t\t * up from var_off)\n\t\t */\n\t\tdst_reg->smin_value = S64_MIN;\n\t\tdst_reg->smax_value = S64_MAX;\n\t\t/* If we might shift our top bit out, then we know nothing */\n\t\tif (dst_reg->umax_value > 1ULL << (63 - umax_val)) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value <<= umin_val;\n\t\t\tdst_reg->umax_value <<= umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_lshift(dst_reg->var_off, umin_val);\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tcase BPF_RSH:\n\t\tif (umax_val >= insn_bitness) {\n\t\t\t/* Shifts greater than 31 or 63 are undefined.\n\t\t\t * This includes shifts by a negative number.\n\t\t\t */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\t/* BPF_RSH is an unsigned shift.  If the value in dst_reg might\n\t\t * be negative, then either:\n\t\t * 1) src_reg might be zero, so the sign bit of the result is\n\t\t *    unknown, so we lose our signed bounds\n\t\t * 2) it's known negative, thus the unsigned bounds capture the\n\t\t *    signed bounds\n\t\t * 3) the signed bounds cross zero, so they tell us nothing\n\t\t *    about the result\n\t\t * If the value in dst_reg is known nonnegative, then again the\n\t\t * unsigned bounts capture the signed bounds.\n\t\t * Thus, in all cases it suffices to blow away our signed bounds\n\t\t * and rely on inferring new ones from the unsigned bounds and\n\t\t * var_off of the result.\n\t\t */\n\t\tdst_reg->smin_value = S64_MIN;\n\t\tdst_reg->smax_value = S64_MAX;\n\t\tdst_reg->var_off = tnum_rshift(dst_reg->var_off, umin_val);\n\t\tdst_reg->umin_value >>= umax_val;\n\t\tdst_reg->umax_value >>= umin_val;\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tcase BPF_ARSH:\n\t\tif (umax_val >= insn_bitness) {\n\t\t\t/* Shifts greater than 31 or 63 are undefined.\n\t\t\t * This includes shifts by a negative number.\n\t\t\t */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Upon reaching here, src_known is true and\n\t\t * umax_val is equal to umin_val.\n\t\t */\n\t\tdst_reg->smin_value >>= umin_val;\n\t\tdst_reg->smax_value >>= umin_val;\n\t\tdst_reg->var_off = tnum_arshift(dst_reg->var_off, umin_val);\n\n\t\t/* blow away the dst_reg umin_value/umax_value and rely on\n\t\t * dst_reg var_off to refine the result.\n\t\t */\n\t\tdst_reg->umin_value = 0;\n\t\tdst_reg->umax_value = U64_MAX;\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tdefault:\n\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\tbreak;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops are (32,32)->32 */\n\t\tcoerce_reg_to_size(dst_reg, 4);\n\t}\n\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\treturn 0;\n}\n\n/* Handles ALU ops other than BPF_END, BPF_NEG and BPF_MOV: computes new min/max\n * and var_off.\n */\nstatic int adjust_reg_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg, *src_reg;\n\tstruct bpf_reg_state *ptr_reg = NULL, off_reg = {0};\n\tu8 opcode = BPF_OP(insn->code);\n\n\tdst_reg = &regs[insn->dst_reg];\n\tsrc_reg = NULL;\n\tif (dst_reg->type != SCALAR_VALUE)\n\t\tptr_reg = dst_reg;\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tsrc_reg = &regs[insn->src_reg];\n\t\tif (src_reg->type != SCALAR_VALUE) {\n\t\t\tif (dst_reg->type != SCALAR_VALUE) {\n\t\t\t\t/* Combining two pointers by any ALU op yields\n\t\t\t\t * an arbitrary scalar. Disallow all math except\n\t\t\t\t * pointer subtraction\n\t\t\t\t */\n\t\t\t\tif (opcode == BPF_SUB && env->allow_ptr_leaks) {\n\t\t\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t\tverbose(env, \"R%d pointer %s pointer prohibited\\n\",\n\t\t\t\t\tinsn->dst_reg,\n\t\t\t\t\tbpf_alu_string[opcode >> 4]);\n\t\t\t\treturn -EACCES;\n\t\t\t} else {\n\t\t\t\t/* scalar += pointer\n\t\t\t\t * This is legal, but we have to reverse our\n\t\t\t\t * src/dest handling in computing the range\n\t\t\t\t */\n\t\t\t\treturn adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t\t       src_reg, dst_reg);\n\t\t\t}\n\t\t} else if (ptr_reg) {\n\t\t\t/* pointer += scalar */\n\t\t\treturn adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t       dst_reg, src_reg);\n\t\t}\n\t} else {\n\t\t/* Pretend the src is a reg with a known value, since we only\n\t\t * need to be able to read from this state.\n\t\t */\n\t\toff_reg.type = SCALAR_VALUE;\n\t\t__mark_reg_known(&off_reg, insn->imm);\n\t\tsrc_reg = &off_reg;\n\t\tif (ptr_reg) /* pointer += K */\n\t\t\treturn adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t       ptr_reg, src_reg);\n\t}\n\n\t/* Got here implies adding two SCALAR_VALUEs */\n\tif (WARN_ON_ONCE(ptr_reg)) {\n\t\tprint_verifier_state(env, state);\n\t\tverbose(env, \"verifier internal error: unexpected ptr_reg\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (WARN_ON(!src_reg)) {\n\t\tprint_verifier_state(env, state);\n\t\tverbose(env, \"verifier internal error: no src_reg\\n\");\n\t\treturn -EINVAL;\n\t}\n\treturn adjust_scalar_min_max_vals(env, insn, dst_reg, *src_reg);\n}\n\n/* check validity of 32-bit and 64-bit arithmetic operations */\nstatic int check_alu_op(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 opcode = BPF_OP(insn->code);\n\tint err;\n\n\tif (opcode == BPF_END || opcode == BPF_NEG) {\n\t\tif (opcode == BPF_NEG) {\n\t\t\tif (BPF_SRC(insn->code) != 0 ||\n\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t    insn->off != 0 || insn->imm != 0) {\n\t\t\t\tverbose(env, \"BPF_NEG uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else {\n\t\t\tif (insn->src_reg != BPF_REG_0 || insn->off != 0 ||\n\t\t\t    (insn->imm != 16 && insn->imm != 32 && insn->imm != 64) ||\n\t\t\t    BPF_CLASS(insn->code) == BPF_ALU64) {\n\t\t\t\tverbose(env, \"BPF_END uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check src operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (is_pointer_value(env, insn->dst_reg)) {\n\t\t\tverbose(env, \"R%d pointer arithmetic prohibited\\n\",\n\t\t\t\tinsn->dst_reg);\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\t/* check dest operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t} else if (opcode == BPF_MOV) {\n\n\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\tif (insn->imm != 0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_MOV uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\t/* check src operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t} else {\n\t\t\tif (insn->src_reg != BPF_REG_0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_MOV uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check dest operand, mark as required later */\n\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP_NO_MARK);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\tstruct bpf_reg_state *src_reg = regs + insn->src_reg;\n\t\t\tstruct bpf_reg_state *dst_reg = regs + insn->dst_reg;\n\n\t\t\tif (BPF_CLASS(insn->code) == BPF_ALU64) {\n\t\t\t\t/* case: R1 = R2\n\t\t\t\t * copy register state to dest reg\n\t\t\t\t */\n\t\t\t\t*dst_reg = *src_reg;\n\t\t\t\tdst_reg->live |= REG_LIVE_WRITTEN;\n\t\t\t} else {\n\t\t\t\t/* R1 = (u32) R2 */\n\t\t\t\tif (is_pointer_value(env, insn->src_reg)) {\n\t\t\t\t\tverbose(env,\n\t\t\t\t\t\t\"R%d partial copy of pointer\\n\",\n\t\t\t\t\t\tinsn->src_reg);\n\t\t\t\t\treturn -EACCES;\n\t\t\t\t} else if (src_reg->type == SCALAR_VALUE) {\n\t\t\t\t\t*dst_reg = *src_reg;\n\t\t\t\t\tdst_reg->live |= REG_LIVE_WRITTEN;\n\t\t\t\t} else {\n\t\t\t\t\tmark_reg_unknown(env, regs,\n\t\t\t\t\t\t\t insn->dst_reg);\n\t\t\t\t}\n\t\t\t\tcoerce_reg_to_size(dst_reg, 4);\n\t\t\t}\n\t\t} else {\n\t\t\t/* case: R = imm\n\t\t\t * remember the value we stored into this reg\n\t\t\t */\n\t\t\t/* clear any state __mark_reg_known doesn't set */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tregs[insn->dst_reg].type = SCALAR_VALUE;\n\t\t\tif (BPF_CLASS(insn->code) == BPF_ALU64) {\n\t\t\t\t__mark_reg_known(regs + insn->dst_reg,\n\t\t\t\t\t\t insn->imm);\n\t\t\t} else {\n\t\t\t\t__mark_reg_known(regs + insn->dst_reg,\n\t\t\t\t\t\t (u32)insn->imm);\n\t\t\t}\n\t\t}\n\n\t} else if (opcode > BPF_END) {\n\t\tverbose(env, \"invalid BPF_ALU opcode %x\\n\", opcode);\n\t\treturn -EINVAL;\n\n\t} else {\t/* all other ALU ops: and, sub, xor, add, ... */\n\n\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\tif (insn->imm != 0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_ALU uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\t/* check src1 operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t} else {\n\t\t\tif (insn->src_reg != BPF_REG_0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_ALU uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check src2 operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif ((opcode == BPF_MOD || opcode == BPF_DIV) &&\n\t\t    BPF_SRC(insn->code) == BPF_K && insn->imm == 0) {\n\t\t\tverbose(env, \"div by zero\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif ((opcode == BPF_LSH || opcode == BPF_RSH ||\n\t\t     opcode == BPF_ARSH) && BPF_SRC(insn->code) == BPF_K) {\n\t\t\tint size = BPF_CLASS(insn->code) == BPF_ALU64 ? 64 : 32;\n\n\t\t\tif (insn->imm < 0 || insn->imm >= size) {\n\t\t\t\tverbose(env, \"invalid shift %d\\n\", insn->imm);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check dest operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP_NO_MARK);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\treturn adjust_reg_min_max_vals(env, insn);\n\t}\n\n\treturn 0;\n}\n\nstatic void find_good_pkt_pointers(struct bpf_verifier_state *vstate,\n\t\t\t\t   struct bpf_reg_state *dst_reg,\n\t\t\t\t   enum bpf_reg_type type,\n\t\t\t\t   bool range_right_open)\n{\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *reg;\n\tu16 new_range;\n\tint i, j;\n\n\tif (dst_reg->off < 0 ||\n\t    (dst_reg->off == 0 && range_right_open))\n\t\t/* This doesn't give us any range */\n\t\treturn;\n\n\tif (dst_reg->umax_value > MAX_PACKET_OFF ||\n\t    dst_reg->umax_value + dst_reg->off > MAX_PACKET_OFF)\n\t\t/* Risk of overflow.  For instance, ptr + (1<<63) may be less\n\t\t * than pkt_end, but that's because it's also less than pkt.\n\t\t */\n\t\treturn;\n\n\tnew_range = dst_reg->off;\n\tif (range_right_open)\n\t\tnew_range--;\n\n\t/* Examples for register markings:\n\t *\n\t * pkt_data in dst register:\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (r2 > pkt_end) goto <handle exception>\n\t *   <access okay>\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (r2 < pkt_end) goto <access okay>\n\t *   <handle exception>\n\t *\n\t *   Where:\n\t *     r2 == dst_reg, pkt_end == src_reg\n\t *     r2=pkt(id=n,off=8,r=0)\n\t *     r3=pkt(id=n,off=0,r=0)\n\t *\n\t * pkt_data in src register:\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (pkt_end >= r2) goto <access okay>\n\t *   <handle exception>\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (pkt_end <= r2) goto <handle exception>\n\t *   <access okay>\n\t *\n\t *   Where:\n\t *     pkt_end == dst_reg, r2 == src_reg\n\t *     r2=pkt(id=n,off=8,r=0)\n\t *     r3=pkt(id=n,off=0,r=0)\n\t *\n\t * Find register r3 and mark its range as r3=pkt(id=n,off=0,r=8)\n\t * or r3=pkt(id=n,off=0,r=8-1), so that range of bytes [r3, r3 + 8)\n\t * and [r3, r3 + 8-1) respectively is safe to access depending on\n\t * the check.\n\t */\n\n\t/* If our ids match, then we must have the same max_value.  And we\n\t * don't care about the other reg's fixed offset, since if it's too big\n\t * the range won't allow anything.\n\t * dst_reg->off is known < MAX_PACKET_OFF, therefore it fits in a u16.\n\t */\n\tfor (i = 0; i < MAX_BPF_REG; i++)\n\t\tif (regs[i].type == type && regs[i].id == dst_reg->id)\n\t\t\t/* keep the maximum range already checked */\n\t\t\tregs[i].range = max(regs[i].range, new_range);\n\n\tfor (j = 0; j <= vstate->curframe; j++) {\n\t\tstate = vstate->frame[j];\n\t\tbpf_for_each_spilled_reg(i, state, reg) {\n\t\t\tif (!reg)\n\t\t\t\tcontinue;\n\t\t\tif (reg->type == type && reg->id == dst_reg->id)\n\t\t\t\treg->range = max(reg->range, new_range);\n\t\t}\n\t}\n}\n\n/* compute branch direction of the expression \"if (reg opcode val) goto target;\"\n * and return:\n *  1 - branch will be taken and \"goto target\" will be executed\n *  0 - branch will not be taken and fall-through to next insn\n * -1 - unknown. Example: \"if (reg < 5)\" is unknown when register value range [0,10]\n */\nstatic int is_branch_taken(struct bpf_reg_state *reg, u64 val, u8 opcode)\n{\n\tif (__is_pointer_value(false, reg))\n\t\treturn -1;\n\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\tif (tnum_is_const(reg->var_off))\n\t\t\treturn !!tnum_equals_const(reg->var_off, val);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\tif (tnum_is_const(reg->var_off))\n\t\t\treturn !tnum_equals_const(reg->var_off, val);\n\t\tbreak;\n\tcase BPF_JSET:\n\t\tif ((~reg->var_off.mask & reg->var_off.value) & val)\n\t\t\treturn 1;\n\t\tif (!((reg->var_off.mask | reg->var_off.value) & val))\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JGT:\n\t\tif (reg->umin_value > val)\n\t\t\treturn 1;\n\t\telse if (reg->umax_value <= val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSGT:\n\t\tif (reg->smin_value > (s64)val)\n\t\t\treturn 1;\n\t\telse if (reg->smax_value < (s64)val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JLT:\n\t\tif (reg->umax_value < val)\n\t\t\treturn 1;\n\t\telse if (reg->umin_value >= val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSLT:\n\t\tif (reg->smax_value < (s64)val)\n\t\t\treturn 1;\n\t\telse if (reg->smin_value >= (s64)val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JGE:\n\t\tif (reg->umin_value >= val)\n\t\t\treturn 1;\n\t\telse if (reg->umax_value < val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSGE:\n\t\tif (reg->smin_value >= (s64)val)\n\t\t\treturn 1;\n\t\telse if (reg->smax_value < (s64)val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JLE:\n\t\tif (reg->umax_value <= val)\n\t\t\treturn 1;\n\t\telse if (reg->umin_value > val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSLE:\n\t\tif (reg->smax_value <= (s64)val)\n\t\t\treturn 1;\n\t\telse if (reg->smin_value > (s64)val)\n\t\t\treturn 0;\n\t\tbreak;\n\t}\n\n\treturn -1;\n}\n\n/* Adjusts the register min/max values in the case that the dst_reg is the\n * variable register that we are working on, and src_reg is a constant or we're\n * simply doing a BPF_K check.\n * In JEQ/JNE cases we also adjust the var_off values.\n */\nstatic void reg_set_min_max(struct bpf_reg_state *true_reg,\n\t\t\t    struct bpf_reg_state *false_reg, u64 val,\n\t\t\t    u8 opcode)\n{\n\t/* If the dst_reg is a pointer, we can't learn anything about its\n\t * variable offset from the compare (unless src_reg were a pointer into\n\t * the same object, but we don't bother with that.\n\t * Since false_reg and true_reg have the same type by construction, we\n\t * only need to check one of them for pointerness.\n\t */\n\tif (__is_pointer_value(false, false_reg))\n\t\treturn;\n\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\t/* If this is false then we know nothing Jon Snow, but if it is\n\t\t * true then we know for sure.\n\t\t */\n\t\t__mark_reg_known(true_reg, val);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\t/* If this is true we know nothing Jon Snow, but if it is false\n\t\t * we know the value for sure;\n\t\t */\n\t\t__mark_reg_known(false_reg, val);\n\t\tbreak;\n\tcase BPF_JSET:\n\t\tfalse_reg->var_off = tnum_and(false_reg->var_off,\n\t\t\t\t\t      tnum_const(~val));\n\t\tif (is_power_of_2(val))\n\t\t\ttrue_reg->var_off = tnum_or(true_reg->var_off,\n\t\t\t\t\t\t    tnum_const(val));\n\t\tbreak;\n\tcase BPF_JGT:\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val);\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JSGT:\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val);\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JLT:\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val);\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val - 1);\n\t\tbreak;\n\tcase BPF_JSLT:\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val);\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val - 1);\n\t\tbreak;\n\tcase BPF_JGE:\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val - 1);\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val);\n\t\tbreak;\n\tcase BPF_JSGE:\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val - 1);\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val);\n\t\tbreak;\n\tcase BPF_JLE:\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val + 1);\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val);\n\t\tbreak;\n\tcase BPF_JSLE:\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val + 1);\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t__reg_deduce_bounds(false_reg);\n\t__reg_deduce_bounds(true_reg);\n\t/* We might have learned some bits from the bounds. */\n\t__reg_bound_offset(false_reg);\n\t__reg_bound_offset(true_reg);\n\t/* Intersecting with the old var_off might have improved our bounds\n\t * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),\n\t * then new var_off is (0; 0x7f...fc) which improves our umax.\n\t */\n\t__update_reg_bounds(false_reg);\n\t__update_reg_bounds(true_reg);\n}\n\n/* Same as above, but for the case that dst_reg holds a constant and src_reg is\n * the variable reg.\n */\nstatic void reg_set_min_max_inv(struct bpf_reg_state *true_reg,\n\t\t\t\tstruct bpf_reg_state *false_reg, u64 val,\n\t\t\t\tu8 opcode)\n{\n\tif (__is_pointer_value(false, false_reg))\n\t\treturn;\n\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\t/* If this is false then we know nothing Jon Snow, but if it is\n\t\t * true then we know for sure.\n\t\t */\n\t\t__mark_reg_known(true_reg, val);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\t/* If this is true we know nothing Jon Snow, but if it is false\n\t\t * we know the value for sure;\n\t\t */\n\t\t__mark_reg_known(false_reg, val);\n\t\tbreak;\n\tcase BPF_JSET:\n\t\tfalse_reg->var_off = tnum_and(false_reg->var_off,\n\t\t\t\t\t      tnum_const(~val));\n\t\tif (is_power_of_2(val))\n\t\t\ttrue_reg->var_off = tnum_or(true_reg->var_off,\n\t\t\t\t\t\t    tnum_const(val));\n\t\tbreak;\n\tcase BPF_JGT:\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val - 1);\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val);\n\t\tbreak;\n\tcase BPF_JSGT:\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val - 1);\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val);\n\t\tbreak;\n\tcase BPF_JLT:\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val + 1);\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val);\n\t\tbreak;\n\tcase BPF_JSLT:\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val + 1);\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val);\n\t\tbreak;\n\tcase BPF_JGE:\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val);\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JSGE:\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val);\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JLE:\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val);\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val - 1);\n\t\tbreak;\n\tcase BPF_JSLE:\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val);\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val - 1);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t__reg_deduce_bounds(false_reg);\n\t__reg_deduce_bounds(true_reg);\n\t/* We might have learned some bits from the bounds. */\n\t__reg_bound_offset(false_reg);\n\t__reg_bound_offset(true_reg);\n\t/* Intersecting with the old var_off might have improved our bounds\n\t * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),\n\t * then new var_off is (0; 0x7f...fc) which improves our umax.\n\t */\n\t__update_reg_bounds(false_reg);\n\t__update_reg_bounds(true_reg);\n}\n\n/* Regs are known to be equal, so intersect their min/max/var_off */\nstatic void __reg_combine_min_max(struct bpf_reg_state *src_reg,\n\t\t\t\t  struct bpf_reg_state *dst_reg)\n{\n\tsrc_reg->umin_value = dst_reg->umin_value = max(src_reg->umin_value,\n\t\t\t\t\t\t\tdst_reg->umin_value);\n\tsrc_reg->umax_value = dst_reg->umax_value = min(src_reg->umax_value,\n\t\t\t\t\t\t\tdst_reg->umax_value);\n\tsrc_reg->smin_value = dst_reg->smin_value = max(src_reg->smin_value,\n\t\t\t\t\t\t\tdst_reg->smin_value);\n\tsrc_reg->smax_value = dst_reg->smax_value = min(src_reg->smax_value,\n\t\t\t\t\t\t\tdst_reg->smax_value);\n\tsrc_reg->var_off = dst_reg->var_off = tnum_intersect(src_reg->var_off,\n\t\t\t\t\t\t\t     dst_reg->var_off);\n\t/* We might have learned new bounds from the var_off. */\n\t__update_reg_bounds(src_reg);\n\t__update_reg_bounds(dst_reg);\n\t/* We might have learned something about the sign bit. */\n\t__reg_deduce_bounds(src_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t/* We might have learned some bits from the bounds. */\n\t__reg_bound_offset(src_reg);\n\t__reg_bound_offset(dst_reg);\n\t/* Intersecting with the old var_off might have improved our bounds\n\t * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),\n\t * then new var_off is (0; 0x7f...fc) which improves our umax.\n\t */\n\t__update_reg_bounds(src_reg);\n\t__update_reg_bounds(dst_reg);\n}\n\nstatic void reg_combine_min_max(struct bpf_reg_state *true_src,\n\t\t\t\tstruct bpf_reg_state *true_dst,\n\t\t\t\tstruct bpf_reg_state *false_src,\n\t\t\t\tstruct bpf_reg_state *false_dst,\n\t\t\t\tu8 opcode)\n{\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\t__reg_combine_min_max(true_src, true_dst);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\t__reg_combine_min_max(false_src, false_dst);\n\t\tbreak;\n\t}\n}\n\nstatic void mark_ptr_or_null_reg(struct bpf_func_state *state,\n\t\t\t\t struct bpf_reg_state *reg, u32 id,\n\t\t\t\t bool is_null)\n{\n\tif (reg_type_may_be_null(reg->type) && reg->id == id) {\n\t\t/* Old offset (both fixed and variable parts) should\n\t\t * have been known-zero, because we don't allow pointer\n\t\t * arithmetic on pointers that might be NULL.\n\t\t */\n\t\tif (WARN_ON_ONCE(reg->smin_value || reg->smax_value ||\n\t\t\t\t !tnum_equals_const(reg->var_off, 0) ||\n\t\t\t\t reg->off)) {\n\t\t\t__mark_reg_known_zero(reg);\n\t\t\treg->off = 0;\n\t\t}\n\t\tif (is_null) {\n\t\t\treg->type = SCALAR_VALUE;\n\t\t} else if (reg->type == PTR_TO_MAP_VALUE_OR_NULL) {\n\t\t\tif (reg->map_ptr->inner_map_meta) {\n\t\t\t\treg->type = CONST_PTR_TO_MAP;\n\t\t\t\treg->map_ptr = reg->map_ptr->inner_map_meta;\n\t\t\t} else {\n\t\t\t\treg->type = PTR_TO_MAP_VALUE;\n\t\t\t}\n\t\t} else if (reg->type == PTR_TO_SOCKET_OR_NULL) {\n\t\t\treg->type = PTR_TO_SOCKET;\n\t\t}\n\t\tif (is_null || !reg_is_refcounted(reg)) {\n\t\t\t/* We don't need id from this point onwards anymore,\n\t\t\t * thus we should better reset it, so that state\n\t\t\t * pruning has chances to take effect.\n\t\t\t */\n\t\t\treg->id = 0;\n\t\t}\n\t}\n}\n\n/* The logic is similar to find_good_pkt_pointers(), both could eventually\n * be folded together at some point.\n */\nstatic void mark_ptr_or_null_regs(struct bpf_verifier_state *vstate, u32 regno,\n\t\t\t\t  bool is_null)\n{\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *reg, *regs = state->regs;\n\tu32 id = regs[regno].id;\n\tint i, j;\n\n\tif (reg_is_refcounted_or_null(&regs[regno]) && is_null)\n\t\t__release_reference_state(state, id);\n\n\tfor (i = 0; i < MAX_BPF_REG; i++)\n\t\tmark_ptr_or_null_reg(state, &regs[i], id, is_null);\n\n\tfor (j = 0; j <= vstate->curframe; j++) {\n\t\tstate = vstate->frame[j];\n\t\tbpf_for_each_spilled_reg(i, state, reg) {\n\t\t\tif (!reg)\n\t\t\t\tcontinue;\n\t\t\tmark_ptr_or_null_reg(state, reg, id, is_null);\n\t\t}\n\t}\n}\n\nstatic bool try_match_pkt_pointers(const struct bpf_insn *insn,\n\t\t\t\t   struct bpf_reg_state *dst_reg,\n\t\t\t\t   struct bpf_reg_state *src_reg,\n\t\t\t\t   struct bpf_verifier_state *this_branch,\n\t\t\t\t   struct bpf_verifier_state *other_branch)\n{\n\tif (BPF_SRC(insn->code) != BPF_X)\n\t\treturn false;\n\n\tswitch (BPF_OP(insn->code)) {\n\tcase BPF_JGT:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' > pkt_end, pkt_meta' > pkt_data */\n\t\t\tfind_good_pkt_pointers(this_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, false);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end > pkt_data', pkt_data > pkt_meta' */\n\t\t\tfind_good_pkt_pointers(other_branch, src_reg,\n\t\t\t\t\t       src_reg->type, true);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase BPF_JLT:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' < pkt_end, pkt_meta' < pkt_data */\n\t\t\tfind_good_pkt_pointers(other_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, true);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end < pkt_data', pkt_data > pkt_meta' */\n\t\t\tfind_good_pkt_pointers(this_branch, src_reg,\n\t\t\t\t\t       src_reg->type, false);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase BPF_JGE:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' >= pkt_end, pkt_meta' >= pkt_data */\n\t\t\tfind_good_pkt_pointers(this_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, true);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end >= pkt_data', pkt_data >= pkt_meta' */\n\t\t\tfind_good_pkt_pointers(other_branch, src_reg,\n\t\t\t\t\t       src_reg->type, false);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase BPF_JLE:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' <= pkt_end, pkt_meta' <= pkt_data */\n\t\t\tfind_good_pkt_pointers(other_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, false);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end <= pkt_data', pkt_data <= pkt_meta' */\n\t\t\tfind_good_pkt_pointers(this_branch, src_reg,\n\t\t\t\t\t       src_reg->type, true);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic int check_cond_jmp_op(struct bpf_verifier_env *env,\n\t\t\t     struct bpf_insn *insn, int *insn_idx)\n{\n\tstruct bpf_verifier_state *this_branch = env->cur_state;\n\tstruct bpf_verifier_state *other_branch;\n\tstruct bpf_reg_state *regs = this_branch->frame[this_branch->curframe]->regs;\n\tstruct bpf_reg_state *dst_reg, *other_branch_regs;\n\tu8 opcode = BPF_OP(insn->code);\n\tint err;\n\n\tif (opcode > BPF_JSLE) {\n\t\tverbose(env, \"invalid BPF_JMP opcode %x\\n\", opcode);\n\t\treturn -EINVAL;\n\t}\n\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tif (insn->imm != 0) {\n\t\t\tverbose(env, \"BPF_JMP uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/* check src1 operand */\n\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (is_pointer_value(env, insn->src_reg)) {\n\t\t\tverbose(env, \"R%d pointer comparison prohibited\\n\",\n\t\t\t\tinsn->src_reg);\n\t\t\treturn -EACCES;\n\t\t}\n\t} else {\n\t\tif (insn->src_reg != BPF_REG_0) {\n\t\t\tverbose(env, \"BPF_JMP uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t/* check src2 operand */\n\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tdst_reg = &regs[insn->dst_reg];\n\n\tif (BPF_SRC(insn->code) == BPF_K) {\n\t\tint pred = is_branch_taken(dst_reg, insn->imm, opcode);\n\n\t\tif (pred == 1) {\n\t\t\t /* only follow the goto, ignore fall-through */\n\t\t\t*insn_idx += insn->off;\n\t\t\treturn 0;\n\t\t} else if (pred == 0) {\n\t\t\t/* only follow fall-through branch, since\n\t\t\t * that's where the program will go\n\t\t\t */\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tother_branch = push_stack(env, *insn_idx + insn->off + 1, *insn_idx,\n\t\t\t\t  false);\n\tif (!other_branch)\n\t\treturn -EFAULT;\n\tother_branch_regs = other_branch->frame[other_branch->curframe]->regs;\n\n\t/* detect if we are comparing against a constant value so we can adjust\n\t * our min/max values for our dst register.\n\t * this is only legit if both are scalars (or pointers to the same\n\t * object, I suppose, but we don't support that right now), because\n\t * otherwise the different base pointers mean the offsets aren't\n\t * comparable.\n\t */\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tif (dst_reg->type == SCALAR_VALUE &&\n\t\t    regs[insn->src_reg].type == SCALAR_VALUE) {\n\t\t\tif (tnum_is_const(regs[insn->src_reg].var_off))\n\t\t\t\treg_set_min_max(&other_branch_regs[insn->dst_reg],\n\t\t\t\t\t\tdst_reg, regs[insn->src_reg].var_off.value,\n\t\t\t\t\t\topcode);\n\t\t\telse if (tnum_is_const(dst_reg->var_off))\n\t\t\t\treg_set_min_max_inv(&other_branch_regs[insn->src_reg],\n\t\t\t\t\t\t    &regs[insn->src_reg],\n\t\t\t\t\t\t    dst_reg->var_off.value, opcode);\n\t\t\telse if (opcode == BPF_JEQ || opcode == BPF_JNE)\n\t\t\t\t/* Comparing for equality, we can combine knowledge */\n\t\t\t\treg_combine_min_max(&other_branch_regs[insn->src_reg],\n\t\t\t\t\t\t    &other_branch_regs[insn->dst_reg],\n\t\t\t\t\t\t    &regs[insn->src_reg],\n\t\t\t\t\t\t    &regs[insn->dst_reg], opcode);\n\t\t}\n\t} else if (dst_reg->type == SCALAR_VALUE) {\n\t\treg_set_min_max(&other_branch_regs[insn->dst_reg],\n\t\t\t\t\tdst_reg, insn->imm, opcode);\n\t}\n\n\t/* detect if R == 0 where R is returned from bpf_map_lookup_elem() */\n\tif (BPF_SRC(insn->code) == BPF_K &&\n\t    insn->imm == 0 && (opcode == BPF_JEQ || opcode == BPF_JNE) &&\n\t    reg_type_may_be_null(dst_reg->type)) {\n\t\t/* Mark all identical registers in each branch as either\n\t\t * safe or unknown depending R == 0 or R != 0 conditional.\n\t\t */\n\t\tmark_ptr_or_null_regs(this_branch, insn->dst_reg,\n\t\t\t\t      opcode == BPF_JNE);\n\t\tmark_ptr_or_null_regs(other_branch, insn->dst_reg,\n\t\t\t\t      opcode == BPF_JEQ);\n\t} else if (!try_match_pkt_pointers(insn, dst_reg, &regs[insn->src_reg],\n\t\t\t\t\t   this_branch, other_branch) &&\n\t\t   is_pointer_value(env, insn->dst_reg)) {\n\t\tverbose(env, \"R%d pointer comparison prohibited\\n\",\n\t\t\tinsn->dst_reg);\n\t\treturn -EACCES;\n\t}\n\tif (env->log.level)\n\t\tprint_verifier_state(env, this_branch->frame[this_branch->curframe]);\n\treturn 0;\n}\n\n/* return the map pointer stored inside BPF_LD_IMM64 instruction */\nstatic struct bpf_map *ld_imm64_to_map_ptr(struct bpf_insn *insn)\n{\n\tu64 imm64 = ((u64) (u32) insn[0].imm) | ((u64) (u32) insn[1].imm) << 32;\n\n\treturn (struct bpf_map *) (unsigned long) imm64;\n}\n\n/* verify BPF_LD_IMM64 instruction */\nstatic int check_ld_imm(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tint err;\n\n\tif (BPF_SIZE(insn->code) != BPF_DW) {\n\t\tverbose(env, \"invalid BPF_LD_IMM insn\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (insn->off != 0) {\n\t\tverbose(env, \"BPF_LD_IMM64 uses reserved fields\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\terr = check_reg_arg(env, insn->dst_reg, DST_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (insn->src_reg == 0) {\n\t\tu64 imm = ((u64)(insn + 1)->imm << 32) | (u32)insn->imm;\n\n\t\tregs[insn->dst_reg].type = SCALAR_VALUE;\n\t\t__mark_reg_known(&regs[insn->dst_reg], imm);\n\t\treturn 0;\n\t}\n\n\t/* replace_map_fd_with_map_ptr() should have caught bad ld_imm64 */\n\tBUG_ON(insn->src_reg != BPF_PSEUDO_MAP_FD);\n\n\tregs[insn->dst_reg].type = CONST_PTR_TO_MAP;\n\tregs[insn->dst_reg].map_ptr = ld_imm64_to_map_ptr(insn);\n\treturn 0;\n}\n\nstatic bool may_access_skb(enum bpf_prog_type type)\n{\n\tswitch (type) {\n\tcase BPF_PROG_TYPE_SOCKET_FILTER:\n\tcase BPF_PROG_TYPE_SCHED_CLS:\n\tcase BPF_PROG_TYPE_SCHED_ACT:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n/* verify safety of LD_ABS|LD_IND instructions:\n * - they can only appear in the programs where ctx == skb\n * - since they are wrappers of function calls, they scratch R1-R5 registers,\n *   preserve R6-R9, and store return value into R0\n *\n * Implicit input:\n *   ctx == skb == R6 == CTX\n *\n * Explicit input:\n *   SRC == any register\n *   IMM == 32-bit immediate\n *\n * Output:\n *   R0 - 8/16/32-bit skb data converted to cpu endianness\n */\nstatic int check_ld_abs(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 mode = BPF_MODE(insn->code);\n\tint i, err;\n\n\tif (!may_access_skb(env->prog->type)) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] instructions not allowed for this program type\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!env->ops->gen_ld_abs) {\n\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (env->subprog_cnt > 1) {\n\t\t/* when program has LD_ABS insn JITs and interpreter assume\n\t\t * that r1 == ctx == skb which is not the case for callees\n\t\t * that can have arbitrary arguments. It's problematic\n\t\t * for main prog as well since JITs would need to analyze\n\t\t * all functions in order to make proper register save/restore\n\t\t * decisions in the main prog. Hence disallow LD_ABS with calls\n\t\t */\n\t\tverbose(env, \"BPF_LD_[ABS|IND] instructions cannot be mixed with bpf-to-bpf calls\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (insn->dst_reg != BPF_REG_0 || insn->off != 0 ||\n\t    BPF_SIZE(insn->code) == BPF_DW ||\n\t    (mode == BPF_ABS && insn->src_reg != BPF_REG_0)) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] uses reserved fields\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* check whether implicit source operand (register R6) is readable */\n\terr = check_reg_arg(env, BPF_REG_6, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\t/* Disallow usage of BPF_LD_[ABS|IND] with reference tracking, as\n\t * gen_ld_abs() may terminate the program at runtime, leading to\n\t * reference leak.\n\t */\n\terr = check_reference_leak(env);\n\tif (err) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] cannot be mixed with socket references\\n\");\n\t\treturn err;\n\t}\n\n\tif (regs[BPF_REG_6].type != PTR_TO_CTX) {\n\t\tverbose(env,\n\t\t\t\"at the time of BPF_LD_ABS|IND R6 != pointer to skb\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (mode == BPF_IND) {\n\t\t/* check explicit source operand */\n\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\t/* reset caller saved regs to unreadable */\n\tfor (i = 0; i < CALLER_SAVED_REGS; i++) {\n\t\tmark_reg_not_init(env, regs, caller_saved[i]);\n\t\tcheck_reg_arg(env, caller_saved[i], DST_OP_NO_MARK);\n\t}\n\n\t/* mark destination R0 register as readable, since it contains\n\t * the value fetched from the packet.\n\t * Already marked as written above.\n\t */\n\tmark_reg_unknown(env, regs, BPF_REG_0);\n\treturn 0;\n}\n\nstatic int check_return_code(struct bpf_verifier_env *env)\n{\n\tstruct bpf_reg_state *reg;\n\tstruct tnum range = tnum_range(0, 1);\n\n\tswitch (env->prog->type) {\n\tcase BPF_PROG_TYPE_CGROUP_SKB:\n\tcase BPF_PROG_TYPE_CGROUP_SOCK:\n\tcase BPF_PROG_TYPE_CGROUP_SOCK_ADDR:\n\tcase BPF_PROG_TYPE_SOCK_OPS:\n\tcase BPF_PROG_TYPE_CGROUP_DEVICE:\n\t\tbreak;\n\tdefault:\n\t\treturn 0;\n\t}\n\n\treg = cur_regs(env) + BPF_REG_0;\n\tif (reg->type != SCALAR_VALUE) {\n\t\tverbose(env, \"At program exit the register R0 is not a known value (%s)\\n\",\n\t\t\treg_type_str[reg->type]);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!tnum_in(range, reg->var_off)) {\n\t\tverbose(env, \"At program exit the register R0 \");\n\t\tif (!tnum_is_unknown(reg->var_off)) {\n\t\t\tchar tn_buf[48];\n\n\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\tverbose(env, \"has value %s\", tn_buf);\n\t\t} else {\n\t\t\tverbose(env, \"has unknown scalar value\");\n\t\t}\n\t\tverbose(env, \" should have been 0 or 1\\n\");\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\n/* non-recursive DFS pseudo code\n * 1  procedure DFS-iterative(G,v):\n * 2      label v as discovered\n * 3      let S be a stack\n * 4      S.push(v)\n * 5      while S is not empty\n * 6            t <- S.pop()\n * 7            if t is what we're looking for:\n * 8                return t\n * 9            for all edges e in G.adjacentEdges(t) do\n * 10               if edge e is already labelled\n * 11                   continue with the next edge\n * 12               w <- G.adjacentVertex(t,e)\n * 13               if vertex w is not discovered and not explored\n * 14                   label e as tree-edge\n * 15                   label w as discovered\n * 16                   S.push(w)\n * 17                   continue at 5\n * 18               else if vertex w is discovered\n * 19                   label e as back-edge\n * 20               else\n * 21                   // vertex w is explored\n * 22                   label e as forward- or cross-edge\n * 23           label t as explored\n * 24           S.pop()\n *\n * convention:\n * 0x10 - discovered\n * 0x11 - discovered and fall-through edge labelled\n * 0x12 - discovered and fall-through and branch edges labelled\n * 0x20 - explored\n */\n\nenum {\n\tDISCOVERED = 0x10,\n\tEXPLORED = 0x20,\n\tFALLTHROUGH = 1,\n\tBRANCH = 2,\n};\n\n#define STATE_LIST_MARK ((struct bpf_verifier_state_list *) -1L)\n\nstatic int *insn_stack;\t/* stack of insns to process */\nstatic int cur_stack;\t/* current stack index */\nstatic int *insn_state;\n\n/* t, w, e - match pseudo-code above:\n * t - index of current instruction\n * w - next instruction\n * e - edge\n */\nstatic int push_insn(int t, int w, int e, struct bpf_verifier_env *env)\n{\n\tif (e == FALLTHROUGH && insn_state[t] >= (DISCOVERED | FALLTHROUGH))\n\t\treturn 0;\n\n\tif (e == BRANCH && insn_state[t] >= (DISCOVERED | BRANCH))\n\t\treturn 0;\n\n\tif (w < 0 || w >= env->prog->len) {\n\t\tverbose_linfo(env, t, \"%d: \", t);\n\t\tverbose(env, \"jump out of range from insn %d to %d\\n\", t, w);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e == BRANCH)\n\t\t/* mark branch target for state pruning */\n\t\tenv->explored_states[w] = STATE_LIST_MARK;\n\n\tif (insn_state[w] == 0) {\n\t\t/* tree-edge */\n\t\tinsn_state[t] = DISCOVERED | e;\n\t\tinsn_state[w] = DISCOVERED;\n\t\tif (cur_stack >= env->prog->len)\n\t\t\treturn -E2BIG;\n\t\tinsn_stack[cur_stack++] = w;\n\t\treturn 1;\n\t} else if ((insn_state[w] & 0xF0) == DISCOVERED) {\n\t\tverbose_linfo(env, t, \"%d: \", t);\n\t\tverbose_linfo(env, w, \"%d: \", w);\n\t\tverbose(env, \"back-edge from insn %d to %d\\n\", t, w);\n\t\treturn -EINVAL;\n\t} else if (insn_state[w] == EXPLORED) {\n\t\t/* forward- or cross-edge */\n\t\tinsn_state[t] = DISCOVERED | e;\n\t} else {\n\t\tverbose(env, \"insn state internal bug\\n\");\n\t\treturn -EFAULT;\n\t}\n\treturn 0;\n}\n\n/* non-recursive depth-first-search to detect loops in BPF program\n * loop == back-edge in directed graph\n */\nstatic int check_cfg(struct bpf_verifier_env *env)\n{\n\tstruct bpf_insn *insns = env->prog->insnsi;\n\tint insn_cnt = env->prog->len;\n\tint ret = 0;\n\tint i, t;\n\n\tinsn_state = kcalloc(insn_cnt, sizeof(int), GFP_KERNEL);\n\tif (!insn_state)\n\t\treturn -ENOMEM;\n\n\tinsn_stack = kcalloc(insn_cnt, sizeof(int), GFP_KERNEL);\n\tif (!insn_stack) {\n\t\tkfree(insn_state);\n\t\treturn -ENOMEM;\n\t}\n\n\tinsn_state[0] = DISCOVERED; /* mark 1st insn as discovered */\n\tinsn_stack[0] = 0; /* 0 is the first instruction */\n\tcur_stack = 1;\n\npeek_stack:\n\tif (cur_stack == 0)\n\t\tgoto check_state;\n\tt = insn_stack[cur_stack - 1];\n\n\tif (BPF_CLASS(insns[t].code) == BPF_JMP) {\n\t\tu8 opcode = BPF_OP(insns[t].code);\n\n\t\tif (opcode == BPF_EXIT) {\n\t\t\tgoto mark_explored;\n\t\t} else if (opcode == BPF_CALL) {\n\t\t\tret = push_insn(t, t + 1, FALLTHROUGH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t\tif (t + 1 < insn_cnt)\n\t\t\t\tenv->explored_states[t + 1] = STATE_LIST_MARK;\n\t\t\tif (insns[t].src_reg == BPF_PSEUDO_CALL) {\n\t\t\t\tenv->explored_states[t] = STATE_LIST_MARK;\n\t\t\t\tret = push_insn(t, t + insns[t].imm + 1, BRANCH, env);\n\t\t\t\tif (ret == 1)\n\t\t\t\t\tgoto peek_stack;\n\t\t\t\telse if (ret < 0)\n\t\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t} else if (opcode == BPF_JA) {\n\t\t\tif (BPF_SRC(insns[t].code) != BPF_K) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t\t/* unconditional jump with single edge */\n\t\t\tret = push_insn(t, t + insns[t].off + 1,\n\t\t\t\t\tFALLTHROUGH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t\t/* tell verifier to check for equivalent states\n\t\t\t * after every call and jump\n\t\t\t */\n\t\t\tif (t + 1 < insn_cnt)\n\t\t\t\tenv->explored_states[t + 1] = STATE_LIST_MARK;\n\t\t} else {\n\t\t\t/* conditional jump with two edges */\n\t\t\tenv->explored_states[t] = STATE_LIST_MARK;\n\t\t\tret = push_insn(t, t + 1, FALLTHROUGH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\n\t\t\tret = push_insn(t, t + insns[t].off + 1, BRANCH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t}\n\t} else {\n\t\t/* all other non-branch instructions with single\n\t\t * fall-through edge\n\t\t */\n\t\tret = push_insn(t, t + 1, FALLTHROUGH, env);\n\t\tif (ret == 1)\n\t\t\tgoto peek_stack;\n\t\telse if (ret < 0)\n\t\t\tgoto err_free;\n\t}\n\nmark_explored:\n\tinsn_state[t] = EXPLORED;\n\tif (cur_stack-- <= 0) {\n\t\tverbose(env, \"pop stack internal bug\\n\");\n\t\tret = -EFAULT;\n\t\tgoto err_free;\n\t}\n\tgoto peek_stack;\n\ncheck_state:\n\tfor (i = 0; i < insn_cnt; i++) {\n\t\tif (insn_state[i] != EXPLORED) {\n\t\t\tverbose(env, \"unreachable insn %d\\n\", i);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\t}\n\tret = 0; /* cfg looks good */\n\nerr_free:\n\tkfree(insn_state);\n\tkfree(insn_stack);\n\treturn ret;\n}\n\n/* The minimum supported BTF func info size */\n#define MIN_BPF_FUNCINFO_SIZE\t8\n#define MAX_FUNCINFO_REC_SIZE\t252\n\nstatic int check_btf_func(struct bpf_verifier_env *env,\n\t\t\t  const union bpf_attr *attr,\n\t\t\t  union bpf_attr __user *uattr)\n{\n\tu32 i, nfuncs, urec_size, min_size, prev_offset;\n\tu32 krec_size = sizeof(struct bpf_func_info);\n\tstruct bpf_func_info *krecord;\n\tconst struct btf_type *type;\n\tstruct bpf_prog *prog;\n\tconst struct btf *btf;\n\tvoid __user *urecord;\n\tint ret = 0;\n\n\tnfuncs = attr->func_info_cnt;\n\tif (!nfuncs)\n\t\treturn 0;\n\n\tif (nfuncs != env->subprog_cnt) {\n\t\tverbose(env, \"number of funcs in func_info doesn't match number of subprogs\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\turec_size = attr->func_info_rec_size;\n\tif (urec_size < MIN_BPF_FUNCINFO_SIZE ||\n\t    urec_size > MAX_FUNCINFO_REC_SIZE ||\n\t    urec_size % sizeof(u32)) {\n\t\tverbose(env, \"invalid func info rec size %u\\n\", urec_size);\n\t\treturn -EINVAL;\n\t}\n\n\tprog = env->prog;\n\tbtf = prog->aux->btf;\n\n\turecord = u64_to_user_ptr(attr->func_info);\n\tmin_size = min_t(u32, krec_size, urec_size);\n\n\tkrecord = kvcalloc(nfuncs, krec_size, GFP_KERNEL | __GFP_NOWARN);\n\tif (!krecord)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < nfuncs; i++) {\n\t\tret = bpf_check_uarg_tail_zero(urecord, krec_size, urec_size);\n\t\tif (ret) {\n\t\t\tif (ret == -E2BIG) {\n\t\t\t\tverbose(env, \"nonzero tailing record in func info\");\n\t\t\t\t/* set the size kernel expects so loader can zero\n\t\t\t\t * out the rest of the record.\n\t\t\t\t */\n\t\t\t\tif (put_user(min_size, &uattr->func_info_rec_size))\n\t\t\t\t\tret = -EFAULT;\n\t\t\t}\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (copy_from_user(&krecord[i], urecord, min_size)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/* check insn_off */\n\t\tif (i == 0) {\n\t\t\tif (krecord[i].insn_off) {\n\t\t\t\tverbose(env,\n\t\t\t\t\t\"nonzero insn_off %u for the first func info record\",\n\t\t\t\t\tkrecord[i].insn_off);\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t} else if (krecord[i].insn_off <= prev_offset) {\n\t\t\tverbose(env,\n\t\t\t\t\"same or smaller insn offset (%u) than previous func info record (%u)\",\n\t\t\t\tkrecord[i].insn_off, prev_offset);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (env->subprog_info[i].start != krecord[i].insn_off) {\n\t\t\tverbose(env, \"func_info BTF section doesn't match subprog layout in BPF program\\n\");\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/* check type_id */\n\t\ttype = btf_type_by_id(btf, krecord[i].type_id);\n\t\tif (!type || BTF_INFO_KIND(type->info) != BTF_KIND_FUNC) {\n\t\t\tverbose(env, \"invalid type id %d in func info\",\n\t\t\t\tkrecord[i].type_id);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tprev_offset = krecord[i].insn_off;\n\t\turecord += urec_size;\n\t}\n\n\tprog->aux->func_info = krecord;\n\tprog->aux->func_info_cnt = nfuncs;\n\treturn 0;\n\nerr_free:\n\tkvfree(krecord);\n\treturn ret;\n}\n\nstatic void adjust_btf_func(struct bpf_verifier_env *env)\n{\n\tint i;\n\n\tif (!env->prog->aux->func_info)\n\t\treturn;\n\n\tfor (i = 0; i < env->subprog_cnt; i++)\n\t\tenv->prog->aux->func_info[i].insn_off = env->subprog_info[i].start;\n}\n\n#define MIN_BPF_LINEINFO_SIZE\t(offsetof(struct bpf_line_info, line_col) + \\\n\t\tsizeof(((struct bpf_line_info *)(0))->line_col))\n#define MAX_LINEINFO_REC_SIZE\tMAX_FUNCINFO_REC_SIZE\n\nstatic int check_btf_line(struct bpf_verifier_env *env,\n\t\t\t  const union bpf_attr *attr,\n\t\t\t  union bpf_attr __user *uattr)\n{\n\tu32 i, s, nr_linfo, ncopy, expected_size, rec_size, prev_offset = 0;\n\tstruct bpf_subprog_info *sub;\n\tstruct bpf_line_info *linfo;\n\tstruct bpf_prog *prog;\n\tconst struct btf *btf;\n\tvoid __user *ulinfo;\n\tint err;\n\n\tnr_linfo = attr->line_info_cnt;\n\tif (!nr_linfo)\n\t\treturn 0;\n\n\trec_size = attr->line_info_rec_size;\n\tif (rec_size < MIN_BPF_LINEINFO_SIZE ||\n\t    rec_size > MAX_LINEINFO_REC_SIZE ||\n\t    rec_size & (sizeof(u32) - 1))\n\t\treturn -EINVAL;\n\n\t/* Need to zero it in case the userspace may\n\t * pass in a smaller bpf_line_info object.\n\t */\n\tlinfo = kvcalloc(nr_linfo, sizeof(struct bpf_line_info),\n\t\t\t GFP_KERNEL | __GFP_NOWARN);\n\tif (!linfo)\n\t\treturn -ENOMEM;\n\n\tprog = env->prog;\n\tbtf = prog->aux->btf;\n\n\ts = 0;\n\tsub = env->subprog_info;\n\tulinfo = u64_to_user_ptr(attr->line_info);\n\texpected_size = sizeof(struct bpf_line_info);\n\tncopy = min_t(u32, expected_size, rec_size);\n\tfor (i = 0; i < nr_linfo; i++) {\n\t\terr = bpf_check_uarg_tail_zero(ulinfo, expected_size, rec_size);\n\t\tif (err) {\n\t\t\tif (err == -E2BIG) {\n\t\t\t\tverbose(env, \"nonzero tailing record in line_info\");\n\t\t\t\tif (put_user(expected_size,\n\t\t\t\t\t     &uattr->line_info_rec_size))\n\t\t\t\t\terr = -EFAULT;\n\t\t\t}\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (copy_from_user(&linfo[i], ulinfo, ncopy)) {\n\t\t\terr = -EFAULT;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/*\n\t\t * Check insn_off to ensure\n\t\t * 1) strictly increasing AND\n\t\t * 2) bounded by prog->len\n\t\t *\n\t\t * The linfo[0].insn_off == 0 check logically falls into\n\t\t * the later \"missing bpf_line_info for func...\" case\n\t\t * because the first linfo[0].insn_off must be the\n\t\t * first sub also and the first sub must have\n\t\t * subprog_info[0].start == 0.\n\t\t */\n\t\tif ((i && linfo[i].insn_off <= prev_offset) ||\n\t\t    linfo[i].insn_off >= prog->len) {\n\t\t\tverbose(env, \"Invalid line_info[%u].insn_off:%u (prev_offset:%u prog->len:%u)\\n\",\n\t\t\t\ti, linfo[i].insn_off, prev_offset,\n\t\t\t\tprog->len);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (!prog->insnsi[linfo[i].insn_off].code) {\n\t\t\tverbose(env,\n\t\t\t\t\"Invalid insn code at line_info[%u].insn_off\\n\",\n\t\t\t\ti);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (!btf_name_by_offset(btf, linfo[i].line_off) ||\n\t\t    !btf_name_by_offset(btf, linfo[i].file_name_off)) {\n\t\t\tverbose(env, \"Invalid line_info[%u].line_off or .file_name_off\\n\", i);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (s != env->subprog_cnt) {\n\t\t\tif (linfo[i].insn_off == sub[s].start) {\n\t\t\t\tsub[s].linfo_idx = i;\n\t\t\t\ts++;\n\t\t\t} else if (sub[s].start < linfo[i].insn_off) {\n\t\t\t\tverbose(env, \"missing bpf_line_info for func#%u\\n\", s);\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t}\n\n\t\tprev_offset = linfo[i].insn_off;\n\t\tulinfo += rec_size;\n\t}\n\n\tif (s != env->subprog_cnt) {\n\t\tverbose(env, \"missing bpf_line_info for %u funcs starting from func#%u\\n\",\n\t\t\tenv->subprog_cnt - s, s);\n\t\terr = -EINVAL;\n\t\tgoto err_free;\n\t}\n\n\tprog->aux->linfo = linfo;\n\tprog->aux->nr_linfo = nr_linfo;\n\n\treturn 0;\n\nerr_free:\n\tkvfree(linfo);\n\treturn err;\n}\n\nstatic int check_btf_info(struct bpf_verifier_env *env,\n\t\t\t  const union bpf_attr *attr,\n\t\t\t  union bpf_attr __user *uattr)\n{\n\tstruct btf *btf;\n\tint err;\n\n\tif (!attr->func_info_cnt && !attr->line_info_cnt)\n\t\treturn 0;\n\n\tbtf = btf_get_by_fd(attr->prog_btf_fd);\n\tif (IS_ERR(btf))\n\t\treturn PTR_ERR(btf);\n\tenv->prog->aux->btf = btf;\n\n\terr = check_btf_func(env, attr, uattr);\n\tif (err)\n\t\treturn err;\n\n\terr = check_btf_line(env, attr, uattr);\n\tif (err)\n\t\treturn err;\n\n\treturn 0;\n}\n\n/* check %cur's range satisfies %old's */\nstatic bool range_within(struct bpf_reg_state *old,\n\t\t\t struct bpf_reg_state *cur)\n{\n\treturn old->umin_value <= cur->umin_value &&\n\t       old->umax_value >= cur->umax_value &&\n\t       old->smin_value <= cur->smin_value &&\n\t       old->smax_value >= cur->smax_value;\n}\n\n/* Maximum number of register states that can exist at once */\n#define ID_MAP_SIZE\t(MAX_BPF_REG + MAX_BPF_STACK / BPF_REG_SIZE)\nstruct idpair {\n\tu32 old;\n\tu32 cur;\n};\n\n/* If in the old state two registers had the same id, then they need to have\n * the same id in the new state as well.  But that id could be different from\n * the old state, so we need to track the mapping from old to new ids.\n * Once we have seen that, say, a reg with old id 5 had new id 9, any subsequent\n * regs with old id 5 must also have new id 9 for the new state to be safe.  But\n * regs with a different old id could still have new id 9, we don't care about\n * that.\n * So we look through our idmap to see if this old id has been seen before.  If\n * so, we require the new id to match; otherwise, we add the id pair to the map.\n */\nstatic bool check_ids(u32 old_id, u32 cur_id, struct idpair *idmap)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < ID_MAP_SIZE; i++) {\n\t\tif (!idmap[i].old) {\n\t\t\t/* Reached an empty slot; haven't seen this id before */\n\t\t\tidmap[i].old = old_id;\n\t\t\tidmap[i].cur = cur_id;\n\t\t\treturn true;\n\t\t}\n\t\tif (idmap[i].old == old_id)\n\t\t\treturn idmap[i].cur == cur_id;\n\t}\n\t/* We ran out of idmap slots, which should be impossible */\n\tWARN_ON_ONCE(1);\n\treturn false;\n}"
    },
    {
        "patch_model": "Replace",
        "change_type": "Assignment",
        "line_old": 915,
        "line_new": 915,
        "critical_vars": [
            "hwc->event_base"
        ],
        "function": "x86_assign_hw_event",
        "filename": "linux/CVE-2011-2521/CVE-2011-2521_CWE-189_fc66c5210ec2539e800e87d7b3a985323c7be96e_perf_event.c.diff",
        "label": "True",
        "function_code": "\nstatic inline void x86_assign_hw_event(struct perf_event *event,\n\t\t\t\tstruct cpu_hw_events *cpuc, int i)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\thwc->idx = cpuc->assign[i];\n\thwc->last_cpu = smp_processor_id();\n\thwc->last_tag = ++cpuc->tags[i];\n\n\tif (hwc->idx == X86_PMC_IDX_FIXED_BTS) {\n\t\thwc->config_base = 0;\n\t\thwc->event_base\t= 0;\n\t} else if (hwc->idx >= X86_PMC_IDX_FIXED) {\n\t\thwc->config_base = MSR_ARCH_PERFMON_FIXED_CTR_CTRL;\n\t\thwc->event_base = MSR_ARCH_PERFMON_FIXED_CTR0 + (hwc->idx - X86_PMC_IDX_FIXED);\n\t} else {\n\t\thwc->config_base = x86_pmu_config_addr(hwc->idx);\n\t\thwc->event_base  = x86_pmu_event_addr(hwc->idx);\n\t}\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "if-Condition",
        "line_old": 93,
        "critical_vars": [
            "skb"
        ],
        "function": "*udp6_ufo_fragment",
        "filename": "linux/CVE-2013-4563/CVE-2013-4563_CWE-189_0e033e04c2678dbbe74a46b23fffb7bb918c288e_udp_offload.c.diff",
        "label": "False",
        "function_code": "\nstatic struct sk_buff *udp6_ufo_fragment(struct sk_buff *skb,\n\t\t\t\t\t netdev_features_t features)\n{\n\tstruct sk_buff *segs = ERR_PTR(-EINVAL);\n\tunsigned int mss;\n\tunsigned int unfrag_ip6hlen, unfrag_len;\n\tstruct frag_hdr *fptr;\n\tu8 *packet_start, *prevhdr;\n\tu8 nexthdr;\n\tu8 frag_hdr_sz = sizeof(struct frag_hdr);\n\tint offset;\n\t__wsum csum;\n\tint tnl_hlen;\n\n\tmss = skb_shinfo(skb)->gso_size;\n\tif (unlikely(skb->len <= mss))\n\t\tgoto out;\n\n\tif (skb_gso_ok(skb, features | NETIF_F_GSO_ROBUST)) {\n\t\t/* Packet is from an untrusted source, reset gso_segs. */\n\t\tint type = skb_shinfo(skb)->gso_type;\n\n\t\tif (unlikely(type & ~(SKB_GSO_UDP |\n\t\t\t\t      SKB_GSO_DODGY |\n\t\t\t\t      SKB_GSO_UDP_TUNNEL |\n\t\t\t\t      SKB_GSO_GRE |\n\t\t\t\t      SKB_GSO_IPIP |\n\t\t\t\t      SKB_GSO_SIT |\n\t\t\t\t      SKB_GSO_MPLS) ||\n\t\t\t     !(type & (SKB_GSO_UDP))))\n\t\t\tgoto out;\n\n\t\tskb_shinfo(skb)->gso_segs = DIV_ROUND_UP(skb->len, mss);\n\n\t\tsegs = NULL;\n\t\tgoto out;\n\t}\n\n\tif (skb->encapsulation && skb_shinfo(skb)->gso_type & SKB_GSO_UDP_TUNNEL)\n\t\tsegs = skb_udp_tunnel_segment(skb, features);\n\telse {\n\t\t/* Do software UFO. Complete and fill in the UDP checksum as HW cannot\n\t\t * do checksum of UDP packets sent as multiple IP fragments.\n\t\t */\n\t\toffset = skb_checksum_start_offset(skb);\n\t\tcsum = skb_checksum(skb, offset, skb->len - offset, 0);\n\t\toffset += skb->csum_offset;\n\t\t*(__sum16 *)(skb->data + offset) = csum_fold(csum);\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\n\t\t/* Check if there is enough headroom to insert fragment header. */\n\t\ttnl_hlen = skb_tnl_header_len(skb);\n\t\tif (skb_headroom(skb) < (tnl_hlen + frag_hdr_sz)) {\n\t\t\tif (gso_pskb_expand_head(skb, tnl_hlen + frag_hdr_sz))\n\t\t\t\tgoto out;\n\t\t}\n\n\t\t/* Find the unfragmentable header and shift it left by frag_hdr_sz\n\t\t * bytes to insert fragment header.\n\t\t */\n\t\tunfrag_ip6hlen = ip6_find_1stfragopt(skb, &prevhdr);\n\t\tnexthdr = *prevhdr;\n\t\t*prevhdr = NEXTHDR_FRAGMENT;\n\t\tunfrag_len = (skb_network_header(skb) - skb_mac_header(skb)) +\n\t\t\t     unfrag_ip6hlen + tnl_hlen;\n\t\tpacket_start = (u8 *) skb->head + SKB_GSO_CB(skb)->mac_offset;\n\t\tmemmove(packet_start-frag_hdr_sz, packet_start, unfrag_len);\n\n\t\tSKB_GSO_CB(skb)->mac_offset -= frag_hdr_sz;\n\t\tskb->mac_header -= frag_hdr_sz;\n\t\tskb->network_header -= frag_hdr_sz;\n\n\t\tfptr = (struct frag_hdr *)(skb_network_header(skb) + unfrag_ip6hlen);\n\t\tfptr->nexthdr = nexthdr;\n\t\tfptr->reserved = 0;\n\t\tipv6_select_ident(fptr, (struct rt6_info *)skb_dst(skb));\n\n\t\t/* Fragment the skb. ipv6 header and the remaining fields of the\n\t\t * fragment header are updated in ipv6_gso_segment()\n\t\t */\n\t\tsegs = skb_segment(skb, features);\n\t}\n\nout:\n\treturn segs;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 93,
        "critical_vars": [
            "skb->mac_header"
        ],
        "function": "*udp6_ufo_fragment",
        "filename": "linux/CVE-2013-4563/CVE-2013-4563_CWE-189_0e033e04c2678dbbe74a46b23fffb7bb918c288e_udp_offload.c.diff",
        "label": "True",
        "function_code": "\nstatic struct sk_buff *udp6_ufo_fragment(struct sk_buff *skb,\n\t\t\t\t\t netdev_features_t features)\n{\n\tstruct sk_buff *segs = ERR_PTR(-EINVAL);\n\tunsigned int mss;\n\tunsigned int unfrag_ip6hlen, unfrag_len;\n\tstruct frag_hdr *fptr;\n\tu8 *packet_start, *prevhdr;\n\tu8 nexthdr;\n\tu8 frag_hdr_sz = sizeof(struct frag_hdr);\n\tint offset;\n\t__wsum csum;\n\tint tnl_hlen;\n\n\tmss = skb_shinfo(skb)->gso_size;\n\tif (unlikely(skb->len <= mss))\n\t\tgoto out;\n\n\tif (skb_gso_ok(skb, features | NETIF_F_GSO_ROBUST)) {\n\t\t/* Packet is from an untrusted source, reset gso_segs. */\n\t\tint type = skb_shinfo(skb)->gso_type;\n\n\t\tif (unlikely(type & ~(SKB_GSO_UDP |\n\t\t\t\t      SKB_GSO_DODGY |\n\t\t\t\t      SKB_GSO_UDP_TUNNEL |\n\t\t\t\t      SKB_GSO_GRE |\n\t\t\t\t      SKB_GSO_IPIP |\n\t\t\t\t      SKB_GSO_SIT |\n\t\t\t\t      SKB_GSO_MPLS) ||\n\t\t\t     !(type & (SKB_GSO_UDP))))\n\t\t\tgoto out;\n\n\t\tskb_shinfo(skb)->gso_segs = DIV_ROUND_UP(skb->len, mss);\n\n\t\tsegs = NULL;\n\t\tgoto out;\n\t}\n\n\tif (skb->encapsulation && skb_shinfo(skb)->gso_type & SKB_GSO_UDP_TUNNEL)\n\t\tsegs = skb_udp_tunnel_segment(skb, features);\n\telse {\n\t\t/* Do software UFO. Complete and fill in the UDP checksum as HW cannot\n\t\t * do checksum of UDP packets sent as multiple IP fragments.\n\t\t */\n\t\toffset = skb_checksum_start_offset(skb);\n\t\tcsum = skb_checksum(skb, offset, skb->len - offset, 0);\n\t\toffset += skb->csum_offset;\n\t\t*(__sum16 *)(skb->data + offset) = csum_fold(csum);\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\n\t\t/* Check if there is enough headroom to insert fragment header. */\n\t\ttnl_hlen = skb_tnl_header_len(skb);\n\t\tif (skb->mac_header < (tnl_hlen + frag_hdr_sz)) {\n\t\t\tif (gso_pskb_expand_head(skb, tnl_hlen + frag_hdr_sz))\n\t\t\t\tgoto out;\n\t\t}\n\n\t\t/* Find the unfragmentable header and shift it left by frag_hdr_sz\n\t\t * bytes to insert fragment header.\n\t\t */\n\t\tunfrag_ip6hlen = ip6_find_1stfragopt(skb, &prevhdr);\n\t\tnexthdr = *prevhdr;\n\t\t*prevhdr = NEXTHDR_FRAGMENT;\n\t\tunfrag_len = (skb_network_header(skb) - skb_mac_header(skb)) +\n\t\t\t     unfrag_ip6hlen + tnl_hlen;\n\t\tpacket_start = (u8 *) skb->head + SKB_GSO_CB(skb)->mac_offset;\n\t\tmemmove(packet_start-frag_hdr_sz, packet_start, unfrag_len);\n\n\t\tSKB_GSO_CB(skb)->mac_offset -= frag_hdr_sz;\n\t\tskb->mac_header -= frag_hdr_sz;\n\t\tskb->network_header -= frag_hdr_sz;\n\n\t\tfptr = (struct frag_hdr *)(skb_network_header(skb) + unfrag_ip6hlen);\n\t\tfptr->nexthdr = nexthdr;\n\t\tfptr->reserved = 0;\n\t\tipv6_select_ident(fptr, (struct rt6_info *)skb_dst(skb));\n\n\t\t/* Fragment the skb. ipv6 header and the remaining fields of the\n\t\t * fragment header are updated in ipv6_gso_segment()\n\t\t */\n\t\tsegs = skb_segment(skb, features);\n\t}\n\nout:\n\treturn segs;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 2481,
        "critical_vars": [
            "newly_acked_sacked",
            "tp->prior_cwnd"
        ],
        "function": "tcp_cwnd_reduction",
        "filename": "linux/CVE-2016-2070/CVE-2016-2070_CWE-189_8b8a321ff72c785ed5e8b4cf6eda20b35d427390_tcp_input.c.diff",
        "label": "True",
        "function_code": "\nstatic void tcp_cwnd_reduction(struct sock *sk, const int prior_unsacked,\n\t\t\t       int fast_rexmit, int flag)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint sndcnt = 0;\n\tint delta = tp->snd_ssthresh - tcp_packets_in_flight(tp);\n\tint newly_acked_sacked = prior_unsacked -\n\t\t\t\t (tp->packets_out - tp->sacked_out);\n\n\tif (newly_acked_sacked <= 0 || WARN_ON_ONCE(!tp->prior_cwnd))\n\t\treturn;\n\n\ttp->prr_delivered += newly_acked_sacked;\n\tif (delta < 0) {\n\t\tu64 dividend = (u64)tp->snd_ssthresh * tp->prr_delivered +\n\t\t\t       tp->prior_cwnd - 1;\n\t\tsndcnt = div_u64(dividend, tp->prior_cwnd) - tp->prr_out;\n\t} else if ((flag & FLAG_RETRANS_DATA_ACKED) &&\n\t\t   !(flag & FLAG_LOST_RETRANS)) {\n\t\tsndcnt = min_t(int, delta,\n\t\t\t       max_t(int, tp->prr_delivered - tp->prr_out,\n\t\t\t\t     newly_acked_sacked) + 1);\n\t} else {\n\t\tsndcnt = min(delta, newly_acked_sacked);\n\t}\n\tsndcnt = max(sndcnt, (fast_rexmit ? 1 : 0));\n\ttp->snd_cwnd = tcp_packets_in_flight(tp) + sndcnt;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "if-Condition",
        "line_old": 844,
        "critical_vars": [
            "apic"
        ],
        "function": "apic_get_tmcct",
        "filename": "linux/CVE-2013-6367/CVE-2013-6367_CWE-189_b963a22e6d1a266a67e9eecc88134713fd54775c_lapic.c.diff",
        "label": "False",
        "function_code": "\nstatic u32 apic_get_tmcct(struct kvm_lapic *apic)\n{\n\tktime_t remaining;\n\ts64 ns;\n\tu32 tmcct;\n\n\tASSERT(apic != NULL);\n\n\t/* if initial count is 0, current count should also be 0 */\n\tif (kvm_apic_get_reg(apic, APIC_TMICT) == 0)\n\t\treturn 0;\n\n\tremaining = hrtimer_get_remaining(&apic->lapic_timer.timer);\n\tif (ktime_to_ns(remaining) < 0)\n\t\tremaining = ktime_set(0, 0);\n\n\tns = mod_64(ktime_to_ns(remaining), apic->lapic_timer.period);\n\ttmcct = div64_u64(ns,\n\t\t\t (APIC_BUS_CYCLE_NS * apic->divide_count));\n\n\treturn tmcct;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 844,
        "critical_vars": [
            "apic->lapic_timer.period",
            "apic"
        ],
        "function": "apic_get_tmcct",
        "filename": "linux/CVE-2013-6367/CVE-2013-6367_CWE-189_b963a22e6d1a266a67e9eecc88134713fd54775c_lapic.c.diff",
        "label": "True",
        "function_code": "\nstatic u32 apic_get_tmcct(struct kvm_lapic *apic)\n{\n\tktime_t remaining;\n\ts64 ns;\n\tu32 tmcct;\n\n\tASSERT(apic != NULL);\n\n\t/* if initial count is 0, current count should also be 0 */\n\tif (kvm_apic_get_reg(apic, APIC_TMICT) == 0 ||\n\t\tapic->lapic_timer.period == 0)\n\t\treturn 0;\n\n\tremaining = hrtimer_get_remaining(&apic->lapic_timer.timer);\n\tif (ktime_to_ns(remaining) < 0)\n\t\tremaining = ktime_set(0, 0);\n\n\tns = mod_64(ktime_to_ns(remaining), apic->lapic_timer.period);\n\ttmcct = div64_u64(ns,\n\t\t\t (APIC_BUS_CYCLE_NS * apic->divide_count));\n\n\treturn tmcct;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 749,
        "critical_vars": [
            "u16"
        ],
        "function": "oz_hcd_pd_reset",
        "filename": "linux/CVE-2015-4001/CVE-2015-4001_CWE-189_b1bb5b49373b61bf9d2c73a4d30058ba6f069e4c_ozhcd.c.diff",
        "label": "True",
        "function_code": "void oz_hcd_pd_reset(void *hpd, void *hport)\n{\n\t/* Cleanup the current configuration and report reset to the core.\n\t */\n\tstruct oz_port *port = hport;\n\tstruct oz_hcd *ozhcd = port->ozhcd;\n\n\toz_dbg(ON, \"PD Reset\\n\");\n\tspin_lock_bh(&port->port_lock);\n\tport->flags |= OZ_PORT_F_CHANGED;\n\tport->status |= USB_PORT_STAT_RESET;\n\tport->status |= (USB_PORT_STAT_C_RESET << 16);\n\tspin_unlock_bh(&port->port_lock);\n\toz_clean_endpoints_for_config(ozhcd->hcd, port);\n\tusb_hcd_poll_rh_status(ozhcd->hcd);\n}"
    },
    {
        "patch_model": "Replace",
        "change_type": "Var-Declaration",
        "line_old": 762,
        "line_new": 762,
        "critical_vars": [
            "copy_len"
        ],
        "function": "oz_hcd_get_desc_cnf",
        "filename": "linux/CVE-2015-4001/CVE-2015-4001_CWE-189_b1bb5b49373b61bf9d2c73a4d30058ba6f069e4c_ozhcd.c.diff",
        "label": "True",
        "function_code": "void oz_hcd_get_desc_cnf(void *hport, u8 req_id, u8 status, const u8 *desc,\n\t\t\tu8 length, u16 offset, u16 total_size)\n{\n\tstruct oz_port *port = hport;\n\tstruct urb *urb;\n\tint err = 0;\n\n\toz_dbg(ON, \"oz_hcd_get_desc_cnf length = %d offs = %d tot_size = %d\\n\",\n\t       length, offset, total_size);\n\turb = oz_find_urb_by_id(port, 0, req_id);\n\tif (!urb)\n\t\treturn;\n\tif (status == 0) {\n\t\tunsigned int copy_len;\n\t\tunsigned int required_size = urb->transfer_buffer_length;\n\n\t\tif (required_size > total_size)\n\t\t\trequired_size = total_size;\n\t\tcopy_len = required_size-offset;\n\t\tif (length <= copy_len)\n\t\t\tcopy_len = length;\n\t\tmemcpy(urb->transfer_buffer+offset, desc, copy_len);\n\t\toffset += copy_len;\n\t\tif (offset < required_size) {\n\t\t\tstruct usb_ctrlrequest *setup =\n\t\t\t\t(struct usb_ctrlrequest *)urb->setup_packet;\n\t\t\tunsigned wvalue = le16_to_cpu(setup->wValue);\n\n\t\t\tif (oz_enqueue_ep_urb(port, 0, 0, urb, req_id))\n\t\t\t\terr = -ENOMEM;\n\t\t\telse if (oz_usb_get_desc_req(port->hpd, req_id,\n\t\t\t\t\tsetup->bRequestType, (u8)(wvalue>>8),\n\t\t\t\t\t(u8)wvalue, setup->wIndex, offset,\n\t\t\t\t\trequired_size-offset)) {\n\t\t\t\toz_dequeue_ep_urb(port, 0, 0, urb);\n\t\t\t\terr = -ENOMEM;\n\t\t\t}\n\t\t\tif (err == 0)\n\t\t\t\treturn;\n\t\t}\n\t}\n\turb->actual_length = total_size;\n\toz_complete_urb(port->ozhcd->hcd, urb, 0);\n}"
    },
    {
        "patch_model": "Replace",
        "change_type": "Assignment",
        "line_old": 763,
        "line_new": 763,
        "critical_vars": [
            "required_size"
        ],
        "function": "oz_hcd_get_desc_cnf",
        "filename": "linux/CVE-2015-4001/CVE-2015-4001_CWE-189_b1bb5b49373b61bf9d2c73a4d30058ba6f069e4c_ozhcd.c.diff",
        "label": "True",
        "function_code": "void oz_hcd_get_desc_cnf(void *hport, u8 req_id, u8 status, const u8 *desc,\n\t\t\tu8 length, u16 offset, u16 total_size)\n{\n\tstruct oz_port *port = hport;\n\tstruct urb *urb;\n\tint err = 0;\n\n\toz_dbg(ON, \"oz_hcd_get_desc_cnf length = %d offs = %d tot_size = %d\\n\",\n\t       length, offset, total_size);\n\turb = oz_find_urb_by_id(port, 0, req_id);\n\tif (!urb)\n\t\treturn;\n\tif (status == 0) {\n\t\tunsigned int copy_len;\n\t\tunsigned int required_size = urb->transfer_buffer_length;\n\n\t\tif (required_size > total_size)\n\t\t\trequired_size = total_size;\n\t\tcopy_len = required_size-offset;\n\t\tif (length <= copy_len)\n\t\t\tcopy_len = length;\n\t\tmemcpy(urb->transfer_buffer+offset, desc, copy_len);\n\t\toffset += copy_len;\n\t\tif (offset < required_size) {\n\t\t\tstruct usb_ctrlrequest *setup =\n\t\t\t\t(struct usb_ctrlrequest *)urb->setup_packet;\n\t\t\tunsigned wvalue = le16_to_cpu(setup->wValue);\n\n\t\t\tif (oz_enqueue_ep_urb(port, 0, 0, urb, req_id))\n\t\t\t\terr = -ENOMEM;\n\t\t\telse if (oz_usb_get_desc_req(port->hpd, req_id,\n\t\t\t\t\tsetup->bRequestType, (u8)(wvalue>>8),\n\t\t\t\t\t(u8)wvalue, setup->wIndex, offset,\n\t\t\t\t\trequired_size-offset)) {\n\t\t\t\toz_dequeue_ep_urb(port, 0, 0, urb);\n\t\t\t\terr = -ENOMEM;\n\t\t\t}\n\t\t\tif (err == 0)\n\t\t\t\treturn;\n\t\t}\n\t}\n\turb->actual_length = total_size;\n\toz_complete_urb(port->ozhcd->hcd, urb, 0);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 1136,
        "critical_vars": [
            "args->num_cliprects"
        ],
        "function": "i915_gem_do_execbuffer",
        "filename": "linux/CVE-2012-2384/CVE-2012-2384_CWE-189_44afb3a04391a74309d16180d1e4f8386fdfa745_i915_gem_execbuffer.c.diff",
        "label": "True",
        "function_code": "\nstatic int\ni915_gem_do_execbuffer(struct drm_device *dev, void *data,\n\t\t       struct drm_file *file,\n\t\t       struct drm_i915_gem_execbuffer2 *args,\n\t\t       struct drm_i915_gem_exec_object2 *exec)\n{\n\tdrm_i915_private_t *dev_priv = dev->dev_private;\n\tstruct list_head objects;\n\tstruct eb_objects *eb;\n\tstruct drm_i915_gem_object *batch_obj;\n\tstruct drm_clip_rect *cliprects = NULL;\n\tstruct intel_ring_buffer *ring;\n\tu32 exec_start, exec_len;\n\tu32 seqno;\n\tu32 mask;\n\tint ret, mode, i;\n\n\tif (!i915_gem_check_execbuffer(args)) {\n\t\tDRM_DEBUG(\"execbuf with invalid offset/length\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tret = validate_exec_list(exec, args->buffer_count);\n\tif (ret)\n\t\treturn ret;\n\n\tswitch (args->flags & I915_EXEC_RING_MASK) {\n\tcase I915_EXEC_DEFAULT:\n\tcase I915_EXEC_RENDER:\n\t\tring = &dev_priv->ring[RCS];\n\t\tbreak;\n\tcase I915_EXEC_BSD:\n\t\tif (!HAS_BSD(dev)) {\n\t\t\tDRM_DEBUG(\"execbuf with invalid ring (BSD)\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tring = &dev_priv->ring[VCS];\n\t\tbreak;\n\tcase I915_EXEC_BLT:\n\t\tif (!HAS_BLT(dev)) {\n\t\t\tDRM_DEBUG(\"execbuf with invalid ring (BLT)\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tring = &dev_priv->ring[BCS];\n\t\tbreak;\n\tdefault:\n\t\tDRM_DEBUG(\"execbuf with unknown ring: %d\\n\",\n\t\t\t  (int)(args->flags & I915_EXEC_RING_MASK));\n\t\treturn -EINVAL;\n\t}\n\n\tmode = args->flags & I915_EXEC_CONSTANTS_MASK;\n\tmask = I915_EXEC_CONSTANTS_MASK;\n\tswitch (mode) {\n\tcase I915_EXEC_CONSTANTS_REL_GENERAL:\n\tcase I915_EXEC_CONSTANTS_ABSOLUTE:\n\tcase I915_EXEC_CONSTANTS_REL_SURFACE:\n\t\tif (ring == &dev_priv->ring[RCS] &&\n\t\t    mode != dev_priv->relative_constants_mode) {\n\t\t\tif (INTEL_INFO(dev)->gen < 4)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tif (INTEL_INFO(dev)->gen > 5 &&\n\t\t\t    mode == I915_EXEC_CONSTANTS_REL_SURFACE)\n\t\t\t\treturn -EINVAL;\n\n\t\t\t/* The HW changed the meaning on this bit on gen6 */\n\t\t\tif (INTEL_INFO(dev)->gen >= 6)\n\t\t\t\tmask &= ~I915_EXEC_CONSTANTS_REL_SURFACE;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tDRM_DEBUG(\"execbuf with unknown constants: %d\\n\", mode);\n\t\treturn -EINVAL;\n\t}\n\n\tif (args->buffer_count < 1) {\n\t\tDRM_DEBUG(\"execbuf with %d buffers\\n\", args->buffer_count);\n\t\treturn -EINVAL;\n\t}\n\n\tif (args->num_cliprects != 0) {\n\t\tif (ring != &dev_priv->ring[RCS]) {\n\t\t\tDRM_DEBUG(\"clip rectangles are only valid with the render ring\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (args->num_cliprects > UINT_MAX / sizeof(*cliprects)) {\n\t\t\tDRM_DEBUG(\"execbuf with %u cliprects\\n\",\n\t\t\t\t  args->num_cliprects);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tcliprects = kmalloc(args->num_cliprects * sizeof(*cliprects),\n\t\t\t\t    GFP_KERNEL);\n\t\tif (cliprects == NULL) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto pre_mutex_err;\n\t\t}\n\n\t\tif (copy_from_user(cliprects,\n\t\t\t\t     (struct drm_clip_rect __user *)(uintptr_t)\n\t\t\t\t     args->cliprects_ptr,\n\t\t\t\t     sizeof(*cliprects)*args->num_cliprects)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto pre_mutex_err;\n\t\t}\n\t}\n\n\tret = i915_mutex_lock_interruptible(dev);\n\tif (ret)\n\t\tgoto pre_mutex_err;\n\n\tif (dev_priv->mm.suspended) {\n\t\tmutex_unlock(&dev->struct_mutex);\n\t\tret = -EBUSY;\n\t\tgoto pre_mutex_err;\n\t}\n\n\teb = eb_create(args->buffer_count);\n\tif (eb == NULL) {\n\t\tmutex_unlock(&dev->struct_mutex);\n\t\tret = -ENOMEM;\n\t\tgoto pre_mutex_err;\n\t}\n\n\t/* Look up object handles */\n\tINIT_LIST_HEAD(&objects);\n\tfor (i = 0; i < args->buffer_count; i++) {\n\t\tstruct drm_i915_gem_object *obj;\n\n\t\tobj = to_intel_bo(drm_gem_object_lookup(dev, file,\n\t\t\t\t\t\t\texec[i].handle));\n\t\tif (&obj->base == NULL) {\n\t\t\tDRM_DEBUG(\"Invalid object handle %d at index %d\\n\",\n\t\t\t\t   exec[i].handle, i);\n\t\t\t/* prevent error path from reading uninitialized data */\n\t\t\tret = -ENOENT;\n\t\t\tgoto err;\n\t\t}\n\n\t\tif (!list_empty(&obj->exec_list)) {\n\t\t\tDRM_DEBUG(\"Object %p [handle %d, index %d] appears more than once in object list\\n\",\n\t\t\t\t   obj, exec[i].handle, i);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err;\n\t\t}\n\n\t\tlist_add_tail(&obj->exec_list, &objects);\n\t\tobj->exec_handle = exec[i].handle;\n\t\tobj->exec_entry = &exec[i];\n\t\teb_add_object(eb, obj);\n\t}\n\n\t/* take note of the batch buffer before we might reorder the lists */\n\tbatch_obj = list_entry(objects.prev,\n\t\t\t       struct drm_i915_gem_object,\n\t\t\t       exec_list);\n\n\t/* Move the objects en-masse into the GTT, evicting if necessary. */\n\tret = i915_gem_execbuffer_reserve(ring, file, &objects);\n\tif (ret)\n\t\tgoto err;\n\n\t/* The objects are in their final locations, apply the relocations. */\n\tret = i915_gem_execbuffer_relocate(dev, eb, &objects);\n\tif (ret) {\n\t\tif (ret == -EFAULT) {\n\t\t\tret = i915_gem_execbuffer_relocate_slow(dev, file, ring,\n\t\t\t\t\t\t\t\t&objects, eb,\n\t\t\t\t\t\t\t\texec,\n\t\t\t\t\t\t\t\targs->buffer_count);\n\t\t\tBUG_ON(!mutex_is_locked(&dev->struct_mutex));\n\t\t}\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\t/* Set the pending read domains for the batch buffer to COMMAND */\n\tif (batch_obj->base.pending_write_domain) {\n\t\tDRM_DEBUG(\"Attempting to use self-modifying batch buffer\\n\");\n\t\tret = -EINVAL;\n\t\tgoto err;\n\t}\n\tbatch_obj->base.pending_read_domains |= I915_GEM_DOMAIN_COMMAND;\n\n\tret = i915_gem_execbuffer_move_to_gpu(ring, &objects);\n\tif (ret)\n\t\tgoto err;\n\n\tseqno = i915_gem_next_request_seqno(ring);\n\tfor (i = 0; i < ARRAY_SIZE(ring->sync_seqno); i++) {\n\t\tif (seqno < ring->sync_seqno[i]) {\n\t\t\t/* The GPU can not handle its semaphore value wrapping,\n\t\t\t * so every billion or so execbuffers, we need to stall\n\t\t\t * the GPU in order to reset the counters.\n\t\t\t */\n\t\t\tret = i915_gpu_idle(dev, true);\n\t\t\tif (ret)\n\t\t\t\tgoto err;\n\n\t\t\tBUG_ON(ring->sync_seqno[i]);\n\t\t}\n\t}\n\n\tif (ring == &dev_priv->ring[RCS] &&\n\t    mode != dev_priv->relative_constants_mode) {\n\t\tret = intel_ring_begin(ring, 4);\n\t\tif (ret)\n\t\t\t\tgoto err;\n\n\t\tintel_ring_emit(ring, MI_NOOP);\n\t\tintel_ring_emit(ring, MI_LOAD_REGISTER_IMM(1));\n\t\tintel_ring_emit(ring, INSTPM);\n\t\tintel_ring_emit(ring, mask << 16 | mode);\n\t\tintel_ring_advance(ring);\n\n\t\tdev_priv->relative_constants_mode = mode;\n\t}\n\n\tif (args->flags & I915_EXEC_GEN7_SOL_RESET) {\n\t\tret = i915_reset_gen7_sol_offsets(dev, ring);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\ttrace_i915_gem_ring_dispatch(ring, seqno);\n\n\texec_start = batch_obj->gtt_offset + args->batch_start_offset;\n\texec_len = args->batch_len;\n\tif (cliprects) {\n\t\tfor (i = 0; i < args->num_cliprects; i++) {\n\t\t\tret = i915_emit_box(dev, &cliprects[i],\n\t\t\t\t\t    args->DR1, args->DR4);\n\t\t\tif (ret)\n\t\t\t\tgoto err;\n\n\t\t\tret = ring->dispatch_execbuffer(ring,\n\t\t\t\t\t\t\texec_start, exec_len);\n\t\t\tif (ret)\n\t\t\t\tgoto err;\n\t\t}\n\t} else {\n\t\tret = ring->dispatch_execbuffer(ring, exec_start, exec_len);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\ti915_gem_execbuffer_move_to_active(&objects, ring, seqno);\n\ti915_gem_execbuffer_retire_commands(dev, file, ring);\n\nerr:\n\teb_destroy(eb);\n\twhile (!list_empty(&objects)) {\n\t\tstruct drm_i915_gem_object *obj;\n\n\t\tobj = list_first_entry(&objects,\n\t\t\t\t       struct drm_i915_gem_object,\n\t\t\t\t       exec_list);\n\t\tlist_del_init(&obj->exec_list);\n\t\tdrm_gem_object_unreference(&obj->base);\n\t}\n\n\tmutex_unlock(&dev->struct_mutex);\n\npre_mutex_err:\n\tkfree(cliprects);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 1137,
        "critical_vars": [
            "args->num_cliprects"
        ],
        "function": "i915_gem_do_execbuffer",
        "filename": "linux/CVE-2012-2384/CVE-2012-2384_CWE-189_44afb3a04391a74309d16180d1e4f8386fdfa745_i915_gem_execbuffer.c.diff",
        "label": "True",
        "function_code": "\nstatic int\ni915_gem_do_execbuffer(struct drm_device *dev, void *data,\n\t\t       struct drm_file *file,\n\t\t       struct drm_i915_gem_execbuffer2 *args,\n\t\t       struct drm_i915_gem_exec_object2 *exec)\n{\n\tdrm_i915_private_t *dev_priv = dev->dev_private;\n\tstruct list_head objects;\n\tstruct eb_objects *eb;\n\tstruct drm_i915_gem_object *batch_obj;\n\tstruct drm_clip_rect *cliprects = NULL;\n\tstruct intel_ring_buffer *ring;\n\tu32 exec_start, exec_len;\n\tu32 seqno;\n\tu32 mask;\n\tint ret, mode, i;\n\n\tif (!i915_gem_check_execbuffer(args)) {\n\t\tDRM_DEBUG(\"execbuf with invalid offset/length\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tret = validate_exec_list(exec, args->buffer_count);\n\tif (ret)\n\t\treturn ret;\n\n\tswitch (args->flags & I915_EXEC_RING_MASK) {\n\tcase I915_EXEC_DEFAULT:\n\tcase I915_EXEC_RENDER:\n\t\tring = &dev_priv->ring[RCS];\n\t\tbreak;\n\tcase I915_EXEC_BSD:\n\t\tif (!HAS_BSD(dev)) {\n\t\t\tDRM_DEBUG(\"execbuf with invalid ring (BSD)\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tring = &dev_priv->ring[VCS];\n\t\tbreak;\n\tcase I915_EXEC_BLT:\n\t\tif (!HAS_BLT(dev)) {\n\t\t\tDRM_DEBUG(\"execbuf with invalid ring (BLT)\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tring = &dev_priv->ring[BCS];\n\t\tbreak;\n\tdefault:\n\t\tDRM_DEBUG(\"execbuf with unknown ring: %d\\n\",\n\t\t\t  (int)(args->flags & I915_EXEC_RING_MASK));\n\t\treturn -EINVAL;\n\t}\n\n\tmode = args->flags & I915_EXEC_CONSTANTS_MASK;\n\tmask = I915_EXEC_CONSTANTS_MASK;\n\tswitch (mode) {\n\tcase I915_EXEC_CONSTANTS_REL_GENERAL:\n\tcase I915_EXEC_CONSTANTS_ABSOLUTE:\n\tcase I915_EXEC_CONSTANTS_REL_SURFACE:\n\t\tif (ring == &dev_priv->ring[RCS] &&\n\t\t    mode != dev_priv->relative_constants_mode) {\n\t\t\tif (INTEL_INFO(dev)->gen < 4)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tif (INTEL_INFO(dev)->gen > 5 &&\n\t\t\t    mode == I915_EXEC_CONSTANTS_REL_SURFACE)\n\t\t\t\treturn -EINVAL;\n\n\t\t\t/* The HW changed the meaning on this bit on gen6 */\n\t\t\tif (INTEL_INFO(dev)->gen >= 6)\n\t\t\t\tmask &= ~I915_EXEC_CONSTANTS_REL_SURFACE;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tDRM_DEBUG(\"execbuf with unknown constants: %d\\n\", mode);\n\t\treturn -EINVAL;\n\t}\n\n\tif (args->buffer_count < 1) {\n\t\tDRM_DEBUG(\"execbuf with %d buffers\\n\", args->buffer_count);\n\t\treturn -EINVAL;\n\t}\n\n\tif (args->num_cliprects != 0) {\n\t\tif (ring != &dev_priv->ring[RCS]) {\n\t\t\tDRM_DEBUG(\"clip rectangles are only valid with the render ring\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (args->num_cliprects > UINT_MAX / sizeof(*cliprects)) {\n\t\t\tDRM_DEBUG(\"execbuf with %u cliprects\\n\",\n\t\t\t\t  args->num_cliprects);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tcliprects = kmalloc(args->num_cliprects * sizeof(*cliprects),\n\t\t\t\t    GFP_KERNEL);\n\t\tif (cliprects == NULL) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto pre_mutex_err;\n\t\t}\n\n\t\tif (copy_from_user(cliprects,\n\t\t\t\t     (struct drm_clip_rect __user *)(uintptr_t)\n\t\t\t\t     args->cliprects_ptr,\n\t\t\t\t     sizeof(*cliprects)*args->num_cliprects)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto pre_mutex_err;\n\t\t}\n\t}\n\n\tret = i915_mutex_lock_interruptible(dev);\n\tif (ret)\n\t\tgoto pre_mutex_err;\n\n\tif (dev_priv->mm.suspended) {\n\t\tmutex_unlock(&dev->struct_mutex);\n\t\tret = -EBUSY;\n\t\tgoto pre_mutex_err;\n\t}\n\n\teb = eb_create(args->buffer_count);\n\tif (eb == NULL) {\n\t\tmutex_unlock(&dev->struct_mutex);\n\t\tret = -ENOMEM;\n\t\tgoto pre_mutex_err;\n\t}\n\n\t/* Look up object handles */\n\tINIT_LIST_HEAD(&objects);\n\tfor (i = 0; i < args->buffer_count; i++) {\n\t\tstruct drm_i915_gem_object *obj;\n\n\t\tobj = to_intel_bo(drm_gem_object_lookup(dev, file,\n\t\t\t\t\t\t\texec[i].handle));\n\t\tif (&obj->base == NULL) {\n\t\t\tDRM_DEBUG(\"Invalid object handle %d at index %d\\n\",\n\t\t\t\t   exec[i].handle, i);\n\t\t\t/* prevent error path from reading uninitialized data */\n\t\t\tret = -ENOENT;\n\t\t\tgoto err;\n\t\t}\n\n\t\tif (!list_empty(&obj->exec_list)) {\n\t\t\tDRM_DEBUG(\"Object %p [handle %d, index %d] appears more than once in object list\\n\",\n\t\t\t\t   obj, exec[i].handle, i);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err;\n\t\t}\n\n\t\tlist_add_tail(&obj->exec_list, &objects);\n\t\tobj->exec_handle = exec[i].handle;\n\t\tobj->exec_entry = &exec[i];\n\t\teb_add_object(eb, obj);\n\t}\n\n\t/* take note of the batch buffer before we might reorder the lists */\n\tbatch_obj = list_entry(objects.prev,\n\t\t\t       struct drm_i915_gem_object,\n\t\t\t       exec_list);\n\n\t/* Move the objects en-masse into the GTT, evicting if necessary. */\n\tret = i915_gem_execbuffer_reserve(ring, file, &objects);\n\tif (ret)\n\t\tgoto err;\n\n\t/* The objects are in their final locations, apply the relocations. */\n\tret = i915_gem_execbuffer_relocate(dev, eb, &objects);\n\tif (ret) {\n\t\tif (ret == -EFAULT) {\n\t\t\tret = i915_gem_execbuffer_relocate_slow(dev, file, ring,\n\t\t\t\t\t\t\t\t&objects, eb,\n\t\t\t\t\t\t\t\texec,\n\t\t\t\t\t\t\t\targs->buffer_count);\n\t\t\tBUG_ON(!mutex_is_locked(&dev->struct_mutex));\n\t\t}\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\t/* Set the pending read domains for the batch buffer to COMMAND */\n\tif (batch_obj->base.pending_write_domain) {\n\t\tDRM_DEBUG(\"Attempting to use self-modifying batch buffer\\n\");\n\t\tret = -EINVAL;\n\t\tgoto err;\n\t}\n\tbatch_obj->base.pending_read_domains |= I915_GEM_DOMAIN_COMMAND;\n\n\tret = i915_gem_execbuffer_move_to_gpu(ring, &objects);\n\tif (ret)\n\t\tgoto err;\n\n\tseqno = i915_gem_next_request_seqno(ring);\n\tfor (i = 0; i < ARRAY_SIZE(ring->sync_seqno); i++) {\n\t\tif (seqno < ring->sync_seqno[i]) {\n\t\t\t/* The GPU can not handle its semaphore value wrapping,\n\t\t\t * so every billion or so execbuffers, we need to stall\n\t\t\t * the GPU in order to reset the counters.\n\t\t\t */\n\t\t\tret = i915_gpu_idle(dev, true);\n\t\t\tif (ret)\n\t\t\t\tgoto err;\n\n\t\t\tBUG_ON(ring->sync_seqno[i]);\n\t\t}\n\t}\n\n\tif (ring == &dev_priv->ring[RCS] &&\n\t    mode != dev_priv->relative_constants_mode) {\n\t\tret = intel_ring_begin(ring, 4);\n\t\tif (ret)\n\t\t\t\tgoto err;\n\n\t\tintel_ring_emit(ring, MI_NOOP);\n\t\tintel_ring_emit(ring, MI_LOAD_REGISTER_IMM(1));\n\t\tintel_ring_emit(ring, INSTPM);\n\t\tintel_ring_emit(ring, mask << 16 | mode);\n\t\tintel_ring_advance(ring);\n\n\t\tdev_priv->relative_constants_mode = mode;\n\t}\n\n\tif (args->flags & I915_EXEC_GEN7_SOL_RESET) {\n\t\tret = i915_reset_gen7_sol_offsets(dev, ring);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\ttrace_i915_gem_ring_dispatch(ring, seqno);\n\n\texec_start = batch_obj->gtt_offset + args->batch_start_offset;\n\texec_len = args->batch_len;\n\tif (cliprects) {\n\t\tfor (i = 0; i < args->num_cliprects; i++) {\n\t\t\tret = i915_emit_box(dev, &cliprects[i],\n\t\t\t\t\t    args->DR1, args->DR4);\n\t\t\tif (ret)\n\t\t\t\tgoto err;\n\n\t\t\tret = ring->dispatch_execbuffer(ring,\n\t\t\t\t\t\t\texec_start, exec_len);\n\t\t\tif (ret)\n\t\t\t\tgoto err;\n\t\t}\n\t} else {\n\t\tret = ring->dispatch_execbuffer(ring, exec_start, exec_len);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\ti915_gem_execbuffer_move_to_active(&objects, ring, seqno);\n\ti915_gem_execbuffer_retire_commands(dev, file, ring);\n\nerr:\n\teb_destroy(eb);\n\twhile (!list_empty(&objects)) {\n\t\tstruct drm_i915_gem_object *obj;\n\n\t\tobj = list_first_entry(&objects,\n\t\t\t\t       struct drm_i915_gem_object,\n\t\t\t\t       exec_list);\n\t\tlist_del_init(&obj->exec_list);\n\t\tdrm_gem_object_unreference(&obj->base);\n\t}\n\n\tmutex_unlock(&dev->struct_mutex);\n\npre_mutex_err:\n\tkfree(cliprects);\n\treturn ret;\n}"
    },
    {
        "patch_model": "Replace",
        "change_type": "Assignment",
        "line_old": 5334,
        "line_new": 5334,
        "critical_vars": [
            "event_id"
        ],
        "function": "sw_perf_event_destroy",
        "filename": "linux/CVE-2013-2094/CVE-2013-2094_CWE-189_8176cced706b5e5d15887584150764894e94e02f_core.c.diff",
        "label": "True",
        "function_code": "\nstatic void sw_perf_event_destroy(struct perf_event *event)\n{\n\tu64 event_id = event->attr.config;\n\n\tWARN_ON(event->parent);\n\n\tstatic_key_slow_dec(&perf_swevent_enabled[event_id]);\n\tswevent_hlist_put(event);\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "if-Condition",
        "line_old": 412,
        "critical_vars": [
            "namelen"
        ],
        "function": "SYSCALL_DEFINE2",
        "filename": "linux/CVE-2011-2209/CVE-2011-2209_CWE-189_21c5977a836e399fc710ff2c5367845ed5c2527f_osf_sys.c.diff",
        "label": "False",
        "function_code": "Function not found"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 412,
        "critical_vars": [
            "len"
        ],
        "function": "SYSCALL_DEFINE2",
        "filename": "linux/CVE-2011-2209/CVE-2011-2209_CWE-189_21c5977a836e399fc710ff2c5367845ed5c2527f_osf_sys.c.diff",
        "label": "True",
        "function_code": "Function not found"
    },
    {
        "patch_model": "Delete",
        "change_type": "if-Condition",
        "line_old": 597,
        "critical_vars": [
            "len"
        ],
        "function": "SYSCALL_DEFINE3",
        "filename": "linux/CVE-2011-2209/CVE-2011-2209_CWE-189_21c5977a836e399fc710ff2c5367845ed5c2527f_osf_sys.c.diff",
        "label": "False",
        "function_code": "Function not found"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 597,
        "critical_vars": [
            "long"
        ],
        "function": "SYSCALL_DEFINE3",
        "filename": "linux/CVE-2011-2209/CVE-2011-2209_CWE-189_21c5977a836e399fc710ff2c5367845ed5c2527f_osf_sys.c.diff",
        "label": "True",
        "function_code": "Function not found"
    },
    {
        "patch_model": "Replace",
        "change_type": "if-Condition",
        "line_old": 652,
        "line_new": 652,
        "critical_vars": [
            "nbytes"
        ],
        "function": "SYSCALL_DEFINE5",
        "filename": "linux/CVE-2011-2209/CVE-2011-2209_CWE-189_21c5977a836e399fc710ff2c5367845ed5c2527f_osf_sys.c.diff",
        "label": "True",
        "function_code": "Function not found"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 1011,
        "critical_vars": [
            "status"
        ],
        "function": "SYSCALL_DEFINE4",
        "filename": "linux/CVE-2011-2209/CVE-2011-2209_CWE-189_21c5977a836e399fc710ff2c5367845ed5c2527f_osf_sys.c.diff",
        "label": "True",
        "function_code": "Function not found"
    },
    {
        "patch_model": "Replace",
        "change_type": "Assignment",
        "line_old": 1019,
        "line_new": 1020,
        "critical_vars": [
            "ret"
        ],
        "function": "SYSCALL_DEFINE4",
        "filename": "linux/CVE-2011-2209/CVE-2011-2209_CWE-189_21c5977a836e399fc710ff2c5367845ed5c2527f_osf_sys.c.diff",
        "label": "True",
        "function_code": "Function not found"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 1028,
        "critical_vars": [
            "err"
        ],
        "function": "SYSCALL_DEFINE4",
        "filename": "linux/CVE-2011-2209/CVE-2011-2209_CWE-189_21c5977a836e399fc710ff2c5367845ed5c2527f_osf_sys.c.diff",
        "label": "True",
        "function_code": "Function not found"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 1494,
        "critical_vars": [
            "iinfo->i_lenEAttr",
            "iinfo->i_lenAlloc"
        ],
        "function": "udf_read_inode",
        "filename": "linux/CVE-2015-4167/CVE-2015-4167_CWE-189_23b133bdc452aa441fcb9b82cbf6dd05cfd342d0_inode.c.diff",
        "label": "True",
        "function_code": "\nstatic int udf_read_inode(struct inode *inode, bool hidden_inode)\n{\n\tstruct buffer_head *bh = NULL;\n\tstruct fileEntry *fe;\n\tstruct extendedFileEntry *efe;\n\tuint16_t ident;\n\tstruct udf_inode_info *iinfo = UDF_I(inode);\n\tstruct udf_sb_info *sbi = UDF_SB(inode->i_sb);\n\tstruct kernel_lb_addr *iloc = &iinfo->i_location;\n\tunsigned int link_count;\n\tunsigned int indirections = 0;\n\tint bs = inode->i_sb->s_blocksize;\n\tint ret = -EIO;\n\nreread:\n\tif (iloc->logicalBlockNum >=\n\t    sbi->s_partmaps[iloc->partitionReferenceNum].s_partition_len) {\n\t\tudf_debug(\"block=%d, partition=%d out of range\\n\",\n\t\t\t  iloc->logicalBlockNum, iloc->partitionReferenceNum);\n\t\treturn -EIO;\n\t}\n\n\t/*\n\t * Set defaults, but the inode is still incomplete!\n\t * Note: get_new_inode() sets the following on a new inode:\n\t *      i_sb = sb\n\t *      i_no = ino\n\t *      i_flags = sb->s_flags\n\t *      i_state = 0\n\t * clean_inode(): zero fills and sets\n\t *      i_count = 1\n\t *      i_nlink = 1\n\t *      i_op = NULL;\n\t */\n\tbh = udf_read_ptagged(inode->i_sb, iloc, 0, &ident);\n\tif (!bh) {\n\t\tudf_err(inode->i_sb, \"(ino %ld) failed !bh\\n\", inode->i_ino);\n\t\treturn -EIO;\n\t}\n\n\tif (ident != TAG_IDENT_FE && ident != TAG_IDENT_EFE &&\n\t    ident != TAG_IDENT_USE) {\n\t\tudf_err(inode->i_sb, \"(ino %ld) failed ident=%d\\n\",\n\t\t\tinode->i_ino, ident);\n\t\tgoto out;\n\t}\n\n\tfe = (struct fileEntry *)bh->b_data;\n\tefe = (struct extendedFileEntry *)bh->b_data;\n\n\tif (fe->icbTag.strategyType == cpu_to_le16(4096)) {\n\t\tstruct buffer_head *ibh;\n\n\t\tibh = udf_read_ptagged(inode->i_sb, iloc, 1, &ident);\n\t\tif (ident == TAG_IDENT_IE && ibh) {\n\t\t\tstruct kernel_lb_addr loc;\n\t\t\tstruct indirectEntry *ie;\n\n\t\t\tie = (struct indirectEntry *)ibh->b_data;\n\t\t\tloc = lelb_to_cpu(ie->indirectICB.extLocation);\n\n\t\t\tif (ie->indirectICB.extLength) {\n\t\t\t\tbrelse(ibh);\n\t\t\t\tmemcpy(&iinfo->i_location, &loc,\n\t\t\t\t       sizeof(struct kernel_lb_addr));\n\t\t\t\tif (++indirections > UDF_MAX_ICB_NESTING) {\n\t\t\t\t\tudf_err(inode->i_sb,\n\t\t\t\t\t\t\"too many ICBs in ICB hierarchy\"\n\t\t\t\t\t\t\" (max %d suppo\n... (function end not found)"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 1497,
        "critical_vars": [
            "iinfo->i_lenAlloc",
            "inode"
        ],
        "function": "udf_read_inode",
        "filename": "linux/CVE-2015-4167/CVE-2015-4167_CWE-189_23b133bdc452aa441fcb9b82cbf6dd05cfd342d0_inode.c.diff",
        "label": "True",
        "function_code": "\nstatic int udf_read_inode(struct inode *inode, bool hidden_inode)\n{\n\tstruct buffer_head *bh = NULL;\n\tstruct fileEntry *fe;\n\tstruct extendedFileEntry *efe;\n\tuint16_t ident;\n\tstruct udf_inode_info *iinfo = UDF_I(inode);\n\tstruct udf_sb_info *sbi = UDF_SB(inode->i_sb);\n\tstruct kernel_lb_addr *iloc = &iinfo->i_location;\n\tunsigned int link_count;\n\tunsigned int indirections = 0;\n\tint bs = inode->i_sb->s_blocksize;\n\tint ret = -EIO;\n\nreread:\n\tif (iloc->logicalBlockNum >=\n\t    sbi->s_partmaps[iloc->partitionReferenceNum].s_partition_len) {\n\t\tudf_debug(\"block=%d, partition=%d out of range\\n\",\n\t\t\t  iloc->logicalBlockNum, iloc->partitionReferenceNum);\n\t\treturn -EIO;\n\t}\n\n\t/*\n\t * Set defaults, but the inode is still incomplete!\n\t * Note: get_new_inode() sets the following on a new inode:\n\t *      i_sb = sb\n\t *      i_no = ino\n\t *      i_flags = sb->s_flags\n\t *      i_state = 0\n\t * clean_inode(): zero fills and sets\n\t *      i_count = 1\n\t *      i_nlink = 1\n\t *      i_op = NULL;\n\t */\n\tbh = udf_read_ptagged(inode->i_sb, iloc, 0, &ident);\n\tif (!bh) {\n\t\tudf_err(inode->i_sb, \"(ino %ld) failed !bh\\n\", inode->i_ino);\n\t\treturn -EIO;\n\t}\n\n\tif (ident != TAG_IDENT_FE && ident != TAG_IDENT_EFE &&\n\t    ident != TAG_IDENT_USE) {\n\t\tudf_err(inode->i_sb, \"(ino %ld) failed ident=%d\\n\",\n\t\t\tinode->i_ino, ident);\n\t\tgoto out;\n\t}\n\n\tfe = (struct fileEntry *)bh->b_data;\n\tefe = (struct extendedFileEntry *)bh->b_data;\n\n\tif (fe->icbTag.strategyType == cpu_to_le16(4096)) {\n\t\tstruct buffer_head *ibh;\n\n\t\tibh = udf_read_ptagged(inode->i_sb, iloc, 1, &ident);\n\t\tif (ident == TAG_IDENT_IE && ibh) {\n\t\t\tstruct kernel_lb_addr loc;\n\t\t\tstruct indirectEntry *ie;\n\n\t\t\tie = (struct indirectEntry *)ibh->b_data;\n\t\t\tloc = lelb_to_cpu(ie->indirectICB.extLocation);\n\n\t\t\tif (ie->indirectICB.extLength) {\n\t\t\t\tbrelse(ibh);\n\t\t\t\tmemcpy(&iinfo->i_location, &loc,\n\t\t\t\t       sizeof(struct kernel_lb_addr));\n\t\t\t\tif (++indirections > UDF_MAX_ICB_NESTING) {\n\t\t\t\t\tudf_err(inode->i_sb,\n\t\t\t\t\t\t\"too many ICBs in ICB hierarchy\"\n\t\t\t\t\t\t\" (max %d suppo\n... (function end not found)"
    },
    {
        "patch_model": "Replace",
        "change_type": "Assignment",
        "line_old": 2009,
        "line_new": 2009,
        "critical_vars": [
            "groups_per_flex"
        ],
        "function": "ext4_fill_flex_info",
        "filename": "linux/CVE-2012-2100/CVE-2012-2100_CWE-189_d50f2ab6f050311dbf7b8f5501b25f0bf64a439b_super.c.diff",
        "label": "True",
        "function_code": "\nstatic int ext4_fill_flex_info(struct super_block *sb)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_group_desc *gdp = NULL;\n\text4_group_t flex_group_count;\n\text4_group_t flex_group;\n\tunsigned int groups_per_flex = 0;\n\tsize_t size;\n\tint i;\n\n\tsbi->s_log_groups_per_flex = sbi->s_es->s_log_groups_per_flex;\n\tif (sbi->s_log_groups_per_flex < 1 || sbi->s_log_groups_per_flex > 31) {\n\t\tsbi->s_log_groups_per_flex = 0;\n\t\treturn 1;\n\t}\n\tgroups_per_flex = 1 << sbi->s_log_groups_per_flex;\n\n\t/* We allocate both existing and potentially added groups */\n\tflex_group_count = ((sbi->s_groups_count + groups_per_flex - 1) +\n\t\t\t((le16_to_cpu(sbi->s_es->s_reserved_gdt_blocks) + 1) <<\n\t\t\t      EXT4_DESC_PER_BLOCK_BITS(sb))) / groups_per_flex;\n\tsize = flex_group_count * sizeof(struct flex_groups);\n\tsbi->s_flex_groups = ext4_kvzalloc(size, GFP_KERNEL);\n\tif (sbi->s_flex_groups == NULL) {\n\t\text4_msg(sb, KERN_ERR, \"not enough memory for %u flex groups\",\n\t\t\t flex_group_count);\n\t\tgoto failed;\n\t}\n\n\tfor (i = 0; i < sbi->s_groups_count; i++) {\n\t\tgdp = ext4_get_group_desc(sb, i, NULL);\n\n\t\tflex_group = ext4_flex_group(sbi, i);\n\t\tatomic_add(ext4_free_inodes_count(sb, gdp),\n\t\t\t   &sbi->s_flex_groups[flex_group].free_inodes);\n\t\tatomic_add(ext4_free_group_clusters(sb, gdp),\n\t\t\t   &sbi->s_flex_groups[flex_group].free_clusters);\n\t\tatomic_add(ext4_used_dirs_count(sb, gdp),\n\t\t\t   &sbi->s_flex_groups[flex_group].used_dirs);\n\t}\n\n\treturn 1;\nfailed:\n\treturn 0;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 2014,
        "critical_vars": [
            "groups_per_flex"
        ],
        "function": "ext4_fill_flex_info",
        "filename": "linux/CVE-2012-2100/CVE-2012-2100_CWE-189_d50f2ab6f050311dbf7b8f5501b25f0bf64a439b_super.c.diff",
        "label": "False",
        "function_code": "\nstatic int ext4_fill_flex_info(struct super_block *sb)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_group_desc *gdp = NULL;\n\text4_group_t flex_group_count;\n\text4_group_t flex_group;\n\tint groups_per_flex = 0;\n\tsize_t size;\n\tint i;\n\n\tsbi->s_log_groups_per_flex = sbi->s_es->s_log_groups_per_flex;\n\tgroups_per_flex = 1 << sbi->s_log_groups_per_flex;\n\n\tif (groups_per_flex < 2) {\n\t\tsbi->s_log_groups_per_flex = 0;\n\t\treturn 1;\n\t}\n\n\t/* We allocate both existing and potentially added groups */\n\tflex_group_count = ((sbi->s_groups_count + groups_per_flex - 1) +\n\t\t\t((le16_to_cpu(sbi->s_es->s_reserved_gdt_blocks) + 1) <<\n\t\t\t      EXT4_DESC_PER_BLOCK_BITS(sb))) / groups_per_flex;\n\tsize = flex_group_count * sizeof(struct flex_groups);\n\tsbi->s_flex_groups = ext4_kvzalloc(size, GFP_KERNEL);\n\tif (sbi->s_flex_groups == NULL) {\n\t\text4_msg(sb, KERN_ERR, \"not enough memory for %u flex groups\",\n\t\t\t flex_group_count);\n\t\tgoto failed;\n\t}\n\n\tfor (i = 0; i < sbi->s_groups_count; i++) {\n\t\tgdp = ext4_get_group_desc(sb, i, NULL);\n\n\t\tflex_group = ext4_flex_group(sbi, i);\n\t\tatomic_add(ext4_free_inodes_count(sb, gdp),\n\t\t\t   &sbi->s_flex_groups[flex_group].free_inodes);\n\t\tatomic_add(ext4_free_group_clusters(sb, gdp),\n\t\t\t   &sbi->s_flex_groups[flex_group].free_clusters);\n\t\tatomic_add(ext4_used_dirs_count(sb, gdp),\n\t\t\t   &sbi->s_flex_groups[flex_group].used_dirs);\n\t}\n\n\treturn 1;\nfailed:\n\treturn 0;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 2014,
        "critical_vars": [
            "sbi->s_log_groups_per_flex"
        ],
        "function": "ext4_fill_flex_info",
        "filename": "linux/CVE-2012-2100/CVE-2012-2100_CWE-189_d50f2ab6f050311dbf7b8f5501b25f0bf64a439b_super.c.diff",
        "label": "True",
        "function_code": "\nstatic int ext4_fill_flex_info(struct super_block *sb)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_group_desc *gdp = NULL;\n\text4_group_t flex_group_count;\n\text4_group_t flex_group;\n\tunsigned int groups_per_flex = 0;\n\tsize_t size;\n\tint i;\n\n\tsbi->s_log_groups_per_flex = sbi->s_es->s_log_groups_per_flex;\n\tif (sbi->s_log_groups_per_flex < 1 || sbi->s_log_groups_per_flex > 31) {\n\t\tsbi->s_log_groups_per_flex = 0;\n\t\treturn 1;\n\t}\n\tgroups_per_flex = 1 << sbi->s_log_groups_per_flex;\n\n\t/* We allocate both existing and potentially added groups */\n\tflex_group_count = ((sbi->s_groups_count + groups_per_flex - 1) +\n\t\t\t((le16_to_cpu(sbi->s_es->s_reserved_gdt_blocks) + 1) <<\n\t\t\t      EXT4_DESC_PER_BLOCK_BITS(sb))) / groups_per_flex;\n\tsize = flex_group_count * sizeof(struct flex_groups);\n\tsbi->s_flex_groups = ext4_kvzalloc(size, GFP_KERNEL);\n\tif (sbi->s_flex_groups == NULL) {\n\t\text4_msg(sb, KERN_ERR, \"not enough memory for %u flex groups\",\n\t\t\t flex_group_count);\n\t\tgoto failed;\n\t}\n\n\tfor (i = 0; i < sbi->s_groups_count; i++) {\n\t\tgdp = ext4_get_group_desc(sb, i, NULL);\n\n\t\tflex_group = ext4_flex_group(sbi, i);\n\t\tatomic_add(ext4_free_inodes_count(sb, gdp),\n\t\t\t   &sbi->s_flex_groups[flex_group].free_inodes);\n\t\tatomic_add(ext4_free_group_clusters(sb, gdp),\n\t\t\t   &sbi->s_flex_groups[flex_group].free_clusters);\n\t\tatomic_add(ext4_used_dirs_count(sb, gdp),\n\t\t\t   &sbi->s_flex_groups[flex_group].used_dirs);\n\t}\n\n\treturn 1;\nfailed:\n\treturn 0;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "if-Condition",
        "line_old": 2016,
        "critical_vars": [
            "groups_per_flex"
        ],
        "function": "ext4_fill_flex_info",
        "filename": "linux/CVE-2012-2100/CVE-2012-2100_CWE-189_d50f2ab6f050311dbf7b8f5501b25f0bf64a439b_super.c.diff",
        "label": "False",
        "function_code": "\nstatic int ext4_fill_flex_info(struct super_block *sb)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_group_desc *gdp = NULL;\n\text4_group_t flex_group_count;\n\text4_group_t flex_group;\n\tint groups_per_flex = 0;\n\tsize_t size;\n\tint i;\n\n\tsbi->s_log_groups_per_flex = sbi->s_es->s_log_groups_per_flex;\n\tgroups_per_flex = 1 << sbi->s_log_groups_per_flex;\n\n\tif (groups_per_flex < 2) {\n\t\tsbi->s_log_groups_per_flex = 0;\n\t\treturn 1;\n\t}\n\n\t/* We allocate both existing and potentially added groups */\n\tflex_group_count = ((sbi->s_groups_count + groups_per_flex - 1) +\n\t\t\t((le16_to_cpu(sbi->s_es->s_reserved_gdt_blocks) + 1) <<\n\t\t\t      EXT4_DESC_PER_BLOCK_BITS(sb))) / groups_per_flex;\n\tsize = flex_group_count * sizeof(struct flex_groups);\n\tsbi->s_flex_groups = ext4_kvzalloc(size, GFP_KERNEL);\n\tif (sbi->s_flex_groups == NULL) {\n\t\text4_msg(sb, KERN_ERR, \"not enough memory for %u flex groups\",\n\t\t\t flex_group_count);\n\t\tgoto failed;\n\t}\n\n\tfor (i = 0; i < sbi->s_groups_count; i++) {\n\t\tgdp = ext4_get_group_desc(sb, i, NULL);\n\n\t\tflex_group = ext4_flex_group(sbi, i);\n\t\tatomic_add(ext4_free_inodes_count(sb, gdp),\n\t\t\t   &sbi->s_flex_groups[flex_group].free_inodes);\n\t\tatomic_add(ext4_free_group_clusters(sb, gdp),\n\t\t\t   &sbi->s_flex_groups[flex_group].free_clusters);\n\t\tatomic_add(ext4_used_dirs_count(sb, gdp),\n\t\t\t   &sbi->s_flex_groups[flex_group].used_dirs);\n\t}\n\n\treturn 1;\nfailed:\n\treturn 0;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 2018,
        "critical_vars": [
            "groups_per_flex"
        ],
        "function": "ext4_fill_flex_info",
        "filename": "linux/CVE-2012-2100/CVE-2012-2100_CWE-189_d50f2ab6f050311dbf7b8f5501b25f0bf64a439b_super.c.diff",
        "label": "True",
        "function_code": "\nstatic int ext4_fill_flex_info(struct super_block *sb)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_group_desc *gdp = NULL;\n\text4_group_t flex_group_count;\n\text4_group_t flex_group;\n\tunsigned int groups_per_flex = 0;\n\tsize_t size;\n\tint i;\n\n\tsbi->s_log_groups_per_flex = sbi->s_es->s_log_groups_per_flex;\n\tif (sbi->s_log_groups_per_flex < 1 || sbi->s_log_groups_per_flex > 31) {\n\t\tsbi->s_log_groups_per_flex = 0;\n\t\treturn 1;\n\t}\n\tgroups_per_flex = 1 << sbi->s_log_groups_per_flex;\n\n\t/* We allocate both existing and potentially added groups */\n\tflex_group_count = ((sbi->s_groups_count + groups_per_flex - 1) +\n\t\t\t((le16_to_cpu(sbi->s_es->s_reserved_gdt_blocks) + 1) <<\n\t\t\t      EXT4_DESC_PER_BLOCK_BITS(sb))) / groups_per_flex;\n\tsize = flex_group_count * sizeof(struct flex_groups);\n\tsbi->s_flex_groups = ext4_kvzalloc(size, GFP_KERNEL);\n\tif (sbi->s_flex_groups == NULL) {\n\t\text4_msg(sb, KERN_ERR, \"not enough memory for %u flex groups\",\n\t\t\t flex_group_count);\n\t\tgoto failed;\n\t}\n\n\tfor (i = 0; i < sbi->s_groups_count; i++) {\n\t\tgdp = ext4_get_group_desc(sb, i, NULL);\n\n\t\tflex_group = ext4_flex_group(sbi, i);\n\t\tatomic_add(ext4_free_inodes_count(sb, gdp),\n\t\t\t   &sbi->s_flex_groups[flex_group].free_inodes);\n\t\tatomic_add(ext4_free_group_clusters(sb, gdp),\n\t\t\t   &sbi->s_flex_groups[flex_group].free_clusters);\n\t\tatomic_add(ext4_used_dirs_count(sb, gdp),\n\t\t\t   &sbi->s_flex_groups[flex_group].used_dirs);\n\t}\n\n\treturn 1;\nfailed:\n\treturn 0;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 1920,
        "critical_vars": [
            "dst[dst_byte_offset]"
        ],
        "function": "ecryptfs_decode_from_filename",
        "filename": "linux/CVE-2014-9683/CVE-2014-9683_CWE-189_942080643bce061c3dd9d5718d3b745dcb39a8bc_crypto.c.diff",
        "label": "False",
        "function_code": "static void\necryptfs_decode_from_filename(unsigned char *dst, size_t *dst_size,\n\t\t\t      const unsigned char *src, size_t src_size)\n{\n\tu8 current_bit_offset = 0;\n\tsize_t src_byte_offset = 0;\n\tsize_t dst_byte_offset = 0;\n\n\tif (dst == NULL) {\n\t\t(*dst_size) = ecryptfs_max_decoded_size(src_size);\n\t\tgoto out;\n\t}\n\twhile (src_byte_offset < src_size) {\n\t\tunsigned char src_byte =\n\t\t\t\tfilename_rev_map[(int)src[src_byte_offset]];\n\n\t\tswitch (current_bit_offset) {\n\t\tcase 0:\n\t\t\tdst[dst_byte_offset] = (src_byte << 2);\n\t\t\tcurrent_bit_offset = 6;\n\t\t\tbreak;\n\t\tcase 6:\n\t\t\tdst[dst_byte_offset++] |= (src_byte >> 4);\n\t\t\tdst[dst_byte_offset] = ((src_byte & 0xF)\n\t\t\t\t\t\t << 4);\n\t\t\tcurrent_bit_offset = 4;\n\t\t\tbreak;\n\t\tcase 4:\n\t\t\tdst[dst_byte_offset++] |= (src_byte >> 2);\n\t\t\tdst[dst_byte_offset] = (src_byte << 6);\n\t\t\tcurrent_bit_offset = 2;\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\tdst[dst_byte_offset++] |= (src_byte);\n\t\t\tdst[dst_byte_offset] = 0;\n\t\t\tcurrent_bit_offset = 0;\n\t\t\tbreak;\n\t\t}\n\t\tsrc_byte_offset++;\n\t}\n\t(*dst_size) = dst_byte_offset;\nout:\n\treturn;\n}"
    },
    {
        "patch_model": "Replace",
        "change_type": "Assignment",
        "line_old": 183,
        "line_new": 185,
        "critical_vars": [
            "new->cid_mask"
        ],
        "function": "recalculate_apic_map",
        "filename": "linux/CVE-2013-6376/CVE-2013-6376_CWE-189_17d68b763f09a9ce824ae23eb62c9efc57b69271_lapic.c.diff",
        "label": "True",
        "function_code": "\nstatic void recalculate_apic_map(struct kvm *kvm)\n{\n\tstruct kvm_apic_map *new, *old = NULL;\n\tstruct kvm_vcpu *vcpu;\n\tint i;\n\n\tnew = kzalloc(sizeof(struct kvm_apic_map), GFP_KERNEL);\n\n\tmutex_lock(&kvm->arch.apic_map_lock);\n\n\tif (!new)\n\t\tgoto out;\n\n\tnew->ldr_bits = 8;\n\t/* flat mode is default */\n\tnew->cid_shift = 8;\n\tnew->cid_mask = 0;\n\tnew->lid_mask = 0xff;\n\n\tkvm_for_each_vcpu(i, vcpu, kvm) {\n\t\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\t\tu16 cid, lid;\n\t\tu32 ldr;\n\n\t\tif (!kvm_apic_present(vcpu))\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * All APICs have to be configured in the same mode by an OS.\n\t\t * We take advatage of this while building logical id loockup\n\t\t * table. After reset APICs are in xapic/flat mode, so if we\n\t\t * find apic with different setting we assume this is the mode\n\t\t * OS wants all apics to be in; build lookup table accordingly.\n\t\t */\n\t\tif (apic_x2apic_mode(apic)) {\n\t\t\tnew->ldr_bits = 32;\n\t\t\tnew->cid_shift = 16;\n\t\t\tnew->cid_mask = (1 << KVM_X2APIC_CID_BITS) - 1;\n\t\t\tnew->lid_mask = 0xffff;\n\t\t} else if (kvm_apic_sw_enabled(apic) &&\n\t\t\t\t!new->cid_mask /* flat mode */ &&\n\t\t\t\tkvm_apic_get_reg(apic, APIC_DFR) == APIC_DFR_CLUSTER) {\n\t\t\tnew->cid_shift = 4;\n\t\t\tnew->cid_mask = 0xf;\n\t\t\tnew->lid_mask = 0xf;\n\t\t}\n\n\t\tnew->phys_map[kvm_apic_id(apic)] = apic;\n\n\t\tldr = kvm_apic_get_reg(apic, APIC_LDR);\n\t\tcid = apic_cluster_id(new, ldr);\n\t\tlid = apic_logical_id(new, ldr);\n\n\t\tif (lid)\n\t\t\tnew->logical_map[cid][ffs(lid) - 1] = apic;\n\t}\nout:\n\told = rcu_dereference_protected(kvm->arch.apic_map,\n\t\t\tlockdep_is_held(&kvm->arch.apic_map_lock));\n\trcu_assign_pointer(kvm->arch.apic_map, new);\n\tmutex_unlock(&kvm->arch.apic_map_lock);\n\n\tif (old)\n\t\tkfree_rcu(old, rcu);\n\n\tkvm_vcpu_request_scan_ioapic(kvm);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 186,
        "critical_vars": [
            "new->lid_mask"
        ],
        "function": "recalculate_apic_map",
        "filename": "linux/CVE-2013-6376/CVE-2013-6376_CWE-189_17d68b763f09a9ce824ae23eb62c9efc57b69271_lapic.c.diff",
        "label": "True",
        "function_code": "\nstatic void recalculate_apic_map(struct kvm *kvm)\n{\n\tstruct kvm_apic_map *new, *old = NULL;\n\tstruct kvm_vcpu *vcpu;\n\tint i;\n\n\tnew = kzalloc(sizeof(struct kvm_apic_map), GFP_KERNEL);\n\n\tmutex_lock(&kvm->arch.apic_map_lock);\n\n\tif (!new)\n\t\tgoto out;\n\n\tnew->ldr_bits = 8;\n\t/* flat mode is default */\n\tnew->cid_shift = 8;\n\tnew->cid_mask = 0;\n\tnew->lid_mask = 0xff;\n\n\tkvm_for_each_vcpu(i, vcpu, kvm) {\n\t\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\t\tu16 cid, lid;\n\t\tu32 ldr;\n\n\t\tif (!kvm_apic_present(vcpu))\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * All APICs have to be configured in the same mode by an OS.\n\t\t * We take advatage of this while building logical id loockup\n\t\t * table. After reset APICs are in xapic/flat mode, so if we\n\t\t * find apic with different setting we assume this is the mode\n\t\t * OS wants all apics to be in; build lookup table accordingly.\n\t\t */\n\t\tif (apic_x2apic_mode(apic)) {\n\t\t\tnew->ldr_bits = 32;\n\t\t\tnew->cid_shift = 16;\n\t\t\tnew->cid_mask = (1 << KVM_X2APIC_CID_BITS) - 1;\n\t\t\tnew->lid_mask = 0xffff;\n\t\t} else if (kvm_apic_sw_enabled(apic) &&\n\t\t\t\t!new->cid_mask /* flat mode */ &&\n\t\t\t\tkvm_apic_get_reg(apic, APIC_DFR) == APIC_DFR_CLUSTER) {\n\t\t\tnew->cid_shift = 4;\n\t\t\tnew->cid_mask = 0xf;\n\t\t\tnew->lid_mask = 0xf;\n\t\t}\n\n\t\tnew->phys_map[kvm_apic_id(apic)] = apic;\n\n\t\tldr = kvm_apic_get_reg(apic, APIC_LDR);\n\t\tcid = apic_cluster_id(new, ldr);\n\t\tlid = apic_logical_id(new, ldr);\n\n\t\tif (lid)\n\t\t\tnew->logical_map[cid][ffs(lid) - 1] = apic;\n\t}\nout:\n\told = rcu_dereference_protected(kvm->arch.apic_map,\n\t\t\tlockdep_is_held(&kvm->arch.apic_map_lock));\n\trcu_assign_pointer(kvm->arch.apic_map, new);\n\tmutex_unlock(&kvm->arch.apic_map_lock);\n\n\tif (old)\n\t\tkfree_rcu(old, rcu);\n\n\tkvm_vcpu_request_scan_ioapic(kvm);\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "if-Condition",
        "line_old": 412,
        "critical_vars": [
            "namelen"
        ],
        "function": "SYSCALL_DEFINE2",
        "filename": "linux/CVE-2011-2208/CVE-2011-2208_CWE-189_21c5977a836e399fc710ff2c5367845ed5c2527f_osf_sys.c.diff",
        "label": "False",
        "function_code": "Function not found"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 412,
        "critical_vars": [
            "len"
        ],
        "function": "SYSCALL_DEFINE2",
        "filename": "linux/CVE-2011-2208/CVE-2011-2208_CWE-189_21c5977a836e399fc710ff2c5367845ed5c2527f_osf_sys.c.diff",
        "label": "True",
        "function_code": "Function not found"
    },
    {
        "patch_model": "Delete",
        "change_type": "if-Condition",
        "line_old": 597,
        "critical_vars": [
            "len"
        ],
        "function": "SYSCALL_DEFINE3",
        "filename": "linux/CVE-2011-2208/CVE-2011-2208_CWE-189_21c5977a836e399fc710ff2c5367845ed5c2527f_osf_sys.c.diff",
        "label": "False",
        "function_code": "Function not found"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 597,
        "critical_vars": [
            "long"
        ],
        "function": "SYSCALL_DEFINE3",
        "filename": "linux/CVE-2011-2208/CVE-2011-2208_CWE-189_21c5977a836e399fc710ff2c5367845ed5c2527f_osf_sys.c.diff",
        "label": "True",
        "function_code": "Function not found"
    },
    {
        "patch_model": "Replace",
        "change_type": "if-Condition",
        "line_old": 652,
        "line_new": 652,
        "critical_vars": [
            "nbytes"
        ],
        "function": "SYSCALL_DEFINE5",
        "filename": "linux/CVE-2011-2208/CVE-2011-2208_CWE-189_21c5977a836e399fc710ff2c5367845ed5c2527f_osf_sys.c.diff",
        "label": "True",
        "function_code": "Function not found"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 1011,
        "critical_vars": [
            "status"
        ],
        "function": "SYSCALL_DEFINE4",
        "filename": "linux/CVE-2011-2208/CVE-2011-2208_CWE-189_21c5977a836e399fc710ff2c5367845ed5c2527f_osf_sys.c.diff",
        "label": "True",
        "function_code": "Function not found"
    },
    {
        "patch_model": "Replace",
        "change_type": "Assignment",
        "line_old": 1019,
        "line_new": 1020,
        "critical_vars": [
            "ret"
        ],
        "function": "SYSCALL_DEFINE4",
        "filename": "linux/CVE-2011-2208/CVE-2011-2208_CWE-189_21c5977a836e399fc710ff2c5367845ed5c2527f_osf_sys.c.diff",
        "label": "True",
        "function_code": "Function not found"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 1028,
        "critical_vars": [
            "err"
        ],
        "function": "SYSCALL_DEFINE4",
        "filename": "linux/CVE-2011-2208/CVE-2011-2208_CWE-189_21c5977a836e399fc710ff2c5367845ed5c2527f_osf_sys.c.diff",
        "label": "True",
        "function_code": "Function not found"
    },
    {
        "patch_model": "Delete",
        "change_type": "if-Condition",
        "line_old": 280,
        "critical_vars": [
            "vma->vm_flags"
        ],
        "function": "*vma_to_resize",
        "filename": "linux/CVE-2011-2496/CVE-2011-2496_CWE-189_982134ba62618c2d69fbbbd166d0a11ee3b7e3d8_mremap.c.diff",
        "label": "False",
        "function_code": "\nstatic struct vm_area_struct *vma_to_resize(unsigned long addr,\n\tunsigned long old_len, unsigned long new_len, unsigned long *p)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma = find_vma(mm, addr);\n\n\tif (!vma || vma->vm_start > addr)\n\t\tgoto Efault;\n\n\tif (is_vm_hugetlb_page(vma))\n\t\tgoto Einval;\n\n\t/* We can't remap across vm area boundaries */\n\tif (old_len > vma->vm_end - addr)\n\t\tgoto Efault;\n\n\tif (vma->vm_flags & (VM_DONTEXPAND | VM_PFNMAP)) {\n\t\tif (new_len > old_len)\n\t\t\tgoto Efault;\n\t}\n\n\tif (vma->vm_flags & VM_LOCKED) {\n\t\tunsigned long locked, lock_limit;\n\t\tlocked = mm->locked_vm << PAGE_SHIFT;\n\t\tlock_limit = rlimit(RLIMIT_MEMLOCK);\n\t\tlocked += new_len - old_len;\n\t\tif (locked > lock_limit && !capable(CAP_IPC_LOCK))\n\t\t\tgoto Eagain;\n\t}\n\n\tif (!may_expand_vm(mm, (new_len - old_len) >> PAGE_SHIFT))\n\t\tgoto Enomem;\n\n\tif (vma->vm_flags & VM_ACCOUNT) {\n\t\tunsigned long charged = (new_len - old_len) >> PAGE_SHIFT;\n\t\tif (security_vm_enough_memory(charged))\n\t\t\tgoto Efault;\n\t\t*p = charged;\n\t}\n\n\treturn vma;\n\nEfault:\t/* very odd choice for most of the cases, but... */\n\treturn ERR_PTR(-EFAULT);\nEinval:\n\treturn ERR_PTR(-EINVAL);\nEnomem:\n\treturn ERR_PTR(-ENOMEM);\nEagain:\n\treturn ERR_PTR(-EAGAIN);\n}\n\nstatic unsigned long mremap_to(unsigned long addr,\n\tunsigned long old_len, unsigned long new_addr,\n\tunsigned long new_len)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long ret = -EINVAL;\n\tunsigned long charged = 0;\n\tunsigned long map_flags;\n\n\tif (new_addr & ~PAGE_MASK)\n\t\tgoto out;\n\n\tif (new_len > TASK_SIZE || new_addr > TASK_SIZE - new_len)\n\t\tgoto out;\n\n\t/* Check if the location we're moving into overlaps the\n\t * old location at all, and fail if it does.\n\t */\n\tif ((new_addr <= addr) && (new_addr+new_len) > addr)\n\t\tgoto out;\n\n\tif ((addr <= new_addr) && (addr+old_len) > new_addr)\n\t\tgoto out;\n\n\tret = security_file_mmap(NULL, 0, 0, 0, new_addr, 1);\n\tif (ret)\n\t\tgoto out;\n\n\tret = do_munmap(mm, new_addr, new_len);\n\tif (ret)\n\t\tgoto out;\n\n\tif (old_len >= new_len) {\n\t\tret = do_munmap(mm, addr+new_len, old_len - new_len);\n\t\tif (ret && old_len != new_len)\n\t\t\tgoto out;\n\t\told_len = new_len;\n\t}\n\n\tvma = vma_to_resize(addr, old_len, new_len, &charged);\n\tif (IS_ERR(vma)) {\n\t\tret = PTR_ERR(vma);\n\t\tgoto out;\n\t}\n\n\tmap_flags = MAP_FIXED;\n\tif (vma->vm_flags & VM_MAYSHARE)\n\t\tmap_flags |= MAP_SHARED;\n\n\tret = get_unmapped_area(vma->vm_file, new_addr, new_len, vma->vm_pgoff +\n\t\t\t\t((addr - vma->vm_start) >> PAGE_SHIFT),\n\t\t\t\tmap_flags);\n\tif (ret & ~PAGE_MASK)\n\t\tgoto out1;\n\n\tret = move_vma(vma, addr, old_len, new_len, new_addr);\n\tif (!(ret & ~PAGE_MASK))\n\t\tgoto out;\nout1:\n\tvm_unacct_memory(charged);\n\nout:\n\treturn ret;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 281,
        "critical_vars": [
            "new_len"
        ],
        "function": "*vma_to_resize",
        "filename": "linux/CVE-2011-2496/CVE-2011-2496_CWE-189_982134ba62618c2d69fbbbd166d0a11ee3b7e3d8_mremap.c.diff",
        "label": "True",
        "function_code": "\nstatic struct vm_area_struct *vma_to_resize(unsigned long addr,\n\tunsigned long old_len, unsigned long new_len, unsigned long *p)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma = find_vma(mm, addr);\n\n\tif (!vma || vma->vm_start > addr)\n\t\tgoto Efault;\n\n\tif (is_vm_hugetlb_page(vma))\n\t\tgoto Einval;\n\n\t/* We can't remap across vm area boundaries */\n\tif (old_len > vma->vm_end - addr)\n\t\tgoto Efault;\n\n\t/* Need to be careful about a growing mapping */\n\tif (new_len > old_len) {\n\t\tunsigned long pgoff;\n\n\t\tif (vma->vm_flags & (VM_DONTEXPAND | VM_PFNMAP))\n\t\t\tgoto Efault;\n\t\tpgoff = (addr - vma->vm_start) >> PAGE_SHIFT;\n\t\tpgoff += vma->vm_pgoff;\n\t\tif (pgoff + (new_len >> PAGE_SHIFT) < pgoff)\n\t\t\tgoto Einval;\n\t}\n\n\tif (vma->vm_flags & VM_LOCKED) {\n\t\tunsigned long locked, lock_limit;\n\t\tlocked = mm->locked_vm << PAGE_SHIFT;\n\t\tlock_limit = rlimit(RLIMIT_MEMLOCK);\n\t\tlocked += new_len - old_len;\n\t\tif (locked > lock_limit && !capable(CAP_IPC_LOCK))\n\t\t\tgoto Eagain;\n\t}\n\n\tif (!may_expand_vm(mm, (new_len - old_len) >> PAGE_SHIFT))\n\t\tgoto Enomem;\n\n\tif (vma->vm_flags & VM_ACCOUNT) {\n\t\tunsigned long charged = (new_len - old_len) >> PAGE_SHIFT;\n\t\tif (security_vm_enough_memory(charged))\n\t\t\tgoto Efault;\n\t\t*p = charged;\n\t}\n\n\treturn vma;\n\nEfault:\t/* very odd choice for most of the cases, but... */\n\treturn ERR_PTR(-EFAULT);\nEinval:\n\treturn ERR_PTR(-EINVAL);\nEnomem:\n\treturn ERR_PTR(-ENOMEM);\nEagain:\n\treturn ERR_PTR(-EAGAIN);\n}\n\nstatic unsigned long mremap_to(unsigned long addr,\n\tunsigned long old_len, unsigned long new_addr,\n\tunsigned long new_len)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long ret = -EINVAL;\n\tunsigned long charged = 0;\n\tunsigned long map_flags;\n\n\tif (new_addr & ~PAGE_MASK)\n\t\tgoto out;\n\n\tif (new_len > TASK_SIZE || new_addr > TASK_SIZE - new_len)\n\t\tgoto out;\n\n\t/* Check if the location we're moving into overlaps the\n\t * old location at all, and fail if it does.\n\t */\n\tif ((new_addr <= addr) && (new_addr+new_len) > addr)\n\t\tgoto out;\n\n\tif ((addr <= new_addr) && (addr+old_len) > new_addr)\n\t\tgoto out;\n\n\tret = security_file_mmap(NULL, 0, 0, 0, new_addr, 1);\n\tif (ret)\n\t\tgoto out;\n\n\tret = do_munmap(mm, new_addr, new_len);\n\tif (ret)\n\t\tgoto out;\n\n\tif (old_len >= new_len) {\n\t\tret = do_munmap(mm, addr+new_len, old_len - new_len);\n\t\tif (ret && old_len != new_len)\n\t\t\tgoto out;\n\t\told_len = new_len;\n\t}\n\n\tvma = vma_to_resize(addr, old_len, new_len, &charged);\n\tif (IS_ERR(vma)) {\n\t\tret = PTR_ERR(vma);\n\t\tgoto out;\n\t}\n\n\tmap_flags = MAP_FIXED;\n\tif (vma->vm_flags & VM_MAYSHARE)\n\t\tmap_flags |= MAP_SHARED;\n\n\tret = get_unmapped_area(vma->vm_file, new_addr, new_len, vma->vm_pgoff +\n\t\t\t\t((addr - vma->vm_start) >> PAGE_SHIFT),\n\t\t\t\tmap_flags);\n\tif (ret & ~PAGE_MASK)\n\t\tgoto out1;\n\n\tret = move_vma(vma, addr, old_len, new_len, new_addr);\n\tif (!(ret & ~PAGE_MASK))\n\t\tgoto out;\nout1:\n\tvm_unacct_memory(charged);\n\nout:\n\treturn ret;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "if-Condition",
        "line_old": 281,
        "critical_vars": [
            "new_len"
        ],
        "function": "*vma_to_resize",
        "filename": "linux/CVE-2011-2496/CVE-2011-2496_CWE-189_982134ba62618c2d69fbbbd166d0a11ee3b7e3d8_mremap.c.diff",
        "label": "False",
        "function_code": "\nstatic struct vm_area_struct *vma_to_resize(unsigned long addr,\n\tunsigned long old_len, unsigned long new_len, unsigned long *p)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma = find_vma(mm, addr);\n\n\tif (!vma || vma->vm_start > addr)\n\t\tgoto Efault;\n\n\tif (is_vm_hugetlb_page(vma))\n\t\tgoto Einval;\n\n\t/* We can't remap across vm area boundaries */\n\tif (old_len > vma->vm_end - addr)\n\t\tgoto Efault;\n\n\tif (vma->vm_flags & (VM_DONTEXPAND | VM_PFNMAP)) {\n\t\tif (new_len > old_len)\n\t\t\tgoto Efault;\n\t}\n\n\tif (vma->vm_flags & VM_LOCKED) {\n\t\tunsigned long locked, lock_limit;\n\t\tlocked = mm->locked_vm << PAGE_SHIFT;\n\t\tlock_limit = rlimit(RLIMIT_MEMLOCK);\n\t\tlocked += new_len - old_len;\n\t\tif (locked > lock_limit && !capable(CAP_IPC_LOCK))\n\t\t\tgoto Eagain;\n\t}\n\n\tif (!may_expand_vm(mm, (new_len - old_len) >> PAGE_SHIFT))\n\t\tgoto Enomem;\n\n\tif (vma->vm_flags & VM_ACCOUNT) {\n\t\tunsigned long charged = (new_len - old_len) >> PAGE_SHIFT;\n\t\tif (security_vm_enough_memory(charged))\n\t\t\tgoto Efault;\n\t\t*p = charged;\n\t}\n\n\treturn vma;\n\nEfault:\t/* very odd choice for most of the cases, but... */\n\treturn ERR_PTR(-EFAULT);\nEinval:\n\treturn ERR_PTR(-EINVAL);\nEnomem:\n\treturn ERR_PTR(-ENOMEM);\nEagain:\n\treturn ERR_PTR(-EAGAIN);\n}\n\nstatic unsigned long mremap_to(unsigned long addr,\n\tunsigned long old_len, unsigned long new_addr,\n\tunsigned long new_len)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long ret = -EINVAL;\n\tunsigned long charged = 0;\n\tunsigned long map_flags;\n\n\tif (new_addr & ~PAGE_MASK)\n\t\tgoto out;\n\n\tif (new_len > TASK_SIZE || new_addr > TASK_SIZE - new_len)\n\t\tgoto out;\n\n\t/* Check if the location we're moving into overlaps the\n\t * old location at all, and fail if it does.\n\t */\n\tif ((new_addr <= addr) && (new_addr+new_len) > addr)\n\t\tgoto out;\n\n\tif ((addr <= new_addr) && (addr+old_len) > new_addr)\n\t\tgoto out;\n\n\tret = security_file_mmap(NULL, 0, 0, 0, new_addr, 1);\n\tif (ret)\n\t\tgoto out;\n\n\tret = do_munmap(mm, new_addr, new_len);\n\tif (ret)\n\t\tgoto out;\n\n\tif (old_len >= new_len) {\n\t\tret = do_munmap(mm, addr+new_len, old_len - new_len);\n\t\tif (ret && old_len != new_len)\n\t\t\tgoto out;\n\t\told_len = new_len;\n\t}\n\n\tvma = vma_to_resize(addr, old_len, new_len, &charged);\n\tif (IS_ERR(vma)) {\n\t\tret = PTR_ERR(vma);\n\t\tgoto out;\n\t}\n\n\tmap_flags = MAP_FIXED;\n\tif (vma->vm_flags & VM_MAYSHARE)\n\t\tmap_flags |= MAP_SHARED;\n\n\tret = get_unmapped_area(vma->vm_file, new_addr, new_len, vma->vm_pgoff +\n\t\t\t\t((addr - vma->vm_start) >> PAGE_SHIFT),\n\t\t\t\tmap_flags);\n\tif (ret & ~PAGE_MASK)\n\t\tgoto out1;\n\n\tret = move_vma(vma, addr, old_len, new_len, new_addr);\n\tif (!(ret & ~PAGE_MASK))\n\t\tgoto out;\nout1:\n\tvm_unacct_memory(charged);\n\nout:\n\treturn ret;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Var-Declaration",
        "line_new": 282,
        "critical_vars": [
            "pgoff"
        ],
        "function": "*vma_to_resize",
        "filename": "linux/CVE-2011-2496/CVE-2011-2496_CWE-189_982134ba62618c2d69fbbbd166d0a11ee3b7e3d8_mremap.c.diff",
        "label": "True",
        "function_code": "\nstatic struct vm_area_struct *vma_to_resize(unsigned long addr,\n\tunsigned long old_len, unsigned long new_len, unsigned long *p)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma = find_vma(mm, addr);\n\n\tif (!vma || vma->vm_start > addr)\n\t\tgoto Efault;\n\n\tif (is_vm_hugetlb_page(vma))\n\t\tgoto Einval;\n\n\t/* We can't remap across vm area boundaries */\n\tif (old_len > vma->vm_end - addr)\n\t\tgoto Efault;\n\n\t/* Need to be careful about a growing mapping */\n\tif (new_len > old_len) {\n\t\tunsigned long pgoff;\n\n\t\tif (vma->vm_flags & (VM_DONTEXPAND | VM_PFNMAP))\n\t\t\tgoto Efault;\n\t\tpgoff = (addr - vma->vm_start) >> PAGE_SHIFT;\n\t\tpgoff += vma->vm_pgoff;\n\t\tif (pgoff + (new_len >> PAGE_SHIFT) < pgoff)\n\t\t\tgoto Einval;\n\t}\n\n\tif (vma->vm_flags & VM_LOCKED) {\n\t\tunsigned long locked, lock_limit;\n\t\tlocked = mm->locked_vm << PAGE_SHIFT;\n\t\tlock_limit = rlimit(RLIMIT_MEMLOCK);\n\t\tlocked += new_len - old_len;\n\t\tif (locked > lock_limit && !capable(CAP_IPC_LOCK))\n\t\t\tgoto Eagain;\n\t}\n\n\tif (!may_expand_vm(mm, (new_len - old_len) >> PAGE_SHIFT))\n\t\tgoto Enomem;\n\n\tif (vma->vm_flags & VM_ACCOUNT) {\n\t\tunsigned long charged = (new_len - old_len) >> PAGE_SHIFT;\n\t\tif (security_vm_enough_memory(charged))\n\t\t\tgoto Efault;\n\t\t*p = charged;\n\t}\n\n\treturn vma;\n\nEfault:\t/* very odd choice for most of the cases, but... */\n\treturn ERR_PTR(-EFAULT);\nEinval:\n\treturn ERR_PTR(-EINVAL);\nEnomem:\n\treturn ERR_PTR(-ENOMEM);\nEagain:\n\treturn ERR_PTR(-EAGAIN);\n}\n\nstatic unsigned long mremap_to(unsigned long addr,\n\tunsigned long old_len, unsigned long new_addr,\n\tunsigned long new_len)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long ret = -EINVAL;\n\tunsigned long charged = 0;\n\tunsigned long map_flags;\n\n\tif (new_addr & ~PAGE_MASK)\n\t\tgoto out;\n\n\tif (new_len > TASK_SIZE || new_addr > TASK_SIZE - new_len)\n\t\tgoto out;\n\n\t/* Check if the location we're moving into overlaps the\n\t * old location at all, and fail if it does.\n\t */\n\tif ((new_addr <= addr) && (new_addr+new_len) > addr)\n\t\tgoto out;\n\n\tif ((addr <= new_addr) && (addr+old_len) > new_addr)\n\t\tgoto out;\n\n\tret = security_file_mmap(NULL, 0, 0, 0, new_addr, 1);\n\tif (ret)\n\t\tgoto out;\n\n\tret = do_munmap(mm, new_addr, new_len);\n\tif (ret)\n\t\tgoto out;\n\n\tif (old_len >= new_len) {\n\t\tret = do_munmap(mm, addr+new_len, old_len - new_len);\n\t\tif (ret && old_len != new_len)\n\t\t\tgoto out;\n\t\told_len = new_len;\n\t}\n\n\tvma = vma_to_resize(addr, old_len, new_len, &charged);\n\tif (IS_ERR(vma)) {\n\t\tret = PTR_ERR(vma);\n\t\tgoto out;\n\t}\n\n\tmap_flags = MAP_FIXED;\n\tif (vma->vm_flags & VM_MAYSHARE)\n\t\tmap_flags |= MAP_SHARED;\n\n\tret = get_unmapped_area(vma->vm_file, new_addr, new_len, vma->vm_pgoff +\n\t\t\t\t((addr - vma->vm_start) >> PAGE_SHIFT),\n\t\t\t\tmap_flags);\n\tif (ret & ~PAGE_MASK)\n\t\tgoto out1;\n\n\tret = move_vma(vma, addr, old_len, new_len, new_addr);\n\tif (!(ret & ~PAGE_MASK))\n\t\tgoto out;\nout1:\n\tvm_unacct_memory(charged);\n\nout:\n\treturn ret;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 284,
        "critical_vars": [
            "vma->vm_flags"
        ],
        "function": "*vma_to_resize",
        "filename": "linux/CVE-2011-2496/CVE-2011-2496_CWE-189_982134ba62618c2d69fbbbd166d0a11ee3b7e3d8_mremap.c.diff",
        "label": "True",
        "function_code": "\nstatic struct vm_area_struct *vma_to_resize(unsigned long addr,\n\tunsigned long old_len, unsigned long new_len, unsigned long *p)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma = find_vma(mm, addr);\n\n\tif (!vma || vma->vm_start > addr)\n\t\tgoto Efault;\n\n\tif (is_vm_hugetlb_page(vma))\n\t\tgoto Einval;\n\n\t/* We can't remap across vm area boundaries */\n\tif (old_len > vma->vm_end - addr)\n\t\tgoto Efault;\n\n\t/* Need to be careful about a growing mapping */\n\tif (new_len > old_len) {\n\t\tunsigned long pgoff;\n\n\t\tif (vma->vm_flags & (VM_DONTEXPAND | VM_PFNMAP))\n\t\t\tgoto Efault;\n\t\tpgoff = (addr - vma->vm_start) >> PAGE_SHIFT;\n\t\tpgoff += vma->vm_pgoff;\n\t\tif (pgoff + (new_len >> PAGE_SHIFT) < pgoff)\n\t\t\tgoto Einval;\n\t}\n\n\tif (vma->vm_flags & VM_LOCKED) {\n\t\tunsigned long locked, lock_limit;\n\t\tlocked = mm->locked_vm << PAGE_SHIFT;\n\t\tlock_limit = rlimit(RLIMIT_MEMLOCK);\n\t\tlocked += new_len - old_len;\n\t\tif (locked > lock_limit && !capable(CAP_IPC_LOCK))\n\t\t\tgoto Eagain;\n\t}\n\n\tif (!may_expand_vm(mm, (new_len - old_len) >> PAGE_SHIFT))\n\t\tgoto Enomem;\n\n\tif (vma->vm_flags & VM_ACCOUNT) {\n\t\tunsigned long charged = (new_len - old_len) >> PAGE_SHIFT;\n\t\tif (security_vm_enough_memory(charged))\n\t\t\tgoto Efault;\n\t\t*p = charged;\n\t}\n\n\treturn vma;\n\nEfault:\t/* very odd choice for most of the cases, but... */\n\treturn ERR_PTR(-EFAULT);\nEinval:\n\treturn ERR_PTR(-EINVAL);\nEnomem:\n\treturn ERR_PTR(-ENOMEM);\nEagain:\n\treturn ERR_PTR(-EAGAIN);\n}\n\nstatic unsigned long mremap_to(unsigned long addr,\n\tunsigned long old_len, unsigned long new_addr,\n\tunsigned long new_len)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long ret = -EINVAL;\n\tunsigned long charged = 0;\n\tunsigned long map_flags;\n\n\tif (new_addr & ~PAGE_MASK)\n\t\tgoto out;\n\n\tif (new_len > TASK_SIZE || new_addr > TASK_SIZE - new_len)\n\t\tgoto out;\n\n\t/* Check if the location we're moving into overlaps the\n\t * old location at all, and fail if it does.\n\t */\n\tif ((new_addr <= addr) && (new_addr+new_len) > addr)\n\t\tgoto out;\n\n\tif ((addr <= new_addr) && (addr+old_len) > new_addr)\n\t\tgoto out;\n\n\tret = security_file_mmap(NULL, 0, 0, 0, new_addr, 1);\n\tif (ret)\n\t\tgoto out;\n\n\tret = do_munmap(mm, new_addr, new_len);\n\tif (ret)\n\t\tgoto out;\n\n\tif (old_len >= new_len) {\n\t\tret = do_munmap(mm, addr+new_len, old_len - new_len);\n\t\tif (ret && old_len != new_len)\n\t\t\tgoto out;\n\t\told_len = new_len;\n\t}\n\n\tvma = vma_to_resize(addr, old_len, new_len, &charged);\n\tif (IS_ERR(vma)) {\n\t\tret = PTR_ERR(vma);\n\t\tgoto out;\n\t}\n\n\tmap_flags = MAP_FIXED;\n\tif (vma->vm_flags & VM_MAYSHARE)\n\t\tmap_flags |= MAP_SHARED;\n\n\tret = get_unmapped_area(vma->vm_file, new_addr, new_len, vma->vm_pgoff +\n\t\t\t\t((addr - vma->vm_start) >> PAGE_SHIFT),\n\t\t\t\tmap_flags);\n\tif (ret & ~PAGE_MASK)\n\t\tgoto out1;\n\n\tret = move_vma(vma, addr, old_len, new_len, new_addr);\n\tif (!(ret & ~PAGE_MASK))\n\t\tgoto out;\nout1:\n\tvm_unacct_memory(charged);\n\nout:\n\treturn ret;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 286,
        "critical_vars": [
            "pgoff"
        ],
        "function": "*vma_to_resize",
        "filename": "linux/CVE-2011-2496/CVE-2011-2496_CWE-189_982134ba62618c2d69fbbbd166d0a11ee3b7e3d8_mremap.c.diff",
        "label": "True",
        "function_code": "\nstatic struct vm_area_struct *vma_to_resize(unsigned long addr,\n\tunsigned long old_len, unsigned long new_len, unsigned long *p)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma = find_vma(mm, addr);\n\n\tif (!vma || vma->vm_start > addr)\n\t\tgoto Efault;\n\n\tif (is_vm_hugetlb_page(vma))\n\t\tgoto Einval;\n\n\t/* We can't remap across vm area boundaries */\n\tif (old_len > vma->vm_end - addr)\n\t\tgoto Efault;\n\n\t/* Need to be careful about a growing mapping */\n\tif (new_len > old_len) {\n\t\tunsigned long pgoff;\n\n\t\tif (vma->vm_flags & (VM_DONTEXPAND | VM_PFNMAP))\n\t\t\tgoto Efault;\n\t\tpgoff = (addr - vma->vm_start) >> PAGE_SHIFT;\n\t\tpgoff += vma->vm_pgoff;\n\t\tif (pgoff + (new_len >> PAGE_SHIFT) < pgoff)\n\t\t\tgoto Einval;\n\t}\n\n\tif (vma->vm_flags & VM_LOCKED) {\n\t\tunsigned long locked, lock_limit;\n\t\tlocked = mm->locked_vm << PAGE_SHIFT;\n\t\tlock_limit = rlimit(RLIMIT_MEMLOCK);\n\t\tlocked += new_len - old_len;\n\t\tif (locked > lock_limit && !capable(CAP_IPC_LOCK))\n\t\t\tgoto Eagain;\n\t}\n\n\tif (!may_expand_vm(mm, (new_len - old_len) >> PAGE_SHIFT))\n\t\tgoto Enomem;\n\n\tif (vma->vm_flags & VM_ACCOUNT) {\n\t\tunsigned long charged = (new_len - old_len) >> PAGE_SHIFT;\n\t\tif (security_vm_enough_memory(charged))\n\t\t\tgoto Efault;\n\t\t*p = charged;\n\t}\n\n\treturn vma;\n\nEfault:\t/* very odd choice for most of the cases, but... */\n\treturn ERR_PTR(-EFAULT);\nEinval:\n\treturn ERR_PTR(-EINVAL);\nEnomem:\n\treturn ERR_PTR(-ENOMEM);\nEagain:\n\treturn ERR_PTR(-EAGAIN);\n}\n\nstatic unsigned long mremap_to(unsigned long addr,\n\tunsigned long old_len, unsigned long new_addr,\n\tunsigned long new_len)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long ret = -EINVAL;\n\tunsigned long charged = 0;\n\tunsigned long map_flags;\n\n\tif (new_addr & ~PAGE_MASK)\n\t\tgoto out;\n\n\tif (new_len > TASK_SIZE || new_addr > TASK_SIZE - new_len)\n\t\tgoto out;\n\n\t/* Check if the location we're moving into overlaps the\n\t * old location at all, and fail if it does.\n\t */\n\tif ((new_addr <= addr) && (new_addr+new_len) > addr)\n\t\tgoto out;\n\n\tif ((addr <= new_addr) && (addr+old_len) > new_addr)\n\t\tgoto out;\n\n\tret = security_file_mmap(NULL, 0, 0, 0, new_addr, 1);\n\tif (ret)\n\t\tgoto out;\n\n\tret = do_munmap(mm, new_addr, new_len);\n\tif (ret)\n\t\tgoto out;\n\n\tif (old_len >= new_len) {\n\t\tret = do_munmap(mm, addr+new_len, old_len - new_len);\n\t\tif (ret && old_len != new_len)\n\t\t\tgoto out;\n\t\told_len = new_len;\n\t}\n\n\tvma = vma_to_resize(addr, old_len, new_len, &charged);\n\tif (IS_ERR(vma)) {\n\t\tret = PTR_ERR(vma);\n\t\tgoto out;\n\t}\n\n\tmap_flags = MAP_FIXED;\n\tif (vma->vm_flags & VM_MAYSHARE)\n\t\tmap_flags |= MAP_SHARED;\n\n\tret = get_unmapped_area(vma->vm_file, new_addr, new_len, vma->vm_pgoff +\n\t\t\t\t((addr - vma->vm_start) >> PAGE_SHIFT),\n\t\t\t\tmap_flags);\n\tif (ret & ~PAGE_MASK)\n\t\tgoto out1;\n\n\tret = move_vma(vma, addr, old_len, new_len, new_addr);\n\tif (!(ret & ~PAGE_MASK))\n\t\tgoto out;\nout1:\n\tvm_unacct_memory(charged);\n\nout:\n\treturn ret;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 287,
        "critical_vars": [
            "pgoff"
        ],
        "function": "*vma_to_resize",
        "filename": "linux/CVE-2011-2496/CVE-2011-2496_CWE-189_982134ba62618c2d69fbbbd166d0a11ee3b7e3d8_mremap.c.diff",
        "label": "True",
        "function_code": "\nstatic struct vm_area_struct *vma_to_resize(unsigned long addr,\n\tunsigned long old_len, unsigned long new_len, unsigned long *p)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma = find_vma(mm, addr);\n\n\tif (!vma || vma->vm_start > addr)\n\t\tgoto Efault;\n\n\tif (is_vm_hugetlb_page(vma))\n\t\tgoto Einval;\n\n\t/* We can't remap across vm area boundaries */\n\tif (old_len > vma->vm_end - addr)\n\t\tgoto Efault;\n\n\t/* Need to be careful about a growing mapping */\n\tif (new_len > old_len) {\n\t\tunsigned long pgoff;\n\n\t\tif (vma->vm_flags & (VM_DONTEXPAND | VM_PFNMAP))\n\t\t\tgoto Efault;\n\t\tpgoff = (addr - vma->vm_start) >> PAGE_SHIFT;\n\t\tpgoff += vma->vm_pgoff;\n\t\tif (pgoff + (new_len >> PAGE_SHIFT) < pgoff)\n\t\t\tgoto Einval;\n\t}\n\n\tif (vma->vm_flags & VM_LOCKED) {\n\t\tunsigned long locked, lock_limit;\n\t\tlocked = mm->locked_vm << PAGE_SHIFT;\n\t\tlock_limit = rlimit(RLIMIT_MEMLOCK);\n\t\tlocked += new_len - old_len;\n\t\tif (locked > lock_limit && !capable(CAP_IPC_LOCK))\n\t\t\tgoto Eagain;\n\t}\n\n\tif (!may_expand_vm(mm, (new_len - old_len) >> PAGE_SHIFT))\n\t\tgoto Enomem;\n\n\tif (vma->vm_flags & VM_ACCOUNT) {\n\t\tunsigned long charged = (new_len - old_len) >> PAGE_SHIFT;\n\t\tif (security_vm_enough_memory(charged))\n\t\t\tgoto Efault;\n\t\t*p = charged;\n\t}\n\n\treturn vma;\n\nEfault:\t/* very odd choice for most of the cases, but... */\n\treturn ERR_PTR(-EFAULT);\nEinval:\n\treturn ERR_PTR(-EINVAL);\nEnomem:\n\treturn ERR_PTR(-ENOMEM);\nEagain:\n\treturn ERR_PTR(-EAGAIN);\n}\n\nstatic unsigned long mremap_to(unsigned long addr,\n\tunsigned long old_len, unsigned long new_addr,\n\tunsigned long new_len)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long ret = -EINVAL;\n\tunsigned long charged = 0;\n\tunsigned long map_flags;\n\n\tif (new_addr & ~PAGE_MASK)\n\t\tgoto out;\n\n\tif (new_len > TASK_SIZE || new_addr > TASK_SIZE - new_len)\n\t\tgoto out;\n\n\t/* Check if the location we're moving into overlaps the\n\t * old location at all, and fail if it does.\n\t */\n\tif ((new_addr <= addr) && (new_addr+new_len) > addr)\n\t\tgoto out;\n\n\tif ((addr <= new_addr) && (addr+old_len) > new_addr)\n\t\tgoto out;\n\n\tret = security_file_mmap(NULL, 0, 0, 0, new_addr, 1);\n\tif (ret)\n\t\tgoto out;\n\n\tret = do_munmap(mm, new_addr, new_len);\n\tif (ret)\n\t\tgoto out;\n\n\tif (old_len >= new_len) {\n\t\tret = do_munmap(mm, addr+new_len, old_len - new_len);\n\t\tif (ret && old_len != new_len)\n\t\t\tgoto out;\n\t\told_len = new_len;\n\t}\n\n\tvma = vma_to_resize(addr, old_len, new_len, &charged);\n\tif (IS_ERR(vma)) {\n\t\tret = PTR_ERR(vma);\n\t\tgoto out;\n\t}\n\n\tmap_flags = MAP_FIXED;\n\tif (vma->vm_flags & VM_MAYSHARE)\n\t\tmap_flags |= MAP_SHARED;\n\n\tret = get_unmapped_area(vma->vm_file, new_addr, new_len, vma->vm_pgoff +\n\t\t\t\t((addr - vma->vm_start) >> PAGE_SHIFT),\n\t\t\t\tmap_flags);\n\tif (ret & ~PAGE_MASK)\n\t\tgoto out1;\n\n\tret = move_vma(vma, addr, old_len, new_len, new_addr);\n\tif (!(ret & ~PAGE_MASK))\n\t\tgoto out;\nout1:\n\tvm_unacct_memory(charged);\n\nout:\n\treturn ret;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 288,
        "critical_vars": [
            "new_len",
            "pgoff"
        ],
        "function": "*vma_to_resize",
        "filename": "linux/CVE-2011-2496/CVE-2011-2496_CWE-189_982134ba62618c2d69fbbbd166d0a11ee3b7e3d8_mremap.c.diff",
        "label": "True",
        "function_code": "\nstatic struct vm_area_struct *vma_to_resize(unsigned long addr,\n\tunsigned long old_len, unsigned long new_len, unsigned long *p)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma = find_vma(mm, addr);\n\n\tif (!vma || vma->vm_start > addr)\n\t\tgoto Efault;\n\n\tif (is_vm_hugetlb_page(vma))\n\t\tgoto Einval;\n\n\t/* We can't remap across vm area boundaries */\n\tif (old_len > vma->vm_end - addr)\n\t\tgoto Efault;\n\n\t/* Need to be careful about a growing mapping */\n\tif (new_len > old_len) {\n\t\tunsigned long pgoff;\n\n\t\tif (vma->vm_flags & (VM_DONTEXPAND | VM_PFNMAP))\n\t\t\tgoto Efault;\n\t\tpgoff = (addr - vma->vm_start) >> PAGE_SHIFT;\n\t\tpgoff += vma->vm_pgoff;\n\t\tif (pgoff + (new_len >> PAGE_SHIFT) < pgoff)\n\t\t\tgoto Einval;\n\t}\n\n\tif (vma->vm_flags & VM_LOCKED) {\n\t\tunsigned long locked, lock_limit;\n\t\tlocked = mm->locked_vm << PAGE_SHIFT;\n\t\tlock_limit = rlimit(RLIMIT_MEMLOCK);\n\t\tlocked += new_len - old_len;\n\t\tif (locked > lock_limit && !capable(CAP_IPC_LOCK))\n\t\t\tgoto Eagain;\n\t}\n\n\tif (!may_expand_vm(mm, (new_len - old_len) >> PAGE_SHIFT))\n\t\tgoto Enomem;\n\n\tif (vma->vm_flags & VM_ACCOUNT) {\n\t\tunsigned long charged = (new_len - old_len) >> PAGE_SHIFT;\n\t\tif (security_vm_enough_memory(charged))\n\t\t\tgoto Efault;\n\t\t*p = charged;\n\t}\n\n\treturn vma;\n\nEfault:\t/* very odd choice for most of the cases, but... */\n\treturn ERR_PTR(-EFAULT);\nEinval:\n\treturn ERR_PTR(-EINVAL);\nEnomem:\n\treturn ERR_PTR(-ENOMEM);\nEagain:\n\treturn ERR_PTR(-EAGAIN);\n}\n\nstatic unsigned long mremap_to(unsigned long addr,\n\tunsigned long old_len, unsigned long new_addr,\n\tunsigned long new_len)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long ret = -EINVAL;\n\tunsigned long charged = 0;\n\tunsigned long map_flags;\n\n\tif (new_addr & ~PAGE_MASK)\n\t\tgoto out;\n\n\tif (new_len > TASK_SIZE || new_addr > TASK_SIZE - new_len)\n\t\tgoto out;\n\n\t/* Check if the location we're moving into overlaps the\n\t * old location at all, and fail if it does.\n\t */\n\tif ((new_addr <= addr) && (new_addr+new_len) > addr)\n\t\tgoto out;\n\n\tif ((addr <= new_addr) && (addr+old_len) > new_addr)\n\t\tgoto out;\n\n\tret = security_file_mmap(NULL, 0, 0, 0, new_addr, 1);\n\tif (ret)\n\t\tgoto out;\n\n\tret = do_munmap(mm, new_addr, new_len);\n\tif (ret)\n\t\tgoto out;\n\n\tif (old_len >= new_len) {\n\t\tret = do_munmap(mm, addr+new_len, old_len - new_len);\n\t\tif (ret && old_len != new_len)\n\t\t\tgoto out;\n\t\told_len = new_len;\n\t}\n\n\tvma = vma_to_resize(addr, old_len, new_len, &charged);\n\tif (IS_ERR(vma)) {\n\t\tret = PTR_ERR(vma);\n\t\tgoto out;\n\t}\n\n\tmap_flags = MAP_FIXED;\n\tif (vma->vm_flags & VM_MAYSHARE)\n\t\tmap_flags |= MAP_SHARED;\n\n\tret = get_unmapped_area(vma->vm_file, new_addr, new_len, vma->vm_pgoff +\n\t\t\t\t((addr - vma->vm_start) >> PAGE_SHIFT),\n\t\t\t\tmap_flags);\n\tif (ret & ~PAGE_MASK)\n\t\tgoto out1;\n\n\tret = move_vma(vma, addr, old_len, new_len, new_addr);\n\tif (!(ret & ~PAGE_MASK))\n\t\tgoto out;\nout1:\n\tvm_unacct_memory(charged);\n\nout:\n\treturn ret;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 662,
        "critical_vars": [
            "sz"
        ],
        "function": "*xt_alloc_table_info",
        "filename": "linux/CVE-2016-3135/CVE-2016-3135_CWE-189_d157bd761585605b7882935ffb86286919f62ea1_x_tables.c.diff",
        "label": "True",
        "function_code": "\nstruct xt_table_info *xt_alloc_table_info(unsigned int size)\n{\n\tstruct xt_table_info *info = NULL;\n\tsize_t sz = sizeof(*info) + size;\n\n\tif (sz < sizeof(*info))\n\t\treturn NULL;\n\n\t/* Pedantry: prevent them from hitting BUG() in vmalloc.c --RR */\n\tif ((SMP_ALIGN(size) >> PAGE_SHIFT) + 2 > totalram_pages)\n\t\treturn NULL;\n\n\tif (sz <= (PAGE_SIZE << PAGE_ALLOC_COSTLY_ORDER))\n\t\tinfo = kmalloc(sz, GFP_KERNEL | __GFP_NOWARN | __GFP_NORETRY);\n\tif (!info) {\n\t\tinfo = vmalloc(sz);\n\t\tif (!info)\n\t\t\treturn NULL;\n\t}\n\tmemset(info, 0, sizeof(*info));\n\tinfo->size = size;\n\treturn info;\n}"
    },
    {
        "patch_model": "Replace",
        "change_type": "Assignment",
        "line_old": 3282,
        "line_new": 3282,
        "critical_vars": [
            "*pos"
        ],
        "function": "build_unc_path_to_root",
        "filename": "linux/CVE-2013-4247/CVE-2013-4247_CWE-189_1fc29bacedeabb278080e31bb9c1ecb49f143c3b_connect.c.diff",
        "label": "True",
        "function_code": "static char *\nbuild_unc_path_to_root(const struct smb_vol *vol,\n\t\tconst struct cifs_sb_info *cifs_sb)\n{\n\tchar *full_path, *pos;\n\tunsigned int pplen = vol->prepath ? strlen(vol->prepath) + 1 : 0;\n\tunsigned int unc_len = strnlen(vol->UNC, MAX_TREE_SIZE + 1);\n\n\tfull_path = kmalloc(unc_len + pplen + 1, GFP_KERNEL);\n\tif (full_path == NULL)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tstrncpy(full_path, vol->UNC, unc_len);\n\tpos = full_path + unc_len;\n\n\tif (pplen) {\n\t\t*pos = CIFS_DIR_SEP(cifs_sb);\n\t\tstrncpy(pos + 1, vol->prepath, pplen);\n\t\tpos += pplen;\n\t}\n\n\t*pos = '\\0'; /* add trailing null */\n\tconvert_delimiter(full_path, CIFS_DIR_SEP(cifs_sb));\n\tcifs_dbg(FYI, \"%s: full_path=%s\\n\", __func__, full_path);\n\treturn full_path;\n}"
    },
    {
        "patch_model": "Replace",
        "change_type": "Fun-Call",
        "line_old": 3283,
        "line_new": 3283,
        "critical_vars": [
            "pplen",
            "vol->prepath",
            "pos"
        ],
        "function": "build_unc_path_to_root",
        "filename": "linux/CVE-2013-4247/CVE-2013-4247_CWE-189_1fc29bacedeabb278080e31bb9c1ecb49f143c3b_connect.c.diff",
        "label": "True",
        "function_code": "static char *\nbuild_unc_path_to_root(const struct smb_vol *vol,\n\t\tconst struct cifs_sb_info *cifs_sb)\n{\n\tchar *full_path, *pos;\n\tunsigned int pplen = vol->prepath ? strlen(vol->prepath) + 1 : 0;\n\tunsigned int unc_len = strnlen(vol->UNC, MAX_TREE_SIZE + 1);\n\n\tfull_path = kmalloc(unc_len + pplen + 1, GFP_KERNEL);\n\tif (full_path == NULL)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tstrncpy(full_path, vol->UNC, unc_len);\n\tpos = full_path + unc_len;\n\n\tif (pplen) {\n\t\t*pos = CIFS_DIR_SEP(cifs_sb);\n\t\tstrncpy(pos + 1, vol->prepath, pplen);\n\t\tpos += pplen;\n\t}\n\n\t*pos = '\\0'; /* add trailing null */\n\tconvert_delimiter(full_path, CIFS_DIR_SEP(cifs_sb));\n\tcifs_dbg(FYI, \"%s: full_path=%s\\n\", __func__, full_path);\n\treturn full_path;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 329,
        "critical_vars": [
            "n"
        ],
        "function": "oz_usb_handle_ep_data",
        "filename": "linux/CVE-2015-4003/CVE-2015-4003_CWE-189_04bf464a5dfd9ade0dda918e44366c2c61fce80b_ozusbsvc1.c.diff",
        "label": "False",
        "function_code": "static void oz_usb_handle_ep_data(struct oz_usb_ctx *usb_ctx,\n\tstruct oz_usb_hdr *usb_hdr, int len)\n{\n\tstruct oz_data *data_hdr = (struct oz_data *)usb_hdr;\n\n\tswitch (data_hdr->format) {\n\tcase OZ_DATA_F_MULTIPLE_FIXED: {\n\t\t\tstruct oz_multiple_fixed *body =\n\t\t\t\t(struct oz_multiple_fixed *)data_hdr;\n\t\t\tu8 *data = body->data;\n\t\t\tint n = (len - sizeof(struct oz_multiple_fixed)+1)\n\t\t\t\t/ body->unit_size;\n\t\t\twhile (n--) {\n\t\t\t\toz_hcd_data_ind(usb_ctx->hport, body->endpoint,\n\t\t\t\t\tdata, body->unit_size);\n\t\t\t\tdata += body->unit_size;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tcase OZ_DATA_F_ISOC_FIXED: {\n\t\t\tstruct oz_isoc_fixed *body =\n\t\t\t\t(struct oz_isoc_fixed *)data_hdr;\n\t\t\tint data_len = len-sizeof(struct oz_isoc_fixed)+1;\n\t\t\tint unit_size = body->unit_size;\n\t\t\tu8 *data = body->data;\n\t\t\tint count;\n\t\t\tint i;\n\n\t\t\tif (!unit_size)\n\t\t\t\tbreak;\n\t\t\tcount = data_len/unit_size;\n\t\t\tfor (i = 0; i < count; i++) {\n\t\t\t\toz_hcd_data_ind(usb_ctx->hport,\n\t\t\t\t\tbody->endpoint, data, unit_size);\n\t\t\t\tdata += unit_size;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\t}\n\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Var-Declaration",
        "line_new": 329,
        "critical_vars": [
            "n"
        ],
        "function": "oz_usb_handle_ep_data",
        "filename": "linux/CVE-2015-4003/CVE-2015-4003_CWE-189_04bf464a5dfd9ade0dda918e44366c2c61fce80b_ozusbsvc1.c.diff",
        "label": "True",
        "function_code": "static void oz_usb_handle_ep_data(struct oz_usb_ctx *usb_ctx,\n\tstruct oz_usb_hdr *usb_hdr, int len)\n{\n\tstruct oz_data *data_hdr = (struct oz_data *)usb_hdr;\n\n\tswitch (data_hdr->format) {\n\tcase OZ_DATA_F_MULTIPLE_FIXED: {\n\t\t\tstruct oz_multiple_fixed *body =\n\t\t\t\t(struct oz_multiple_fixed *)data_hdr;\n\t\t\tu8 *data = body->data;\n\t\t\tint n;\n\t\t\tif (!body->unit_size)\n\t\t\t\tbreak;\n\t\t\tn = (len - sizeof(struct oz_multiple_fixed)+1)\n\t\t\t\t/ body->unit_size;\n\t\t\twhile (n--) {\n\t\t\t\toz_hcd_data_ind(usb_ctx->hport, body->endpoint,\n\t\t\t\t\tdata, body->unit_size);\n\t\t\t\tdata += body->unit_size;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tcase OZ_DATA_F_ISOC_FIXED: {\n\t\t\tstruct oz_isoc_fixed *body =\n\t\t\t\t(struct oz_isoc_fixed *)data_hdr;\n\t\t\tint data_len = len-sizeof(struct oz_isoc_fixed)+1;\n\t\t\tint unit_size = body->unit_size;\n\t\t\tu8 *data = body->data;\n\t\t\tint count;\n\t\t\tint i;\n\n\t\t\tif (!unit_size)\n\t\t\t\tbreak;\n\t\t\tcount = data_len/unit_size;\n\t\t\tfor (i = 0; i < count; i++) {\n\t\t\t\toz_hcd_data_ind(usb_ctx->hport,\n\t\t\t\t\tbody->endpoint, data, unit_size);\n\t\t\t\tdata += unit_size;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\t}\n\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 330,
        "critical_vars": [
            "body->unit_size"
        ],
        "function": "oz_usb_handle_ep_data",
        "filename": "linux/CVE-2015-4003/CVE-2015-4003_CWE-189_04bf464a5dfd9ade0dda918e44366c2c61fce80b_ozusbsvc1.c.diff",
        "label": "True",
        "function_code": "static void oz_usb_handle_ep_data(struct oz_usb_ctx *usb_ctx,\n\tstruct oz_usb_hdr *usb_hdr, int len)\n{\n\tstruct oz_data *data_hdr = (struct oz_data *)usb_hdr;\n\n\tswitch (data_hdr->format) {\n\tcase OZ_DATA_F_MULTIPLE_FIXED: {\n\t\t\tstruct oz_multiple_fixed *body =\n\t\t\t\t(struct oz_multiple_fixed *)data_hdr;\n\t\t\tu8 *data = body->data;\n\t\t\tint n;\n\t\t\tif (!body->unit_size)\n\t\t\t\tbreak;\n\t\t\tn = (len - sizeof(struct oz_multiple_fixed)+1)\n\t\t\t\t/ body->unit_size;\n\t\t\twhile (n--) {\n\t\t\t\toz_hcd_data_ind(usb_ctx->hport, body->endpoint,\n\t\t\t\t\tdata, body->unit_size);\n\t\t\t\tdata += body->unit_size;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tcase OZ_DATA_F_ISOC_FIXED: {\n\t\t\tstruct oz_isoc_fixed *body =\n\t\t\t\t(struct oz_isoc_fixed *)data_hdr;\n\t\t\tint data_len = len-sizeof(struct oz_isoc_fixed)+1;\n\t\t\tint unit_size = body->unit_size;\n\t\t\tu8 *data = body->data;\n\t\t\tint count;\n\t\t\tint i;\n\n\t\t\tif (!unit_size)\n\t\t\t\tbreak;\n\t\t\tcount = data_len/unit_size;\n\t\t\tfor (i = 0; i < count; i++) {\n\t\t\t\toz_hcd_data_ind(usb_ctx->hport,\n\t\t\t\t\tbody->endpoint, data, unit_size);\n\t\t\t\tdata += unit_size;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\t}\n\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 332,
        "critical_vars": [
            "n"
        ],
        "function": "oz_usb_handle_ep_data",
        "filename": "linux/CVE-2015-4003/CVE-2015-4003_CWE-189_04bf464a5dfd9ade0dda918e44366c2c61fce80b_ozusbsvc1.c.diff",
        "label": "True",
        "function_code": "static void oz_usb_handle_ep_data(struct oz_usb_ctx *usb_ctx,\n\tstruct oz_usb_hdr *usb_hdr, int len)\n{\n\tstruct oz_data *data_hdr = (struct oz_data *)usb_hdr;\n\n\tswitch (data_hdr->format) {\n\tcase OZ_DATA_F_MULTIPLE_FIXED: {\n\t\t\tstruct oz_multiple_fixed *body =\n\t\t\t\t(struct oz_multiple_fixed *)data_hdr;\n\t\t\tu8 *data = body->data;\n\t\t\tint n;\n\t\t\tif (!body->unit_size)\n\t\t\t\tbreak;\n\t\t\tn = (len - sizeof(struct oz_multiple_fixed)+1)\n\t\t\t\t/ body->unit_size;\n\t\t\twhile (n--) {\n\t\t\t\toz_hcd_data_ind(usb_ctx->hport, body->endpoint,\n\t\t\t\t\tdata, body->unit_size);\n\t\t\t\tdata += body->unit_size;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tcase OZ_DATA_F_ISOC_FIXED: {\n\t\t\tstruct oz_isoc_fixed *body =\n\t\t\t\t(struct oz_isoc_fixed *)data_hdr;\n\t\t\tint data_len = len-sizeof(struct oz_isoc_fixed)+1;\n\t\t\tint unit_size = body->unit_size;\n\t\t\tu8 *data = body->data;\n\t\t\tint count;\n\t\t\tint i;\n\n\t\t\tif (!unit_size)\n\t\t\t\tbreak;\n\t\t\tcount = data_len/unit_size;\n\t\t\tfor (i = 0; i < count; i++) {\n\t\t\t\toz_hcd_data_ind(usb_ctx->hport,\n\t\t\t\t\tbody->endpoint, data, unit_size);\n\t\t\t\tdata += unit_size;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\t}\n\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 375,
        "critical_vars": [
            "buffer"
        ],
        "function": "chk_mem_check",
        "filename": "platform_bionic/CVE-2012-2674/CVE-2012-2674_CWE-189_7f5aa4f35e23fd37425b3a5041737cdf58f87385_malloc_debug_leak.c.diff",
        "label": "False",
        "function_code": "static int chk_mem_check(void*       mem,\n                         size_t*     allocated,\n                         const char* func)\n{\n    char*  buffer;\n    size_t offset, bytes;\n    int    i;\n    char*  buf;\n\n    /* first check the bytes in the sentinel header */\n    buf = (char*)mem - CHK_SENTINEL_HEAD_SIZE;\n    for (i=0 ; i<CHK_SENTINEL_HEAD_SIZE ; i++) {\n        if (buf[i] != CHK_SENTINEL_VALUE) {\n            assert_log_message(\n                \"*** %s CHECK: buffer %p \"\n                \"corrupted %d bytes before allocation\",\n                func, mem, CHK_SENTINEL_HEAD_SIZE-i);\n            return -1;\n        }\n    }\n\n    /* then the ones in the sentinel trailer */\n    buffer = (char*)mem - CHK_SENTINEL_HEAD_SIZE;\n    offset = dlmalloc_usable_size(buffer) - sizeof(size_t);\n    bytes  = *(size_t *)(buffer + offset);\n\n    buf = (char*)mem + bytes;\n    for (i=CHK_SENTINEL_TAIL_SIZE-1 ; i>=0 ; i--) {\n        if (buf[i] != CHK_SENTINEL_VALUE) {\n            assert_log_message(\n                \"*** %s CHECK: buffer %p, size=%lu, \"\n                \"corrupted %d bytes after allocation\",\n                func, buffer, bytes, i+1);\n            return -1;\n        }\n    }\n\n    *allocated = bytes;\n    return 0;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 375,
        "critical_vars": [
            "size"
        ],
        "function": "chk_mem_check",
        "filename": "platform_bionic/CVE-2012-2674/CVE-2012-2674_CWE-189_7f5aa4f35e23fd37425b3a5041737cdf58f87385_malloc_debug_leak.c.diff",
        "label": "True",
        "function_code": "static int chk_mem_check(void*       mem,\n                         size_t*     allocated,\n                         const char* func)\n{\n    char*  buffer;\n    size_t offset, bytes;\n    int    i;\n    char*  buf;\n\n    /* first check the bytes in the sentinel header */\n    buf = (char*)mem - CHK_SENTINEL_HEAD_SIZE;\n    for (i=0 ; i<CHK_SENTINEL_HEAD_SIZE ; i++) {\n        if (buf[i] != CHK_SENTINEL_VALUE) {\n            assert_log_message(\n                \"*** %s CHECK: buffer %p \"\n                \"corrupted %d bytes before allocation\",\n                func, mem, CHK_SENTINEL_HEAD_SIZE-i);\n            return -1;\n        }\n    }\n\n    /* then the ones in the sentinel trailer */\n    buffer = (char*)mem - CHK_SENTINEL_HEAD_SIZE;\n    offset = dlmalloc_usable_size(buffer) - sizeof(size_t);\n    bytes  = *(size_t *)(buffer + offset);\n\n    buf = (char*)mem + bytes;\n    for (i=CHK_SENTINEL_TAIL_SIZE-1 ; i>=0 ; i--) {\n        if (buf[i] != CHK_SENTINEL_VALUE) {\n            assert_log_message(\n                \"*** %s CHECK: buffer %p, size=%lu, \"\n                \"corrupted %d bytes after allocation\",\n                func, buffer, bytes, i+1);\n            return -1;\n        }\n    }\n\n    *allocated = bytes;\n    return 0;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 376,
        "critical_vars": [
            "size"
        ],
        "function": "chk_mem_check",
        "filename": "platform_bionic/CVE-2012-2674/CVE-2012-2674_CWE-189_7f5aa4f35e23fd37425b3a5041737cdf58f87385_malloc_debug_leak.c.diff",
        "label": "True",
        "function_code": "static int chk_mem_check(void*       mem,\n                         size_t*     allocated,\n                         const char* func)\n{\n    char*  buffer;\n    size_t offset, bytes;\n    int    i;\n    char*  buf;\n\n    /* first check the bytes in the sentinel header */\n    buf = (char*)mem - CHK_SENTINEL_HEAD_SIZE;\n    for (i=0 ; i<CHK_SENTINEL_HEAD_SIZE ; i++) {\n        if (buf[i] != CHK_SENTINEL_VALUE) {\n            assert_log_message(\n                \"*** %s CHECK: buffer %p \"\n                \"corrupted %d bytes before allocation\",\n                func, mem, CHK_SENTINEL_HEAD_SIZE-i);\n            return -1;\n        }\n    }\n\n    /* then the ones in the sentinel trailer */\n    buffer = (char*)mem - CHK_SENTINEL_HEAD_SIZE;\n    offset = dlmalloc_usable_size(buffer) - sizeof(size_t);\n    bytes  = *(size_t *)(buffer + offset);\n\n    buf = (char*)mem + bytes;\n    for (i=CHK_SENTINEL_TAIL_SIZE-1 ; i>=0 ; i--) {\n        if (buf[i] != CHK_SENTINEL_VALUE) {\n            assert_log_message(\n                \"*** %s CHECK: buffer %p, size=%lu, \"\n                \"corrupted %d bytes after allocation\",\n                func, buffer, bytes, i+1);\n            return -1;\n        }\n    }\n\n    *allocated = bytes;\n    return 0;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 379,
        "critical_vars": [
            "buffer"
        ],
        "function": "chk_mem_check",
        "filename": "platform_bionic/CVE-2012-2674/CVE-2012-2674_CWE-189_7f5aa4f35e23fd37425b3a5041737cdf58f87385_malloc_debug_leak.c.diff",
        "label": "True",
        "function_code": "static int chk_mem_check(void*       mem,\n                         size_t*     allocated,\n                         const char* func)\n{\n    char*  buffer;\n    size_t offset, bytes;\n    int    i;\n    char*  buf;\n\n    /* first check the bytes in the sentinel header */\n    buf = (char*)mem - CHK_SENTINEL_HEAD_SIZE;\n    for (i=0 ; i<CHK_SENTINEL_HEAD_SIZE ; i++) {\n        if (buf[i] != CHK_SENTINEL_VALUE) {\n            assert_log_message(\n                \"*** %s CHECK: buffer %p \"\n                \"corrupted %d bytes before allocation\",\n                func, mem, CHK_SENTINEL_HEAD_SIZE-i);\n            return -1;\n        }\n    }\n\n    /* then the ones in the sentinel trailer */\n    buffer = (char*)mem - CHK_SENTINEL_HEAD_SIZE;\n    offset = dlmalloc_usable_size(buffer) - sizeof(size_t);\n    bytes  = *(size_t *)(buffer + offset);\n\n    buf = (char*)mem + bytes;\n    for (i=CHK_SENTINEL_TAIL_SIZE-1 ; i>=0 ; i--) {\n        if (buf[i] != CHK_SENTINEL_VALUE) {\n            assert_log_message(\n                \"*** %s CHECK: buffer %p, size=%lu, \"\n                \"corrupted %d bytes after allocation\",\n                func, buffer, bytes, i+1);\n            return -1;\n        }\n    }\n\n    *allocated = bytes;\n    return 0;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 508,
        "critical_vars": [
            "base"
        ],
        "function": "leak_malloc",
        "filename": "platform_bionic/CVE-2012-2674/CVE-2012-2674_CWE-189_7f5aa4f35e23fd37425b3a5041737cdf58f87385_malloc_debug_leak.c.diff",
        "label": "False",
        "function_code": "\nvoid* leak_malloc(size_t bytes)\n{\n    // allocate enough space infront of the allocation to store the pointer for\n    // the alloc structure. This will making free'ing the structer really fast!\n\n    // 1. allocate enough memory and include our header\n    // 2. set the base pointer to be right after our header\n\n    void* base = dlmalloc(bytes + sizeof(AllocationEntry));\n    if (base != NULL) {\n        pthread_mutex_lock(&gAllocationsMutex);\n\n            intptr_t backtrace[BACKTRACE_SIZE];\n            size_t numEntries = get_backtrace(backtrace, BACKTRACE_SIZE);\n\n            AllocationEntry* header = (AllocationEntry*)base;\n            header->entry = record_backtrace(backtrace, numEntries, bytes);\n            header->guard = GUARD;\n\n            // now increment base to point to after our header.\n            // this should just work since our header is 8 bytes.\n            base = (AllocationEntry*)base + 1;\n\n        pthread_mutex_unlock(&gAllocationsMutex);\n    }\n\n    return base;\n}\n\nvoid leak_free(void* mem)\n{\n    if (mem != NULL) {\n        pthread_mutex_lock(&gAllocationsMutex);\n\n        // check the guard to make sure it is valid\n        AllocationEntry* header = (AllocationEntry*)mem - 1;\n\n        if (header->guard != GUARD) {\n            // could be a memaligned block\n            if (((void**)mem)[-1] == MEMALIGN_GUARD) {\n                mem = ((void**)mem)[-2];\n                header = (AllocationEntry*)mem - 1;\n            }\n        }\n\n        if (header->guard == GUARD || is_valid_entry(header->entry)) {\n            // decrement the allocations\n            HashEntry* entry = header->entry;\n            entry->allocations--;\n            if (entry->allocations <= 0) {\n                remove_entry(entry);\n                dlfree(entry);\n            }\n\n            // now free the memory!\n            dlfree(header);\n        } else {\n            debug_log(\"WARNING bad header guard: '0x%x'! and invalid entry: %p\\n\",\n                    header->guard, header->entry);\n        }\n\n        pthread_mutex_unlock(&gAllocationsMutex);\n    }\n}\n\nvoid* leak_calloc(size_t n_elements, size_t elem_size)\n{\n    size_t  size;\n    void*   ptr;\n\n    /* Fail on overflow - just to be safe even though this code runs only\n     * within the debugging C library, not the production one */\n    if (n_elements && MAX_SIZE_T / n_elements < elem_size) {\n        return NULL;\n    }\n    size = n_elements * elem_size;\n    ptr  = leak_malloc(size);\n    if (ptr != NULL) {\n        memset(ptr, 0, size);\n    }\n    return ptr;\n}\n\nvoid* leak_realloc(void* oldMem, size_t bytes)\n{\n    if (oldMem == NULL) {\n        return leak_malloc(bytes);\n    }\n    void* newMem = NULL;\n    AllocationEntry* header = (AllocationEntry*)oldMem - 1;\n    if (header && header->guard == GUARD) {\n        size_t oldSize = header->entry->size & ~SIZE_FLAG_MASK;\n        newMem = leak_malloc(bytes);\n        if (newMem != NULL) {\n            size_t copySize = (oldSize <= bytes) ? oldSize : bytes;\n            memcpy(newMem, oldMem, copySize);\n            leak_free(oldMem);\n        }\n    } else {\n        newMem = dlrealloc(oldMem, bytes);\n    }\n    return newMem;\n}\n\nvoid* leak_memalign(size_t alignment, size_t bytes)\n{\n    // we can just use malloc\n    if (alignment <= MALLOC_ALIGNMENT)\n        return leak_malloc(bytes);\n\n    // need to make sure it's a power of two\n    if (alignment & (alignment-1))\n        alignment = 1L << (31 - __builtin_clz(alignment));\n\n    // here, aligment is at least MALLOC_ALIGNMENT<<1 bytes\n    // we will align by at least MALLOC_ALIGNMENT bytes\n    // and at most alignment-MALLOC_ALIGNMENT bytes\n    size_t size = (alignment-MALLOC_ALIGNMENT) + bytes;\n    void* base = leak_malloc(size);\n    if (base != NULL) {\n        intptr_t ptr = (intptr_t)base;\n        if ((ptr % alignment) == 0)\n            return base;\n\n        // align the pointer\n        ptr += ((-ptr) % alignment);\n\n        // there is always enough space for the base pointer and the guard\n        ((void**)ptr)[-1] = MEMALIGN_GUARD;\n        ((void**)ptr)[-2] = base;\n\n        return (void*)ptr;\n    }\n    return base;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 512,
        "critical_vars": [
            "size"
        ],
        "function": "leak_malloc",
        "filename": "platform_bionic/CVE-2012-2674/CVE-2012-2674_CWE-189_7f5aa4f35e23fd37425b3a5041737cdf58f87385_malloc_debug_leak.c.diff",
        "label": "True",
        "function_code": "\nvoid* leak_malloc(size_t bytes)\n{\n    // allocate enough space infront of the allocation to store the pointer for\n    // the alloc structure. This will making free'ing the structer really fast!\n\n    // 1. allocate enough memory and include our header\n    // 2. set the base pointer to be right after our header\n\n    size_t size = bytes + sizeof(AllocationEntry);\n    if (size < bytes) { // Overflow.\n        return NULL;\n    }\n\n    void* base = dlmalloc(size);\n    if (base != NULL) {\n        pthread_mutex_lock(&gAllocationsMutex);\n\n            intptr_t backtrace[BACKTRACE_SIZE];\n            size_t numEntries = get_backtrace(backtrace, BACKTRACE_SIZE);\n\n            AllocationEntry* header = (AllocationEntry*)base;\n            header->entry = record_backtrace(backtrace, numEntries, bytes);\n            header->guard = GUARD;\n\n            // now increment base to point to after our header.\n            // this should just work since our header is 8 bytes.\n            base = (AllocationEntry*)base + 1;\n\n        pthread_mutex_unlock(&gAllocationsMutex);\n    }\n\n    return base;\n}\n\nvoid leak_free(void* mem)\n{\n    if (mem != NULL) {\n        pthread_mutex_lock(&gAllocationsMutex);\n\n        // check the guard to make sure it is valid\n        AllocationEntry* header = (AllocationEntry*)mem - 1;\n\n        if (header->guard != GUARD) {\n            // could be a memaligned block\n            if (((void**)mem)[-1] == MEMALIGN_GUARD) {\n                mem = ((void**)mem)[-2];\n                header = (AllocationEntry*)mem - 1;\n            }\n        }\n\n        if (header->guard == GUARD || is_valid_entry(header->entry)) {\n            // decrement the allocations\n            HashEntry* entry = header->entry;\n            entry->allocations--;\n            if (entry->allocations <= 0) {\n                remove_entry(entry);\n                dlfree(entry);\n            }\n\n            // now free the memory!\n            dlfree(header);\n        } else {\n            debug_log(\"WARNING bad header guard: '0x%x'! and invalid entry: %p\\n\",\n                    header->guard, header->entry);\n        }\n\n        pthread_mutex_unlock(&gAllocationsMutex);\n    }\n}\n\nvoid* leak_calloc(size_t n_elements, size_t elem_size)\n{\n    size_t  size;\n    void*   ptr;\n\n    /* Fail on overflow - just to be safe even though this code runs only\n     * within the debugging C library, not the production one */\n    if (n_elements && MAX_SIZE_T / n_elements < elem_size) {\n        return NULL;\n    }\n    size = n_elements * elem_size;\n    ptr  = leak_malloc(size);\n    if (ptr != NULL) {\n        memset(ptr, 0, size);\n    }\n    return ptr;\n}\n\nvoid* leak_realloc(void* oldMem, size_t bytes)\n{\n    if (oldMem == NULL) {\n        return leak_malloc(bytes);\n    }\n    void* newMem = NULL;\n    AllocationEntry* header = (AllocationEntry*)oldMem - 1;\n    if (header && header->guard == GUARD) {\n        size_t oldSize = header->entry->size & ~SIZE_FLAG_MASK;\n        newMem = leak_malloc(bytes);\n        if (newMem != NULL) {\n            size_t copySize = (oldSize <= bytes) ? oldSize : bytes;\n            memcpy(newMem, oldMem, copySize);\n            leak_free(oldMem);\n        }\n    } else {\n        newMem = dlrealloc(oldMem, bytes);\n    }\n    return newMem;\n}\n\nvoid* leak_memalign(size_t alignment, size_t bytes)\n{\n    // we can just use malloc\n    if (alignment <= MALLOC_ALIGNMENT)\n        return leak_malloc(bytes);\n\n    // need to make sure it's a power of two\n    if (alignment & (alignment-1))\n        alignment = 1L << (31 - __builtin_clz(alignment));\n\n    // here, aligment is at least MALLOC_ALIGNMENT<<1 bytes\n    // we will align by at least MALLOC_ALIGNMENT bytes\n    // and at most alignment-MALLOC_ALIGNMENT bytes\n    size_t size = (alignment-MALLOC_ALIGNMENT) + bytes;\n    if (size < bytes) { // Overflow.\n        return NULL;\n    }\n\n    void* base = leak_malloc(size);\n    if (base != NULL) {\n        intptr_t ptr = (intptr_t)base;\n        if ((ptr % alignment) == 0)\n            return base;\n\n        // align the pointer\n        ptr += ((-ptr) % alignment);\n\n        // there is always enough space for the base pointer and the guard\n        ((void**)ptr)[-1] = MEMALIGN_GUARD;\n        ((void**)ptr)[-2] = base;\n\n        return (void*)ptr;\n    }\n    return base;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 513,
        "critical_vars": [
            "size"
        ],
        "function": "leak_malloc",
        "filename": "platform_bionic/CVE-2012-2674/CVE-2012-2674_CWE-189_7f5aa4f35e23fd37425b3a5041737cdf58f87385_malloc_debug_leak.c.diff",
        "label": "True",
        "function_code": "\nvoid* leak_malloc(size_t bytes)\n{\n    // allocate enough space infront of the allocation to store the pointer for\n    // the alloc structure. This will making free'ing the structer really fast!\n\n    // 1. allocate enough memory and include our header\n    // 2. set the base pointer to be right after our header\n\n    size_t size = bytes + sizeof(AllocationEntry);\n    if (size < bytes) { // Overflow.\n        return NULL;\n    }\n\n    void* base = dlmalloc(size);\n    if (base != NULL) {\n        pthread_mutex_lock(&gAllocationsMutex);\n\n            intptr_t backtrace[BACKTRACE_SIZE];\n            size_t numEntries = get_backtrace(backtrace, BACKTRACE_SIZE);\n\n            AllocationEntry* header = (AllocationEntry*)base;\n            header->entry = record_backtrace(backtrace, numEntries, bytes);\n            header->guard = GUARD;\n\n            // now increment base to point to after our header.\n            // this should just work since our header is 8 bytes.\n            base = (AllocationEntry*)base + 1;\n\n        pthread_mutex_unlock(&gAllocationsMutex);\n    }\n\n    return base;\n}\n\nvoid leak_free(void* mem)\n{\n    if (mem != NULL) {\n        pthread_mutex_lock(&gAllocationsMutex);\n\n        // check the guard to make sure it is valid\n        AllocationEntry* header = (AllocationEntry*)mem - 1;\n\n        if (header->guard != GUARD) {\n            // could be a memaligned block\n            if (((void**)mem)[-1] == MEMALIGN_GUARD) {\n                mem = ((void**)mem)[-2];\n                header = (AllocationEntry*)mem - 1;\n            }\n        }\n\n        if (header->guard == GUARD || is_valid_entry(header->entry)) {\n            // decrement the allocations\n            HashEntry* entry = header->entry;\n            entry->allocations--;\n            if (entry->allocations <= 0) {\n                remove_entry(entry);\n                dlfree(entry);\n            }\n\n            // now free the memory!\n            dlfree(header);\n        } else {\n            debug_log(\"WARNING bad header guard: '0x%x'! and invalid entry: %p\\n\",\n                    header->guard, header->entry);\n        }\n\n        pthread_mutex_unlock(&gAllocationsMutex);\n    }\n}\n\nvoid* leak_calloc(size_t n_elements, size_t elem_size)\n{\n    size_t  size;\n    void*   ptr;\n\n    /* Fail on overflow - just to be safe even though this code runs only\n     * within the debugging C library, not the production one */\n    if (n_elements && MAX_SIZE_T / n_elements < elem_size) {\n        return NULL;\n    }\n    size = n_elements * elem_size;\n    ptr  = leak_malloc(size);\n    if (ptr != NULL) {\n        memset(ptr, 0, size);\n    }\n    return ptr;\n}\n\nvoid* leak_realloc(void* oldMem, size_t bytes)\n{\n    if (oldMem == NULL) {\n        return leak_malloc(bytes);\n    }\n    void* newMem = NULL;\n    AllocationEntry* header = (AllocationEntry*)oldMem - 1;\n    if (header && header->guard == GUARD) {\n        size_t oldSize = header->entry->size & ~SIZE_FLAG_MASK;\n        newMem = leak_malloc(bytes);\n        if (newMem != NULL) {\n            size_t copySize = (oldSize <= bytes) ? oldSize : bytes;\n            memcpy(newMem, oldMem, copySize);\n            leak_free(oldMem);\n        }\n    } else {\n        newMem = dlrealloc(oldMem, bytes);\n    }\n    return newMem;\n}\n\nvoid* leak_memalign(size_t alignment, size_t bytes)\n{\n    // we can just use malloc\n    if (alignment <= MALLOC_ALIGNMENT)\n        return leak_malloc(bytes);\n\n    // need to make sure it's a power of two\n    if (alignment & (alignment-1))\n        alignment = 1L << (31 - __builtin_clz(alignment));\n\n    // here, aligment is at least MALLOC_ALIGNMENT<<1 bytes\n    // we will align by at least MALLOC_ALIGNMENT bytes\n    // and at most alignment-MALLOC_ALIGNMENT bytes\n    size_t size = (alignment-MALLOC_ALIGNMENT) + bytes;\n    if (size < bytes) { // Overflow.\n        return NULL;\n    }\n\n    void* base = leak_malloc(size);\n    if (base != NULL) {\n        intptr_t ptr = (intptr_t)base;\n        if ((ptr % alignment) == 0)\n            return base;\n\n        // align the pointer\n        ptr += ((-ptr) % alignment);\n\n        // there is always enough space for the base pointer and the guard\n        ((void**)ptr)[-1] = MEMALIGN_GUARD;\n        ((void**)ptr)[-2] = base;\n\n        return (void*)ptr;\n    }\n    return base;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 517,
        "critical_vars": [
            "base"
        ],
        "function": "leak_malloc",
        "filename": "platform_bionic/CVE-2012-2674/CVE-2012-2674_CWE-189_7f5aa4f35e23fd37425b3a5041737cdf58f87385_malloc_debug_leak.c.diff",
        "label": "True",
        "function_code": "\nvoid* leak_malloc(size_t bytes)\n{\n    // allocate enough space infront of the allocation to store the pointer for\n    // the alloc structure. This will making free'ing the structer really fast!\n\n    // 1. allocate enough memory and include our header\n    // 2. set the base pointer to be right after our header\n\n    size_t size = bytes + sizeof(AllocationEntry);\n    if (size < bytes) { // Overflow.\n        return NULL;\n    }\n\n    void* base = dlmalloc(size);\n    if (base != NULL) {\n        pthread_mutex_lock(&gAllocationsMutex);\n\n            intptr_t backtrace[BACKTRACE_SIZE];\n            size_t numEntries = get_backtrace(backtrace, BACKTRACE_SIZE);\n\n            AllocationEntry* header = (AllocationEntry*)base;\n            header->entry = record_backtrace(backtrace, numEntries, bytes);\n            header->guard = GUARD;\n\n            // now increment base to point to after our header.\n            // this should just work since our header is 8 bytes.\n            base = (AllocationEntry*)base + 1;\n\n        pthread_mutex_unlock(&gAllocationsMutex);\n    }\n\n    return base;\n}\n\nvoid leak_free(void* mem)\n{\n    if (mem != NULL) {\n        pthread_mutex_lock(&gAllocationsMutex);\n\n        // check the guard to make sure it is valid\n        AllocationEntry* header = (AllocationEntry*)mem - 1;\n\n        if (header->guard != GUARD) {\n            // could be a memaligned block\n            if (((void**)mem)[-1] == MEMALIGN_GUARD) {\n                mem = ((void**)mem)[-2];\n                header = (AllocationEntry*)mem - 1;\n            }\n        }\n\n        if (header->guard == GUARD || is_valid_entry(header->entry)) {\n            // decrement the allocations\n            HashEntry* entry = header->entry;\n            entry->allocations--;\n            if (entry->allocations <= 0) {\n                remove_entry(entry);\n                dlfree(entry);\n            }\n\n            // now free the memory!\n            dlfree(header);\n        } else {\n            debug_log(\"WARNING bad header guard: '0x%x'! and invalid entry: %p\\n\",\n                    header->guard, header->entry);\n        }\n\n        pthread_mutex_unlock(&gAllocationsMutex);\n    }\n}\n\nvoid* leak_calloc(size_t n_elements, size_t elem_size)\n{\n    size_t  size;\n    void*   ptr;\n\n    /* Fail on overflow - just to be safe even though this code runs only\n     * within the debugging C library, not the production one */\n    if (n_elements && MAX_SIZE_T / n_elements < elem_size) {\n        return NULL;\n    }\n    size = n_elements * elem_size;\n    ptr  = leak_malloc(size);\n    if (ptr != NULL) {\n        memset(ptr, 0, size);\n    }\n    return ptr;\n}\n\nvoid* leak_realloc(void* oldMem, size_t bytes)\n{\n    if (oldMem == NULL) {\n        return leak_malloc(bytes);\n    }\n    void* newMem = NULL;\n    AllocationEntry* header = (AllocationEntry*)oldMem - 1;\n    if (header && header->guard == GUARD) {\n        size_t oldSize = header->entry->size & ~SIZE_FLAG_MASK;\n        newMem = leak_malloc(bytes);\n        if (newMem != NULL) {\n            size_t copySize = (oldSize <= bytes) ? oldSize : bytes;\n            memcpy(newMem, oldMem, copySize);\n            leak_free(oldMem);\n        }\n    } else {\n        newMem = dlrealloc(oldMem, bytes);\n    }\n    return newMem;\n}\n\nvoid* leak_memalign(size_t alignment, size_t bytes)\n{\n    // we can just use malloc\n    if (alignment <= MALLOC_ALIGNMENT)\n        return leak_malloc(bytes);\n\n    // need to make sure it's a power of two\n    if (alignment & (alignment-1))\n        alignment = 1L << (31 - __builtin_clz(alignment));\n\n    // here, aligment is at least MALLOC_ALIGNMENT<<1 bytes\n    // we will align by at least MALLOC_ALIGNMENT bytes\n    // and at most alignment-MALLOC_ALIGNMENT bytes\n    size_t size = (alignment-MALLOC_ALIGNMENT) + bytes;\n    if (size < bytes) { // Overflow.\n        return NULL;\n    }\n\n    void* base = leak_malloc(size);\n    if (base != NULL) {\n        intptr_t ptr = (intptr_t)base;\n        if ((ptr % alignment) == 0)\n            return base;\n\n        // align the pointer\n        ptr += ((-ptr) % alignment);\n\n        // there is always enough space for the base pointer and the guard\n        ((void**)ptr)[-1] = MEMALIGN_GUARD;\n        ((void**)ptr)[-2] = base;\n\n        return (void*)ptr;\n    }\n    return base;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 627,
        "critical_vars": [
            "size"
        ],
        "function": "leak_memalign",
        "filename": "platform_bionic/CVE-2012-2674/CVE-2012-2674_CWE-189_7f5aa4f35e23fd37425b3a5041737cdf58f87385_malloc_debug_leak.c.diff",
        "label": "True",
        "function_code": "\nvoid* leak_memalign(size_t alignment, size_t bytes)\n{\n    // we can just use malloc\n    if (alignment <= MALLOC_ALIGNMENT)\n        return leak_malloc(bytes);\n\n    // need to make sure it's a power of two\n    if (alignment & (alignment-1))\n        alignment = 1L << (31 - __builtin_clz(alignment));\n\n    // here, aligment is at least MALLOC_ALIGNMENT<<1 bytes\n    // we will align by at least MALLOC_ALIGNMENT bytes\n    // and at most alignment-MALLOC_ALIGNMENT bytes\n    size_t size = (alignment-MALLOC_ALIGNMENT) + bytes;\n    if (size < bytes) { // Overflow.\n        return NULL;\n    }\n\n    void* base = leak_malloc(size);\n    if (base != NULL) {\n        intptr_t ptr = (intptr_t)base;\n        if ((ptr % alignment) == 0)\n            return base;\n\n        // align the pointer\n        ptr += ((-ptr) % alignment);\n\n        // there is always enough space for the base pointer and the guard\n        ((void**)ptr)[-1] = MEMALIGN_GUARD;\n        ((void**)ptr)[-2] = base;\n\n        return (void*)ptr;\n    }\n    return base;\n}\n\n/* Initializes malloc debugging framework.\n * See comments on MallocDebugInit in malloc_debug_common.h\n */\nint malloc_debug_initialize(void)\n{\n    // We don't really have anything that requires initialization here.\n    return 0;\n}"
    },
    {
        "patch_model": "Replace",
        "change_type": "Assignment",
        "line_old": 256,
        "line_new": 256,
        "critical_vars": [
            "cmd"
        ],
        "function": "*php_escape_shell_cmd",
        "filename": "php-src/CVE-2016-1904/CVE-2016-1904_CWE-189_2871c70efaaaa0f102557a17c727fd4d5204dd4b_exec.c.diff",
        "label": "True",
        "function_code": "PHPAPI zend_string *php_escape_shell_cmd(char *str)\n{\n\tregister int x, y, l = (int)strlen(str);\n\tsize_t estimate = (2 * l) + 1;\n\tzend_string *cmd;\n#ifndef PHP_WIN32\n\tchar *p = NULL;\n#endif\n\n\n\tcmd = zend_string_safe_alloc(2, l, 0, 0);\n\n\tfor (x = 0, y = 0; x < l; x++) {\n\t\tint mb_len = php_mblen(str + x, (l - x));\n\n\t\t/* skip non-valid multibyte characters */\n\t\tif (mb_len < 0) {\n\t\t\tcontinue;\n\t\t} else if (mb_len > 1) {\n\t\t\tmemcpy(ZSTR_VAL(cmd) + y, str + x, mb_len);\n\t\t\ty += mb_len;\n\t\t\tx += mb_len - 1;\n\t\t\tcontinue;\n\t\t}\n\n\t\tswitch (str[x]) {\n#ifndef PHP_WIN32\n\t\t\tcase '\"':\n\t\t\tcase '\\'':\n\t\t\t\tif (!p && (p = memchr(str + x + 1, str[x], l - x - 1))) {\n\t\t\t\t\t/* noop */\n\t\t\t\t} else if (p && *p == str[x]) {\n\t\t\t\t\tp = NULL;\n\t\t\t\t} else {\n\t\t\t\t\tZSTR_VAL(cmd)[y++] = '\\\\';\n\t\t\t\t}\n\t\t\t\tZSTR_VAL(cmd)[y++] = str[x];\n\t\t\t\tbreak;\n#else\n\t\t\t/* % is Windows specific for environmental variables, ^%PATH% will \n\t\t\t\toutput PATH while ^%PATH^% will not. escapeshellcmd->val will escape all % and !.\n\t\t\t*/\n\t\t\tcase '%':\n\t\t\tcase '!':\n\t\t\tcase '\"':\n\t\t\tcase '\\'':\n#endif\n\t\t\tcase '#': /* This is character-set independent */\n\t\t\tcase '&':\n\t\t\tcase ';':\n\t\t\tcase '`':\n\t\t\tcase '|':\n\t\t\tcase '*':\n\t\t\tcase '?':\n\t\t\tcase '~':\n\t\t\tcase '<':\n\t\t\tcase '>':\n\t\t\tcase '^':\n\t\t\tcase '(':\n\t\t\tcase ')':\n\t\t\tcase '[':\n\t\t\tcase ']':\n\t\t\tcase '{':\n\t\t\tcase '}':\n\t\t\tcase '$':\n\t\t\tcase '\\\\':\n\t\t\tcase '\\x0A': /* excluding these two */\n\t\t\tcase '\\xFF':\n#ifdef PHP_WIN32\n\t\t\t\tZSTR_VAL(cmd)[y++] = '^';\n#else\n\t\t\t\tZSTR_VAL(cmd)[y++] = '\\\\';\n#endif\n\t\t\t\t/* fall-through */\n\t\t\tdefault:\n\t\t\t\tZSTR_VAL(cmd)[y++] = str[x];\n\n\t\t}\n\t}\n\tZSTR_VAL(cmd)[y] = '\\0';\n\n\tif ((estimate - y) > 4096) {\n\t\t/* realloc if the estimate was way overill\n\t\t * Arbitrary cutoff point of 4096 */\n\t\tcmd = zend_string_truncate(cmd, y, 0);\n\t}\n\n\tZSTR_LEN(cmd) = y;\n\n\treturn cmd;\n}"
    },
    {
        "patch_model": "Replace",
        "change_type": "Assignment",
        "line_old": 348,
        "line_new": 348,
        "critical_vars": [
            "cmd"
        ],
        "function": "*php_escape_shell_arg",
        "filename": "php-src/CVE-2016-1904/CVE-2016-1904_CWE-189_2871c70efaaaa0f102557a17c727fd4d5204dd4b_exec.c.diff",
        "label": "True",
        "function_code": "PHPAPI zend_string *php_escape_shell_arg(char *str)\n{\n\tint x, y = 0, l = (int)strlen(str);\n\tzend_string *cmd;\n\tsize_t estimate = (4 * l) + 3;\n\n\n\tcmd = zend_string_safe_alloc(4, l, 2, 0); /* worst case */\n\n#ifdef PHP_WIN32\n\tZSTR_VAL(cmd)[y++] = '\"';\n#else\n\tZSTR_VAL(cmd)[y++] = '\\'';\n#endif\n\n\tfor (x = 0; x < l; x++) {\n\t\tint mb_len = php_mblen(str + x, (l - x));\n\n\t\t/* skip non-valid multibyte characters */\n\t\tif (mb_len < 0) {\n\t\t\tcontinue;\n\t\t} else if (mb_len > 1) {\n\t\t\tmemcpy(ZSTR_VAL(cmd) + y, str + x, mb_len);\n\t\t\ty += mb_len;\n\t\t\tx += mb_len - 1;\n\t\t\tcontinue;\n\t\t}\n\n\t\tswitch (str[x]) {\n#ifdef PHP_WIN32\n\t\tcase '\"':\n\t\tcase '%':\n\t\tcase '!':\n\t\t\tZSTR_VAL(cmd)[y++] = ' ';\n\t\t\tbreak;\n#else\n\t\tcase '\\'':\n\t\t\tZSTR_VAL(cmd)[y++] = '\\'';\n\t\t\tZSTR_VAL(cmd)[y++] = '\\\\';\n\t\t\tZSTR_VAL(cmd)[y++] = '\\'';\n#endif\n\t\t\t/* fall-through */\n\t\tdefault:\n\t\t\tZSTR_VAL(cmd)[y++] = str[x];\n\t\t}\n\t}\n#ifdef PHP_WIN32\n\tif (y > 0 && '\\\\' == ZSTR_VAL(cmd)[y - 1]) {\n\t\tint k = 0, n = y - 1;\n\t\tfor (; n >= 0 && '\\\\' == ZSTR_VAL(cmd)[n]; n--, k++);\n\t\tif (k % 2) {\n\t\t\tZSTR_VAL(cmd)[y++] = '\\\\';\n\t\t}\n\t}\n\n\tZSTR_VAL(cmd)[y++] = '\"';\n#else\n\tZSTR_VAL(cmd)[y++] = '\\'';\n#endif\n\tZSTR_VAL(cmd)[y] = '\\0';\n\n\tif ((estimate - y) > 4096) {\n\t\t/* realloc if the estimate was way overill\n\t\t * Arbitrary cutoff point of 4096 */\n\t\tcmd = zend_string_truncate(cmd, y, 0);\n\t}\n\tZSTR_LEN(cmd) = y;\n\treturn cmd;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 823,
        "critical_vars": [
            "q"
        ],
        "function": "cdf_read_property_info",
        "filename": "php-src/CVE-2014-3587/CVE-2014-3587_CWE-189_7ba1409a1aee5925180de546057ddd84ff267947_cdf.c.diff",
        "label": "True",
        "function_code": "\nint\ncdf_read_property_info(const cdf_stream_t *sst, const cdf_header_t *h,\n    uint32_t offs, cdf_property_info_t **info, size_t *count, size_t *maxcount)\n{\n\tconst cdf_section_header_t *shp;\n\tcdf_section_header_t sh;\n\tconst uint8_t *p, *q, *e;\n\tint16_t s16;\n\tint32_t s32;\n\tuint32_t u32;\n\tint64_t s64;\n\tuint64_t u64;\n\tcdf_timestamp_t tp;\n\tsize_t i, o, o4, nelements, j;\n\tcdf_property_info_t *inp;\n\n\tif (offs > UINT32_MAX / 4) {\n\t\terrno = EFTYPE;\n\t\tgoto out;\n\t}\n\tshp = CAST(const cdf_section_header_t *, (const void *)\n\t    ((const char *)sst->sst_tab + offs));\n\tif (cdf_check_stream_offset(sst, h, shp, sizeof(*shp), __LINE__) == -1)\n\t\tgoto out;\n\tsh.sh_len = CDF_TOLE4(shp->sh_len);\n#define CDF_SHLEN_LIMIT (UINT32_MAX / 8)\n\tif (sh.sh_len > CDF_SHLEN_LIMIT) {\n\t\terrno = EFTYPE;\n\t\tgoto out;\n\t}\n\tsh.sh_properties = CDF_TOLE4(shp->sh_properties);\n#define CDF_PROP_LIMIT (UINT32_MAX / (4 * sizeof(*inp)))\n\tif (sh.sh_properties > CDF_PROP_LIMIT)\n\t\tgoto out;\n\tDPRINTF((\"section len: %u properties %u\\n\", sh.sh_len,\n\t    sh.sh_properties));\n\tif (*maxcount) {\n\t\tif (*maxcount > CDF_PROP_LIMIT)\n\t\t\tgoto out;\n\t\t*maxcount += sh.sh_properties;\n\t\tinp = CAST(cdf_property_info_t *,\n\t\t    realloc(*info, *maxcount * sizeof(*inp)));\n\t} else {\n\t\t*maxcount = sh.sh_properties;\n\t\tinp = CAST(cdf_property_info_t *,\n\t\t    malloc(*maxcount * sizeof(*inp)));\n\t}\n\tif (inp == NULL)\n\t\tgoto out;\n\t*info = inp;\n\tinp += *count;\n\t*count += sh.sh_properties;\n\tp = CAST(const uint8_t *, (const void *)\n\t    ((const char *)(const void *)sst->sst_tab +\n\t    offs + sizeof(sh)));\n\te = CAST(const uint8_t *, (const void *)\n\t    (((const char *)(const void *)shp) + sh.sh_len));\n\tif (cdf_check_stream_offset(sst, h, e, 0, __LINE__) == -1)\n\t\tgoto out;\n\tfor (i = 0; i < sh.sh_properties; i++) {\n\t\tsize_t ofs, tail = (i << 1) + 1;\n\t\tif (cdf_check_stream_offset(sst, h, p, tail * sizeof(uint32_t),\n\t\t    __LINE__) == -1)\n\t\t\tgoto out;\n\t\tofs = CDF_GETUINT32(p, tail);\n\t\tq = (const uint8_t *)(const void *)\n\t\t    ((const char *)(const void *)p + ofs\n\t\t    - 2 * sizeof(uint32_t));\n\t\tif (q < p || q > e) {\n\t\t\tDPRINTF((\"Ran of the end %p > %p\\n\", q, e));\n\t\t\tgoto out;\n\t\t}\n\t\tinp[i].pi_id = CDF_GETUINT32(p, i << 1);\n\t\tinp[i].pi_type = CDF_GETUINT32(q, 0);\n\t\tDPRINTF((\"%\" SIZE_T_FORMAT \"u) id=%x type=%x offs=0x%tx,0x%x\\n\",\n\t\t    i, inp[i].pi_id, inp[i].pi_type, q - p, offs));\n\t\tif (inp[i].pi_type & CDF_VECTOR) {\n\t\t\tnelements = CDF_GETUINT32(q, 1);\n\t\t\tif (nelements == 0) {\n\t\t\t\tDPRINTF((\"CDF_VECTOR with nelements == 0\\n\"));\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\to = 2;\n\t\t} else {\n\t\t\tnelements = 1;\n\t\t\to = 1;\n\t\t}\n\t\to4 = o * sizeof(uint32_t);\n\t\tif (inp[i].pi_type & (CDF_ARRAY|CDF_BYREF|CDF_RESERVED))\n\t\t\tgoto unknown;\n\t\tswitch (inp[i].pi_type & CDF_TYPEMASK) {\n\t\tcase CDF_NULL:\n\t\tcase CDF_EMPTY:\n\t\t\tbreak;\n\t\tcase CDF_SIGNED16:\n\t\t\tif (inp[i].pi_type & CDF_VECTOR)\n\t\t\t\tgoto unknown;\n\t\t\t(void)memcpy(&s16, &q[o4], sizeof(s16));\n\t\t\tinp[i].pi_s16 = CDF_TOLE2(s16);\n\t\t\tbreak;\n\t\tcase CDF_SIGNED32:\n\t\t\tif (inp[i].pi_type & CDF_VECTOR)\n\t\t\t\tgoto unknown;\n\t\t\t(void)memcpy(&s32, &q[o4], sizeof(s32));\n\t\t\tinp[i].pi_s32 = CDF_TOLE4((uint32_t)s32);\n\t\t\tbreak;\n\t\tcase CDF_BOOL:\n\t\tcase CDF_UNSIGNED32:\n\t\t\tif (inp[i].pi_type & CDF_VECTOR)\n\t\t\t\tgoto unknown;\n\t\t\t(void)memcpy(&u32, &q[o4], sizeof(u32));\n\t\t\tinp[i].pi_u32 = CDF_TOLE4(u32);\n\t\t\tbreak;\n\t\tcase CDF_SIGNED64:\n\t\t\tif (inp[i].pi_type & CDF_VECTOR)\n\t\t\t\tgoto unknown;\n\t\t\t(void)memcpy(&s64, &q[o4], sizeof(s64));\n\t\t\tinp[i].pi_s64 = CDF_TOLE8((uint64_t)s64);\n\t\t\tbreak;\n\t\tcase CDF_UNSIGNED64:\n\t\t\tif (inp[i].pi_type & CDF_VECTOR)\n\t\t\t\tgoto unknown;\n\t\t\t(void)memcpy(&u64, &q[o4], sizeof(u64));\n\t\t\tinp[i].pi_u64 = CDF_TOLE8((uint64_t)u64);\n\t\t\tbreak;\n\t\tcase CDF_FLOAT:\n\t\t\tif (inp[i].pi_type & CDF_VECTOR)\n\t\t\t\tgoto unknown;\n\t\t\t(void)memcpy(&u32, &q[o4], sizeof(u32));\n\t\t\tu32 = CDF_TOLE4(u32);\n\t\t\tmemcpy(&inp[i].pi_f, &u32, sizeof(inp[i].pi_f));\n\t\t\tbreak;\n\t\tcase CDF_DOUBLE:\n\t\t\tif (inp[i].pi_type & CDF_VECTOR)\n\t\t\t\tgoto unknown;\n\t\t\t(void)memcpy(&u64, &q[o4], sizeof(u64));\n\t\t\tu64 = CDF_TOLE8((uint64_t)u64);\n\t\t\tmemcpy(&inp[i].pi_d, &u64, sizeof(inp[i].pi_d));\n\t\t\tbreak;\n\t\tcase CDF_LENGTH32_STRING:\n\t\tcase CDF_LENGTH32_WSTRING:\n\t\t\tif (nelements > 1) {\n\t\t\t\tsize_t nelem = inp - *info;\n\t\t\t\tif (*maxcount > CDF_PROP_LIMIT\n\t\t\t\t    || nelements > CDF_PROP_LIMIT)\n\t\t\t\t\tgoto out;\n\t\t\t\t*maxcount += nelements;\n\t\t\t\tinp = CAST(cdf_property_info_t *,\n\t\t\t\t    realloc(*info, *maxcount * sizeof(*inp)));\n\t\t\t\tif (inp == NULL)\n\t\t\t\t\tgoto out;\n\t\t\t\t*info = inp;\n\t\t\t\tinp = *info + nelem;\n\t\t\t}\n\t\t\tDPRINTF((\"nelements = %\" SIZE_T_FORMAT \"u\\n\",\n\t\t\t    nelements));\n\t\t\tfor (j = 0; j < nelements && i < sh.sh_properties; \n\t\t\t    j++, i++) \n\t\t\t{\n\t\t\t\tuint32_t l = CDF_GETUINT32(q, o);\n\t\t\t\tinp[i].pi_str.s_len = l;\n\t\t\t\tinp[i].pi_str.s_buf = (const char *)\n\t\t\t\t    (const void *)(&q[o4 + sizeof(l)]);\n\t\t\t\tDPRINTF((\"l = %d, r = %\" SIZE_T_FORMAT\n\t\t\t\t    \"u, s = %s\\n\", l,\n\t\t\t\t    CDF_ROUND(l, sizeof(l)),\n\t\t\t\t    inp[i].pi_str.s_buf));\n\t\t\t\tif (l & 1)\n\t\t\t\t\tl++;\n\t\t\t\to += l >> 1;\n\t\t\t\tif (q + o >= e)\n\t\t\t\t\tgoto out;\n\t\t\t\to4 = o * sizeof(uint32_t);\n\t\t\t}\n\t\t\ti--;\n\t\t\tbreak;\n\t\tcase CDF_FILETIME:\n\t\t\tif (inp[i].pi_type & CDF_VECTOR)\n\t\t\t\tgoto unknown;\n\t\t\t(void)memcpy(&tp, &q[o4], sizeof(tp));\n\t\t\tinp[i].pi_tp = CDF_TOLE8((uint64_t)tp);\n\t\t\tbreak;\n\t\tcase CDF_CLIPBOARD:\n\t\t\tif (inp[i].pi_type & CDF_VECTOR)\n\t\t\t\tgoto unknown;\n\t\t\tbreak;\n\t\tdefault:\n\t\tunknown:\n\t\t\tDPRINTF((\"Don't know how to deal with %x\\n\",\n\t\t\t    inp[i].pi_type));\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn 0;\nout:\n\tfree(*info);\n\treturn -1;\n}"
    },
    {
        "patch_model": "Replace",
        "change_type": "if-Condition",
        "line_old": 382,
        "line_new": 386,
        "critical_vars": [
            "n"
        ],
        "function": "malloc",
        "filename": "bdwgc/CVE-2012-2673/CVE-2012-2673_CWE-189_6a93f8e5bcad22137f41b6c60a1c7384baaec2b3_malloc.c.diff",
        "label": "True",
        "function_code": "\nvoid * malloc(size_t lb)\n{\n    /* It might help to manually inline the GC_malloc call here.        */\n    /* But any decent compiler should reduce the extra procedure call   */\n    /* to at most a jump instruction in this case.                      */\n#   if defined(I386) && defined(GC_SOLARIS_THREADS)\n      /*\n       * Thread initialisation can call malloc before\n       * we're ready for it.\n       * It's not clear that this is enough to help matters.\n       * The thread implementation may well call malloc at other\n       * inopportune times.\n       */\n      if (!EXPECT(GC_is_initialized, TRUE)) return sbrk(lb);\n#   endif /* I386 && GC_SOLARIS_THREADS */\n    return((void *)REDIRECT_MALLOC(lb));\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 380,
        "critical_vars": [
            "lb",
            "n"
        ],
        "function": "malloc",
        "filename": "bdwgc/CVE-2012-2673/CVE-2012-2673_CWE-189_e10c1eb9908c2774c16b3148b30d2f3823d66a9a_malloc.c.diff",
        "label": "True",
        "function_code": "\nvoid * malloc(size_t lb)\n{\n    /* It might help to manually inline the GC_malloc call here.        */\n    /* But any decent compiler should reduce the extra procedure call   */\n    /* to at most a jump instruction in this case.                      */\n#   if defined(I386) && defined(GC_SOLARIS_THREADS)\n      /*\n       * Thread initialisation can call malloc before\n       * we're ready for it.\n       * It's not clear that this is enough to help matters.\n       * The thread implementation may well call malloc at other\n       * inopportune times.\n       */\n      if (!EXPECT(GC_is_initialized, TRUE)) return sbrk(lb);\n#   endif /* I386 && GC_SOLARIS_THREADS */\n    return((void *)REDIRECT_MALLOC(lb));\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "if-Condition",
        "line_old": 386,
        "critical_vars": [
            "lb",
            "n"
        ],
        "function": "malloc",
        "filename": "bdwgc/CVE-2012-2673/CVE-2012-2673_CWE-189_83231d0ab5ed60015797c3d1ad9056295ac3b2bb_malloc.c.diff",
        "label": "False",
        "function_code": "\nvoid * malloc(size_t lb)\n{\n    /* It might help to manually inline the GC_malloc call here.        */\n    /* But any decent compiler should reduce the extra procedure call   */\n    /* to at most a jump instruction in this case.                      */\n#   if defined(I386) && defined(GC_SOLARIS_THREADS)\n      /*\n       * Thread initialisation can call malloc before\n       * we're ready for it.\n       * It's not clear that this is enough to help matters.\n       * The thread implementation may well call malloc at other\n       * inopportune times.\n       */\n      if (!EXPECT(GC_is_initialized, TRUE)) return sbrk(lb);\n#   endif /* I386 && GC_SOLARIS_THREADS */\n    return((void *)REDIRECT_MALLOC(lb));\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 172,
        "critical_vars": [
            "lb_rounded"
        ],
        "function": "GC_generic_malloc",
        "filename": "bdwgc/CVE-2012-2673/CVE-2012-2673_CWE-189_be9df82919960214ee4b9d3313523bff44fd99e1_malloc.c.diff",
        "label": "True",
        "function_code": "\nGC_API void * GC_CALL GC_generic_malloc(size_t lb, int k)\n{\n    void * result;\n    DCL_LOCK_STATE;\n\n    if (EXPECT(GC_have_errors, FALSE))\n      GC_print_all_errors();\n    GC_INVOKE_FINALIZERS();\n    if (SMALL_OBJ(lb)) {\n        LOCK();\n        result = GC_generic_malloc_inner((word)lb, k);\n        UNLOCK();\n    } else {\n        size_t lg;\n        size_t lb_rounded;\n        word n_blocks;\n        GC_bool init;\n        lg = ROUNDED_UP_GRANULES(lb);\n        lb_rounded = GRANULES_TO_BYTES(lg);\n        if (lb_rounded < lb)\n            return((*GC_get_oom_fn())(lb));\n        n_blocks = OBJ_SZ_TO_BLOCKS(lb_rounded);\n        init = GC_obj_kinds[k].ok_init;\n        LOCK();\n        result = (ptr_t)GC_alloc_large(lb_rounded, k, 0);\n        if (0 != result) {\n          if (GC_debugging_started) {\n            BZERO(result, n_blocks * HBLKSIZE);\n          } else {\n#           ifdef THREADS\n              /* Clear any memory that might be used for GC descriptors */\n              /* before we release the lock.                            */\n                ((word *)result)[0] = 0;\n                ((word *)result)[1] = 0;\n                ((word *)result)[GRANULES_TO_WORDS(lg)-1] = 0;\n                ((word *)result)[GRANULES_TO_WORDS(lg)-2] = 0;\n#           endif\n          }\n        }\n        GC_bytes_allocd += lb_rounded;\n        UNLOCK();\n        if (init && !GC_debugging_started && 0 != result) {\n            BZERO(result, n_blocks * HBLKSIZE);\n        }\n    }\n    if (0 == result) {\n        return((*GC_get_oom_fn())(lb));\n    } else {\n        return(result);\n    }\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 185,
        "critical_vars": [
            "lb_rounded"
        ],
        "function": "GC_generic_malloc_ignore_off_page",
        "filename": "bdwgc/CVE-2012-2673/CVE-2012-2673_CWE-189_be9df82919960214ee4b9d3313523bff44fd99e1_mallocx.c.diff",
        "label": "True",
        "function_code": "GC_INNER void * GC_generic_malloc_ignore_off_page(size_t lb, int k)\n{\n    void *result;\n    size_t lg;\n    size_t lb_rounded;\n    word n_blocks;\n    GC_bool init;\n    DCL_LOCK_STATE;\n\n    if (SMALL_OBJ(lb))\n        return(GC_generic_malloc((word)lb, k));\n    lg = ROUNDED_UP_GRANULES(lb);\n    lb_rounded = GRANULES_TO_BYTES(lg);\n    if (lb_rounded < lb)\n        return((*GC_get_oom_fn())(lb));\n    n_blocks = OBJ_SZ_TO_BLOCKS(lb_rounded);\n    init = GC_obj_kinds[k].ok_init;\n    if (EXPECT(GC_have_errors, FALSE))\n      GC_print_all_errors();\n    GC_INVOKE_FINALIZERS();\n    LOCK();\n    result = (ptr_t)GC_alloc_large(ADD_SLOP(lb), k, IGNORE_OFF_PAGE);\n    if (0 != result) {\n        if (GC_debugging_started) {\n            BZERO(result, n_blocks * HBLKSIZE);\n        } else {\n#           ifdef THREADS\n              /* Clear any memory that might be used for GC descriptors */\n              /* before we release the lock.                          */\n                ((word *)result)[0] = 0;\n                ((word *)result)[1] = 0;\n                ((word *)result)[GRANULES_TO_WORDS(lg)-1] = 0;\n                ((word *)result)[GRANULES_TO_WORDS(lg)-2] = 0;\n#           endif\n        }\n    }\n    GC_bytes_allocd += lb_rounded;\n    if (0 == result) {\n        GC_oom_func oom_fn = GC_oom_fn;\n        UNLOCK();\n        return((*oom_fn)(lb));\n    } else {\n        UNLOCK();\n        if (init && !GC_debugging_started) {\n            BZERO(result, n_blocks * HBLKSIZE);\n        }\n        return(result);\n    }\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 509,
        "critical_vars": [
            "bits_left"
        ],
        "function": "lzxd_decompress",
        "filename": "libmspack/CVE-2015-4471/CVE-2015-4471_CWE-189_18b6a2cc0b87536015bedd4f7763e6b02d5aa4f3_lzxd.c.diff",
        "label": "True",
        "function_code": "\nint lzxd_decompress(struct lzxd_stream *lzx, off_t out_bytes) {\n  /* bitstream and huffman reading variables */\n  register unsigned int bit_buffer;\n  register int bits_left, i=0;\n  unsigned char *i_ptr, *i_end;\n  register unsigned short sym;\n\n  int match_length, length_footer, extra, verbatim_bits, bytes_todo;\n  int this_run, main_element, aligned_bits, j;\n  unsigned char *window, *runsrc, *rundest, buf[12];\n  unsigned int frame_size=0, end_frame, match_offset, window_posn;\n  unsigned int R0, R1, R2;\n\n  /* easy answers */\n  if (!lzx || (out_bytes < 0)) return MSPACK_ERR_ARGS;\n  if (lzx->error) return lzx->error;\n\n  /* flush out any stored-up bytes before we begin */\n  i = lzx->o_end - lzx->o_ptr;\n  if ((off_t) i > out_bytes) i = (int) out_bytes;\n  if (i) {\n    if (lzx->sys->write(lzx->output, lzx->o_ptr, i) != i) {\n      return lzx->error = MSPACK_ERR_WRITE;\n    }\n    lzx->o_ptr  += i;\n    lzx->offset += i;\n    out_bytes   -= i;\n  }\n  if (out_bytes == 0) return MSPACK_ERR_OK;\n\n  /* restore local state */\n  RESTORE_BITS;\n  window = lzx->window;\n  window_posn = lzx->window_posn;\n  R0 = lzx->R0;\n  R1 = lzx->R1;\n  R2 = lzx->R2;\n\n  end_frame = (unsigned int)((lzx->offset + out_bytes) / LZX_FRAME_SIZE) + 1;\n\n  while (lzx->frame < end_frame) {\n    /* have we reached the reset interval? (if there is one?) */\n    if (lzx->reset_interval && ((lzx->frame % lzx->reset_interval) == 0)) {\n      if (lzx->block_remaining) {\n\tD((\"%d bytes remaining at reset interval\", lzx->block_remaining))\n\treturn lzx->error = MSPACK_ERR_DECRUNCH;\n      }\n\n      /* re-read the intel header and reset the huffman lengths */\n      lzxd_reset_state(lzx);\n      R0 = lzx->R0;\n      R1 = lzx->R1;\n      R2 = lzx->R2;\n    }\n\n    /* LZX DELTA format has chunk_size, not present in LZX format */\n    if (lzx->is_delta) {\n      ENSURE_BITS(16);\n      REMOVE_BITS(16);\n    }\n\n    /* read header if necessary */\n    if (!lzx->header_read) {\n      /* read 1 bit. if bit=0, intel filesize = 0.\n       * if bit=1, read \n... (function end not found)"
    },
    {
        "patch_model": "Delete",
        "change_type": "if-Condition",
        "line_old": 510,
        "critical_vars": [
            "bits_left"
        ],
        "function": "lzxd_decompress",
        "filename": "libmspack/CVE-2015-4471/CVE-2015-4471_CWE-189_18b6a2cc0b87536015bedd4f7763e6b02d5aa4f3_lzxd.c.diff",
        "label": "False",
        "function_code": "\nint lzxd_decompress(struct lzxd_stream *lzx, off_t out_bytes) {\n  /* bitstream and huffman reading variables */\n  register unsigned int bit_buffer;\n  register int bits_left, i=0;\n  unsigned char *i_ptr, *i_end;\n  register unsigned short sym;\n\n  int match_length, length_footer, extra, verbatim_bits, bytes_todo;\n  int this_run, main_element, aligned_bits, j;\n  unsigned char *window, *runsrc, *rundest, buf[12];\n  unsigned int frame_size=0, end_frame, match_offset, window_posn;\n  unsigned int R0, R1, R2;\n\n  /* easy answers */\n  if (!lzx || (out_bytes < 0)) return MSPACK_ERR_ARGS;\n  if (lzx->error) return lzx->error;\n\n  /* flush out any stored-up bytes before we begin */\n  i = lzx->o_end - lzx->o_ptr;\n  if ((off_t) i > out_bytes) i = (int) out_bytes;\n  if (i) {\n    if (lzx->sys->write(lzx->output, lzx->o_ptr, i) != i) {\n      return lzx->error = MSPACK_ERR_WRITE;\n    }\n    lzx->o_ptr  += i;\n    lzx->offset += i;\n    out_bytes   -= i;\n  }\n  if (out_bytes == 0) return MSPACK_ERR_OK;\n\n  /* restore local state */\n  RESTORE_BITS;\n  window = lzx->window;\n  window_posn = lzx->window_posn;\n  R0 = lzx->R0;\n  R1 = lzx->R1;\n  R2 = lzx->R2;\n\n  end_frame = (unsigned int)((lzx->offset + out_bytes) / LZX_FRAME_SIZE) + 1;\n\n  while (lzx->frame < end_frame) {\n    /* have we reached the reset interval? (if there is one?) */\n    if (lzx->reset_interval && ((lzx->frame % lzx->reset_interval) == 0)) {\n      if (lzx->block_remaining) {\n\tD((\"%d bytes remaining at reset interval\", lzx->block_remaining))\n\treturn lzx->error = MSPACK_ERR_DECRUNCH;\n      }\n\n      /* re-read the intel header and reset the huffman lengths */\n      lzxd_reset_state(lzx);\n      R0 = lzx->R0;\n      R1 = lzx->R1;\n      R2 = lzx->R2;\n    }\n\n    /* LZX DELTA format has chunk_size, not present in LZX format */\n    if (lzx->is_delta) {\n      ENSURE_BITS(16);\n      REMOVE_BITS(16);\n    }\n\n    /* read header if necessary */\n    if (!lzx->header_read) {\n      /* read 1 bit. if bit=0, intel filesize = 0.\n       * if bit=1, read \n... (function end not found)"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 2021,
        "critical_vars": [
            "flags"
        ],
        "function": "nedpmalloc",
        "filename": "nedmalloc/CVE-2012-2675/CVE-2012-2675_CWE-189_2965eca30c408c13473c4146a9d47d547d288db1_nedmalloc.c.diff",
        "label": "False",
        "function_code": "Function not found"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 2021,
        "critical_vars": [
            "bytes"
        ],
        "function": "nedpmalloc",
        "filename": "nedmalloc/CVE-2012-2675/CVE-2012-2675_CWE-189_2965eca30c408c13473c4146a9d47d547d288db1_nedmalloc.c.diff",
        "label": "True",
        "function_code": "Function not found"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 2023,
        "critical_vars": [
            "no",
            "bytes",
            "size"
        ],
        "function": "nedpmalloc",
        "filename": "nedmalloc/CVE-2012-2675/CVE-2012-2675_CWE-189_2965eca30c408c13473c4146a9d47d547d288db1_nedmalloc.c.diff",
        "label": "True",
        "function_code": "Function not found"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 2025,
        "critical_vars": [
            "flags"
        ],
        "function": "nedpmalloc",
        "filename": "nedmalloc/CVE-2012-2675/CVE-2012-2675_CWE-189_2965eca30c408c13473c4146a9d47d547d288db1_nedmalloc.c.diff",
        "label": "True",
        "function_code": "Function not found"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 331,
        "critical_vars": [
            "size"
        ],
        "function": "*CallMalloc",
        "filename": "nedmalloc/CVE-2012-2675/CVE-2012-2675_CWE-189_1a759756639ab7543b650a10c2d77a0ffc7a2000_nedmalloc.c.diff",
        "label": "False",
        "function_code": "Function not found"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 331,
        "critical_vars": [
            "bytes"
        ],
        "function": "*CallMalloc",
        "filename": "nedmalloc/CVE-2012-2675/CVE-2012-2675_CWE-189_1a759756639ab7543b650a10c2d77a0ffc7a2000_nedmalloc.c.diff",
        "label": "True",
        "function_code": "Function not found"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 333,
        "critical_vars": [
            "bytes"
        ],
        "function": "*CallMalloc",
        "filename": "nedmalloc/CVE-2012-2675/CVE-2012-2675_CWE-189_1a759756639ab7543b650a10c2d77a0ffc7a2000_nedmalloc.c.diff",
        "label": "True",
        "function_code": "Function not found"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 335,
        "critical_vars": [
            "size"
        ],
        "function": "*CallMalloc",
        "filename": "nedmalloc/CVE-2012-2675/CVE-2012-2675_CWE-189_1a759756639ab7543b650a10c2d77a0ffc7a2000_nedmalloc.c.diff",
        "label": "True",
        "function_code": "Function not found"
    },
    {
        "patch_model": "Delete",
        "change_type": "Fun-Call",
        "line_old": 201,
        "critical_vars": [
            "mobj",
            "size"
        ],
        "function": "*alloc_ta_mem",
        "filename": "optee_os/CVE-2019-1010294/CVE-2019-1010294_CWE-189_7e768f8a473409215fe3fff8f6e31f8a3a0103c6_user_ta.c.diff",
        "label": "False",
        "function_code": "\nstatic struct mobj *alloc_ta_mem(size_t size)\n{\n#ifdef CFG_PAGED_USER_TA\n\treturn mobj_paged_alloc(size);\n#else\n\tstruct mobj *mobj = mobj_mm_alloc(mobj_sec_ddr, size, &tee_mm_sec_ddr);\n\n\tif (mobj)\n\t\tmemset(mobj_get_va(mobj, 0), 0, size);\n\treturn mobj;\n#endif\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 201,
        "critical_vars": [
            "granularity"
        ],
        "function": "*alloc_ta_mem",
        "filename": "optee_os/CVE-2019-1010294/CVE-2019-1010294_CWE-189_7e768f8a473409215fe3fff8f6e31f8a3a0103c6_user_ta.c.diff",
        "label": "True",
        "function_code": "\nstatic struct mobj *alloc_ta_mem(size_t size)\n{\n#ifdef CFG_PAGED_USER_TA\n\treturn mobj_paged_alloc(size);\n#else\n\tstruct mobj *mobj = mobj_mm_alloc(mobj_sec_ddr, size, &tee_mm_sec_ddr);\n\n\tif (mobj) {\n\t\tsize_t granularity = BIT(tee_mm_sec_ddr.shift);\n\n\t\t/* Round up to allocation granularity size */\n\t\tmemset(mobj_get_va(mobj, 0), 0, ROUNDUP(size, granularity));\n\t}\n\treturn mobj;\n#endif\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 204,
        "critical_vars": [
            "granularity",
            "mobj",
            "size"
        ],
        "function": "*alloc_ta_mem",
        "filename": "optee_os/CVE-2019-1010294/CVE-2019-1010294_CWE-189_7e768f8a473409215fe3fff8f6e31f8a3a0103c6_user_ta.c.diff",
        "label": "True",
        "function_code": "\nstatic struct mobj *alloc_ta_mem(size_t size)\n{\n#ifdef CFG_PAGED_USER_TA\n\treturn mobj_paged_alloc(size);\n#else\n\tstruct mobj *mobj = mobj_mm_alloc(mobj_sec_ddr, size, &tee_mm_sec_ddr);\n\n\tif (mobj) {\n\t\tsize_t granularity = BIT(tee_mm_sec_ddr.shift);\n\n\t\t/* Round up to allocation granularity size */\n\t\tmemset(mobj_get_va(mobj, 0), 0, ROUNDUP(size, granularity));\n\t}\n\treturn mobj;\n#endif\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "if-Condition",
        "line_old": 366,
        "critical_vars": [
            "length",
            "index"
        ],
        "function": "ByteVector::mid",
        "filename": "taglib/CVE-2012-1584/CVE-2012-1584_CWE-189_dcdf4fd954e3213c355746fa15b7480461972308_tbytevector.cpp.diff",
        "label": "False",
        "function_code": "Function not found"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 366,
        "critical_vars": [
            "length"
        ],
        "function": "ByteVector::mid",
        "filename": "taglib/CVE-2012-1584/CVE-2012-1584_CWE-189_dcdf4fd954e3213c355746fa15b7480461972308_tbytevector.cpp.diff",
        "label": "True",
        "function_code": "Function not found"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 517,
        "critical_vars": [
            "state.sumlen",
            "state.polstr"
        ],
        "function": "parse_tsquery",
        "filename": "postgres/CVE-2014-0064/CVE-2014-0064_CWE-189_31400a673325147e1205326008e32135a78b4d8a_tsquery.c.diff",
        "label": "True",
        "function_code": "TSQuery\nparse_tsquery(char *buf,\n\t\t\t  PushFunction pushval,\n\t\t\t  Datum opaque,\n\t\t\t  bool isplain)\n{\n\tstruct TSQueryParserStateData state;\n\tint\t\t\ti;\n\tTSQuery\t\tquery;\n\tint\t\t\tcommonlen;\n\tQueryItem  *ptr;\n\tListCell   *cell;\n\n\t/* init state */\n\tstate.buffer = buf;\n\tstate.buf = buf;\n\tstate.state = (isplain) ? WAITSINGLEOPERAND : WAITFIRSTOPERAND;\n\tstate.count = 0;\n\tstate.polstr = NIL;\n\n\t/* init value parser's state */\n\tstate.valstate = init_tsvector_parser(state.buffer, true, true);\n\n\t/* init list of operand */\n\tstate.sumlen = 0;\n\tstate.lenop = 64;\n\tstate.curop = state.op = (char *) palloc(state.lenop);\n\t*(state.curop) = '\\0';\n\n\t/* parse query & make polish notation (postfix, but in reverse order) */\n\tmakepol(&state, pushval, opaque);\n\n\tclose_tsvector_parser(state.valstate);\n\n\tif (list_length(state.polstr) == 0)\n\t{\n\t\tereport(NOTICE,\n\t\t\t\t(errmsg(\"text-search query doesn't contain lexemes: \\\"%s\\\"\",\n\t\t\t\t\t\tstate.buffer)));\n\t\tquery = (TSQuery) palloc(HDRSIZETQ);\n\t\tSET_VARSIZE(query, HDRSIZETQ);\n\t\tquery->size = 0;\n\t\treturn query;\n\t}\n\n\tif (TSQUERY_TOO_BIG(list_length(state.polstr), state.sumlen))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t\t errmsg(\"tsquery is too large\")));\n\tcommonlen = COMPUTESIZE(list_length(state.polstr), state.sumlen);\n\n\t/* Pack the QueryItems in the final TSQuery struct to return to caller */\n\tquery = (TSQuery) palloc0(commonlen);\n\tSET_VARSIZE(query, commonlen);\n\tquery->size = list_length(state.polstr);\n\tptr = GETQUERY(query);\n\n\t/* Copy QueryItems to TSQuery */\n\ti = 0;\n\tforeach(cell, state.polstr)\n\t{\n\t\tQueryItem  *item = (QueryItem *) lfirst(cell);\n\n\t\tswitch (item->type)\n\t\t{\n\t\t\tcase QI_VAL:\n\t\t\t\tmemcpy(&ptr[i], item, sizeof(QueryOperand));\n\t\t\t\tbreak;\n\t\t\tcase QI_VALSTOP:\n\t\t\t\tptr[i].type = QI_VALSTOP;\n\t\t\t\tbreak;\n\t\t\tcase QI_OPR:\n\t\t\t\tmemcpy(&ptr[i], item, sizeof(QueryOperator));\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\telog(ERROR, \"unrecognized QueryItem type: %d\", item->type);\n\t\t}\n\t\ti++;\n\t}\n\n\t/* Copy all the operand strings to TSQuery */\n\tmemcpy((void \n... (function end not found)"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 337,
        "critical_vars": [
            "nnode",
            "sumlen"
        ],
        "function": "QTN2QT",
        "filename": "postgres/CVE-2014-0064/CVE-2014-0064_CWE-189_31400a673325147e1205326008e32135a78b4d8a_tsquery_util.c.diff",
        "label": "True",
        "function_code": "\nTSQuery\nQTN2QT(QTNode *in)\n{\n\tTSQuery\t\tout;\n\tint\t\t\tlen;\n\tint\t\t\tsumlen = 0,\n\t\t\t\tnnode = 0;\n\tQTN2QTState state;\n\n\tcntsize(in, &sumlen, &nnode);\n\n\tif (TSQUERY_TOO_BIG(nnode, sumlen))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t\t errmsg(\"tsquery is too large\")));\n\tlen = COMPUTESIZE(nnode, sumlen);\n\n\tout = (TSQuery) palloc0(len);\n\tSET_VARSIZE(out, len);\n\tout->size = nnode;\n\n\tstate.curitem = GETQUERY(out);\n\tstate.operand = state.curoperand = GETOPERAND(out);\n\n\tfillQT(&state, in);\n\treturn out;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 443,
        "critical_vars": [
            "pcount"
        ],
        "function": "hstore_recv",
        "filename": "postgres/CVE-2014-0064/CVE-2014-0064_CWE-189_31400a673325147e1205326008e32135a78b4d8a_hstore_io.c.diff",
        "label": "True",
        "function_code": "Datum\t\thstore_recv(PG_FUNCTION_ARGS);\nDatum\nhstore_recv(PG_FUNCTION_ARGS)\n{\n\tint32\t\tbuflen;\n\tHStore\t   *out;\n\tPairs\t   *pairs;\n\tint32\t\ti;\n\tint32\t\tpcount;\n\tStringInfo\tbuf = (StringInfo) PG_GETARG_POINTER(0);\n\n\tpcount = pq_getmsgint(buf, 4);\n\n\tif (pcount == 0)\n\t{\n\t\tout = hstorePairs(NULL, 0, 0);\n\t\tPG_RETURN_POINTER(out);\n\t}\n\n\tif (pcount < 0 || pcount > MaxAllocSize / sizeof(Pairs))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t  errmsg(\"number of pairs (%d) exceeds the maximum allowed (%d)\",\n\t\t\t\t\t pcount, (int) (MaxAllocSize / sizeof(Pairs)))));\n\tpairs = palloc(pcount * sizeof(Pairs));\n\n\tfor (i = 0; i < pcount; ++i)\n\t{\n\t\tint\t\t\trawlen = pq_getmsgint(buf, 4);\n\t\tint\t\t\tlen;\n\n\t\tif (rawlen < 0)\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_NULL_VALUE_NOT_ALLOWED),\n\t\t\t\t\t errmsg(\"null value not allowed for hstore key\")));\n\n\t\tpairs[i].key = pq_getmsgtext(buf, rawlen, &len);\n\t\tpairs[i].keylen = hstoreCheckKeyLen(len);\n\t\tpairs[i].needfree = true;\n\n\t\trawlen = pq_getmsgint(buf, 4);\n\t\tif (rawlen < 0)\n\t\t{\n\t\t\tpairs[i].val = NULL;\n\t\t\tpairs[i].vallen = 0;\n\t\t\tpairs[i].isnull = true;\n\t\t}\n\t\telse\n\t\t{\n\t\t\tpairs[i].val = pq_getmsgtext(buf, rawlen, &len);\n\t\t\tpairs[i].vallen = hstoreCheckValLen(len);\n\t\t\tpairs[i].isnull = false;\n\t\t}\n\t}\n\n\tpcount = hstoreUniquePairs(pairs, pcount, &buflen);\n\n\tout = hstorePairs(pairs, pcount, buflen);\n\n\tPG_RETURN_POINTER(out);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 444,
        "critical_vars": [
            "Pairs",
            "pcount"
        ],
        "function": "hstore_recv",
        "filename": "postgres/CVE-2014-0064/CVE-2014-0064_CWE-189_31400a673325147e1205326008e32135a78b4d8a_hstore_io.c.diff",
        "label": "True",
        "function_code": "Datum\t\thstore_recv(PG_FUNCTION_ARGS);\nDatum\nhstore_recv(PG_FUNCTION_ARGS)\n{\n\tint32\t\tbuflen;\n\tHStore\t   *out;\n\tPairs\t   *pairs;\n\tint32\t\ti;\n\tint32\t\tpcount;\n\tStringInfo\tbuf = (StringInfo) PG_GETARG_POINTER(0);\n\n\tpcount = pq_getmsgint(buf, 4);\n\n\tif (pcount == 0)\n\t{\n\t\tout = hstorePairs(NULL, 0, 0);\n\t\tPG_RETURN_POINTER(out);\n\t}\n\n\tif (pcount < 0 || pcount > MaxAllocSize / sizeof(Pairs))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t  errmsg(\"number of pairs (%d) exceeds the maximum allowed (%d)\",\n\t\t\t\t\t pcount, (int) (MaxAllocSize / sizeof(Pairs)))));\n\tpairs = palloc(pcount * sizeof(Pairs));\n\n\tfor (i = 0; i < pcount; ++i)\n\t{\n\t\tint\t\t\trawlen = pq_getmsgint(buf, 4);\n\t\tint\t\t\tlen;\n\n\t\tif (rawlen < 0)\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_NULL_VALUE_NOT_ALLOWED),\n\t\t\t\t\t errmsg(\"null value not allowed for hstore key\")));\n\n\t\tpairs[i].key = pq_getmsgtext(buf, rawlen, &len);\n\t\tpairs[i].keylen = hstoreCheckKeyLen(len);\n\t\tpairs[i].needfree = true;\n\n\t\trawlen = pq_getmsgint(buf, 4);\n\t\tif (rawlen < 0)\n\t\t{\n\t\t\tpairs[i].val = NULL;\n\t\t\tpairs[i].vallen = 0;\n\t\t\tpairs[i].isnull = true;\n\t\t}\n\t\telse\n\t\t{\n\t\t\tpairs[i].val = pq_getmsgtext(buf, rawlen, &len);\n\t\t\tpairs[i].vallen = hstoreCheckValLen(len);\n\t\t\tpairs[i].isnull = false;\n\t\t}\n\t}\n\n\tpcount = hstoreUniquePairs(pairs, pcount, &buflen);\n\n\tout = hstorePairs(pairs, pcount, buflen);\n\n\tPG_RETURN_POINTER(out);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 564,
        "critical_vars": [
            "key_count"
        ],
        "function": "hstore_from_arrays",
        "filename": "postgres/CVE-2014-0064/CVE-2014-0064_CWE-189_31400a673325147e1205326008e32135a78b4d8a_hstore_io.c.diff",
        "label": "True",
        "function_code": "Datum\t\thstore_from_arrays(PG_FUNCTION_ARGS);\nDatum\nhstore_from_arrays(PG_FUNCTION_ARGS)\n{\n\tint32\t\tbuflen;\n\tHStore\t   *out;\n\tPairs\t   *pairs;\n\tDatum\t   *key_datums;\n\tbool\t   *key_nulls;\n\tint\t\t\tkey_count;\n\tDatum\t   *value_datums;\n\tbool\t   *value_nulls;\n\tint\t\t\tvalue_count;\n\tArrayType  *key_array;\n\tArrayType  *value_array;\n\tint\t\t\ti;\n\n\tif (PG_ARGISNULL(0))\n\t\tPG_RETURN_NULL();\n\n\tkey_array = PG_GETARG_ARRAYTYPE_P(0);\n\n\tAssert(ARR_ELEMTYPE(key_array) == TEXTOID);\n\n\t/*\n\t * must check >1 rather than != 1 because empty arrays have 0 dimensions,\n\t * not 1\n\t */\n\n\tif (ARR_NDIM(key_array) > 1)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_ARRAY_SUBSCRIPT_ERROR),\n\t\t\t\t errmsg(\"wrong number of array subscripts\")));\n\n\tdeconstruct_array(key_array,\n\t\t\t\t\t  TEXTOID, -1, false, 'i',\n\t\t\t\t\t  &key_datums, &key_nulls, &key_count);\n\n\t/* see discussion in hstoreArrayToPairs() */\n\tif (key_count > MaxAllocSize / sizeof(Pairs))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t  errmsg(\"number of pairs (%d) exceeds the maximum allowed (%d)\",\n\t\t\t\t\t key_count, (int) (MaxAllocSize / sizeof(Pairs)))));\n\n\t/* value_array might be NULL */\n\n\tif (PG_ARGISNULL(1))\n\t{\n\t\tvalue_array = NULL;\n\t\tvalue_count = key_count;\n\t\tvalue_datums = NULL;\n\t\tvalue_nulls = NULL;\n\t}\n\telse\n\t{\n\t\tvalue_array = PG_GETARG_ARRAYTYPE_P(1);\n\n\t\tAssert(ARR_ELEMTYPE(value_array) == TEXTOID);\n\n\t\tif (ARR_NDIM(value_array) > 1)\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_ARRAY_SUBSCRIPT_ERROR),\n\t\t\t\t\t errmsg(\"wrong number of array subscripts\")));\n\n\t\tif ((ARR_NDIM(key_array) > 0 || ARR_NDIM(value_array) > 0) &&\n\t\t\t(ARR_NDIM(key_array) != ARR_NDIM(value_array) ||\n\t\t\t ARR_DIMS(key_array)[0] != ARR_DIMS(value_array)[0] ||\n\t\t\t ARR_LBOUND(key_array)[0] != ARR_LBOUND(value_array)[0]))\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_ARRAY_SUBSCRIPT_ERROR),\n\t\t\t\t\t errmsg(\"arrays must have same bounds\")));\n\n\t\tdeconstruct_array(value_array,\n\t\t\t\t\t\t  TEXTOID, -1, false, 'i',\n\t\t\t\t\t\t  &value_datums, &value_nulls, &value_count);\n\n\t\tAssert(key_count == value_count);\n\t}\n\n\tpairs = palloc(key_count * sizeof(Pairs));\n\n\tfor (i = 0; i < key_count; ++i)\n\t{\n\t\tif (key_nulls[i])\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_NULL_VALUE_NOT_ALLOWED),\n\t\t\t\t\t errmsg(\"null value not allowed for hstore key\")));\n\n\t\tif (!value_nulls || value_nulls[i])\n\t\t{\n\t\t\tpairs[i].key = VARDATA_ANY(key_datums[i]);\n\t\t\tpairs[i].val = NULL;\n\t\t\tpairs[i].keylen = hstoreCheckKeyLen(VARSIZE_ANY_EXHDR(key_datums[i]));\n\t\t\tpairs[i].vallen = 4;\n\t\t\tpairs[i].isnull = true;\n\t\t\tpairs[i].needfree = false;\n\t\t}\n\t\telse\n\t\t{\n\t\t\tpairs[i].key = VARDATA_ANY(key_datums[i]);\n\t\t\tpairs[i].val = VARDATA_ANY(value_datums[i]);\n\t\t\tpairs[i].keylen = hstoreCheckKeyLen(VARSIZE_ANY_EXHDR(key_datums[i]));\n\t\t\tpairs[i].vallen = hstoreCheckValLen(VARSIZE_ANY_EXHDR(value_datums[i]));\n\t\t\tpairs[i].isnull = false;\n\t\t\tpairs[i].needfree = false;\n\t\t}\n\t}\n\n\tkey_count = hstoreUniquePairs(pairs, key_count, &buflen);\n\n\tout = hstorePairs(pairs, key_count, buflen);\n\n\tPG_RETURN_POINTER(out);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 565,
        "critical_vars": [
            "Pairs",
            "key_count"
        ],
        "function": "hstore_from_arrays",
        "filename": "postgres/CVE-2014-0064/CVE-2014-0064_CWE-189_31400a673325147e1205326008e32135a78b4d8a_hstore_io.c.diff",
        "label": "True",
        "function_code": "Datum\t\thstore_from_arrays(PG_FUNCTION_ARGS);\nDatum\nhstore_from_arrays(PG_FUNCTION_ARGS)\n{\n\tint32\t\tbuflen;\n\tHStore\t   *out;\n\tPairs\t   *pairs;\n\tDatum\t   *key_datums;\n\tbool\t   *key_nulls;\n\tint\t\t\tkey_count;\n\tDatum\t   *value_datums;\n\tbool\t   *value_nulls;\n\tint\t\t\tvalue_count;\n\tArrayType  *key_array;\n\tArrayType  *value_array;\n\tint\t\t\ti;\n\n\tif (PG_ARGISNULL(0))\n\t\tPG_RETURN_NULL();\n\n\tkey_array = PG_GETARG_ARRAYTYPE_P(0);\n\n\tAssert(ARR_ELEMTYPE(key_array) == TEXTOID);\n\n\t/*\n\t * must check >1 rather than != 1 because empty arrays have 0 dimensions,\n\t * not 1\n\t */\n\n\tif (ARR_NDIM(key_array) > 1)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_ARRAY_SUBSCRIPT_ERROR),\n\t\t\t\t errmsg(\"wrong number of array subscripts\")));\n\n\tdeconstruct_array(key_array,\n\t\t\t\t\t  TEXTOID, -1, false, 'i',\n\t\t\t\t\t  &key_datums, &key_nulls, &key_count);\n\n\t/* see discussion in hstoreArrayToPairs() */\n\tif (key_count > MaxAllocSize / sizeof(Pairs))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t  errmsg(\"number of pairs (%d) exceeds the maximum allowed (%d)\",\n\t\t\t\t\t key_count, (int) (MaxAllocSize / sizeof(Pairs)))));\n\n\t/* value_array might be NULL */\n\n\tif (PG_ARGISNULL(1))\n\t{\n\t\tvalue_array = NULL;\n\t\tvalue_count = key_count;\n\t\tvalue_datums = NULL;\n\t\tvalue_nulls = NULL;\n\t}\n\telse\n\t{\n\t\tvalue_array = PG_GETARG_ARRAYTYPE_P(1);\n\n\t\tAssert(ARR_ELEMTYPE(value_array) == TEXTOID);\n\n\t\tif (ARR_NDIM(value_array) > 1)\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_ARRAY_SUBSCRIPT_ERROR),\n\t\t\t\t\t errmsg(\"wrong number of array subscripts\")));\n\n\t\tif ((ARR_NDIM(key_array) > 0 || ARR_NDIM(value_array) > 0) &&\n\t\t\t(ARR_NDIM(key_array) != ARR_NDIM(value_array) ||\n\t\t\t ARR_DIMS(key_array)[0] != ARR_DIMS(value_array)[0] ||\n\t\t\t ARR_LBOUND(key_array)[0] != ARR_LBOUND(value_array)[0]))\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_ARRAY_SUBSCRIPT_ERROR),\n\t\t\t\t\t errmsg(\"arrays must have same bounds\")));\n\n\t\tdeconstruct_array(value_array,\n\t\t\t\t\t\t  TEXTOID, -1, false, 'i',\n\t\t\t\t\t\t  &value_datums, &value_nulls, &value_count);\n\n\t\tAssert(key_count == value_count);\n\t}\n\n\tpairs = palloc(key_count * sizeof(Pairs));\n\n\tfor (i = 0; i < key_count; ++i)\n\t{\n\t\tif (key_nulls[i])\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_NULL_VALUE_NOT_ALLOWED),\n\t\t\t\t\t errmsg(\"null value not allowed for hstore key\")));\n\n\t\tif (!value_nulls || value_nulls[i])\n\t\t{\n\t\t\tpairs[i].key = VARDATA_ANY(key_datums[i]);\n\t\t\tpairs[i].val = NULL;\n\t\t\tpairs[i].keylen = hstoreCheckKeyLen(VARSIZE_ANY_EXHDR(key_datums[i]));\n\t\t\tpairs[i].vallen = 4;\n\t\t\tpairs[i].isnull = true;\n\t\t\tpairs[i].needfree = false;\n\t\t}\n\t\telse\n\t\t{\n\t\t\tpairs[i].key = VARDATA_ANY(key_datums[i]);\n\t\t\tpairs[i].val = VARDATA_ANY(value_datums[i]);\n\t\t\tpairs[i].keylen = hstoreCheckKeyLen(VARSIZE_ANY_EXHDR(key_datums[i]));\n\t\t\tpairs[i].vallen = hstoreCheckValLen(VARSIZE_ANY_EXHDR(value_datums[i]));\n\t\t\tpairs[i].isnull = false;\n\t\t\tpairs[i].needfree = false;\n\t\t}\n\t}\n\n\tkey_count = hstoreUniquePairs(pairs, key_count, &buflen);\n\n\tout = hstorePairs(pairs, key_count, buflen);\n\n\tPG_RETURN_POINTER(out);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 693,
        "critical_vars": [
            "count"
        ],
        "function": "hstore_from_array",
        "filename": "postgres/CVE-2014-0064/CVE-2014-0064_CWE-189_31400a673325147e1205326008e32135a78b4d8a_hstore_io.c.diff",
        "label": "True",
        "function_code": "Datum\t\thstore_from_array(PG_FUNCTION_ARGS);\nDatum\nhstore_from_array(PG_FUNCTION_ARGS)\n{\n\tArrayType  *in_array = PG_GETARG_ARRAYTYPE_P(0);\n\tint\t\t\tndims = ARR_NDIM(in_array);\n\tint\t\t\tcount;\n\tint32\t\tbuflen;\n\tHStore\t   *out;\n\tPairs\t   *pairs;\n\tDatum\t   *in_datums;\n\tbool\t   *in_nulls;\n\tint\t\t\tin_count;\n\tint\t\t\ti;\n\n\tAssert(ARR_ELEMTYPE(in_array) == TEXTOID);\n\n\tswitch (ndims)\n\t{\n\t\tcase 0:\n\t\t\tout = hstorePairs(NULL, 0, 0);\n\t\t\tPG_RETURN_POINTER(out);\n\n\t\tcase 1:\n\t\t\tif ((ARR_DIMS(in_array)[0]) % 2)\n\t\t\t\tereport(ERROR,\n\t\t\t\t\t\t(errcode(ERRCODE_ARRAY_SUBSCRIPT_ERROR),\n\t\t\t\t\t\t errmsg(\"array must have even number of elements\")));\n\t\t\tbreak;\n\n\t\tcase 2:\n\t\t\tif ((ARR_DIMS(in_array)[1]) != 2)\n\t\t\t\tereport(ERROR,\n\t\t\t\t\t\t(errcode(ERRCODE_ARRAY_SUBSCRIPT_ERROR),\n\t\t\t\t\t\t errmsg(\"array must have two columns\")));\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_ARRAY_SUBSCRIPT_ERROR),\n\t\t\t\t\t errmsg(\"wrong number of array subscripts\")));\n\t}\n\n\tdeconstruct_array(in_array,\n\t\t\t\t\t  TEXTOID, -1, false, 'i',\n\t\t\t\t\t  &in_datums, &in_nulls, &in_count);\n\n\tcount = in_count / 2;\n\n\t/* see discussion in hstoreArrayToPairs() */\n\tif (count > MaxAllocSize / sizeof(Pairs))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t  errmsg(\"number of pairs (%d) exceeds the maximum allowed (%d)\",\n\t\t\t\t\t count, (int) (MaxAllocSize / sizeof(Pairs)))));\n\n\tpairs = palloc(count * sizeof(Pairs));\n\n\tfor (i = 0; i < count; ++i)\n\t{\n\t\tif (in_nulls[i * 2])\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_NULL_VALUE_NOT_ALLOWED),\n\t\t\t\t\t errmsg(\"null value not allowed for hstore key\")));\n\n\t\tif (in_nulls[i * 2 + 1])\n\t\t{\n\t\t\tpairs[i].key = VARDATA_ANY(in_datums[i * 2]);\n\t\t\tpairs[i].val = NULL;\n\t\t\tpairs[i].keylen = hstoreCheckKeyLen(VARSIZE_ANY_EXHDR(in_datums[i * 2]));\n\t\t\tpairs[i].vallen = 4;\n\t\t\tpairs[i].isnull = true;\n\t\t\tpairs[i].needfree = false;\n\t\t}\n\t\telse\n\t\t{\n\t\t\tpairs[i].key = VARDATA_ANY(in_datums[i * 2]);\n\t\t\tpairs[i].val = VARDATA_ANY(in_datums[i * 2 + 1]);\n\t\t\tpairs[i].keylen = hstoreCheckKeyLen(VARSIZE_ANY_EXHDR(in_datums[i * 2]));\n\t\t\tpairs[i].vallen = hstoreCheckValLen(VARSIZE_ANY_EXHDR(in_datums[i * 2 + 1]));\n\t\t\tpairs[i].isnull = false;\n\t\t\tpairs[i].needfree = false;\n\t\t}\n\t}\n\n\tcount = hstoreUniquePairs(pairs, count, &buflen);\n\n\tout = hstorePairs(pairs, count, buflen);\n\n\tPG_RETURN_POINTER(out);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 694,
        "critical_vars": [
            "Pairs",
            "count"
        ],
        "function": "hstore_from_array",
        "filename": "postgres/CVE-2014-0064/CVE-2014-0064_CWE-189_31400a673325147e1205326008e32135a78b4d8a_hstore_io.c.diff",
        "label": "True",
        "function_code": "Datum\t\thstore_from_array(PG_FUNCTION_ARGS);\nDatum\nhstore_from_array(PG_FUNCTION_ARGS)\n{\n\tArrayType  *in_array = PG_GETARG_ARRAYTYPE_P(0);\n\tint\t\t\tndims = ARR_NDIM(in_array);\n\tint\t\t\tcount;\n\tint32\t\tbuflen;\n\tHStore\t   *out;\n\tPairs\t   *pairs;\n\tDatum\t   *in_datums;\n\tbool\t   *in_nulls;\n\tint\t\t\tin_count;\n\tint\t\t\ti;\n\n\tAssert(ARR_ELEMTYPE(in_array) == TEXTOID);\n\n\tswitch (ndims)\n\t{\n\t\tcase 0:\n\t\t\tout = hstorePairs(NULL, 0, 0);\n\t\t\tPG_RETURN_POINTER(out);\n\n\t\tcase 1:\n\t\t\tif ((ARR_DIMS(in_array)[0]) % 2)\n\t\t\t\tereport(ERROR,\n\t\t\t\t\t\t(errcode(ERRCODE_ARRAY_SUBSCRIPT_ERROR),\n\t\t\t\t\t\t errmsg(\"array must have even number of elements\")));\n\t\t\tbreak;\n\n\t\tcase 2:\n\t\t\tif ((ARR_DIMS(in_array)[1]) != 2)\n\t\t\t\tereport(ERROR,\n\t\t\t\t\t\t(errcode(ERRCODE_ARRAY_SUBSCRIPT_ERROR),\n\t\t\t\t\t\t errmsg(\"array must have two columns\")));\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_ARRAY_SUBSCRIPT_ERROR),\n\t\t\t\t\t errmsg(\"wrong number of array subscripts\")));\n\t}\n\n\tdeconstruct_array(in_array,\n\t\t\t\t\t  TEXTOID, -1, false, 'i',\n\t\t\t\t\t  &in_datums, &in_nulls, &in_count);\n\n\tcount = in_count / 2;\n\n\t/* see discussion in hstoreArrayToPairs() */\n\tif (count > MaxAllocSize / sizeof(Pairs))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t  errmsg(\"number of pairs (%d) exceeds the maximum allowed (%d)\",\n\t\t\t\t\t count, (int) (MaxAllocSize / sizeof(Pairs)))));\n\n\tpairs = palloc(count * sizeof(Pairs));\n\n\tfor (i = 0; i < count; ++i)\n\t{\n\t\tif (in_nulls[i * 2])\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_NULL_VALUE_NOT_ALLOWED),\n\t\t\t\t\t errmsg(\"null value not allowed for hstore key\")));\n\n\t\tif (in_nulls[i * 2 + 1])\n\t\t{\n\t\t\tpairs[i].key = VARDATA_ANY(in_datums[i * 2]);\n\t\t\tpairs[i].val = NULL;\n\t\t\tpairs[i].keylen = hstoreCheckKeyLen(VARSIZE_ANY_EXHDR(in_datums[i * 2]));\n\t\t\tpairs[i].vallen = 4;\n\t\t\tpairs[i].isnull = true;\n\t\t\tpairs[i].needfree = false;\n\t\t}\n\t\telse\n\t\t{\n\t\t\tpairs[i].key = VARDATA_ANY(in_datums[i * 2]);\n\t\t\tpairs[i].val = VARDATA_ANY(in_datums[i * 2 + 1]);\n\t\t\tpairs[i].keylen = hstoreCheckKeyLen(VARSIZE_ANY_EXHDR(in_datums[i * 2]));\n\t\t\tpairs[i].vallen = hstoreCheckValLen(VARSIZE_ANY_EXHDR(in_datums[i * 2 + 1]));\n\t\t\tpairs[i].isnull = false;\n\t\t\tpairs[i].needfree = false;\n\t\t}\n\t}\n\n\tcount = hstoreUniquePairs(pairs, count, &buflen);\n\n\tout = hstorePairs(pairs, count, buflen);\n\n\tPG_RETURN_POINTER(out);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 830,
        "critical_vars": [
            "ncolumns"
        ],
        "function": "hstore_from_record",
        "filename": "postgres/CVE-2014-0064/CVE-2014-0064_CWE-189_31400a673325147e1205326008e32135a78b4d8a_hstore_io.c.diff",
        "label": "True",
        "function_code": "Datum\t\thstore_from_record(PG_FUNCTION_ARGS);\nDatum\nhstore_from_record(PG_FUNCTION_ARGS)\n{\n\tHeapTupleHeader rec;\n\tint32\t\tbuflen;\n\tHStore\t   *out;\n\tPairs\t   *pairs;\n\tOid\t\t\ttupType;\n\tint32\t\ttupTypmod;\n\tTupleDesc\ttupdesc;\n\tHeapTupleData tuple;\n\tRecordIOData *my_extra;\n\tint\t\t\tncolumns;\n\tint\t\t\ti,\n\t\t\t\tj;\n\tDatum\t   *values;\n\tbool\t   *nulls;\n\n\tif (PG_ARGISNULL(0))\n\t{\n\t\tOid\t\t\targtype = get_fn_expr_argtype(fcinfo->flinfo, 0);\n\n\t\t/*\n\t\t * have no tuple to look at, so the only source of type info is the\n\t\t * argtype. The lookup_rowtype_tupdesc call below will error out if we\n\t\t * don't have a known composite type oid here.\n\t\t */\n\t\ttupType = argtype;\n\t\ttupTypmod = -1;\n\n\t\trec = NULL;\n\t}\n\telse\n\t{\n\t\trec = PG_GETARG_HEAPTUPLEHEADER(0);\n\n\t\t/* Extract type info from the tuple itself */\n\t\ttupType = HeapTupleHeaderGetTypeId(rec);\n\t\ttupTypmod = HeapTupleHeaderGetTypMod(rec);\n\t}\n\n\ttupdesc = lookup_rowtype_tupdesc(tupType, tupTypmod);\n\tncolumns = tupdesc->natts;\n\n\t/*\n\t * We arrange to look up the needed I/O info just once per series of\n\t * calls, assuming the record type doesn't change underneath us.\n\t */\n\tmy_extra = (RecordIOData *) fcinfo->flinfo->fn_extra;\n\tif (my_extra == NULL ||\n\t\tmy_extra->ncolumns != ncolumns)\n\t{\n\t\tfcinfo->flinfo->fn_extra =\n\t\t\tMemoryContextAlloc(fcinfo->flinfo->fn_mcxt,\n\t\t\t\t\t\t\t   sizeof(RecordIOData) - sizeof(ColumnIOData)\n\t\t\t\t\t\t\t   + ncolumns * sizeof(ColumnIOData));\n\t\tmy_extra = (RecordIOData *) fcinfo->flinfo->fn_extra;\n\t\tmy_extra->record_type = InvalidOid;\n\t\tmy_extra->record_typmod = 0;\n\t}\n\n\tif (my_extra->record_type != tupType ||\n\t\tmy_extra->record_typmod != tupTypmod)\n\t{\n\t\tMemSet(my_extra, 0,\n\t\t\t   sizeof(RecordIOData) - sizeof(ColumnIOData)\n\t\t\t   + ncolumns * sizeof(ColumnIOData));\n\t\tmy_extra->record_type = tupType;\n\t\tmy_extra->record_typmod = tupTypmod;\n\t\tmy_extra->ncolumns = ncolumns;\n\t}\n\n\tAssert(ncolumns <= MaxTupleAttributeNumber);\t\t/* thus, no overflow */\n\tpairs = palloc(ncolumns * sizeof(Pairs));\n\n\tif (rec)\n\t{\n\t\t/* Build a temporary HeapTuple control str\n... (function end not found)"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 459,
        "critical_vars": [
            "avail"
        ],
        "function": "txid_snapshot_recv",
        "filename": "postgres/CVE-2014-0064/CVE-2014-0064_CWE-189_31400a673325147e1205326008e32135a78b4d8a_txid.c.diff",
        "label": "False",
        "function_code": " * txid_snapshot_recv(internal) returns txid_snapshot\n *\n *\t\tbinary input function for type txid_snapshot\n *\n *\t\tformat: int4 nxip, int8 xmin, int8 xmax, int8 xip\n */\nDatum\ntxid_snapshot_recv(PG_FUNCTION_ARGS)\n{\n\tStringInfo\tbuf = (StringInfo) PG_GETARG_POINTER(0);\n\tTxidSnapshot *snap;\n\ttxid\t\tlast = 0;\n\tint\t\t\tnxip;\n\tint\t\t\ti;\n\tint\t\t\tavail;\n\tint\t\t\texpect;\n\ttxid\t\txmin,\n\t\t\t\txmax;\n\n\t/*\n\t * load nxip and check for nonsense.\n\t *\n\t * (nxip > avail) check is against int overflows in 'expect'.\n\t */\n\tnxip = pq_getmsgint(buf, 4);\n\tavail = buf->len - buf->cursor;\n\texpect = 8 + 8 + nxip * 8;\n\tif (nxip < 0 || nxip > avail || expect > avail)\n\t\tgoto bad_format;\n\n\txmin = pq_getmsgint64(buf);\n\txmax = pq_getmsgint64(buf);\n\tif (xmin == 0 || xmax == 0 || xmin > xmax || xmax > MAX_TXID)\n\t\tgoto bad_format;\n\n\tsnap = palloc(TXID_SNAPSHOT_SIZE(nxip));\n\tsnap->xmin = xmin;\n\tsnap->xmax = xmax;\n\tsnap->nxip = nxip;\n\tSET_VARSIZE(snap, TXID_SNAPSHOT_SIZE(nxip));\n\n\tfor (i = 0; i < nxip; i++)\n\t{\n\t\ttxid\t\tcur = pq_getmsgint64(buf);\n\n\t\tif (cur <= last || cur < xmin || cur >= xmax)\n\t\t\tgoto bad_format;\n\t\tsnap->xip[i] = cur;\n\t\tlast = cur;\n\t}\n\tPG_RETURN_POINTER(snap);\n\nbad_format:\n\telog(ERROR, \"invalid snapshot data\");\n\treturn (Datum) NULL;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 464,
        "critical_vars": [
            "nxip"
        ],
        "function": "txid_snapshot_recv",
        "filename": "postgres/CVE-2014-0064/CVE-2014-0064_CWE-189_31400a673325147e1205326008e32135a78b4d8a_txid.c.diff",
        "label": "True",
        "function_code": " * txid_snapshot_recv(internal) returns txid_snapshot\n *\n *\t\tbinary input function for type txid_snapshot\n *\n *\t\tformat: int4 nxip, int8 xmin, int8 xmax, int8 xip\n */\nDatum\ntxid_snapshot_recv(PG_FUNCTION_ARGS)\n{\n\tStringInfo\tbuf = (StringInfo) PG_GETARG_POINTER(0);\n\tTxidSnapshot *snap;\n\ttxid\t\tlast = 0;\n\tint\t\t\tnxip;\n\tint\t\t\ti;\n\ttxid\t\txmin,\n\t\t\t\txmax;\n\n\t/* load and validate nxip */\n\tnxip = pq_getmsgint(buf, 4);\n\tif (nxip < 0 || nxip > TXID_SNAPSHOT_MAX_NXIP)\n\t\tgoto bad_format;\n\n\txmin = pq_getmsgint64(buf);\n\txmax = pq_getmsgint64(buf);\n\tif (xmin == 0 || xmax == 0 || xmin > xmax || xmax > MAX_TXID)\n\t\tgoto bad_format;\n\n\tsnap = palloc(TXID_SNAPSHOT_SIZE(nxip));\n\tsnap->xmin = xmin;\n\tsnap->xmax = xmax;\n\tsnap->nxip = nxip;\n\tSET_VARSIZE(snap, TXID_SNAPSHOT_SIZE(nxip));\n\n\tfor (i = 0; i < nxip; i++)\n\t{\n\t\ttxid\t\tcur = pq_getmsgint64(buf);\n\n\t\tif (cur <= last || cur < xmin || cur >= xmax)\n\t\t\tgoto bad_format;\n\t\tsnap->xip[i] = cur;\n\t\tlast = cur;\n\t}\n\tPG_RETURN_POINTER(snap);\n\nbad_format:\n\telog(ERROR, \"invalid snapshot data\");\n\treturn (Datum) NULL;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 460,
        "critical_vars": [
            "expect"
        ],
        "function": "txid_snapshot_recv",
        "filename": "postgres/CVE-2014-0064/CVE-2014-0064_CWE-189_31400a673325147e1205326008e32135a78b4d8a_txid.c.diff",
        "label": "False",
        "function_code": " * txid_snapshot_recv(internal) returns txid_snapshot\n *\n *\t\tbinary input function for type txid_snapshot\n *\n *\t\tformat: int4 nxip, int8 xmin, int8 xmax, int8 xip\n */\nDatum\ntxid_snapshot_recv(PG_FUNCTION_ARGS)\n{\n\tStringInfo\tbuf = (StringInfo) PG_GETARG_POINTER(0);\n\tTxidSnapshot *snap;\n\ttxid\t\tlast = 0;\n\tint\t\t\tnxip;\n\tint\t\t\ti;\n\tint\t\t\tavail;\n\tint\t\t\texpect;\n\ttxid\t\txmin,\n\t\t\t\txmax;\n\n\t/*\n\t * load nxip and check for nonsense.\n\t *\n\t * (nxip > avail) check is against int overflows in 'expect'.\n\t */\n\tnxip = pq_getmsgint(buf, 4);\n\tavail = buf->len - buf->cursor;\n\texpect = 8 + 8 + nxip * 8;\n\tif (nxip < 0 || nxip > avail || expect > avail)\n\t\tgoto bad_format;\n\n\txmin = pq_getmsgint64(buf);\n\txmax = pq_getmsgint64(buf);\n\tif (xmin == 0 || xmax == 0 || xmin > xmax || xmax > MAX_TXID)\n\t\tgoto bad_format;\n\n\tsnap = palloc(TXID_SNAPSHOT_SIZE(nxip));\n\tsnap->xmin = xmin;\n\tsnap->xmax = xmax;\n\tsnap->nxip = nxip;\n\tSET_VARSIZE(snap, TXID_SNAPSHOT_SIZE(nxip));\n\n\tfor (i = 0; i < nxip; i++)\n\t{\n\t\ttxid\t\tcur = pq_getmsgint64(buf);\n\n\t\tif (cur <= last || cur < xmin || cur >= xmax)\n\t\t\tgoto bad_format;\n\t\tsnap->xip[i] = cur;\n\t\tlast = cur;\n\t}\n\tPG_RETURN_POINTER(snap);\n\nbad_format:\n\telog(ERROR, \"invalid snapshot data\");\n\treturn (Datum) NULL;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "if-Condition",
        "line_old": 461,
        "critical_vars": [
            "nxip",
            "expect"
        ],
        "function": "txid_snapshot_recv",
        "filename": "postgres/CVE-2014-0064/CVE-2014-0064_CWE-189_31400a673325147e1205326008e32135a78b4d8a_txid.c.diff",
        "label": "False",
        "function_code": " * txid_snapshot_recv(internal) returns txid_snapshot\n *\n *\t\tbinary input function for type txid_snapshot\n *\n *\t\tformat: int4 nxip, int8 xmin, int8 xmax, int8 xip\n */\nDatum\ntxid_snapshot_recv(PG_FUNCTION_ARGS)\n{\n\tStringInfo\tbuf = (StringInfo) PG_GETARG_POINTER(0);\n\tTxidSnapshot *snap;\n\ttxid\t\tlast = 0;\n\tint\t\t\tnxip;\n\tint\t\t\ti;\n\tint\t\t\tavail;\n\tint\t\t\texpect;\n\ttxid\t\txmin,\n\t\t\t\txmax;\n\n\t/*\n\t * load nxip and check for nonsense.\n\t *\n\t * (nxip > avail) check is against int overflows in 'expect'.\n\t */\n\tnxip = pq_getmsgint(buf, 4);\n\tavail = buf->len - buf->cursor;\n\texpect = 8 + 8 + nxip * 8;\n\tif (nxip < 0 || nxip > avail || expect > avail)\n\t\tgoto bad_format;\n\n\txmin = pq_getmsgint64(buf);\n\txmax = pq_getmsgint64(buf);\n\tif (xmin == 0 || xmax == 0 || xmin > xmax || xmax > MAX_TXID)\n\t\tgoto bad_format;\n\n\tsnap = palloc(TXID_SNAPSHOT_SIZE(nxip));\n\tsnap->xmin = xmin;\n\tsnap->xmax = xmax;\n\tsnap->nxip = nxip;\n\tSET_VARSIZE(snap, TXID_SNAPSHOT_SIZE(nxip));\n\n\tfor (i = 0; i < nxip; i++)\n\t{\n\t\ttxid\t\tcur = pq_getmsgint64(buf);\n\n\t\tif (cur <= last || cur < xmin || cur >= xmax)\n\t\t\tgoto bad_format;\n\t\tsnap->xip[i] = cur;\n\t\tlast = cur;\n\t}\n\tPG_RETURN_POINTER(snap);\n\nbad_format:\n\telog(ERROR, \"invalid snapshot data\");\n\treturn (Datum) NULL;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 101,
        "critical_vars": [
            "key_count"
        ],
        "function": "hstoreArrayToPairs",
        "filename": "postgres/CVE-2014-0064/CVE-2014-0064_CWE-189_31400a673325147e1205326008e32135a78b4d8a_hstore_op.c.diff",
        "label": "True",
        "function_code": "\nPairs *\nhstoreArrayToPairs(ArrayType *a, int *npairs)\n{\n\tDatum\t   *key_datums;\n\tbool\t   *key_nulls;\n\tint\t\t\tkey_count;\n\tPairs\t   *key_pairs;\n\tint\t\t\tbufsiz;\n\tint\t\t\ti,\n\t\t\t\tj;\n\n\tdeconstruct_array(a,\n\t\t\t\t\t  TEXTOID, -1, false, 'i',\n\t\t\t\t\t  &key_datums, &key_nulls, &key_count);\n\n\tif (key_count == 0)\n\t{\n\t\t*npairs = 0;\n\t\treturn NULL;\n\t}\n\n\t/*\n\t * A text array uses at least eight bytes per element, so any overflow in\n\t * \"key_count * sizeof(Pairs)\" is small enough for palloc() to catch.\n\t * However, credible improvements to the array format could invalidate\n\t * that assumption.  Therefore, use an explicit check rather than relying\n\t * on palloc() to complain.\n\t */\n\tif (key_count > MaxAllocSize / sizeof(Pairs))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t  errmsg(\"number of pairs (%d) exceeds the maximum allowed (%d)\",\n\t\t\t\t\t key_count, (int) (MaxAllocSize / sizeof(Pairs)))));\n\n\tkey_pairs = palloc(sizeof(Pairs) * key_count);\n\n\tfor (i = 0, j = 0; i < key_count; i++)\n\t{\n\t\tif (!key_nulls[i])\n\t\t{\n\t\t\tkey_pairs[j].key = VARDATA(key_datums[i]);\n\t\t\tkey_pairs[j].keylen = VARSIZE(key_datums[i]) - VARHDRSZ;\n\t\t\tkey_pairs[j].val = NULL;\n\t\t\tkey_pairs[j].vallen = 0;\n\t\t\tkey_pairs[j].needfree = 0;\n\t\t\tkey_pairs[j].isnull = 1;\n\t\t\tj++;\n\t\t}\n\t}\n\n\t*npairs = hstoreUniquePairs(key_pairs, j, &bufsiz);\n\n\treturn key_pairs;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 102,
        "critical_vars": [
            "Pairs",
            "key_count"
        ],
        "function": "hstoreArrayToPairs",
        "filename": "postgres/CVE-2014-0064/CVE-2014-0064_CWE-189_31400a673325147e1205326008e32135a78b4d8a_hstore_op.c.diff",
        "label": "True",
        "function_code": "\nPairs *\nhstoreArrayToPairs(ArrayType *a, int *npairs)\n{\n\tDatum\t   *key_datums;\n\tbool\t   *key_nulls;\n\tint\t\t\tkey_count;\n\tPairs\t   *key_pairs;\n\tint\t\t\tbufsiz;\n\tint\t\t\ti,\n\t\t\t\tj;\n\n\tdeconstruct_array(a,\n\t\t\t\t\t  TEXTOID, -1, false, 'i',\n\t\t\t\t\t  &key_datums, &key_nulls, &key_count);\n\n\tif (key_count == 0)\n\t{\n\t\t*npairs = 0;\n\t\treturn NULL;\n\t}\n\n\t/*\n\t * A text array uses at least eight bytes per element, so any overflow in\n\t * \"key_count * sizeof(Pairs)\" is small enough for palloc() to catch.\n\t * However, credible improvements to the array format could invalidate\n\t * that assumption.  Therefore, use an explicit check rather than relying\n\t * on palloc() to complain.\n\t */\n\tif (key_count > MaxAllocSize / sizeof(Pairs))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t  errmsg(\"number of pairs (%d) exceeds the maximum allowed (%d)\",\n\t\t\t\t\t key_count, (int) (MaxAllocSize / sizeof(Pairs)))));\n\n\tkey_pairs = palloc(sizeof(Pairs) * key_count);\n\n\tfor (i = 0, j = 0; i < key_count; i++)\n\t{\n\t\tif (!key_nulls[i])\n\t\t{\n\t\t\tkey_pairs[j].key = VARDATA(key_datums[i]);\n\t\t\tkey_pairs[j].keylen = VARSIZE(key_datums[i]) - VARHDRSZ;\n\t\t\tkey_pairs[j].val = NULL;\n\t\t\tkey_pairs[j].vallen = 0;\n\t\t\tkey_pairs[j].needfree = 0;\n\t\t\tkey_pairs[j].isnull = 1;\n\t\t\tj++;\n\t\t}\n\t}\n\n\t*npairs = hstoreUniquePairs(key_pairs, j, &bufsiz);\n\n\treturn key_pairs;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 160,
        "critical_vars": [
            "slen"
        ],
        "function": "bit_in",
        "filename": "postgres/CVE-2014-0064/CVE-2014-0064_CWE-189_31400a673325147e1205326008e32135a78b4d8a_varbit.c.diff",
        "label": "True",
        "function_code": "Datum\nbit_in(PG_FUNCTION_ARGS)\n{\n\tchar\t   *input_string = PG_GETARG_CSTRING(0);\n\n#ifdef NOT_USED\n\tOid\t\t\ttypelem = PG_GETARG_OID(1);\n#endif\n\tint32\t\tatttypmod = PG_GETARG_INT32(2);\n\tVarBit\t   *result;\t\t\t/* The resulting bit string\t\t\t  */\n\tchar\t   *sp;\t\t\t\t/* pointer into the character string  */\n\tbits8\t   *r;\t\t\t\t/* pointer into the result */\n\tint\t\t\tlen,\t\t\t/* Length of the whole data structure */\n\t\t\t\tbitlen,\t\t\t/* Number of bits in the bit string   */\n\t\t\t\tslen;\t\t\t/* Length of the input string\t\t  */\n\tbool\t\tbit_not_hex;\t/* false = hex string  true = bit string */\n\tint\t\t\tbc;\n\tbits8\t\tx = 0;\n\n\t/* Check that the first character is a b or an x */\n\tif (input_string[0] == 'b' || input_string[0] == 'B')\n\t{\n\t\tbit_not_hex = true;\n\t\tsp = input_string + 1;\n\t}\n\telse if (input_string[0] == 'x' || input_string[0] == 'X')\n\t{\n\t\tbit_not_hex = false;\n\t\tsp = input_string + 1;\n\t}\n\telse\n\t{\n\t\t/*\n\t\t * Otherwise it's binary.  This allows things like cast('1001' as bit)\n\t\t * to work transparently.\n\t\t */\n\t\tbit_not_hex = true;\n\t\tsp = input_string;\n\t}\n\n\t/*\n\t * Determine bitlength from input string.  MaxAllocSize ensures a regular\n\t * input is small enough, but we must check hex input.\n\t */\n\tslen = strlen(sp);\n\tif (bit_not_hex)\n\t\tbitlen = slen;\n\telse\n\t{\n\t\tif (slen > VARBITMAXLEN / 4)\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t\t errmsg(\"bit string length exceeds the maximum allowed (%d)\",\n\t\t\t\t\t\tVARBITMAXLEN)));\n\t\tbitlen = slen * 4;\n\t}\n\n\t/*\n\t * Sometimes atttypmod is not supplied. If it is supplied we need to make\n\t * sure that the bitstring fits.\n\t */\n\tif (atttypmod <= 0)\n\t\tatttypmod = bitlen;\n\telse if (bitlen != atttypmod)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_STRING_DATA_LENGTH_MISMATCH),\n\t\t\t\t errmsg(\"bit string length %d does not match type bit(%d)\",\n\t\t\t\t\t\tbitlen, atttypmod)));\n\n\tlen = VARBITTOTALLEN(atttypmod);\n\t/* set to 0 so that *r is always initialised and string is zero-padded */\n\tresult = (VarBit *) palloc0(len);\n\tSET_VARSIZE(result, len);\n\tVARBITLEN(result) = atttypmod;\n\n\tr = VARBITS(result);\n\tif (bit_not_hex)\n\t{\n\t\t/* Parse the bit representation of the string */\n\t\t/* We know it fits, as bitlen was compared to atttypmod */\n\t\tx = HIGHBIT;\n\t\tfor (; *sp; sp++)\n\t\t{\n\t\t\tif (*sp == '1')\n\t\t\t\t*r |= x;\n\t\t\telse if (*sp != '0')\n\t\t\t\tereport(ERROR,\n\t\t\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t\t\t\t errmsg(\"\\\"%c\\\" is not a valid binary digit\",\n\t\t\t\t\t\t\t\t*sp)));\n\n\t\t\tx >>= 1;\n\t\t\tif (x == 0)\n\t\t\t{\n\t\t\t\tx = HIGHBIT;\n\t\t\t\tr++;\n\t\t\t}\n\t\t}\n\t}\n\telse\n\t{\n\t\t/* Parse the hex representation of the string */\n\t\tfor (bc = 0; *sp; sp++)\n\t\t{\n\t\t\tif (*sp >= '0' && *sp <= '9')\n\t\t\t\tx = (bits8) (*sp - '0');\n\t\t\telse if (*sp >= 'A' && *sp <= 'F')\n\t\t\t\tx = (bits8) (*sp - 'A') + 10;\n\t\t\telse if (*sp >= 'a' && *sp <= 'f')\n\t\t\t\tx = (bits8) (*sp - 'a') + 10;\n\t\t\telse\n\t\t\t\tereport(ERROR,\n\t\t\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t\t\t\t errmsg(\"\\\"%c\\\" is not a valid hexadecimal digit\",\n\t\t\t\t\t\t\t\t*sp)));\n\n\t\t\tif (bc)\n\t\t\t{\n\t\t\t\t*r++ |= x;\n\t\t\t\tbc = 0;\n\t\t\t}\n\t\t\telse\n\t\t\t{\n\t\t\t\t*r = x << 4;\n\t\t\t\tbc = 1;\n\t\t\t}\n\t\t}\n\t}\n\n\tPG_RETURN_VARBIT_P(result);\n}\n\n\nDatum\nbit_out(PG_FUNCTION_ARGS)\n{\n#if 1\n\t/* same as varbit output */\n\treturn varbit_out(fcinfo);\n#else\n\n\t/*\n\t * This is how one would print a hex string, in case someone wants to\n\t * write a formatting function.\n\t */\n\tVarBit\t   *s = PG_GETARG_VARBIT_P(0);\n\tchar\t   *result,\n\t\t\t   *r;\n\tbits8\t   *sp;\n\tint\t\t\ti,\n\t\t\t\tlen,\n\t\t\t\tbitlen;\n\n\tbitlen = VARBITLEN(s);\n\tlen = (bitlen + 3) / 4;\n\tresult = (char *) palloc(len + 2);\n\tsp = VARBITS(s);\n\tr = result;\n\t*r++ = 'X';\n\t/* we cheat by knowing that we store full bytes zero padded */\n\tfor (i = 0; i < len; i += 2, sp++)\n\t{\n\t\t*r++ = HEXDIG((*sp) >> 4);\n\t\t*r++ = HEXDIG((*sp) & 0xF);\n\t}\n\n\t/*\n\t * Go back one step if we printed a hex number that was not part of the\n\t * bitstring anymore\n\t */\n\tif (i > len)\n\t\tr--;\n\t*r = '\\0';\n\n\tPG_RETURN_CSTRING(result);\n#endif\n}\n\n/*\n *\t\tbit_recv\t\t\t- converts external binary format to bit\n */\nDatum\nbit_recv(PG_FUNCTION_ARGS)\n{\n\tStringInfo\tbuf = (StringInfo) PG_GETARG_POINTER(0);\n\n#ifdef NOT_USED\n\tOid\t\t\ttypelem = PG_GETARG_OID(1);\n#endif\n\tint32\t\tatttypmod = PG_GETARG_INT32(2);\n\tVarBit\t   *result;\n\tint\t\t\tlen,\n\t\t\t\tbitlen;\n\tint\t\t\tipad;\n\tbits8\t\tmask;\n\n\tbitlen = pq_getmsgint(buf, sizeof(int32));\n\tif (bitlen < 0)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_BINARY_REPRESENTATION),\n\t\t\t\t errmsg(\"invalid length in external bit string\")));\n\n\t/*\n\t * Sometimes atttypmod is not supplied. If it is supplied we need to make\n\t * sure that the bitstring fits.\n\t */\n\tif (atttypmod > 0 && bitlen != atttypmod)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_STRING_DATA_LENGTH_MISMATCH),\n\t\t\t\t errmsg(\"bit string length %d does not match type bit(%d)\",\n\t\t\t\t\t\tbitlen, atttypmod)));\n\n\tlen = VARBITTOTALLEN(bitlen);\n\tresult = (VarBit *) palloc(len);\n\tSET_VARSIZE(result, len);\n\tVARBITLEN(result) = bitlen;\n\n\tpq_copymsgbytes(buf, (char *) VARBITS(result), VARBITBYTES(result));\n\n\t/* Make sure last byte is zero-padded if needed */\n\tipad = VARBITPAD(result);\n\tif (ipad > 0)\n\t{\n\t\tmask = BITMASK << ipad;\n\t\t*(VARBITS(result) + VARBITBYTES(result) - 1) &= mask;\n\t}\n\n\tPG_RETURN_VARBIT_P(result);\n}\n\n/*\n *\t\tbit_send\t\t\t- converts bit to binary format\n */\nDatum\nbit_send(PG_FUNCTION_ARGS)\n{\n\t/* Exactly the same as varbit_send, so share code */\n\treturn varbit_send(fcinfo);\n}\n\n/*\n * bit()\n * Converts a bit() type to a specific internal length.\n * len is the bitlength specified in the column definition.\n *\n * If doing implicit cast, raise error when source data is wrong length.\n * If doing explicit cast, silently truncate or zero-pad to specified length.\n */\nDatum\nbit(PG_FUNCTION_ARGS)\n{\n\tVarBit\t   *arg = PG_GETARG_VARBIT_P(0);\n\tint32\t\tlen = PG_GETARG_INT32(1);\n\tbool\t\tisExplicit = PG_GETARG_BOOL(2);\n\tVarBit\t   *result;\n\tint\t\t\trlen;\n\tint\t\t\tipad;\n\tbits8\t\tmask;\n\n\t/* No work if typmod is invalid or supplied data matches it already */\n\tif (len <= 0 || len == VARBITLEN(arg))\n\t\tPG_RETURN_VARBIT_P(arg);\n\n\tif (!isExplicit)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_STRING_DATA_LENGTH_MISMATCH),\n\t\t\t\t errmsg(\"bit string length %d does not match type bit(%d)\",\n\t\t\t\t\t\tVARBITLEN(arg), len)));\n\n\trlen = VARBITTOTALLEN(len);\n\t/* set to 0 so that string is zero-padded */\n\tresult = (VarBit *) palloc0(rlen);\n\tSET_VARSIZE(result, rlen);\n\tVARBITLEN(result) = len;\n\n\tmemcpy(VARBITS(result), VARBITS(arg),\n\t\t   Min(VARBITBYTES(result), VARBITBYTES(arg)));\n\n\t/*\n\t * Make sure last byte is zero-padded if needed.  This is useless but safe\n\t * if source data was shorter than target length (we assume the last byte\n\t * of the source data was itself correctly zero-padded).\n\t */\n\tipad = VARBITPAD(result);\n\tif (ipad > 0)\n\t{\n\t\tmask = BITMASK << ipad;\n\t\t*(VARBITS(result) + VARBITBYTES(result) - 1) &= mask;\n\t}\n\n\tPG_RETURN_VARBIT_P(result);\n}\n\nDatum\nbittypmodin(PG_FUNCTION_ARGS)\n{\n\tArrayType  *ta = PG_GETARG_ARRAYTYPE_P(0);\n\n\tPG_RETURN_INT32(anybit_typmodin(ta, \"bit\"));\n}\n\nDatum\nbittypmodout(PG_FUNCTION_ARGS)\n{\n\tint32\t\ttypmod = PG_GETARG_INT32(0);\n\n\tPG_RETURN_CSTRING(anybit_typmodout(typmod));\n}\n\n\n/*\n * varbit_in -\n *\t  converts a string to the internal representation of a bitstring.\n *\t\tThis is the same as bit_in except that atttypmod is taken as\n *\t\tthe maximum length, not the exact length to force the bitstring to.\n */\nDatum\nvarbit_in(PG_FUNCTION_ARGS)\n{\n\tchar\t   *input_string = PG_GETARG_CSTRING(0);\n\n#ifdef NOT_USED\n\tOid\t\t\ttypelem = PG_GETARG_OID(1);\n#endif\n\tint32\t\tatttypmod = PG_GETARG_INT32(2);\n\tVarBit\t   *result;\t\t\t/* The resulting bit string\t\t\t  */\n\tchar\t   *sp;\t\t\t\t/* pointer into the character string  */\n\tbits8\t   *r;\t\t\t\t/* pointer into the result */\n\tint\t\t\tlen,\t\t\t/* Length of the whole data structure */\n\t\t\t\tbitlen,\t\t\t/* Number of bits in the bit string   */\n\t\t\t\tslen;\t\t\t/* Length of the input string\t\t  */\n\tbool\t\tbit_not_hex;\t/* false = hex string  true = bit string */\n\tint\t\t\tbc;\n\tbits8\t\tx = 0;\n\n\t/* Check that the first character is a b or an x */\n\tif (input_string[0] == 'b' || input_string[0] == 'B')\n\t{\n\t\tbit_not_hex = true;\n\t\tsp = input_string + 1;\n\t}\n\telse if (input_string[0] == 'x' || input_string[0] == 'X')\n\t{\n\t\tbit_not_hex = false;\n\t\tsp = input_string + 1;\n\t}\n\telse\n\t{\n\t\tbit_not_hex = true;\n\t\tsp = input_string;\n\t}\n\n\t/*\n\t * Determine bitlength from input string.  MaxAllocSize ensures a regular\n\t * input is small enough, but we must check hex input.\n\t */\n\tslen = strlen(sp);\n\tif (bit_not_hex)\n\t\tbitlen = slen;\n\telse\n\t{\n\t\tif (slen > VARBITMAXLEN / 4)\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t\t errmsg(\"bit string length exceeds the maximum allowed (%d)\",\n\t\t\t\t\t\tVARBITMAXLEN)));\n\t\tbitlen = slen * 4;\n\t}\n\n\t/*\n\t * Sometimes atttypmod is not supplied. If it is supplied we need to make\n\t * sure that the bitstring fits.\n\t */\n\tif (atttypmod <= 0)\n\t\tatttypmod = bitlen;\n\telse if (bitlen > atttypmod)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_STRING_DATA_RIGHT_TRUNCATION),\n\t\t\t\t errmsg(\"bit string too long for type bit varying(%d)\",\n\t\t\t\t\t\tatttypmod)));\n\n\tlen = VARBITTOTALLEN(bitlen);\n\t/* set to 0 so that *r is always initialised and string is zero-padded */\n\tresult = (VarBit *) palloc0(len);\n\tSET_VARSIZE(result, len);\n\tVARBITLEN(result) = Min(bitlen, atttypmod);\n\n\tr = VARBITS(result);\n\tif (bit_not_hex)\n\t{\n\t\t/* Parse the bit representation of the string */\n\t\t/* We know it fits, as bitlen was compared to atttypmod */\n\t\tx = HIGHBIT;\n\t\tfor (; *sp; sp++)\n\t\t{\n\t\t\tif (*sp == '1')\n\t\t\t\t*r |= x;\n\t\t\telse if (*sp != '0')\n\t\t\t\tereport(ERROR,\n\t\t\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t\t\t\t errmsg(\"\\\"%c\\\" is not a valid binary digit\",\n\t\t\t\t\t\t\t\t*sp)));\n\n\t\t\tx >>= 1;\n\t\t\tif (x == 0)\n\t\t\t{\n\t\t\t\tx = HIGHBIT;\n\t\t\t\tr++;\n\t\t\t}\n\t\t}\n\t}\n\telse\n\t{\n\t\t/* Parse the hex representation of the string */\n\t\tfor (bc = 0; *sp; sp++)\n\t\t{\n\t\t\tif (*sp >= '0' && *sp <= '9')\n\t\t\t\tx = (bits8) (*sp - '0');\n\t\t\telse if (*sp >= 'A' && *sp <= 'F')\n\t\t\t\tx = (bits8) (*sp - 'A') + 10;\n\t\t\telse if (*sp >= 'a' && *sp <= 'f')\n\t\t\t\tx = (bits8) (*sp - 'a') + 10;\n\t\t\telse\n\t\t\t\tereport(ERROR,\n\t\t\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t\t\t\t errmsg(\"\\\"%c\\\" is not a valid hexadecimal digit\",\n\t\t\t\t\t\t\t\t*sp)));\n\n\t\t\tif (bc)\n\t\t\t{\n\t\t\t\t*r++ |= x;\n\t\t\t\tbc = 0;\n\t\t\t}\n\t\t\telse\n\t\t\t{\n\t\t\t\t*r = x << 4;\n\t\t\t\tbc = 1;\n\t\t\t}\n\t\t}\n\t}\n\n\tPG_RETURN_VARBIT_P(result);\n}\n\n/*\n * varbit_out -\n *\t  Prints the string as bits to preserve length accurately\n *\n * XXX varbit_recv() and hex input to varbit_in() can load a value that this\n * cannot emit.  Consider using hex output for such values.\n */\nDatum\nvarbit_out(PG_FUNCTION_ARGS)\n{\n\tVarBit\t   *s = PG_GETARG_VARBIT_P(0);\n\tchar\t   *result,\n\t\t\t   *r;\n\tbits8\t   *sp;\n\tbits8\t\tx;\n\tint\t\t\ti,\n\t\t\t\tk,\n\t\t\t\tlen;\n\n\tlen = VARBITLEN(s);\n\tresult = (char *) palloc(len + 1);\n\tsp = VARBITS(s);\n\tr = result;\n\tfor (i = 0; i <= len - BITS_PER_BYTE; i += BITS_PER_BYTE, sp++)\n\t{\n\t\t/* print full bytes */\n\t\tx = *sp;\n\t\tfor (k = 0; k < BITS_PER_BYTE; k++)\n\t\t{\n\t\t\t*r++ = IS_HIGHBIT_SET(x) ? '1' : '0';\n\t\t\tx <<= 1;\n\t\t}\n\t}\n\tif (i < len)\n\t{\n\t\t/* print the last partial byte */\n\t\tx = *sp;\n\t\tfor (k = i; k < len; k++)\n\t\t{\n\t\t\t*r++ = IS_HIGHBIT_SET(x) ? '1' : '0';\n\t\t\tx <<= 1;\n\t\t}\n\t}\n\t*r = '\\0';\n\n\tPG_RETURN_CSTRING(result);\n}\n\n/*\n *\t\tvarbit_recv\t\t\t- converts external binary format to varbit\n *\n * External format is the bitlen as an int32, then the byte array.\n */\nDatum\nvarbit_recv(PG_FUNCTION_ARGS)\n{\n\tStringInfo\tbuf = (StringInfo) PG_GETARG_POINTER(0);\n\n#ifdef NOT_USED\n\tOid\t\t\ttypelem = PG_GETARG_OID(1);\n#endif\n\tint32\t\tatttypmod = PG_GETARG_INT32(2);\n\tVarBit\t   *result;\n\tint\t\t\tlen,\n\t\t\t\tbitlen;\n\tint\t\t\tipad;\n\tbits8\t\tmask;\n\n\tbitlen = pq_getmsgint(buf, sizeof(int32));\n\tif (bitlen < 0)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_BINARY_REPRESENTATION),\n\t\t\t\t errmsg(\"invalid length in external bit string\")));\n\n\t/*\n\t * Sometimes atttypmod is not supplied. If it is supplied we need to make\n\t * sure that the bitstring fits.\n\t */\n\tif (atttypmod > 0 && bitlen > atttypmod)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_STRING_DATA_RIGHT_TRUNCATION),\n\t\t\t\t errmsg(\"bit string too long for type bit varying(%d)\",\n\t\t\t\t\t\tatttypmod)));\n\n\tlen = VARBITTOTALLEN(bitlen);\n\tresult = (VarBit *) palloc(len);\n\tSET_VARSIZE(result, len);\n\tVARBITLEN(result) = bitlen;\n\n\tpq_copymsgbytes(buf, (char *) VARBITS(result), VARBITBYTES(result));\n\n\t/* Make sure last byte is zero-padded if needed */\n\tipad = VARBITPAD(result);\n\tif (ipad > 0)\n\t{\n\t\tmask = BITMASK << ipad;\n\t\t*(VARBITS(result) + VARBITBYTES(result) - 1) &= mask;\n\t}\n\n\tPG_RETURN_VARBIT_P(result);\n}\n\n/*\n *\t\tvarbit_send\t\t\t- converts varbit to binary format\n */\nDatum\nvarbit_send(PG_FUNCTION_ARGS)\n{\n\tVarBit\t   *s = PG_GETARG_VARBIT_P(0);\n\tStringInfoData buf;\n\n\tpq_begintypsend(&buf);\n\tpq_sendint(&buf, VARBITLEN(s), sizeof(int32));\n\tpq_sendbytes(&buf, (char *) VARBITS(s), VARBITBYTES(s));\n\tPG_RETURN_BYTEA_P(pq_endtypsend(&buf));\n}\n\n/*\n * varbit_transform()\n * Flatten calls to varbit's length coercion function that set the new maximum\n * length >= the previous maximum length.  We can ignore the isExplicit\n * argument, since that only affects truncation cases.\n */\nDatum\nvarbit_transform(PG_FUNCTION_ARGS)\n{\n\tFuncExpr   *expr = (FuncExpr *) PG_GETARG_POINTER(0);\n\tNode\t   *ret = NULL;\n\tNode\t   *typmod;\n\n\tAssert(IsA(expr, FuncExpr));\n\tAssert(list_length(expr->args) >= 2);\n\n\ttypmod = (Node *) lsecond(expr->args);\n\n\tif (IsA(typmod, Const) &&!((Const *) typmod)->constisnull)\n\t{\n\t\tNode\t   *source = (Node *) linitial(expr->args);\n\t\tint32\t\tnew_typmod = DatumGetInt32(((Const *) typmod)->constvalue);\n\t\tint32\t\told_max = exprTypmod(source);\n\t\tint32\t\tnew_max = new_typmod;\n\n\t\t/* Note: varbit() treats typmod 0 as invalid, so we do too */\n\t\tif (new_max <= 0 || (old_max > 0 && old_max <= new_max))\n\t\t\tret = relabel_to_typmod(source, new_typmod);\n\t}\n\n\tPG_RETURN_POINTER(ret);\n}\n\n/*\n * varbit()\n * Converts a varbit() type to a specific internal length.\n * len is the maximum bitlength specified in the column definition.\n *\n * If doing implicit cast, raise error when source data is too long.\n * If doing explicit cast, silently truncate to max length.\n */\nDatum\nvarbit(PG_FUNCTION_ARGS)\n{\n\tVarBit\t   *arg = PG_GETARG_VARBIT_P(0);\n\tint32\t\tlen = PG_GETARG_INT32(1);\n\tbool\t\tisExplicit = PG_GETARG_BOOL(2);\n\tVarBit\t   *result;\n\tint\t\t\trlen;\n\tint\t\t\tipad;\n\tbits8\t\tmask;\n\n\t/* No work if typmod is invalid or supplied data matches it already */\n\tif (len <= 0 || len >= VARBITLEN(arg))\n\t\tPG_RETURN_VARBIT_P(arg);\n\n\tif (!isExplicit)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_STRING_DATA_RIGHT_TRUNCATION),\n\t\t\t\t errmsg(\"bit string too long for type bit varying(%d)\",\n\t\t\t\t\t\tlen)));\n\n\trlen = VARBITTOTALLEN(len);\n\tresult = (VarBit *) palloc(rlen);\n\tSET_VARSIZE(result, rlen);\n\tVARBITLEN(result) = len;\n\n\tmemcpy(VARBITS(result), VARBITS(arg), VARBITBYTES(result));\n\n\t/* Make sure last byte is zero-padded if needed */\n\tipad = VARBITPAD(result);\n\tif (ipad > 0)\n\t{\n\t\tmask = BITMASK << ipad;\n\t\t*(VARBITS(result) + VARBITBYTES(result) - 1) &= mask;\n\t}\n\n\tPG_RETURN_VARBIT_P(result);\n}\n\nDatum\nvarbittypmodin(PG_FUNCTION_ARGS)\n{\n\tArrayType  *ta = PG_GETARG_ARRAYTYPE_P(0);\n\n\tPG_RETURN_INT32(anybit_typmodin(ta, \"varbit\"));\n}\n\nDatum\nvarbittypmodout(PG_FUNCTION_ARGS)\n{\n\tint32\t\ttypmod = PG_GETARG_INT32(0);\n\n\tPG_RETURN_CSTRING(anybit_typmodout(typmod));\n}\n\n\n/*\n * Comparison operators\n *\n * We only need one set of comparison operators for bitstrings, as the lengths\n * are stored in the same way for zero-padded and varying bit strings.\n *\n * Note that the standard is not unambiguous about the comparison between\n * zero-padded bit strings and varying bitstrings. If the same value is written\n * into a zero padded bitstring as into a varying bitstring, but the zero\n * padded bitstring has greater length, it will be bigger.\n *\n * Zeros from the beginning of a bitstring cannot simply be ignored, as they\n * may be part of a bit string and may be significant.\n *\n * Note: btree indexes need these routines not to leak memory; therefore,\n * be careful to free working copies of toasted datums.  Most places don't\n * need to be so careful.\n */\n\n/*\n * bit_cmp\n *\n * Compares two bitstrings and returns <0, 0, >0 depending on whether the first\n * string is smaller, equal, or bigger than the second. All bits are considered\n * and additional zero bits may make one string smaller/larger than the other,\n * even if their zero-padded values would be the same.\n */\nstatic int32\nbit_cmp(VarBit *arg1, VarBit *arg2)\n{\n\tint\t\t\tbitlen1,\n\t\t\t\tbytelen1,\n\t\t\t\tbitlen2,\n\t\t\t\tbytelen2;\n\tint32\t\tcmp;\n\n\tbytelen1 = VARBITBYTES(arg1);\n\tbytelen2 = VARBITBYTES(arg2);\n\n\tcmp = memcmp(VARBITS(arg1), VARBITS(arg2), Min(bytelen1, bytelen2));\n\tif (cmp == 0)\n\t{\n\t\tbitlen1 = VARBITLEN(arg1);\n\t\tbitlen2 = VARBITLEN(arg2);\n\t\tif (bitlen1 != bitlen2)\n\t\t\tcmp = (bitlen1 < bitlen2) ? -1 : 1;\n\t}\n\treturn cmp;\n}\n\nDatum\nbiteq(PG_FUNCTION_ARGS)\n{\n\tVarBit\t   *arg1 = PG_GETARG_VARBIT_P(0);\n\tVarBit\t   *arg2 = PG_GETARG_VARBIT_P(1);\n\tbool\t\tresult;\n\tint\t\t\tbitlen1,\n\t\t\t\tbitlen2;\n\n\tbitlen1 = VARBITLEN(arg1);\n\tbitlen2 = VARBITLEN(arg2);\n\n\t/* fast path for different-length inputs */\n\tif (bitlen1 != bitlen2)\n\t\tresult = false;\n\telse\n\t\tresult = (bit_cmp(arg1, arg2) == 0);\n\n\tPG_FREE_IF_COPY(arg1, 0);\n\tPG_FREE_IF_COPY(arg2, 1);\n\n\tPG_RETURN_BOOL(result);\n}\n\nDatum\nbitne(PG_FUNCTION_ARGS)\n{\n\tVarBit\t   *arg1 = PG_GETARG_VARBIT_P(0);\n\tVarBit\t   *arg2 = PG_GETARG_VARBIT_P(1);\n\tbool\t\tresult;\n\tint\t\t\tbitlen1,\n\t\t\t\tbitlen2;\n\n\tbitlen1 = VARBITLEN(arg1);\n\tbitlen2 = VARBITLEN(arg2);\n\n\t/* fast path for different-length inputs */\n\tif (bitlen1 != bitlen2)\n\t\tresult = true;\n\telse\n\t\tresult = (bit_cmp(arg1, arg2) != 0);\n\n\tPG_FREE_IF_COPY(arg1, 0);\n\tPG_FREE_IF_COPY(arg2, 1);\n\n\tPG_RETURN_BOOL(result);\n}\n\nDatum\nbitlt(PG_FUNCTION_ARGS)\n{\n\tVarBit\t   *arg1 = PG_GETARG_VARBIT_P(0);\n\tVarBit\t   *arg2 = PG_GETARG_VARBIT_P(1);\n\tbool\t\tresult;\n\n\tresult = (bit_cmp(arg1, arg2) < 0);\n\n\tPG_FREE_IF_COPY(arg1, 0);\n\tPG_FREE_IF_COPY(arg2, 1);\n\n\tPG_RETURN_BOOL(result);\n}\n\nDatum\nbitle(PG_FUNCTION_ARGS)\n{\n\tVarBit\t   *arg1 = PG_GETARG_VARBIT_P(0);\n\tVarBit\t   *arg2 = PG_GETARG_VARBIT_P(1);\n\tbool\t\tresult;\n\n\tresult = (bit_cmp(arg1, arg2) <= 0);\n\n\tPG_FREE_IF_COPY(arg1, 0);\n\tPG_FREE_IF_COPY(arg2, 1);\n\n\tPG_RETURN_BOOL(result);\n}\n\nDatum\nbitgt(PG_FUNCTION_ARGS)\n{\n\tVarBit\t   *arg1 = PG_GETARG_VARBIT_P(0);\n\tVarBit\t   *arg2 = PG_GETARG_VARBIT_P(1);\n\tbool\t\tresult;\n\n\tresult = (bit_cmp(arg1, arg2) > 0);\n\n\tPG_FREE_IF_COPY(arg1, 0);\n\tPG_FREE_IF_COPY(arg2, 1);\n\n\tPG_RETURN_BOOL(result);\n}\n\nDatum\nbitge(PG_FUNCTION_ARGS)\n{\n\tVarBit\t   *arg1 = PG_GETARG_VARBIT_P(0);\n\tVarBit\t   *arg2 = PG_GETARG_VARBIT_P(1);\n\tbool\t\tresult;\n\n\tresult = (bit_cmp(arg1, arg2) >= 0);\n\n\tPG_FREE_IF_COPY(arg1, 0);\n\tPG_FREE_IF_COPY(arg2, 1);\n\n\tPG_RETURN_BOOL(result);\n}\n\nDatum\nbitcmp(PG_FUNCTION_ARGS)\n{\n\tVarBit\t   *arg1 = PG_GETARG_VARBIT_P(0);\n\tVarBit\t   *arg2 = PG_GETARG_VARBIT_P(1);\n\tint32\t\tresult;\n\n\tresult = bit_cmp(arg1, arg2);\n\n\tPG_FREE_IF_COPY(arg1, 0);\n\tPG_FREE_IF_COPY(arg2, 1);\n\n\tPG_RETURN_INT32(result);\n}\n\n/*\n * bitcat\n * Concatenation of bit strings\n */\nDatum\nbitcat(PG_FUNCTION_ARGS)\n{\n\tVarBit\t   *arg1 = PG_GETARG_VARBIT_P(0);\n\tVarBit\t   *arg2 = PG_GETARG_VARBIT_P(1);\n\n\tPG_RETURN_VARBIT_P(bit_catenate(arg1, arg2));\n}\n\nstatic VarBit *\nbit_catenate(VarBit *arg1, VarBit *arg2)\n{\n\tVarBit\t   *result;\n\tint\t\t\tbitlen1,\n\t\t\t\tbitlen2,\n\t\t\t\tbytelen,\n\t\t\t\tbit1pad,\n\t\t\t\tbit2shift;\n\tbits8\t   *pr,\n\t\t\t   *pa;\n\n\tbitlen1 = VARBITLEN(arg1);\n\tbitlen2 = VARBITLEN(arg2);\n\n\tif (bitlen1 > VARBITMAXLEN - bitlen2)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t\t errmsg(\"bit string length exceeds the maximum allowed (%d)\",\n\t\t\t\t\t\tVARBITMAXLEN)));\n\tbytelen = VARBITTOTALLEN(bitlen1 + bitlen2);\n\n\tresult = (VarBit *) palloc(bytelen);\n\tSET_VARSIZE(result, bytelen);\n\tVARBITLEN(result) = bitlen1 + bitlen2;\n\n\t/* Copy the first bitstring in */\n\tmemcpy(VARBITS(result), VARBITS(arg1), VARBITBYTES(arg1));\n\n\t/* Copy the second bit string */\n\tbit1pad = VARBITPAD(arg1);\n\tif (bit1pad == 0)\n\t{\n\t\tmemcpy(VARBITS(result) + VARBITBYTES(arg1), VARBITS(arg2),\n\t\t\t   VARBITBYTES(arg2));\n\t}\n\telse if (bitlen2 > 0)\n\t{\n\t\t/* We need to shift all the bits to fit */\n\t\tbit2shift = BITS_PER_BYTE - bit1pad;\n\t\tpr = VARBITS(result) + VARBITBYTES(arg1) - 1;\n\t\tfor (pa = VARBITS(arg2); pa < VARBITEND(arg2); pa++)\n\t\t{\n\t\t\t*pr |= ((*pa >> bit2shift) & BITMASK);\n\t\t\tpr++;\n\t\t\tif (pr < VARBITEND(result))\n\t\t\t\t*pr = (*pa << bit1pad) & BITMASK;\n\t\t}\n\t}\n\n\treturn result;\n}\n\n/*\n * bitsubstr\n * retrieve a substring from the bit string.\n * Note, s is 1-based.\n * SQL draft 6.10 9)\n */\nDatum\nbitsubstr(PG_FUNCTION_ARGS)\n{\n\tPG_RETURN_VARBIT_P(bitsubstring(PG_GETARG_VARBIT_P(0),\n\t\t\t\t\t\t\t\t\tPG_GETARG_INT32(1),\n\t\t\t\t\t\t\t\t\tPG_GETARG_INT32(2),\n\t\t\t\t\t\t\t\t\tfalse));\n}\n\nDatum\nbitsubstr_no_len(PG_FUNCTION_ARGS)\n{\n\tPG_RETURN_VARBIT_P(bitsubstring(PG_GETARG_VARBIT_P(0),\n\t\t\t\t\t\t\t\t\tPG_GETARG_INT32(1),\n\t\t\t\t\t\t\t\t\t-1, true));\n}\n\nstatic VarBit *\nbitsubstring(VarBit *arg, int32 s, int32 l, bool length_not_specified)\n{\n\tVarBit\t   *result;\n\tint\t\t\tbitlen,\n\t\t\t\trbitlen,\n\t\t\t\tlen,\n\t\t\t\tipad = 0,\n\t\t\t\tishift,\n\t\t\t\ti;\n\tint\t\t\te,\n\t\t\t\ts1,\n\t\t\t\te1;\n\tbits8\t\tmask,\n\t\t\t   *r,\n\t\t\t   *ps;\n\n\tbitlen = VARBITLEN(arg);\n\ts1 = Max(s, 1);\n\t/* If we do not have an upper bound, use end of string */\n\tif (length_not_specified)\n\t{\n\t\te1 = bitlen + 1;\n\t}\n\telse\n\t{\n\t\te = s + l;\n\n\t\t/*\n\t\t * A negative value for L is the only way for the end position to be\n\t\t * before the start. SQL99 says to throw an error.\n\t\t */\n\t\tif (e < s)\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_SUBSTRING_ERROR),\n\t\t\t\t\t errmsg(\"negative substring length not allowed\")));\n\t\te1 = Min(e, bitlen + 1);\n\t}\n\tif (s1 > bitlen || e1 <= s1)\n\t{\n\t\t/* Need to return a zero-length bitstring */\n\t\tlen = VARBITTOTALLEN(0);\n\t\tresult = (VarBit *) palloc(len);\n\t\tSET_VARSIZE(result, len);\n\t\tVARBITLEN(result) = 0;\n\t}\n\telse\n\t{\n\t\t/*\n\t\t * OK, we've got a true substring starting at position s1-1 and ending\n\t\t * at position e1-1\n\t\t */\n\t\trbitlen = e1 - s1;\n\t\tlen = VARBITTOTALLEN(rbitlen);\n\t\tresult = (VarBit *) palloc(len);\n\t\tSET_VARSIZE(result, len);\n\t\tVARBITLEN(result) = rbitlen;\n\t\tlen -= VARHDRSZ + VARBITHDRSZ;\n\t\t/* Are we copying from a byte boundary? */\n\t\tif ((s1 - 1) % BITS_PER_BYTE == 0)\n\t\t{\n\t\t\t/* Yep, we are copying bytes */\n\t\t\tmemcpy(VARBITS(result), VARBITS(arg) + (s1 - 1) / BITS_PER_BYTE,\n\t\t\t\t   len);\n\t\t}\n\t\telse\n\t\t{\n\t\t\t/* Figure out how much we need to shift the sequence by */\n\t\t\tishift = (s1 - 1) % BITS_PER_BYTE;\n\t\t\tr = VARBITS(result);\n\t\t\tps = VARBITS(arg) + (s1 - 1) / BITS_PER_BYTE;\n\t\t\tfor (i = 0; i < len; i++)\n\t\t\t{\n\t\t\t\t*r = (*ps << ishift) & BITMASK;\n\t\t\t\tif ((++ps) < VARBITEND(arg))\n\t\t\t\t\t*r |= *ps >> (BITS_PER_BYTE - ishift);\n\t\t\t\tr++;\n\t\t\t}\n\t\t}\n\t\t/* Do we need to pad at the end? */\n\t\tipad = VARBITPAD(result);\n\t\tif (ipad > 0)\n\t\t{\n\t\t\tmask = BITMASK << ipad;\n\t\t\t*(VARBITS(result) + len - 1) &= mask;\n\t\t}\n\t}\n\n\treturn result;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 472,
        "critical_vars": [
            "slen"
        ],
        "function": "varbit_in",
        "filename": "postgres/CVE-2014-0064/CVE-2014-0064_CWE-189_31400a673325147e1205326008e32135a78b4d8a_varbit.c.diff",
        "label": "True",
        "function_code": "Datum\nvarbit_in(PG_FUNCTION_ARGS)\n{\n\tchar\t   *input_string = PG_GETARG_CSTRING(0);\n\n#ifdef NOT_USED\n\tOid\t\t\ttypelem = PG_GETARG_OID(1);\n#endif\n\tint32\t\tatttypmod = PG_GETARG_INT32(2);\n\tVarBit\t   *result;\t\t\t/* The resulting bit string\t\t\t  */\n\tchar\t   *sp;\t\t\t\t/* pointer into the character string  */\n\tbits8\t   *r;\t\t\t\t/* pointer into the result */\n\tint\t\t\tlen,\t\t\t/* Length of the whole data structure */\n\t\t\t\tbitlen,\t\t\t/* Number of bits in the bit string   */\n\t\t\t\tslen;\t\t\t/* Length of the input string\t\t  */\n\tbool\t\tbit_not_hex;\t/* false = hex string  true = bit string */\n\tint\t\t\tbc;\n\tbits8\t\tx = 0;\n\n\t/* Check that the first character is a b or an x */\n\tif (input_string[0] == 'b' || input_string[0] == 'B')\n\t{\n\t\tbit_not_hex = true;\n\t\tsp = input_string + 1;\n\t}\n\telse if (input_string[0] == 'x' || input_string[0] == 'X')\n\t{\n\t\tbit_not_hex = false;\n\t\tsp = input_string + 1;\n\t}\n\telse\n\t{\n\t\tbit_not_hex = true;\n\t\tsp = input_string;\n\t}\n\n\t/*\n\t * Determine bitlength from input string.  MaxAllocSize ensures a regular\n\t * input is small enough, but we must check hex input.\n\t */\n\tslen = strlen(sp);\n\tif (bit_not_hex)\n\t\tbitlen = slen;\n\telse\n\t{\n\t\tif (slen > VARBITMAXLEN / 4)\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t\t errmsg(\"bit string length exceeds the maximum allowed (%d)\",\n\t\t\t\t\t\tVARBITMAXLEN)));\n\t\tbitlen = slen * 4;\n\t}\n\n\t/*\n\t * Sometimes atttypmod is not supplied. If it is supplied we need to make\n\t * sure that the bitstring fits.\n\t */\n\tif (atttypmod <= 0)\n\t\tatttypmod = bitlen;\n\telse if (bitlen > atttypmod)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_STRING_DATA_RIGHT_TRUNCATION),\n\t\t\t\t errmsg(\"bit string too long for type bit varying(%d)\",\n\t\t\t\t\t\tatttypmod)));\n\n\tlen = VARBITTOTALLEN(bitlen);\n\t/* set to 0 so that *r is always initialised and string is zero-padded */\n\tresult = (VarBit *) palloc0(len);\n\tSET_VARSIZE(result, len);\n\tVARBITLEN(result) = Min(bitlen, atttypmod);\n\n\tr = VARBITS(result);\n\tif (bit_not_hex)\n\t{\n\t\t/* Parse the bit representation of the string */\n\t\t/* We know it fits, as bitlen was compared to atttypmod */\n\t\tx = HIGHBIT;\n\t\tfor (; *sp; sp++)\n\t\t{\n\t\t\tif (*sp == '1')\n\t\t\t\t*r |= x;\n\t\t\telse if (*sp != '0')\n\t\t\t\tereport(ERROR,\n\t\t\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t\t\t\t errmsg(\"\\\"%c\\\" is not a valid binary digit\",\n\t\t\t\t\t\t\t\t*sp)));\n\n\t\t\tx >>= 1;\n\t\t\tif (x == 0)\n\t\t\t{\n\t\t\t\tx = HIGHBIT;\n\t\t\t\tr++;\n\t\t\t}\n\t\t}\n\t}\n\telse\n\t{\n\t\t/* Parse the hex representation of the string */\n\t\tfor (bc = 0; *sp; sp++)\n\t\t{\n\t\t\tif (*sp >= '0' && *sp <= '9')\n\t\t\t\tx = (bits8) (*sp - '0');\n\t\t\telse if (*sp >= 'A' && *sp <= 'F')\n\t\t\t\tx = (bits8) (*sp - 'A') + 10;\n\t\t\telse if (*sp >= 'a' && *sp <= 'f')\n\t\t\t\tx = (bits8) (*sp - 'a') + 10;\n\t\t\telse\n\t\t\t\tereport(ERROR,\n\t\t\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t\t\t\t errmsg(\"\\\"%c\\\" is not a valid hexadecimal digit\",\n\t\t\t\t\t\t\t\t*sp)));\n\n\t\t\tif (bc)\n\t\t\t{\n\t\t\t\t*r++ |= x;\n\t\t\t\tbc = 0;\n\t\t\t}\n\t\t\telse\n\t\t\t{\n\t\t\t\t*r = x << 4;\n\t\t\t\tbc = 1;\n\t\t\t}\n\t\t}\n\t}\n\n\tPG_RETURN_VARBIT_P(result);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 970,
        "critical_vars": [
            "bitlen1"
        ],
        "function": "bit_catenate",
        "filename": "postgres/CVE-2014-0064/CVE-2014-0064_CWE-189_31400a673325147e1205326008e32135a78b4d8a_varbit.c.diff",
        "label": "True",
        "function_code": "\nstatic VarBit *bit_catenate(VarBit *arg1, VarBit *arg2);\nstatic VarBit *bitsubstring(VarBit *arg, int32 s, int32 l,\n\t\t\t bool length_not_specified);\nstatic VarBit *bit_overlay(VarBit *t1, VarBit *t2, int sp, int sl);\n\n\n/*\n * common code for bittypmodin and varbittypmodin\n */\nstatic int32\nanybit_typmodin(ArrayType *ta, const char *typename)\n{\n\tint32\t\ttypmod;\n\tint32\t   *tl;\n\tint\t\t\tn;\n\n\ttl = ArrayGetIntegerTypmods(ta, &n);\n\n\t/*\n\t * we're not too tense about good error message here because grammar\n\t * shouldn't allow wrong number of modifiers for BIT\n\t */\n\tif (n != 1)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_PARAMETER_VALUE),\n\t\t\t\t errmsg(\"invalid type modifier\")));\n\n\tif (*tl < 1)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_PARAMETER_VALUE),\n\t\t\t\t errmsg(\"length for type %s must be at least 1\",\n\t\t\t\t\t\ttypename)));\n\tif (*tl > (MaxAttrSize * BITS_PER_BYTE))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_PARAMETER_VALUE),\n\t\t\t\t errmsg(\"length for type %s cannot exceed %d\",\n\t\t\t\t\t\ttypename, MaxAttrSize * BITS_PER_BYTE)));\n\n\ttypmod = *tl;\n\n\treturn typmod;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 68,
        "critical_vars": [
            "num"
        ],
        "function": "ltree_in",
        "filename": "postgres/CVE-2014-0064/CVE-2014-0064_CWE-189_31400a673325147e1205326008e32135a78b4d8a_ltree_io.c.diff",
        "label": "True",
        "function_code": "\nDatum\nltree_in(PG_FUNCTION_ARGS)\n{\n\tchar\t   *buf = (char *) PG_GETARG_POINTER(0);\n\tchar\t   *ptr;\n\tnodeitem   *list,\n\t\t\t   *lptr;\n\tint\t\t\tnum = 0,\n\t\t\t\ttotallen = 0;\n\tint\t\t\tstate = LTPRS_WAITNAME;\n\tltree\t   *result;\n\tltree_level *curlevel;\n\tint\t\t\tcharlen;\n\tint\t\t\tpos = 0;\n\n\tptr = buf;\n\twhile (*ptr)\n\t{\n\t\tcharlen = pg_mblen(ptr);\n\t\tif (charlen == 1 && t_iseq(ptr, '.'))\n\t\t\tnum++;\n\t\tptr += charlen;\n\t}\n\n\tif (num + 1 > MaxAllocSize / sizeof(nodeitem))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t errmsg(\"number of levels (%d) exceeds the maximum allowed (%d)\",\n\t\t\t\t\tnum + 1, (int) (MaxAllocSize / sizeof(nodeitem)))));\n\tlist = lptr = (nodeitem *) palloc(sizeof(nodeitem) * (num + 1));\n\tptr = buf;\n\twhile (*ptr)\n\t{\n\t\tcharlen = pg_mblen(ptr);\n\n\t\tif (state == LTPRS_WAITNAME)\n\t\t{\n\t\t\tif (ISALNUM(ptr))\n\t\t\t{\n\t\t\t\tlptr->start = ptr;\n\t\t\t\tlptr->wlen = 0;\n\t\t\t\tstate = LTPRS_WAITDELIM;\n\t\t\t}\n\t\t\telse\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse if (state == LTPRS_WAITDELIM)\n\t\t{\n\t\t\tif (charlen == 1 && t_iseq(ptr, '.'))\n\t\t\t{\n\t\t\t\tlptr->len = ptr - lptr->start;\n\t\t\t\tif (lptr->wlen > 255)\n\t\t\t\t\tereport(ERROR,\n\t\t\t\t\t\t\t(errcode(ERRCODE_NAME_TOO_LONG),\n\t\t\t\t\t\t\t errmsg(\"name of level is too long\"),\n\t\t\t\t\t\t\t errdetail(\"Name length is %d, must \"\n\t\t\t\t\t\t\t\t\t   \"be < 256, in position %d.\",\n\t\t\t\t\t\t\t\t\t   lptr->wlen, pos)));\n\n\t\t\t\ttotallen += MAXALIGN(lptr->len + LEVEL_HDRSIZE);\n\t\t\t\tlptr++;\n\t\t\t\tstate = LTPRS_WAITNAME;\n\t\t\t}\n\t\t\telse if (!ISALNUM(ptr))\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse\n\t\t\t/* internal error */\n\t\t\telog(ERROR, \"internal error in parser\");\n\n\t\tptr += charlen;\n\t\tlptr->wlen++;\n\t\tpos++;\n\t}\n\n\tif (state == LTPRS_WAITDELIM)\n\t{\n\t\tlptr->len = ptr - lptr->start;\n\t\tif (lptr->wlen > 255)\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_NAME_TOO_LONG),\n\t\t\t\t\t errmsg(\"name of level is too long\"),\n\t\t\t\t\t errdetail(\"Name length is %d, must \"\n\t\t\t\t\t\t\t   \"be < 256, in position %d.\",\n\t\t\t\t\t\t\t   lptr->wlen, pos)));\n\n\t\ttotallen += MAXALIGN(lptr->len + LEVEL_HDRSIZE);\n\t\tlptr++;\n\t}\n\telse if (!(state == LTPRS_WAITNAME && lptr == list))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_SYNTAX_ERROR),\n\t\t\t\t errmsg(\"syntax error\"),\n\t\t\t\t errdetail(\"Unexpected end of line.\")));\n\n\tresult = (ltree *) palloc0(LTREE_HDRSIZE + totallen);\n\tSET_VARSIZE(result, LTREE_HDRSIZE + totallen);\n\tresult->numlevel = lptr - list;\n\tcurlevel = LTREE_FIRST(result);\n\tlptr = list;\n\twhile (lptr - list < result->numlevel)\n\t{\n\t\tcurlevel->len = (uint16) lptr->len;\n\t\tmemcpy(curlevel->name, lptr->start, lptr->len);\n\t\tcurlevel = LEVEL_NEXT(curlevel);\n\t\tlptr++;\n\t}\n\n\tpfree(list);\n\tPG_RETURN_POINTER(result);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 69,
        "critical_vars": [
            "num",
            "nodeitem"
        ],
        "function": "ltree_in",
        "filename": "postgres/CVE-2014-0064/CVE-2014-0064_CWE-189_31400a673325147e1205326008e32135a78b4d8a_ltree_io.c.diff",
        "label": "True",
        "function_code": "\nDatum\nltree_in(PG_FUNCTION_ARGS)\n{\n\tchar\t   *buf = (char *) PG_GETARG_POINTER(0);\n\tchar\t   *ptr;\n\tnodeitem   *list,\n\t\t\t   *lptr;\n\tint\t\t\tnum = 0,\n\t\t\t\ttotallen = 0;\n\tint\t\t\tstate = LTPRS_WAITNAME;\n\tltree\t   *result;\n\tltree_level *curlevel;\n\tint\t\t\tcharlen;\n\tint\t\t\tpos = 0;\n\n\tptr = buf;\n\twhile (*ptr)\n\t{\n\t\tcharlen = pg_mblen(ptr);\n\t\tif (charlen == 1 && t_iseq(ptr, '.'))\n\t\t\tnum++;\n\t\tptr += charlen;\n\t}\n\n\tif (num + 1 > MaxAllocSize / sizeof(nodeitem))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t errmsg(\"number of levels (%d) exceeds the maximum allowed (%d)\",\n\t\t\t\t\tnum + 1, (int) (MaxAllocSize / sizeof(nodeitem)))));\n\tlist = lptr = (nodeitem *) palloc(sizeof(nodeitem) * (num + 1));\n\tptr = buf;\n\twhile (*ptr)\n\t{\n\t\tcharlen = pg_mblen(ptr);\n\n\t\tif (state == LTPRS_WAITNAME)\n\t\t{\n\t\t\tif (ISALNUM(ptr))\n\t\t\t{\n\t\t\t\tlptr->start = ptr;\n\t\t\t\tlptr->wlen = 0;\n\t\t\t\tstate = LTPRS_WAITDELIM;\n\t\t\t}\n\t\t\telse\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse if (state == LTPRS_WAITDELIM)\n\t\t{\n\t\t\tif (charlen == 1 && t_iseq(ptr, '.'))\n\t\t\t{\n\t\t\t\tlptr->len = ptr - lptr->start;\n\t\t\t\tif (lptr->wlen > 255)\n\t\t\t\t\tereport(ERROR,\n\t\t\t\t\t\t\t(errcode(ERRCODE_NAME_TOO_LONG),\n\t\t\t\t\t\t\t errmsg(\"name of level is too long\"),\n\t\t\t\t\t\t\t errdetail(\"Name length is %d, must \"\n\t\t\t\t\t\t\t\t\t   \"be < 256, in position %d.\",\n\t\t\t\t\t\t\t\t\t   lptr->wlen, pos)));\n\n\t\t\t\ttotallen += MAXALIGN(lptr->len + LEVEL_HDRSIZE);\n\t\t\t\tlptr++;\n\t\t\t\tstate = LTPRS_WAITNAME;\n\t\t\t}\n\t\t\telse if (!ISALNUM(ptr))\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse\n\t\t\t/* internal error */\n\t\t\telog(ERROR, \"internal error in parser\");\n\n\t\tptr += charlen;\n\t\tlptr->wlen++;\n\t\tpos++;\n\t}\n\n\tif (state == LTPRS_WAITDELIM)\n\t{\n\t\tlptr->len = ptr - lptr->start;\n\t\tif (lptr->wlen > 255)\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_NAME_TOO_LONG),\n\t\t\t\t\t errmsg(\"name of level is too long\"),\n\t\t\t\t\t errdetail(\"Name length is %d, must \"\n\t\t\t\t\t\t\t   \"be < 256, in position %d.\",\n\t\t\t\t\t\t\t   lptr->wlen, pos)));\n\n\t\ttotallen += MAXALIGN(lptr->len + LEVEL_HDRSIZE);\n\t\tlptr++;\n\t}\n\telse if (!(state == LTPRS_WAITNAME && lptr == list))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_SYNTAX_ERROR),\n\t\t\t\t errmsg(\"syntax error\"),\n\t\t\t\t errdetail(\"Unexpected end of line.\")));\n\n\tresult = (ltree *) palloc0(LTREE_HDRSIZE + totallen);\n\tSET_VARSIZE(result, LTREE_HDRSIZE + totallen);\n\tresult->numlevel = lptr - list;\n\tcurlevel = LTREE_FIRST(result);\n\tlptr = list;\n\twhile (lptr - list < result->numlevel)\n\t{\n\t\tcurlevel->len = (uint16) lptr->len;\n\t\tmemcpy(curlevel->name, lptr->start, lptr->len);\n\t\tcurlevel = LEVEL_NEXT(curlevel);\n\t\tlptr++;\n\t}\n\n\tpfree(list);\n\tPG_RETURN_POINTER(result);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 237,
        "critical_vars": [
            "num"
        ],
        "function": "lquery_in",
        "filename": "postgres/CVE-2014-0064/CVE-2014-0064_CWE-189_31400a673325147e1205326008e32135a78b4d8a_ltree_io.c.diff",
        "label": "True",
        "function_code": "\nDatum\nlquery_in(PG_FUNCTION_ARGS)\n{\n\tchar\t   *buf = (char *) PG_GETARG_POINTER(0);\n\tchar\t   *ptr;\n\tint\t\t\tnum = 0,\n\t\t\t\ttotallen = 0,\n\t\t\t\tnumOR = 0;\n\tint\t\t\tstate = LQPRS_WAITLEVEL;\n\tlquery\t   *result;\n\tnodeitem   *lptr = NULL;\n\tlquery_level *cur,\n\t\t\t   *curqlevel,\n\t\t\t   *tmpql;\n\tlquery_variant *lrptr = NULL;\n\tbool\t\thasnot = false;\n\tbool\t\twasbad = false;\n\tint\t\t\tcharlen;\n\tint\t\t\tpos = 0;\n\n\tptr = buf;\n\twhile (*ptr)\n\t{\n\t\tcharlen = pg_mblen(ptr);\n\n\t\tif (charlen == 1)\n\t\t{\n\t\t\tif (t_iseq(ptr, '.'))\n\t\t\t\tnum++;\n\t\t\telse if (t_iseq(ptr, '|'))\n\t\t\t\tnumOR++;\n\t\t}\n\n\t\tptr += charlen;\n\t}\n\n\tnum++;\n\tif (num > MaxAllocSize / ITEMSIZE)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t errmsg(\"number of levels (%d) exceeds the maximum allowed (%d)\",\n\t\t\t\t\tnum, (int) (MaxAllocSize / ITEMSIZE))));\n\tcurqlevel = tmpql = (lquery_level *) palloc0(ITEMSIZE * num);\n\tptr = buf;\n\twhile (*ptr)\n\t{\n\t\tcharlen = pg_mblen(ptr);\n\n\t\tif (state == LQPRS_WAITLEVEL)\n\t\t{\n\t\t\tif (ISALNUM(ptr))\n\t\t\t{\n\t\t\t\tGETVAR(curqlevel) = lptr = (nodeitem *) palloc0(sizeof(nodeitem) * (numOR + 1));\n\t\t\t\tlptr->start = ptr;\n\t\t\t\tstate = LQPRS_WAITDELIM;\n\t\t\t\tcurqlevel->numvar = 1;\n\t\t\t}\n\t\t\telse if (charlen == 1 && t_iseq(ptr, '!'))\n\t\t\t{\n\t\t\t\tGETVAR(curqlevel) = lptr = (nodeitem *) palloc0(sizeof(nodeitem) * (numOR + 1));\n\t\t\t\tlptr->start = ptr + 1;\n\t\t\t\tstate = LQPRS_WAITDELIM;\n\t\t\t\tcurqlevel->numvar = 1;\n\t\t\t\tcurqlevel->flag |= LQL_NOT;\n\t\t\t\thasnot = true;\n\t\t\t}\n\t\t\telse if (charlen == 1 && t_iseq(ptr, '*'))\n\t\t\t\tstate = LQPRS_WAITOPEN;\n\t\t\telse\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse if (state == LQPRS_WAITVAR)\n\t\t{\n\t\t\tif (ISALNUM(ptr))\n\t\t\t{\n\t\t\t\tlptr++;\n\t\t\t\tlptr->start = ptr;\n\t\t\t\tstate = LQPRS_WAITDELIM;\n\t\t\t\tcurqlevel->numvar++;\n\t\t\t}\n\t\t\telse\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse if (state == LQPRS_WAITDELIM)\n\t\t{\n\t\t\tif (charlen == 1 && t_iseq(ptr, '@'))\n\t\t\t{\n\t\t\t\tif (lptr->start == ptr)\n\t\t\t\t\tUNCHAR;\n\t\t\t\tlptr->flag |= LVAR_INCASE;\n\t\t\t\tcurqlevel->flag |= LVAR_INCASE;\n\t\t\t}\n\t\t\telse if (charlen == 1 && t_iseq(ptr, '*'))\n\t\t\t{\n\t\t\t\tif (lptr->start == ptr)\n\t\t\t\t\tUNCHAR;\n\t\t\t\tlptr->flag |= LVAR_ANYEND;\n\t\t\t\tcurqlevel->flag |= LVAR_ANYEND;\n\t\t\t}\n\t\t\telse if (charlen == 1 && t_iseq(ptr, '%'))\n\t\t\t{\n\t\t\t\tif (lptr->start == ptr)\n\t\t\t\t\tUNCHAR;\n\t\t\t\tlptr->flag |= LVAR_SUBLEXEME;\n\t\t\t\tcurqlevel->flag |= LVAR_SUBLEXEME;\n\t\t\t}\n\t\t\telse if (charlen == 1 && t_iseq(ptr, '|'))\n\t\t\t{\n\t\t\t\tlptr->len = ptr - lptr->start -\n\t\t\t\t\t((lptr->flag & LVAR_SUBLEXEME) ? 1 : 0) -\n\t\t\t\t\t((lptr->flag & LVAR_INCASE) ? 1 : 0) -\n\t\t\t\t\t((lptr->flag & LVAR_ANYEND) ? 1 : 0);\n\t\t\t\tif (lptr->wlen > 255)\n\t\t\t\t\tereport(ERROR,\n\t\t\t\t\t\t\t(errcode(ERRCODE_NAME_TOO_LONG),\n\t\t\t\t\t\t\t errmsg(\"name of level is too long\"),\n\t\t\t\t\t\t\t errdetail(\"Name length is %d, must \"\n\t\t\t\t\t\t\t\t\t   \"be < 256, in position %d.\",\n\t\t\t\t\t\t\t\t\t   lptr->wlen, pos)));\n\n\t\t\t\tstate = LQPRS_WAITVAR;\n\t\t\t}\n\t\t\telse if (charlen == 1 && t_iseq(ptr, '.'))\n\t\t\t{\n\t\t\t\tlptr->len = ptr - lptr->start -\n\t\t\t\t\t((lptr->flag & LVAR_SUBLEXEME) ? 1 : 0) -\n\t\t\t\t\t((lptr->flag & LVAR_INCASE) ? 1 : 0) -\n\t\t\t\t\t((lptr->flag & LVAR_ANYEND) ? 1 : 0);\n\t\t\t\tif (lptr->wlen > 255)\n\t\t\t\t\tereport(ERROR,\n\t\t\t\t\t\t\t(errcode(ERRCODE_NAME_TOO_LONG),\n\t\t\t\t\t\t\t errmsg(\"name of level is too long\"),\n\t\t\t\t\t\t\t errdetail(\"Name length is %d, must \"\n\t\t\t\t\t\t\t\t\t   \"be < 256, in position %d.\",\n\t\t\t\t\t\t\t\t\t   lptr->wlen, pos)));\n\n\t\t\t\tstate = LQPRS_WAITLEVEL;\n\t\t\t\tcurqlevel = NEXTLEV(curqlevel);\n\t\t\t}\n\t\t\telse if (ISALNUM(ptr))\n\t\t\t{\n\t\t\t\tif (lptr->flag)\n\t\t\t\t\tUNCHAR;\n\t\t\t}\n\t\t\telse\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse if (state == LQPRS_WAITOPEN)\n\t\t{\n\t\t\tif (charlen == 1 && t_iseq(ptr, '{'))\n\t\t\t\tstate = LQPRS_WAITFNUM;\n\t\t\telse if (charlen == 1 && t_iseq(ptr, '.'))\n\t\t\t{\n\t\t\t\tcurqlevel->low = 0;\n\t\t\t\tcurqlevel->high = 0xffff;\n\t\t\t\tcurqlevel = NEXTLEV(curqlevel);\n\t\t\t\tstate = LQPRS_WAITLEVEL;\n\t\t\t}\n\t\t\telse\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse if (state == LQPRS_WAITFNUM)\n\t\t{\n\t\t\tif (charlen == 1 && t_iseq(ptr, ','))\n\t\t\t\tstate = LQPRS_WAITSNUM;\n\t\t\telse if (t_isdigit(ptr))\n\t\t\t{\n\t\t\t\tcurqlevel->low = atoi(ptr);\n\t\t\t\tstate = LQPRS_WAITND;\n\t\t\t}\n\t\t\telse\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse if (state == LQPRS_WAITSNUM)\n\t\t{\n\t\t\tif (t_isdigit(ptr))\n\t\t\t{\n\t\t\t\tcurqlevel->high = atoi(ptr);\n\t\t\t\tstate = LQPRS_WAITCLOSE;\n\t\t\t}\n\t\t\telse if (charlen == 1 && t_iseq(ptr, '}'))\n\t\t\t{\n\t\t\t\tcurqlevel->high = 0xffff;\n\t\t\t\tstate = LQPRS_WAITEND;\n\t\t\t}\n\t\t\telse\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse if (state == LQPRS_WAITCLOSE)\n\t\t{\n\t\t\tif (charlen == 1 && t_iseq(ptr, '}'))\n\t\t\t\tstate = LQPRS_WAITEND;\n\t\t\telse if (!t_isdigit(ptr))\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse if (state == LQPRS_WAITND)\n\t\t{\n\t\t\tif (charlen == 1 && t_iseq(ptr, '}'))\n\t\t\t{\n\t\t\t\tcurqlevel->high = curqlevel->low;\n\t\t\t\tstate = LQPRS_WAITEND;\n\t\t\t}\n\t\t\telse if (charlen == 1 && t_iseq(ptr, ','))\n\t\t\t\tstate = LQPRS_WAITSNUM;\n\t\t\telse if (!t_isdigit(ptr))\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse if (state == LQPRS_WAITEND)\n\t\t{\n\t\t\tif (charlen == 1 && t_iseq(ptr, '.'))\n\t\t\t{\n\t\t\t\tstate = LQPRS_WAITLEVEL;\n\t\t\t\tcurqlevel = NEXTLEV(curqlevel);\n\t\t\t}\n\t\t\telse\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse\n\t\t\t/* internal error */\n\t\t\telog(ERROR, \"internal error in parser\");\n\n\t\tptr += charlen;\n\t\tif (state == LQPRS_WAITDELIM)\n\t\t\tlptr->wlen++;\n\t\tpos++;\n\t}\n\n\tif (state == LQPRS_WAITDELIM)\n\t{\n\t\tif (lptr->start == ptr)\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_SYNTAX_ERROR),\n\t\t\t\t\t errmsg(\"syntax error\"),\n\t\t\t\t\t errdetail(\"Unexpected end of line.\")));\n\n\t\tlptr->len = ptr - lptr->start -\n\t\t\t((lptr->flag & LVAR_SUBLEXEME) ? 1 : 0) -\n\t\t\t((lptr->flag & LVAR_INCASE) ? 1 : 0) -\n\t\t\t((lptr->flag & LVAR_ANYEND) ? 1 : 0);\n\t\tif (lptr->len == 0)\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_SYNTAX_ERROR),\n\t\t\t\t\t errmsg(\"syntax error\"),\n\t\t\t\t\t errdetail(\"Unexpected end of line.\")));\n\n\t\tif (lptr->wlen > 255)\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_NAME_TOO_LONG),\n\t\t\t\t\t errmsg(\"name of level is too long\"),\n\t\t\t\t\t errdetail(\"Name length is %d, must \"\n\t\t\t\t\t\t\t   \"be < 256, in position %d.\",\n\t\t\t\t\t\t\t   lptr->wlen, pos)));\n\t}\n\telse if (state == LQPRS_WAITOPEN)\n\t\tcurqlevel->high = 0xffff;\n\telse if (state != LQPRS_WAITEND)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_SYNTAX_ERROR),\n\t\t\t\t errmsg(\"syntax error\"),\n\t\t\t\t errdetail(\"Unexpected end of line.\")));\n\n\tcurqlevel = tmpql;\n\ttotallen = LQUERY_HDRSIZE;\n\twhile ((char *) curqlevel - (char *) tmpql < num * ITEMSIZE)\n\t{\n\t\ttotallen += LQL_HDRSIZE;\n\t\tif (curqlevel->numvar)\n\t\t{\n\t\t\tlptr = GETVAR(curqlevel);\n\t\t\twhile (lptr - GETVAR(curqlevel) < curqlevel->numvar)\n\t\t\t{\n\t\t\t\ttotallen += MAXALIGN(LVAR_HDRSIZE + lptr->len);\n\t\t\t\tlptr++;\n\t\t\t}\n\t\t}\n\t\telse if (curqlevel->low > curqlevel->high)\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_SYNTAX_ERROR),\n\t\t\t\t\t errmsg(\"syntax error\"),\n\t\t\t\t\t errdetail(\"Low limit(%d) is greater than upper(%d).\",\n\t\t\t\t\t\t\t   curqlevel->low, curqlevel->high)));\n\n\t\tcurqlevel = NEXTLEV(curqlevel);\n\t}\n\n\tresult = (lquery *) palloc0(totallen);\n\tSET_VARSIZE(result, totallen);\n\tresult->numlevel = num;\n\tresult->firstgood = 0;\n\tresult->flag = 0;\n\tif (hasnot)\n\t\tresult->flag |= LQUERY_HASNOT;\n\tcur = LQUERY_FIRST(result);\n\tcurqlevel = tmpql;\n\twhile ((char *) curqlevel - (char *) tmpql < num * ITEMSIZE)\n\t{\n\t\tmemcpy(cur, curqlevel, LQL_HDRSIZE);\n\t\tcur->totallen = LQL_HDRSIZE;\n\t\tif (curqlevel->numvar)\n\t\t{\n\t\t\tlrptr = LQL_FIRST(cur);\n\t\t\tlptr = GETVAR(curqlevel);\n\t\t\twhile (lptr - GETVAR(curqlevel) < curqlevel->numvar)\n\t\t\t{\n\t\t\t\tcur->totallen += MAXALIGN(LVAR_HDRSIZE + lptr->len);\n\t\t\t\tlrptr->len = lptr->len;\n\t\t\t\tlrptr->flag = lptr->flag;\n\t\t\t\tlrptr->val = ltree_crc32_sz(lptr->start, lptr->len);\n\t\t\t\tmemcpy(lrptr->name, lptr->start, lptr->len);\n\t\t\t\tlptr++;\n\t\t\t\tlrptr = LVAR_NEXT(lrptr);\n\t\t\t}\n\t\t\tpfree(GETVAR(curqlevel));\n\t\t\tif (cur->numvar > 1 || cur->flag != 0)\n\t\t\t\twasbad = true;\n\t\t\telse if (wasbad == false)\n\t\t\t\t(result->firstgood)++;\n\t\t}\n\t\telse\n\t\t\twasbad = true;\n\t\tcurqlevel = NEXTLEV(curqlevel);\n\t\tcur = LQL_NEXT(cur);\n\t}\n\n\tpfree(tmpql);\n\tPG_RETURN_POINTER(result);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 238,
        "critical_vars": [
            "num",
            "MaxAllocSize"
        ],
        "function": "lquery_in",
        "filename": "postgres/CVE-2014-0064/CVE-2014-0064_CWE-189_31400a673325147e1205326008e32135a78b4d8a_ltree_io.c.diff",
        "label": "True",
        "function_code": "\nDatum\nlquery_in(PG_FUNCTION_ARGS)\n{\n\tchar\t   *buf = (char *) PG_GETARG_POINTER(0);\n\tchar\t   *ptr;\n\tint\t\t\tnum = 0,\n\t\t\t\ttotallen = 0,\n\t\t\t\tnumOR = 0;\n\tint\t\t\tstate = LQPRS_WAITLEVEL;\n\tlquery\t   *result;\n\tnodeitem   *lptr = NULL;\n\tlquery_level *cur,\n\t\t\t   *curqlevel,\n\t\t\t   *tmpql;\n\tlquery_variant *lrptr = NULL;\n\tbool\t\thasnot = false;\n\tbool\t\twasbad = false;\n\tint\t\t\tcharlen;\n\tint\t\t\tpos = 0;\n\n\tptr = buf;\n\twhile (*ptr)\n\t{\n\t\tcharlen = pg_mblen(ptr);\n\n\t\tif (charlen == 1)\n\t\t{\n\t\t\tif (t_iseq(ptr, '.'))\n\t\t\t\tnum++;\n\t\t\telse if (t_iseq(ptr, '|'))\n\t\t\t\tnumOR++;\n\t\t}\n\n\t\tptr += charlen;\n\t}\n\n\tnum++;\n\tif (num > MaxAllocSize / ITEMSIZE)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t errmsg(\"number of levels (%d) exceeds the maximum allowed (%d)\",\n\t\t\t\t\tnum, (int) (MaxAllocSize / ITEMSIZE))));\n\tcurqlevel = tmpql = (lquery_level *) palloc0(ITEMSIZE * num);\n\tptr = buf;\n\twhile (*ptr)\n\t{\n\t\tcharlen = pg_mblen(ptr);\n\n\t\tif (state == LQPRS_WAITLEVEL)\n\t\t{\n\t\t\tif (ISALNUM(ptr))\n\t\t\t{\n\t\t\t\tGETVAR(curqlevel) = lptr = (nodeitem *) palloc0(sizeof(nodeitem) * (numOR + 1));\n\t\t\t\tlptr->start = ptr;\n\t\t\t\tstate = LQPRS_WAITDELIM;\n\t\t\t\tcurqlevel->numvar = 1;\n\t\t\t}\n\t\t\telse if (charlen == 1 && t_iseq(ptr, '!'))\n\t\t\t{\n\t\t\t\tGETVAR(curqlevel) = lptr = (nodeitem *) palloc0(sizeof(nodeitem) * (numOR + 1));\n\t\t\t\tlptr->start = ptr + 1;\n\t\t\t\tstate = LQPRS_WAITDELIM;\n\t\t\t\tcurqlevel->numvar = 1;\n\t\t\t\tcurqlevel->flag |= LQL_NOT;\n\t\t\t\thasnot = true;\n\t\t\t}\n\t\t\telse if (charlen == 1 && t_iseq(ptr, '*'))\n\t\t\t\tstate = LQPRS_WAITOPEN;\n\t\t\telse\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse if (state == LQPRS_WAITVAR)\n\t\t{\n\t\t\tif (ISALNUM(ptr))\n\t\t\t{\n\t\t\t\tlptr++;\n\t\t\t\tlptr->start = ptr;\n\t\t\t\tstate = LQPRS_WAITDELIM;\n\t\t\t\tcurqlevel->numvar++;\n\t\t\t}\n\t\t\telse\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse if (state == LQPRS_WAITDELIM)\n\t\t{\n\t\t\tif (charlen == 1 && t_iseq(ptr, '@'))\n\t\t\t{\n\t\t\t\tif (lptr->start == ptr)\n\t\t\t\t\tUNCHAR;\n\t\t\t\tlptr->flag |= LVAR_INCASE;\n\t\t\t\tcurqlevel->flag |= LVAR_INCASE;\n\t\t\t}\n\t\t\telse if (charlen == 1 && t_iseq(ptr, '*'))\n\t\t\t{\n\t\t\t\tif (lptr->start == ptr)\n\t\t\t\t\tUNCHAR;\n\t\t\t\tlptr->flag |= LVAR_ANYEND;\n\t\t\t\tcurqlevel->flag |= LVAR_ANYEND;\n\t\t\t}\n\t\t\telse if (charlen == 1 && t_iseq(ptr, '%'))\n\t\t\t{\n\t\t\t\tif (lptr->start == ptr)\n\t\t\t\t\tUNCHAR;\n\t\t\t\tlptr->flag |= LVAR_SUBLEXEME;\n\t\t\t\tcurqlevel->flag |= LVAR_SUBLEXEME;\n\t\t\t}\n\t\t\telse if (charlen == 1 && t_iseq(ptr, '|'))\n\t\t\t{\n\t\t\t\tlptr->len = ptr - lptr->start -\n\t\t\t\t\t((lptr->flag & LVAR_SUBLEXEME) ? 1 : 0) -\n\t\t\t\t\t((lptr->flag & LVAR_INCASE) ? 1 : 0) -\n\t\t\t\t\t((lptr->flag & LVAR_ANYEND) ? 1 : 0);\n\t\t\t\tif (lptr->wlen > 255)\n\t\t\t\t\tereport(ERROR,\n\t\t\t\t\t\t\t(errcode(ERRCODE_NAME_TOO_LONG),\n\t\t\t\t\t\t\t errmsg(\"name of level is too long\"),\n\t\t\t\t\t\t\t errdetail(\"Name length is %d, must \"\n\t\t\t\t\t\t\t\t\t   \"be < 256, in position %d.\",\n\t\t\t\t\t\t\t\t\t   lptr->wlen, pos)));\n\n\t\t\t\tstate = LQPRS_WAITVAR;\n\t\t\t}\n\t\t\telse if (charlen == 1 && t_iseq(ptr, '.'))\n\t\t\t{\n\t\t\t\tlptr->len = ptr - lptr->start -\n\t\t\t\t\t((lptr->flag & LVAR_SUBLEXEME) ? 1 : 0) -\n\t\t\t\t\t((lptr->flag & LVAR_INCASE) ? 1 : 0) -\n\t\t\t\t\t((lptr->flag & LVAR_ANYEND) ? 1 : 0);\n\t\t\t\tif (lptr->wlen > 255)\n\t\t\t\t\tereport(ERROR,\n\t\t\t\t\t\t\t(errcode(ERRCODE_NAME_TOO_LONG),\n\t\t\t\t\t\t\t errmsg(\"name of level is too long\"),\n\t\t\t\t\t\t\t errdetail(\"Name length is %d, must \"\n\t\t\t\t\t\t\t\t\t   \"be < 256, in position %d.\",\n\t\t\t\t\t\t\t\t\t   lptr->wlen, pos)));\n\n\t\t\t\tstate = LQPRS_WAITLEVEL;\n\t\t\t\tcurqlevel = NEXTLEV(curqlevel);\n\t\t\t}\n\t\t\telse if (ISALNUM(ptr))\n\t\t\t{\n\t\t\t\tif (lptr->flag)\n\t\t\t\t\tUNCHAR;\n\t\t\t}\n\t\t\telse\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse if (state == LQPRS_WAITOPEN)\n\t\t{\n\t\t\tif (charlen == 1 && t_iseq(ptr, '{'))\n\t\t\t\tstate = LQPRS_WAITFNUM;\n\t\t\telse if (charlen == 1 && t_iseq(ptr, '.'))\n\t\t\t{\n\t\t\t\tcurqlevel->low = 0;\n\t\t\t\tcurqlevel->high = 0xffff;\n\t\t\t\tcurqlevel = NEXTLEV(curqlevel);\n\t\t\t\tstate = LQPRS_WAITLEVEL;\n\t\t\t}\n\t\t\telse\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse if (state == LQPRS_WAITFNUM)\n\t\t{\n\t\t\tif (charlen == 1 && t_iseq(ptr, ','))\n\t\t\t\tstate = LQPRS_WAITSNUM;\n\t\t\telse if (t_isdigit(ptr))\n\t\t\t{\n\t\t\t\tcurqlevel->low = atoi(ptr);\n\t\t\t\tstate = LQPRS_WAITND;\n\t\t\t}\n\t\t\telse\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse if (state == LQPRS_WAITSNUM)\n\t\t{\n\t\t\tif (t_isdigit(ptr))\n\t\t\t{\n\t\t\t\tcurqlevel->high = atoi(ptr);\n\t\t\t\tstate = LQPRS_WAITCLOSE;\n\t\t\t}\n\t\t\telse if (charlen == 1 && t_iseq(ptr, '}'))\n\t\t\t{\n\t\t\t\tcurqlevel->high = 0xffff;\n\t\t\t\tstate = LQPRS_WAITEND;\n\t\t\t}\n\t\t\telse\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse if (state == LQPRS_WAITCLOSE)\n\t\t{\n\t\t\tif (charlen == 1 && t_iseq(ptr, '}'))\n\t\t\t\tstate = LQPRS_WAITEND;\n\t\t\telse if (!t_isdigit(ptr))\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse if (state == LQPRS_WAITND)\n\t\t{\n\t\t\tif (charlen == 1 && t_iseq(ptr, '}'))\n\t\t\t{\n\t\t\t\tcurqlevel->high = curqlevel->low;\n\t\t\t\tstate = LQPRS_WAITEND;\n\t\t\t}\n\t\t\telse if (charlen == 1 && t_iseq(ptr, ','))\n\t\t\t\tstate = LQPRS_WAITSNUM;\n\t\t\telse if (!t_isdigit(ptr))\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse if (state == LQPRS_WAITEND)\n\t\t{\n\t\t\tif (charlen == 1 && t_iseq(ptr, '.'))\n\t\t\t{\n\t\t\t\tstate = LQPRS_WAITLEVEL;\n\t\t\t\tcurqlevel = NEXTLEV(curqlevel);\n\t\t\t}\n\t\t\telse\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse\n\t\t\t/* internal error */\n\t\t\telog(ERROR, \"internal error in parser\");\n\n\t\tptr += charlen;\n\t\tif (state == LQPRS_WAITDELIM)\n\t\t\tlptr->wlen++;\n\t\tpos++;\n\t}\n\n\tif (state == LQPRS_WAITDELIM)\n\t{\n\t\tif (lptr->start == ptr)\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_SYNTAX_ERROR),\n\t\t\t\t\t errmsg(\"syntax error\"),\n\t\t\t\t\t errdetail(\"Unexpected end of line.\")));\n\n\t\tlptr->len = ptr - lptr->start -\n\t\t\t((lptr->flag & LVAR_SUBLEXEME) ? 1 : 0) -\n\t\t\t((lptr->flag & LVAR_INCASE) ? 1 : 0) -\n\t\t\t((lptr->flag & LVAR_ANYEND) ? 1 : 0);\n\t\tif (lptr->len == 0)\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_SYNTAX_ERROR),\n\t\t\t\t\t errmsg(\"syntax error\"),\n\t\t\t\t\t errdetail(\"Unexpected end of line.\")));\n\n\t\tif (lptr->wlen > 255)\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_NAME_TOO_LONG),\n\t\t\t\t\t errmsg(\"name of level is too long\"),\n\t\t\t\t\t errdetail(\"Name length is %d, must \"\n\t\t\t\t\t\t\t   \"be < 256, in position %d.\",\n\t\t\t\t\t\t\t   lptr->wlen, pos)));\n\t}\n\telse if (state == LQPRS_WAITOPEN)\n\t\tcurqlevel->high = 0xffff;\n\telse if (state != LQPRS_WAITEND)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_SYNTAX_ERROR),\n\t\t\t\t errmsg(\"syntax error\"),\n\t\t\t\t errdetail(\"Unexpected end of line.\")));\n\n\tcurqlevel = tmpql;\n\ttotallen = LQUERY_HDRSIZE;\n\twhile ((char *) curqlevel - (char *) tmpql < num * ITEMSIZE)\n\t{\n\t\ttotallen += LQL_HDRSIZE;\n\t\tif (curqlevel->numvar)\n\t\t{\n\t\t\tlptr = GETVAR(curqlevel);\n\t\t\twhile (lptr - GETVAR(curqlevel) < curqlevel->numvar)\n\t\t\t{\n\t\t\t\ttotallen += MAXALIGN(LVAR_HDRSIZE + lptr->len);\n\t\t\t\tlptr++;\n\t\t\t}\n\t\t}\n\t\telse if (curqlevel->low > curqlevel->high)\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_SYNTAX_ERROR),\n\t\t\t\t\t errmsg(\"syntax error\"),\n\t\t\t\t\t errdetail(\"Low limit(%d) is greater than upper(%d).\",\n\t\t\t\t\t\t\t   curqlevel->low, curqlevel->high)));\n\n\t\tcurqlevel = NEXTLEV(curqlevel);\n\t}\n\n\tresult = (lquery *) palloc0(totallen);\n\tSET_VARSIZE(result, totallen);\n\tresult->numlevel = num;\n\tresult->firstgood = 0;\n\tresult->flag = 0;\n\tif (hasnot)\n\t\tresult->flag |= LQUERY_HASNOT;\n\tcur = LQUERY_FIRST(result);\n\tcurqlevel = tmpql;\n\twhile ((char *) curqlevel - (char *) tmpql < num * ITEMSIZE)\n\t{\n\t\tmemcpy(cur, curqlevel, LQL_HDRSIZE);\n\t\tcur->totallen = LQL_HDRSIZE;\n\t\tif (curqlevel->numvar)\n\t\t{\n\t\t\tlrptr = LQL_FIRST(cur);\n\t\t\tlptr = GETVAR(curqlevel);\n\t\t\twhile (lptr - GETVAR(curqlevel) < curqlevel->numvar)\n\t\t\t{\n\t\t\t\tcur->totallen += MAXALIGN(LVAR_HDRSIZE + lptr->len);\n\t\t\t\tlrptr->len = lptr->len;\n\t\t\t\tlrptr->flag = lptr->flag;\n\t\t\t\tlrptr->val = ltree_crc32_sz(lptr->start, lptr->len);\n\t\t\t\tmemcpy(lrptr->name, lptr->start, lptr->len);\n\t\t\t\tlptr++;\n\t\t\t\tlrptr = LVAR_NEXT(lrptr);\n\t\t\t}\n\t\t\tpfree(GETVAR(curqlevel));\n\t\t\tif (cur->numvar > 1 || cur->flag != 0)\n\t\t\t\twasbad = true;\n\t\t\telse if (wasbad == false)\n\t\t\t\t(result->firstgood)++;\n\t\t}\n\t\telse\n\t\t\twasbad = true;\n\t\tcurqlevel = NEXTLEV(curqlevel);\n\t\tcur = LQL_NEXT(cur);\n\t}\n\n\tpfree(tmpql);\n\tPG_RETURN_POINTER(result);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 514,
        "critical_vars": [
            "state.num"
        ],
        "function": "bqarr_in",
        "filename": "postgres/CVE-2014-0064/CVE-2014-0064_CWE-189_31400a673325147e1205326008e32135a78b4d8a__int_bool.c.diff",
        "label": "True",
        "function_code": "Datum\nbqarr_in(PG_FUNCTION_ARGS)\n{\n\tchar\t   *buf = (char *) PG_GETARG_POINTER(0);\n\tWORKSTATE\tstate;\n\tint32\t\ti;\n\tQUERYTYPE  *query;\n\tint32\t\tcommonlen;\n\tITEM\t   *ptr;\n\tNODE\t   *tmp;\n\tint32\t\tpos = 0;\n\n#ifdef BS_DEBUG\n\tStringInfoData pbuf;\n#endif\n\n\tstate.buf = buf;\n\tstate.state = WAITOPERAND;\n\tstate.count = 0;\n\tstate.num = 0;\n\tstate.str = NULL;\n\n\t/* make polish notation (postfix, but in reverse order) */\n\tmakepol(&state);\n\tif (!state.num)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_PARAMETER_VALUE),\n\t\t\t\t errmsg(\"empty query\")));\n\n\tif (state.num > QUERYTYPEMAXITEMS)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\terrmsg(\"number of query items (%d) exceeds the maximum allowed (%d)\",\n\t\t\t   state.num, (int) QUERYTYPEMAXITEMS)));\n\tcommonlen = COMPUTESIZE(state.num);\n\n\tquery = (QUERYTYPE *) palloc(commonlen);\n\tSET_VARSIZE(query, commonlen);\n\tquery->size = state.num;\n\tptr = GETQUERY(query);\n\n\tfor (i = state.num - 1; i >= 0; i--)\n\t{\n\t\tptr[i].type = state.str->type;\n\t\tptr[i].val = state.str->val;\n\t\ttmp = state.str->next;\n\t\tpfree(state.str);\n\t\tstate.str = tmp;\n\t}\n\n\tpos = query->size - 1;\n\tfindoprnd(ptr, &pos);\n#ifdef BS_DEBUG\n\tinitStringInfo(&pbuf);\n\tfor (i = 0; i < query->size; i++)\n\t{\n\t\tif (ptr[i].type == OPR)\n\t\t\tappendStringInfo(&pbuf, \"%c(%d) \", ptr[i].val, ptr[i].left);\n\t\telse\n\t\t\tappendStringInfo(&pbuf, \"%d \", ptr[i].val);\n\t}\n\telog(DEBUG3, \"POR: %s\", pbuf.data);\n\tpfree(pbuf.data);\n#endif\n\n\tPG_RETURN_POINTER(query);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 515,
        "critical_vars": [
            "state.num"
        ],
        "function": "bqarr_in",
        "filename": "postgres/CVE-2014-0064/CVE-2014-0064_CWE-189_31400a673325147e1205326008e32135a78b4d8a__int_bool.c.diff",
        "label": "True",
        "function_code": "Datum\nbqarr_in(PG_FUNCTION_ARGS)\n{\n\tchar\t   *buf = (char *) PG_GETARG_POINTER(0);\n\tWORKSTATE\tstate;\n\tint32\t\ti;\n\tQUERYTYPE  *query;\n\tint32\t\tcommonlen;\n\tITEM\t   *ptr;\n\tNODE\t   *tmp;\n\tint32\t\tpos = 0;\n\n#ifdef BS_DEBUG\n\tStringInfoData pbuf;\n#endif\n\n\tstate.buf = buf;\n\tstate.state = WAITOPERAND;\n\tstate.count = 0;\n\tstate.num = 0;\n\tstate.str = NULL;\n\n\t/* make polish notation (postfix, but in reverse order) */\n\tmakepol(&state);\n\tif (!state.num)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_PARAMETER_VALUE),\n\t\t\t\t errmsg(\"empty query\")));\n\n\tif (state.num > QUERYTYPEMAXITEMS)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\terrmsg(\"number of query items (%d) exceeds the maximum allowed (%d)\",\n\t\t\t   state.num, (int) QUERYTYPEMAXITEMS)));\n\tcommonlen = COMPUTESIZE(state.num);\n\n\tquery = (QUERYTYPE *) palloc(commonlen);\n\tSET_VARSIZE(query, commonlen);\n\tquery->size = state.num;\n\tptr = GETQUERY(query);\n\n\tfor (i = state.num - 1; i >= 0; i--)\n\t{\n\t\tptr[i].type = state.str->type;\n\t\tptr[i].val = state.str->val;\n\t\ttmp = state.str->next;\n\t\tpfree(state.str);\n\t\tstate.str = tmp;\n\t}\n\n\tpos = query->size - 1;\n\tfindoprnd(ptr, &pos);\n#ifdef BS_DEBUG\n\tinitStringInfo(&pbuf);\n\tfor (i = 0; i < query->size; i++)\n\t{\n\t\tif (ptr[i].type == OPR)\n\t\t\tappendStringInfo(&pbuf, \"%c(%d) \", ptr[i].val, ptr[i].left);\n\t\telse\n\t\t\tappendStringInfo(&pbuf, \"%d \", ptr[i].val);\n\t}\n\telog(DEBUG3, \"POR: %s\", pbuf.data);\n\tpfree(pbuf.data);\n#endif\n\n\tPG_RETURN_POINTER(query);\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 1387,
        "critical_vars": [
            "size"
        ],
        "function": "path_in",
        "filename": "postgres/CVE-2014-0064/CVE-2014-0064_CWE-189_31400a673325147e1205326008e32135a78b4d8a_geo_ops.c.diff",
        "label": "False",
        "function_code": "\n\nDatum\npath_in(PG_FUNCTION_ARGS)\n{\n\tchar\t   *str = PG_GETARG_CSTRING(0);\n\tPATH\t   *path;\n\tint\t\t\tisopen;\n\tchar\t   *s;\n\tint\t\t\tnpts;\n\tint\t\t\tsize;\n\tint\t\t\tdepth = 0;\n\n\tif ((npts = pair_count(str, ',')) <= 0)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t\t errmsg(\"invalid input syntax for type path: \\\"%s\\\"\", str)));\n\n\ts = str;\n\twhile (isspace((unsigned char) *s))\n\t\ts++;\n\n\t/* skip single leading paren */\n\tif ((*s == LDELIM) && (strrchr(s, LDELIM) == s))\n\t{\n\t\ts++;\n\t\tdepth++;\n\t}\n\n\tsize = offsetof(PATH, p[0]) +sizeof(path->p[0]) * npts;\n\tpath = (PATH *) palloc(size);\n\n\tSET_VARSIZE(path, size);\n\tpath->npts = npts;\n\n\tif ((!path_decode(TRUE, npts, s, &isopen, &s, &(path->p[0])))\n\t&& (!((depth == 0) && (*s == '\\0'))) && !((depth >= 1) && (*s == RDELIM)))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t\t errmsg(\"invalid input syntax for type path: \\\"%s\\\"\", str)));\n\n\tpath->closed = (!isopen);\n\t/* prevent instability in unused pad bytes */\n\tpath->dummy = 0;\n\n\tPG_RETURN_PATH_P(path);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 1388,
        "critical_vars": [
            "base_size"
        ],
        "function": "path_in",
        "filename": "postgres/CVE-2014-0064/CVE-2014-0064_CWE-189_31400a673325147e1205326008e32135a78b4d8a_geo_ops.c.diff",
        "label": "True",
        "function_code": "\n\nDatum\npath_in(PG_FUNCTION_ARGS)\n{\n\tchar\t   *str = PG_GETARG_CSTRING(0);\n\tPATH\t   *path;\n\tint\t\t\tisopen;\n\tchar\t   *s;\n\tint\t\t\tnpts;\n\tint\t\t\tsize;\n\tint\t\t\tbase_size;\n\tint\t\t\tdepth = 0;\n\n\tif ((npts = pair_count(str, ',')) <= 0)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t\t errmsg(\"invalid input syntax for type path: \\\"%s\\\"\", str)));\n\n\ts = str;\n\twhile (isspace((unsigned char) *s))\n\t\ts++;\n\n\t/* skip single leading paren */\n\tif ((*s == LDELIM) && (strrchr(s, LDELIM) == s))\n\t{\n\t\ts++;\n\t\tdepth++;\n\t}\n\n\tbase_size = sizeof(path->p[0]) * npts;\n\tsize = offsetof(PATH, p[0]) + base_size;\n\n\t/* Check for integer overflow */\n\tif (base_size / npts != sizeof(path->p[0]) || size <= base_size)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t\t errmsg(\"too many points requested\")));\n\n\tpath = (PATH *) palloc(size);\n\n\tSET_VARSIZE(path, size);\n\tpath->npts = npts;\n\n\tif ((!path_decode(TRUE, npts, s, &isopen, &s, &(path->p[0])))\n\t&& (!((depth == 0) && (*s == '\\0'))) && !((depth >= 1) && (*s == RDELIM)))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t\t errmsg(\"invalid input syntax for type path: \\\"%s\\\"\", str)));\n\n\tpath->closed = (!isopen);\n\t/* prevent instability in unused pad bytes */\n\tpath->dummy = 0;\n\n\tPG_RETURN_PATH_P(path);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 1389,
        "critical_vars": [
            "size"
        ],
        "function": "path_in",
        "filename": "postgres/CVE-2014-0064/CVE-2014-0064_CWE-189_31400a673325147e1205326008e32135a78b4d8a_geo_ops.c.diff",
        "label": "True",
        "function_code": "\n\nDatum\npath_in(PG_FUNCTION_ARGS)\n{\n\tchar\t   *str = PG_GETARG_CSTRING(0);\n\tPATH\t   *path;\n\tint\t\t\tisopen;\n\tchar\t   *s;\n\tint\t\t\tnpts;\n\tint\t\t\tsize;\n\tint\t\t\tbase_size;\n\tint\t\t\tdepth = 0;\n\n\tif ((npts = pair_count(str, ',')) <= 0)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t\t errmsg(\"invalid input syntax for type path: \\\"%s\\\"\", str)));\n\n\ts = str;\n\twhile (isspace((unsigned char) *s))\n\t\ts++;\n\n\t/* skip single leading paren */\n\tif ((*s == LDELIM) && (strrchr(s, LDELIM) == s))\n\t{\n\t\ts++;\n\t\tdepth++;\n\t}\n\n\tbase_size = sizeof(path->p[0]) * npts;\n\tsize = offsetof(PATH, p[0]) + base_size;\n\n\t/* Check for integer overflow */\n\tif (base_size / npts != sizeof(path->p[0]) || size <= base_size)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t\t errmsg(\"too many points requested\")));\n\n\tpath = (PATH *) palloc(size);\n\n\tSET_VARSIZE(path, size);\n\tpath->npts = npts;\n\n\tif ((!path_decode(TRUE, npts, s, &isopen, &s, &(path->p[0])))\n\t&& (!((depth == 0) && (*s == '\\0'))) && !((depth >= 1) && (*s == RDELIM)))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t\t errmsg(\"invalid input syntax for type path: \\\"%s\\\"\", str)));\n\n\tpath->closed = (!isopen);\n\t/* prevent instability in unused pad bytes */\n\tpath->dummy = 0;\n\n\tPG_RETURN_PATH_P(path);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 1392,
        "critical_vars": [
            "base_size",
            "npts",
            "size"
        ],
        "function": "path_in",
        "filename": "postgres/CVE-2014-0064/CVE-2014-0064_CWE-189_31400a673325147e1205326008e32135a78b4d8a_geo_ops.c.diff",
        "label": "True",
        "function_code": "\n\nDatum\npath_in(PG_FUNCTION_ARGS)\n{\n\tchar\t   *str = PG_GETARG_CSTRING(0);\n\tPATH\t   *path;\n\tint\t\t\tisopen;\n\tchar\t   *s;\n\tint\t\t\tnpts;\n\tint\t\t\tsize;\n\tint\t\t\tbase_size;\n\tint\t\t\tdepth = 0;\n\n\tif ((npts = pair_count(str, ',')) <= 0)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t\t errmsg(\"invalid input syntax for type path: \\\"%s\\\"\", str)));\n\n\ts = str;\n\twhile (isspace((unsigned char) *s))\n\t\ts++;\n\n\t/* skip single leading paren */\n\tif ((*s == LDELIM) && (strrchr(s, LDELIM) == s))\n\t{\n\t\ts++;\n\t\tdepth++;\n\t}\n\n\tbase_size = sizeof(path->p[0]) * npts;\n\tsize = offsetof(PATH, p[0]) + base_size;\n\n\t/* Check for integer overflow */\n\tif (base_size / npts != sizeof(path->p[0]) || size <= base_size)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t\t errmsg(\"too many points requested\")));\n\n\tpath = (PATH *) palloc(size);\n\n\tSET_VARSIZE(path, size);\n\tpath->npts = npts;\n\n\tif ((!path_decode(TRUE, npts, s, &isopen, &s, &(path->p[0])))\n\t&& (!((depth == 0) && (*s == '\\0'))) && !((depth >= 1) && (*s == RDELIM)))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t\t errmsg(\"invalid input syntax for type path: \\\"%s\\\"\", str)));\n\n\tpath->closed = (!isopen);\n\t/* prevent instability in unused pad bytes */\n\tpath->dummy = 0;\n\n\tPG_RETURN_PATH_P(path);\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 3440,
        "critical_vars": [
            "size"
        ],
        "function": "poly_in",
        "filename": "postgres/CVE-2014-0064/CVE-2014-0064_CWE-189_31400a673325147e1205326008e32135a78b4d8a_geo_ops.c.diff",
        "label": "False",
        "function_code": "Datum\npoly_in(PG_FUNCTION_ARGS)\n{\n\tchar\t   *str = PG_GETARG_CSTRING(0);\n\tPOLYGON    *poly;\n\tint\t\t\tnpts;\n\tint\t\t\tsize;\n\tint\t\t\tisopen;\n\tchar\t   *s;\n\n\tif ((npts = pair_count(str, ',')) <= 0)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t  errmsg(\"invalid input syntax for type polygon: \\\"%s\\\"\", str)));\n\n\tsize = offsetof(POLYGON, p[0]) +sizeof(poly->p[0]) * npts;\n\tpoly = (POLYGON *) palloc0(size);\t/* zero any holes */\n\n\tSET_VARSIZE(poly, size);\n\tpoly->npts = npts;\n\n\tif ((!path_decode(FALSE, npts, str, &isopen, &s, &(poly->p[0])))\n\t\t|| (*s != '\\0'))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t  errmsg(\"invalid input syntax for type polygon: \\\"%s\\\"\", str)));\n\n\tmake_bound_box(poly);\n\n\tPG_RETURN_POLYGON_P(poly);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 3450,
        "critical_vars": [
            "base_size"
        ],
        "function": "poly_in",
        "filename": "postgres/CVE-2014-0064/CVE-2014-0064_CWE-189_31400a673325147e1205326008e32135a78b4d8a_geo_ops.c.diff",
        "label": "True",
        "function_code": "Datum\npoly_in(PG_FUNCTION_ARGS)\n{\n\tchar\t   *str = PG_GETARG_CSTRING(0);\n\tPOLYGON    *poly;\n\tint\t\t\tnpts;\n\tint\t\t\tsize;\n\tint\t\t\tbase_size;\n\tint\t\t\tisopen;\n\tchar\t   *s;\n\n\tif ((npts = pair_count(str, ',')) <= 0)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t  errmsg(\"invalid input syntax for type polygon: \\\"%s\\\"\", str)));\n\n\tbase_size = sizeof(poly->p[0]) * npts;\n\tsize = offsetof(POLYGON, p[0]) + base_size;\n\n\t/* Check for integer overflow */\n\tif (base_size / npts != sizeof(poly->p[0]) || size <= base_size)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t\t errmsg(\"too many points requested\")));\n\n\tpoly = (POLYGON *) palloc0(size);\t/* zero any holes */\n\n\tSET_VARSIZE(poly, size);\n\tpoly->npts = npts;\n\n\tif ((!path_decode(FALSE, npts, str, &isopen, &s, &(poly->p[0])))\n\t\t|| (*s != '\\0'))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t  errmsg(\"invalid input syntax for type polygon: \\\"%s\\\"\", str)));\n\n\tmake_bound_box(poly);\n\n\tPG_RETURN_POLYGON_P(poly);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 3451,
        "critical_vars": [
            "size"
        ],
        "function": "poly_in",
        "filename": "postgres/CVE-2014-0064/CVE-2014-0064_CWE-189_31400a673325147e1205326008e32135a78b4d8a_geo_ops.c.diff",
        "label": "True",
        "function_code": "Datum\npoly_in(PG_FUNCTION_ARGS)\n{\n\tchar\t   *str = PG_GETARG_CSTRING(0);\n\tPOLYGON    *poly;\n\tint\t\t\tnpts;\n\tint\t\t\tsize;\n\tint\t\t\tbase_size;\n\tint\t\t\tisopen;\n\tchar\t   *s;\n\n\tif ((npts = pair_count(str, ',')) <= 0)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t  errmsg(\"invalid input syntax for type polygon: \\\"%s\\\"\", str)));\n\n\tbase_size = sizeof(poly->p[0]) * npts;\n\tsize = offsetof(POLYGON, p[0]) + base_size;\n\n\t/* Check for integer overflow */\n\tif (base_size / npts != sizeof(poly->p[0]) || size <= base_size)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t\t errmsg(\"too many points requested\")));\n\n\tpoly = (POLYGON *) palloc0(size);\t/* zero any holes */\n\n\tSET_VARSIZE(poly, size);\n\tpoly->npts = npts;\n\n\tif ((!path_decode(FALSE, npts, str, &isopen, &s, &(poly->p[0])))\n\t\t|| (*s != '\\0'))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t  errmsg(\"invalid input syntax for type polygon: \\\"%s\\\"\", str)));\n\n\tmake_bound_box(poly);\n\n\tPG_RETURN_POLYGON_P(poly);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 3454,
        "critical_vars": [
            "base_size",
            "npts",
            "size"
        ],
        "function": "poly_in",
        "filename": "postgres/CVE-2014-0064/CVE-2014-0064_CWE-189_31400a673325147e1205326008e32135a78b4d8a_geo_ops.c.diff",
        "label": "True",
        "function_code": "Datum\npoly_in(PG_FUNCTION_ARGS)\n{\n\tchar\t   *str = PG_GETARG_CSTRING(0);\n\tPOLYGON    *poly;\n\tint\t\t\tnpts;\n\tint\t\t\tsize;\n\tint\t\t\tbase_size;\n\tint\t\t\tisopen;\n\tchar\t   *s;\n\n\tif ((npts = pair_count(str, ',')) <= 0)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t  errmsg(\"invalid input syntax for type polygon: \\\"%s\\\"\", str)));\n\n\tbase_size = sizeof(poly->p[0]) * npts;\n\tsize = offsetof(POLYGON, p[0]) + base_size;\n\n\t/* Check for integer overflow */\n\tif (base_size / npts != sizeof(poly->p[0]) || size <= base_size)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t\t errmsg(\"too many points requested\")));\n\n\tpoly = (POLYGON *) palloc0(size);\t/* zero any holes */\n\n\tSET_VARSIZE(poly, size);\n\tpoly->npts = npts;\n\n\tif ((!path_decode(FALSE, npts, str, &isopen, &s, &(poly->p[0])))\n\t\t|| (*s != '\\0'))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t  errmsg(\"invalid input syntax for type polygon: \\\"%s\\\"\", str)));\n\n\tmake_bound_box(poly);\n\n\tPG_RETURN_POLYGON_P(poly);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 350,
        "critical_vars": [
            "state.num",
            "state.sumlen"
        ],
        "function": "queryin",
        "filename": "postgres/CVE-2014-0064/CVE-2014-0064_CWE-189_31400a673325147e1205326008e32135a78b4d8a_ltxtquery_io.c.diff",
        "label": "True",
        "function_code": "static ltxtquery *\nqueryin(char *buf)\n{\n\tQPRS_STATE\tstate;\n\tint32\t\ti;\n\tltxtquery  *query;\n\tint32\t\tcommonlen;\n\tITEM\t   *ptr;\n\tNODE\t   *tmp;\n\tint32\t\tpos = 0;\n\n#ifdef BS_DEBUG\n\tchar\t\tpbuf[16384],\n\t\t\t   *cur;\n#endif\n\n\t/* init state */\n\tstate.buf = buf;\n\tstate.state = WAITOPERAND;\n\tstate.count = 0;\n\tstate.num = 0;\n\tstate.str = NULL;\n\n\t/* init list of operand */\n\tstate.sumlen = 0;\n\tstate.lenop = 64;\n\tstate.curop = state.op = (char *) palloc(state.lenop);\n\t*(state.curop) = '\\0';\n\n\t/* parse query & make polish notation (postfix, but in reverse order) */\n\tmakepol(&state);\n\tif (!state.num)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_SYNTAX_ERROR),\n\t\t\t\t errmsg(\"syntax error\"),\n\t\t\t\t errdetail(\"Empty query.\")));\n\n\tif (LTXTQUERY_TOO_BIG(state.num, state.sumlen))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t\t errmsg(\"ltxtquery is too large\")));\n\tcommonlen = COMPUTESIZE(state.num, state.sumlen);\n\n\tquery = (ltxtquery *) palloc(commonlen);\n\tSET_VARSIZE(query, commonlen);\n\tquery->size = state.num;\n\tptr = GETQUERY(query);\n\n\t/* set item in polish notation */\n\tfor (i = 0; i < state.num; i++)\n\t{\n\t\tptr[i].type = state.str->type;\n\t\tptr[i].val = state.str->val;\n\t\tptr[i].distance = state.str->distance;\n\t\tptr[i].length = state.str->length;\n\t\tptr[i].flag = state.str->flag;\n\t\ttmp = state.str->next;\n\t\tpfree(state.str);\n\t\tstate.str = tmp;\n\t}\n\n\t/* set user friendly-operand view */\n\tmemcpy((void *) GETOPERAND(query), (void *) state.op, state.sumlen);\n\tpfree(state.op);\n\n\t/* set left operand's position for every operator */\n\tpos = 0;\n\tfindoprnd(ptr, &pos);\n\n\treturn query;\n}\n\n/*\n * in without morphology\n */\nDatum\nltxtq_in(PG_FUNCTION_ARGS)\n{\n\tPG_RETURN_POINTER(queryin((char *) PG_GETARG_POINTER(0)));\n}\n\n/*\n * out function\n */\ntypedef struct\n{\n\tITEM\t   *curpol;\n\tchar\t   *buf;\n\tchar\t   *cur;\n\tchar\t   *op;\n\tint32\t\tbuflen;\n} INFIX;\n\n#define RESIZEBUF(inf,addsize) \\\nwhile( ( (inf)->cur - (inf)->buf ) + (addsize) + 1 >= (inf)->buflen ) \\\n{ \\\n\tint32 len = (inf)->cur - (inf)->buf; \\\n\t(inf)->bufl\n... (function end not found)"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 459,
        "critical_vars": [
            "avail"
        ],
        "function": "txid_snapshot_recv",
        "filename": "postgres/CVE-2014-2669/CVE-2014-2669_CWE-189_31400a673325147e1205326008e32135a78b4d8a_txid.c.diff",
        "label": "False",
        "function_code": " * txid_snapshot_recv(internal) returns txid_snapshot\n *\n *\t\tbinary input function for type txid_snapshot\n *\n *\t\tformat: int4 nxip, int8 xmin, int8 xmax, int8 xip\n */\nDatum\ntxid_snapshot_recv(PG_FUNCTION_ARGS)\n{\n\tStringInfo\tbuf = (StringInfo) PG_GETARG_POINTER(0);\n\tTxidSnapshot *snap;\n\ttxid\t\tlast = 0;\n\tint\t\t\tnxip;\n\tint\t\t\ti;\n\tint\t\t\tavail;\n\tint\t\t\texpect;\n\ttxid\t\txmin,\n\t\t\t\txmax;\n\n\t/*\n\t * load nxip and check for nonsense.\n\t *\n\t * (nxip > avail) check is against int overflows in 'expect'.\n\t */\n\tnxip = pq_getmsgint(buf, 4);\n\tavail = buf->len - buf->cursor;\n\texpect = 8 + 8 + nxip * 8;\n\tif (nxip < 0 || nxip > avail || expect > avail)\n\t\tgoto bad_format;\n\n\txmin = pq_getmsgint64(buf);\n\txmax = pq_getmsgint64(buf);\n\tif (xmin == 0 || xmax == 0 || xmin > xmax || xmax > MAX_TXID)\n\t\tgoto bad_format;\n\n\tsnap = palloc(TXID_SNAPSHOT_SIZE(nxip));\n\tsnap->xmin = xmin;\n\tsnap->xmax = xmax;\n\tsnap->nxip = nxip;\n\tSET_VARSIZE(snap, TXID_SNAPSHOT_SIZE(nxip));\n\n\tfor (i = 0; i < nxip; i++)\n\t{\n\t\ttxid\t\tcur = pq_getmsgint64(buf);\n\n\t\tif (cur <= last || cur < xmin || cur >= xmax)\n\t\t\tgoto bad_format;\n\t\tsnap->xip[i] = cur;\n\t\tlast = cur;\n\t}\n\tPG_RETURN_POINTER(snap);\n\nbad_format:\n\telog(ERROR, \"invalid snapshot data\");\n\treturn (Datum) NULL;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 464,
        "critical_vars": [
            "nxip"
        ],
        "function": "txid_snapshot_recv",
        "filename": "postgres/CVE-2014-2669/CVE-2014-2669_CWE-189_31400a673325147e1205326008e32135a78b4d8a_txid.c.diff",
        "label": "True",
        "function_code": " * txid_snapshot_recv(internal) returns txid_snapshot\n *\n *\t\tbinary input function for type txid_snapshot\n *\n *\t\tformat: int4 nxip, int8 xmin, int8 xmax, int8 xip\n */\nDatum\ntxid_snapshot_recv(PG_FUNCTION_ARGS)\n{\n\tStringInfo\tbuf = (StringInfo) PG_GETARG_POINTER(0);\n\tTxidSnapshot *snap;\n\ttxid\t\tlast = 0;\n\tint\t\t\tnxip;\n\tint\t\t\ti;\n\ttxid\t\txmin,\n\t\t\t\txmax;\n\n\t/* load and validate nxip */\n\tnxip = pq_getmsgint(buf, 4);\n\tif (nxip < 0 || nxip > TXID_SNAPSHOT_MAX_NXIP)\n\t\tgoto bad_format;\n\n\txmin = pq_getmsgint64(buf);\n\txmax = pq_getmsgint64(buf);\n\tif (xmin == 0 || xmax == 0 || xmin > xmax || xmax > MAX_TXID)\n\t\tgoto bad_format;\n\n\tsnap = palloc(TXID_SNAPSHOT_SIZE(nxip));\n\tsnap->xmin = xmin;\n\tsnap->xmax = xmax;\n\tsnap->nxip = nxip;\n\tSET_VARSIZE(snap, TXID_SNAPSHOT_SIZE(nxip));\n\n\tfor (i = 0; i < nxip; i++)\n\t{\n\t\ttxid\t\tcur = pq_getmsgint64(buf);\n\n\t\tif (cur <= last || cur < xmin || cur >= xmax)\n\t\t\tgoto bad_format;\n\t\tsnap->xip[i] = cur;\n\t\tlast = cur;\n\t}\n\tPG_RETURN_POINTER(snap);\n\nbad_format:\n\telog(ERROR, \"invalid snapshot data\");\n\treturn (Datum) NULL;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 460,
        "critical_vars": [
            "expect"
        ],
        "function": "txid_snapshot_recv",
        "filename": "postgres/CVE-2014-2669/CVE-2014-2669_CWE-189_31400a673325147e1205326008e32135a78b4d8a_txid.c.diff",
        "label": "False",
        "function_code": " * txid_snapshot_recv(internal) returns txid_snapshot\n *\n *\t\tbinary input function for type txid_snapshot\n *\n *\t\tformat: int4 nxip, int8 xmin, int8 xmax, int8 xip\n */\nDatum\ntxid_snapshot_recv(PG_FUNCTION_ARGS)\n{\n\tStringInfo\tbuf = (StringInfo) PG_GETARG_POINTER(0);\n\tTxidSnapshot *snap;\n\ttxid\t\tlast = 0;\n\tint\t\t\tnxip;\n\tint\t\t\ti;\n\tint\t\t\tavail;\n\tint\t\t\texpect;\n\ttxid\t\txmin,\n\t\t\t\txmax;\n\n\t/*\n\t * load nxip and check for nonsense.\n\t *\n\t * (nxip > avail) check is against int overflows in 'expect'.\n\t */\n\tnxip = pq_getmsgint(buf, 4);\n\tavail = buf->len - buf->cursor;\n\texpect = 8 + 8 + nxip * 8;\n\tif (nxip < 0 || nxip > avail || expect > avail)\n\t\tgoto bad_format;\n\n\txmin = pq_getmsgint64(buf);\n\txmax = pq_getmsgint64(buf);\n\tif (xmin == 0 || xmax == 0 || xmin > xmax || xmax > MAX_TXID)\n\t\tgoto bad_format;\n\n\tsnap = palloc(TXID_SNAPSHOT_SIZE(nxip));\n\tsnap->xmin = xmin;\n\tsnap->xmax = xmax;\n\tsnap->nxip = nxip;\n\tSET_VARSIZE(snap, TXID_SNAPSHOT_SIZE(nxip));\n\n\tfor (i = 0; i < nxip; i++)\n\t{\n\t\ttxid\t\tcur = pq_getmsgint64(buf);\n\n\t\tif (cur <= last || cur < xmin || cur >= xmax)\n\t\t\tgoto bad_format;\n\t\tsnap->xip[i] = cur;\n\t\tlast = cur;\n\t}\n\tPG_RETURN_POINTER(snap);\n\nbad_format:\n\telog(ERROR, \"invalid snapshot data\");\n\treturn (Datum) NULL;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "if-Condition",
        "line_old": 461,
        "critical_vars": [
            "nxip",
            "expect"
        ],
        "function": "txid_snapshot_recv",
        "filename": "postgres/CVE-2014-2669/CVE-2014-2669_CWE-189_31400a673325147e1205326008e32135a78b4d8a_txid.c.diff",
        "label": "False",
        "function_code": " * txid_snapshot_recv(internal) returns txid_snapshot\n *\n *\t\tbinary input function for type txid_snapshot\n *\n *\t\tformat: int4 nxip, int8 xmin, int8 xmax, int8 xip\n */\nDatum\ntxid_snapshot_recv(PG_FUNCTION_ARGS)\n{\n\tStringInfo\tbuf = (StringInfo) PG_GETARG_POINTER(0);\n\tTxidSnapshot *snap;\n\ttxid\t\tlast = 0;\n\tint\t\t\tnxip;\n\tint\t\t\ti;\n\tint\t\t\tavail;\n\tint\t\t\texpect;\n\ttxid\t\txmin,\n\t\t\t\txmax;\n\n\t/*\n\t * load nxip and check for nonsense.\n\t *\n\t * (nxip > avail) check is against int overflows in 'expect'.\n\t */\n\tnxip = pq_getmsgint(buf, 4);\n\tavail = buf->len - buf->cursor;\n\texpect = 8 + 8 + nxip * 8;\n\tif (nxip < 0 || nxip > avail || expect > avail)\n\t\tgoto bad_format;\n\n\txmin = pq_getmsgint64(buf);\n\txmax = pq_getmsgint64(buf);\n\tif (xmin == 0 || xmax == 0 || xmin > xmax || xmax > MAX_TXID)\n\t\tgoto bad_format;\n\n\tsnap = palloc(TXID_SNAPSHOT_SIZE(nxip));\n\tsnap->xmin = xmin;\n\tsnap->xmax = xmax;\n\tsnap->nxip = nxip;\n\tSET_VARSIZE(snap, TXID_SNAPSHOT_SIZE(nxip));\n\n\tfor (i = 0; i < nxip; i++)\n\t{\n\t\ttxid\t\tcur = pq_getmsgint64(buf);\n\n\t\tif (cur <= last || cur < xmin || cur >= xmax)\n\t\t\tgoto bad_format;\n\t\tsnap->xip[i] = cur;\n\t\tlast = cur;\n\t}\n\tPG_RETURN_POINTER(snap);\n\nbad_format:\n\telog(ERROR, \"invalid snapshot data\");\n\treturn (Datum) NULL;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 337,
        "critical_vars": [
            "nnode",
            "sumlen"
        ],
        "function": "QTN2QT",
        "filename": "postgres/CVE-2014-2669/CVE-2014-2669_CWE-189_31400a673325147e1205326008e32135a78b4d8a_tsquery_util.c.diff",
        "label": "True",
        "function_code": "\nTSQuery\nQTN2QT(QTNode *in)\n{\n\tTSQuery\t\tout;\n\tint\t\t\tlen;\n\tint\t\t\tsumlen = 0,\n\t\t\t\tnnode = 0;\n\tQTN2QTState state;\n\n\tcntsize(in, &sumlen, &nnode);\n\n\tif (TSQUERY_TOO_BIG(nnode, sumlen))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t\t errmsg(\"tsquery is too large\")));\n\tlen = COMPUTESIZE(nnode, sumlen);\n\n\tout = (TSQuery) palloc0(len);\n\tSET_VARSIZE(out, len);\n\tout->size = nnode;\n\n\tstate.curitem = GETQUERY(out);\n\tstate.operand = state.curoperand = GETOPERAND(out);\n\n\tfillQT(&state, in);\n\treturn out;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 514,
        "critical_vars": [
            "state.num"
        ],
        "function": "bqarr_in",
        "filename": "postgres/CVE-2014-2669/CVE-2014-2669_CWE-189_31400a673325147e1205326008e32135a78b4d8a__int_bool.c.diff",
        "label": "True",
        "function_code": "Datum\nbqarr_in(PG_FUNCTION_ARGS)\n{\n\tchar\t   *buf = (char *) PG_GETARG_POINTER(0);\n\tWORKSTATE\tstate;\n\tint32\t\ti;\n\tQUERYTYPE  *query;\n\tint32\t\tcommonlen;\n\tITEM\t   *ptr;\n\tNODE\t   *tmp;\n\tint32\t\tpos = 0;\n\n#ifdef BS_DEBUG\n\tStringInfoData pbuf;\n#endif\n\n\tstate.buf = buf;\n\tstate.state = WAITOPERAND;\n\tstate.count = 0;\n\tstate.num = 0;\n\tstate.str = NULL;\n\n\t/* make polish notation (postfix, but in reverse order) */\n\tmakepol(&state);\n\tif (!state.num)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_PARAMETER_VALUE),\n\t\t\t\t errmsg(\"empty query\")));\n\n\tif (state.num > QUERYTYPEMAXITEMS)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\terrmsg(\"number of query items (%d) exceeds the maximum allowed (%d)\",\n\t\t\t   state.num, (int) QUERYTYPEMAXITEMS)));\n\tcommonlen = COMPUTESIZE(state.num);\n\n\tquery = (QUERYTYPE *) palloc(commonlen);\n\tSET_VARSIZE(query, commonlen);\n\tquery->size = state.num;\n\tptr = GETQUERY(query);\n\n\tfor (i = state.num - 1; i >= 0; i--)\n\t{\n\t\tptr[i].type = state.str->type;\n\t\tptr[i].val = state.str->val;\n\t\ttmp = state.str->next;\n\t\tpfree(state.str);\n\t\tstate.str = tmp;\n\t}\n\n\tpos = query->size - 1;\n\tfindoprnd(ptr, &pos);\n#ifdef BS_DEBUG\n\tinitStringInfo(&pbuf);\n\tfor (i = 0; i < query->size; i++)\n\t{\n\t\tif (ptr[i].type == OPR)\n\t\t\tappendStringInfo(&pbuf, \"%c(%d) \", ptr[i].val, ptr[i].left);\n\t\telse\n\t\t\tappendStringInfo(&pbuf, \"%d \", ptr[i].val);\n\t}\n\telog(DEBUG3, \"POR: %s\", pbuf.data);\n\tpfree(pbuf.data);\n#endif\n\n\tPG_RETURN_POINTER(query);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 515,
        "critical_vars": [
            "state.num"
        ],
        "function": "bqarr_in",
        "filename": "postgres/CVE-2014-2669/CVE-2014-2669_CWE-189_31400a673325147e1205326008e32135a78b4d8a__int_bool.c.diff",
        "label": "True",
        "function_code": "Datum\nbqarr_in(PG_FUNCTION_ARGS)\n{\n\tchar\t   *buf = (char *) PG_GETARG_POINTER(0);\n\tWORKSTATE\tstate;\n\tint32\t\ti;\n\tQUERYTYPE  *query;\n\tint32\t\tcommonlen;\n\tITEM\t   *ptr;\n\tNODE\t   *tmp;\n\tint32\t\tpos = 0;\n\n#ifdef BS_DEBUG\n\tStringInfoData pbuf;\n#endif\n\n\tstate.buf = buf;\n\tstate.state = WAITOPERAND;\n\tstate.count = 0;\n\tstate.num = 0;\n\tstate.str = NULL;\n\n\t/* make polish notation (postfix, but in reverse order) */\n\tmakepol(&state);\n\tif (!state.num)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_PARAMETER_VALUE),\n\t\t\t\t errmsg(\"empty query\")));\n\n\tif (state.num > QUERYTYPEMAXITEMS)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\terrmsg(\"number of query items (%d) exceeds the maximum allowed (%d)\",\n\t\t\t   state.num, (int) QUERYTYPEMAXITEMS)));\n\tcommonlen = COMPUTESIZE(state.num);\n\n\tquery = (QUERYTYPE *) palloc(commonlen);\n\tSET_VARSIZE(query, commonlen);\n\tquery->size = state.num;\n\tptr = GETQUERY(query);\n\n\tfor (i = state.num - 1; i >= 0; i--)\n\t{\n\t\tptr[i].type = state.str->type;\n\t\tptr[i].val = state.str->val;\n\t\ttmp = state.str->next;\n\t\tpfree(state.str);\n\t\tstate.str = tmp;\n\t}\n\n\tpos = query->size - 1;\n\tfindoprnd(ptr, &pos);\n#ifdef BS_DEBUG\n\tinitStringInfo(&pbuf);\n\tfor (i = 0; i < query->size; i++)\n\t{\n\t\tif (ptr[i].type == OPR)\n\t\t\tappendStringInfo(&pbuf, \"%c(%d) \", ptr[i].val, ptr[i].left);\n\t\telse\n\t\t\tappendStringInfo(&pbuf, \"%d \", ptr[i].val);\n\t}\n\telog(DEBUG3, \"POR: %s\", pbuf.data);\n\tpfree(pbuf.data);\n#endif\n\n\tPG_RETURN_POINTER(query);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 517,
        "critical_vars": [
            "state.sumlen",
            "state.polstr"
        ],
        "function": "parse_tsquery",
        "filename": "postgres/CVE-2014-2669/CVE-2014-2669_CWE-189_31400a673325147e1205326008e32135a78b4d8a_tsquery.c.diff",
        "label": "True",
        "function_code": "TSQuery\nparse_tsquery(char *buf,\n\t\t\t  PushFunction pushval,\n\t\t\t  Datum opaque,\n\t\t\t  bool isplain)\n{\n\tstruct TSQueryParserStateData state;\n\tint\t\t\ti;\n\tTSQuery\t\tquery;\n\tint\t\t\tcommonlen;\n\tQueryItem  *ptr;\n\tListCell   *cell;\n\n\t/* init state */\n\tstate.buffer = buf;\n\tstate.buf = buf;\n\tstate.state = (isplain) ? WAITSINGLEOPERAND : WAITFIRSTOPERAND;\n\tstate.count = 0;\n\tstate.polstr = NIL;\n\n\t/* init value parser's state */\n\tstate.valstate = init_tsvector_parser(state.buffer, true, true);\n\n\t/* init list of operand */\n\tstate.sumlen = 0;\n\tstate.lenop = 64;\n\tstate.curop = state.op = (char *) palloc(state.lenop);\n\t*(state.curop) = '\\0';\n\n\t/* parse query & make polish notation (postfix, but in reverse order) */\n\tmakepol(&state, pushval, opaque);\n\n\tclose_tsvector_parser(state.valstate);\n\n\tif (list_length(state.polstr) == 0)\n\t{\n\t\tereport(NOTICE,\n\t\t\t\t(errmsg(\"text-search query doesn't contain lexemes: \\\"%s\\\"\",\n\t\t\t\t\t\tstate.buffer)));\n\t\tquery = (TSQuery) palloc(HDRSIZETQ);\n\t\tSET_VARSIZE(query, HDRSIZETQ);\n\t\tquery->size = 0;\n\t\treturn query;\n\t}\n\n\tif (TSQUERY_TOO_BIG(list_length(state.polstr), state.sumlen))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t\t errmsg(\"tsquery is too large\")));\n\tcommonlen = COMPUTESIZE(list_length(state.polstr), state.sumlen);\n\n\t/* Pack the QueryItems in the final TSQuery struct to return to caller */\n\tquery = (TSQuery) palloc0(commonlen);\n\tSET_VARSIZE(query, commonlen);\n\tquery->size = list_length(state.polstr);\n\tptr = GETQUERY(query);\n\n\t/* Copy QueryItems to TSQuery */\n\ti = 0;\n\tforeach(cell, state.polstr)\n\t{\n\t\tQueryItem  *item = (QueryItem *) lfirst(cell);\n\n\t\tswitch (item->type)\n\t\t{\n\t\t\tcase QI_VAL:\n\t\t\t\tmemcpy(&ptr[i], item, sizeof(QueryOperand));\n\t\t\t\tbreak;\n\t\t\tcase QI_VALSTOP:\n\t\t\t\tptr[i].type = QI_VALSTOP;\n\t\t\t\tbreak;\n\t\t\tcase QI_OPR:\n\t\t\t\tmemcpy(&ptr[i], item, sizeof(QueryOperator));\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\telog(ERROR, \"unrecognized QueryItem type: %d\", item->type);\n\t\t}\n\t\ti++;\n\t}\n\n\t/* Copy all the operand strings to TSQuery */\n\tmemcpy((void \n... (function end not found)"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 1387,
        "critical_vars": [
            "size"
        ],
        "function": "path_in",
        "filename": "postgres/CVE-2014-2669/CVE-2014-2669_CWE-189_31400a673325147e1205326008e32135a78b4d8a_geo_ops.c.diff",
        "label": "False",
        "function_code": "\n\nDatum\npath_in(PG_FUNCTION_ARGS)\n{\n\tchar\t   *str = PG_GETARG_CSTRING(0);\n\tPATH\t   *path;\n\tint\t\t\tisopen;\n\tchar\t   *s;\n\tint\t\t\tnpts;\n\tint\t\t\tsize;\n\tint\t\t\tdepth = 0;\n\n\tif ((npts = pair_count(str, ',')) <= 0)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t\t errmsg(\"invalid input syntax for type path: \\\"%s\\\"\", str)));\n\n\ts = str;\n\twhile (isspace((unsigned char) *s))\n\t\ts++;\n\n\t/* skip single leading paren */\n\tif ((*s == LDELIM) && (strrchr(s, LDELIM) == s))\n\t{\n\t\ts++;\n\t\tdepth++;\n\t}\n\n\tsize = offsetof(PATH, p[0]) +sizeof(path->p[0]) * npts;\n\tpath = (PATH *) palloc(size);\n\n\tSET_VARSIZE(path, size);\n\tpath->npts = npts;\n\n\tif ((!path_decode(TRUE, npts, s, &isopen, &s, &(path->p[0])))\n\t&& (!((depth == 0) && (*s == '\\0'))) && !((depth >= 1) && (*s == RDELIM)))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t\t errmsg(\"invalid input syntax for type path: \\\"%s\\\"\", str)));\n\n\tpath->closed = (!isopen);\n\t/* prevent instability in unused pad bytes */\n\tpath->dummy = 0;\n\n\tPG_RETURN_PATH_P(path);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 1388,
        "critical_vars": [
            "base_size"
        ],
        "function": "path_in",
        "filename": "postgres/CVE-2014-2669/CVE-2014-2669_CWE-189_31400a673325147e1205326008e32135a78b4d8a_geo_ops.c.diff",
        "label": "True",
        "function_code": "\n\nDatum\npath_in(PG_FUNCTION_ARGS)\n{\n\tchar\t   *str = PG_GETARG_CSTRING(0);\n\tPATH\t   *path;\n\tint\t\t\tisopen;\n\tchar\t   *s;\n\tint\t\t\tnpts;\n\tint\t\t\tsize;\n\tint\t\t\tbase_size;\n\tint\t\t\tdepth = 0;\n\n\tif ((npts = pair_count(str, ',')) <= 0)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t\t errmsg(\"invalid input syntax for type path: \\\"%s\\\"\", str)));\n\n\ts = str;\n\twhile (isspace((unsigned char) *s))\n\t\ts++;\n\n\t/* skip single leading paren */\n\tif ((*s == LDELIM) && (strrchr(s, LDELIM) == s))\n\t{\n\t\ts++;\n\t\tdepth++;\n\t}\n\n\tbase_size = sizeof(path->p[0]) * npts;\n\tsize = offsetof(PATH, p[0]) + base_size;\n\n\t/* Check for integer overflow */\n\tif (base_size / npts != sizeof(path->p[0]) || size <= base_size)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t\t errmsg(\"too many points requested\")));\n\n\tpath = (PATH *) palloc(size);\n\n\tSET_VARSIZE(path, size);\n\tpath->npts = npts;\n\n\tif ((!path_decode(TRUE, npts, s, &isopen, &s, &(path->p[0])))\n\t&& (!((depth == 0) && (*s == '\\0'))) && !((depth >= 1) && (*s == RDELIM)))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t\t errmsg(\"invalid input syntax for type path: \\\"%s\\\"\", str)));\n\n\tpath->closed = (!isopen);\n\t/* prevent instability in unused pad bytes */\n\tpath->dummy = 0;\n\n\tPG_RETURN_PATH_P(path);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 1389,
        "critical_vars": [
            "size"
        ],
        "function": "path_in",
        "filename": "postgres/CVE-2014-2669/CVE-2014-2669_CWE-189_31400a673325147e1205326008e32135a78b4d8a_geo_ops.c.diff",
        "label": "True",
        "function_code": "\n\nDatum\npath_in(PG_FUNCTION_ARGS)\n{\n\tchar\t   *str = PG_GETARG_CSTRING(0);\n\tPATH\t   *path;\n\tint\t\t\tisopen;\n\tchar\t   *s;\n\tint\t\t\tnpts;\n\tint\t\t\tsize;\n\tint\t\t\tbase_size;\n\tint\t\t\tdepth = 0;\n\n\tif ((npts = pair_count(str, ',')) <= 0)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t\t errmsg(\"invalid input syntax for type path: \\\"%s\\\"\", str)));\n\n\ts = str;\n\twhile (isspace((unsigned char) *s))\n\t\ts++;\n\n\t/* skip single leading paren */\n\tif ((*s == LDELIM) && (strrchr(s, LDELIM) == s))\n\t{\n\t\ts++;\n\t\tdepth++;\n\t}\n\n\tbase_size = sizeof(path->p[0]) * npts;\n\tsize = offsetof(PATH, p[0]) + base_size;\n\n\t/* Check for integer overflow */\n\tif (base_size / npts != sizeof(path->p[0]) || size <= base_size)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t\t errmsg(\"too many points requested\")));\n\n\tpath = (PATH *) palloc(size);\n\n\tSET_VARSIZE(path, size);\n\tpath->npts = npts;\n\n\tif ((!path_decode(TRUE, npts, s, &isopen, &s, &(path->p[0])))\n\t&& (!((depth == 0) && (*s == '\\0'))) && !((depth >= 1) && (*s == RDELIM)))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t\t errmsg(\"invalid input syntax for type path: \\\"%s\\\"\", str)));\n\n\tpath->closed = (!isopen);\n\t/* prevent instability in unused pad bytes */\n\tpath->dummy = 0;\n\n\tPG_RETURN_PATH_P(path);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 1392,
        "critical_vars": [
            "base_size",
            "npts",
            "size"
        ],
        "function": "path_in",
        "filename": "postgres/CVE-2014-2669/CVE-2014-2669_CWE-189_31400a673325147e1205326008e32135a78b4d8a_geo_ops.c.diff",
        "label": "True",
        "function_code": "\n\nDatum\npath_in(PG_FUNCTION_ARGS)\n{\n\tchar\t   *str = PG_GETARG_CSTRING(0);\n\tPATH\t   *path;\n\tint\t\t\tisopen;\n\tchar\t   *s;\n\tint\t\t\tnpts;\n\tint\t\t\tsize;\n\tint\t\t\tbase_size;\n\tint\t\t\tdepth = 0;\n\n\tif ((npts = pair_count(str, ',')) <= 0)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t\t errmsg(\"invalid input syntax for type path: \\\"%s\\\"\", str)));\n\n\ts = str;\n\twhile (isspace((unsigned char) *s))\n\t\ts++;\n\n\t/* skip single leading paren */\n\tif ((*s == LDELIM) && (strrchr(s, LDELIM) == s))\n\t{\n\t\ts++;\n\t\tdepth++;\n\t}\n\n\tbase_size = sizeof(path->p[0]) * npts;\n\tsize = offsetof(PATH, p[0]) + base_size;\n\n\t/* Check for integer overflow */\n\tif (base_size / npts != sizeof(path->p[0]) || size <= base_size)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t\t errmsg(\"too many points requested\")));\n\n\tpath = (PATH *) palloc(size);\n\n\tSET_VARSIZE(path, size);\n\tpath->npts = npts;\n\n\tif ((!path_decode(TRUE, npts, s, &isopen, &s, &(path->p[0])))\n\t&& (!((depth == 0) && (*s == '\\0'))) && !((depth >= 1) && (*s == RDELIM)))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t\t errmsg(\"invalid input syntax for type path: \\\"%s\\\"\", str)));\n\n\tpath->closed = (!isopen);\n\t/* prevent instability in unused pad bytes */\n\tpath->dummy = 0;\n\n\tPG_RETURN_PATH_P(path);\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 3440,
        "critical_vars": [
            "size"
        ],
        "function": "poly_in",
        "filename": "postgres/CVE-2014-2669/CVE-2014-2669_CWE-189_31400a673325147e1205326008e32135a78b4d8a_geo_ops.c.diff",
        "label": "False",
        "function_code": "Datum\npoly_in(PG_FUNCTION_ARGS)\n{\n\tchar\t   *str = PG_GETARG_CSTRING(0);\n\tPOLYGON    *poly;\n\tint\t\t\tnpts;\n\tint\t\t\tsize;\n\tint\t\t\tisopen;\n\tchar\t   *s;\n\n\tif ((npts = pair_count(str, ',')) <= 0)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t  errmsg(\"invalid input syntax for type polygon: \\\"%s\\\"\", str)));\n\n\tsize = offsetof(POLYGON, p[0]) +sizeof(poly->p[0]) * npts;\n\tpoly = (POLYGON *) palloc0(size);\t/* zero any holes */\n\n\tSET_VARSIZE(poly, size);\n\tpoly->npts = npts;\n\n\tif ((!path_decode(FALSE, npts, str, &isopen, &s, &(poly->p[0])))\n\t\t|| (*s != '\\0'))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t  errmsg(\"invalid input syntax for type polygon: \\\"%s\\\"\", str)));\n\n\tmake_bound_box(poly);\n\n\tPG_RETURN_POLYGON_P(poly);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 3450,
        "critical_vars": [
            "base_size"
        ],
        "function": "poly_in",
        "filename": "postgres/CVE-2014-2669/CVE-2014-2669_CWE-189_31400a673325147e1205326008e32135a78b4d8a_geo_ops.c.diff",
        "label": "True",
        "function_code": "Datum\npoly_in(PG_FUNCTION_ARGS)\n{\n\tchar\t   *str = PG_GETARG_CSTRING(0);\n\tPOLYGON    *poly;\n\tint\t\t\tnpts;\n\tint\t\t\tsize;\n\tint\t\t\tbase_size;\n\tint\t\t\tisopen;\n\tchar\t   *s;\n\n\tif ((npts = pair_count(str, ',')) <= 0)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t  errmsg(\"invalid input syntax for type polygon: \\\"%s\\\"\", str)));\n\n\tbase_size = sizeof(poly->p[0]) * npts;\n\tsize = offsetof(POLYGON, p[0]) + base_size;\n\n\t/* Check for integer overflow */\n\tif (base_size / npts != sizeof(poly->p[0]) || size <= base_size)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t\t errmsg(\"too many points requested\")));\n\n\tpoly = (POLYGON *) palloc0(size);\t/* zero any holes */\n\n\tSET_VARSIZE(poly, size);\n\tpoly->npts = npts;\n\n\tif ((!path_decode(FALSE, npts, str, &isopen, &s, &(poly->p[0])))\n\t\t|| (*s != '\\0'))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t  errmsg(\"invalid input syntax for type polygon: \\\"%s\\\"\", str)));\n\n\tmake_bound_box(poly);\n\n\tPG_RETURN_POLYGON_P(poly);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 3451,
        "critical_vars": [
            "size"
        ],
        "function": "poly_in",
        "filename": "postgres/CVE-2014-2669/CVE-2014-2669_CWE-189_31400a673325147e1205326008e32135a78b4d8a_geo_ops.c.diff",
        "label": "True",
        "function_code": "Datum\npoly_in(PG_FUNCTION_ARGS)\n{\n\tchar\t   *str = PG_GETARG_CSTRING(0);\n\tPOLYGON    *poly;\n\tint\t\t\tnpts;\n\tint\t\t\tsize;\n\tint\t\t\tbase_size;\n\tint\t\t\tisopen;\n\tchar\t   *s;\n\n\tif ((npts = pair_count(str, ',')) <= 0)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t  errmsg(\"invalid input syntax for type polygon: \\\"%s\\\"\", str)));\n\n\tbase_size = sizeof(poly->p[0]) * npts;\n\tsize = offsetof(POLYGON, p[0]) + base_size;\n\n\t/* Check for integer overflow */\n\tif (base_size / npts != sizeof(poly->p[0]) || size <= base_size)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t\t errmsg(\"too many points requested\")));\n\n\tpoly = (POLYGON *) palloc0(size);\t/* zero any holes */\n\n\tSET_VARSIZE(poly, size);\n\tpoly->npts = npts;\n\n\tif ((!path_decode(FALSE, npts, str, &isopen, &s, &(poly->p[0])))\n\t\t|| (*s != '\\0'))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t  errmsg(\"invalid input syntax for type polygon: \\\"%s\\\"\", str)));\n\n\tmake_bound_box(poly);\n\n\tPG_RETURN_POLYGON_P(poly);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 3454,
        "critical_vars": [
            "base_size",
            "npts",
            "size"
        ],
        "function": "poly_in",
        "filename": "postgres/CVE-2014-2669/CVE-2014-2669_CWE-189_31400a673325147e1205326008e32135a78b4d8a_geo_ops.c.diff",
        "label": "True",
        "function_code": "Datum\npoly_in(PG_FUNCTION_ARGS)\n{\n\tchar\t   *str = PG_GETARG_CSTRING(0);\n\tPOLYGON    *poly;\n\tint\t\t\tnpts;\n\tint\t\t\tsize;\n\tint\t\t\tbase_size;\n\tint\t\t\tisopen;\n\tchar\t   *s;\n\n\tif ((npts = pair_count(str, ',')) <= 0)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t  errmsg(\"invalid input syntax for type polygon: \\\"%s\\\"\", str)));\n\n\tbase_size = sizeof(poly->p[0]) * npts;\n\tsize = offsetof(POLYGON, p[0]) + base_size;\n\n\t/* Check for integer overflow */\n\tif (base_size / npts != sizeof(poly->p[0]) || size <= base_size)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t\t errmsg(\"too many points requested\")));\n\n\tpoly = (POLYGON *) palloc0(size);\t/* zero any holes */\n\n\tSET_VARSIZE(poly, size);\n\tpoly->npts = npts;\n\n\tif ((!path_decode(FALSE, npts, str, &isopen, &s, &(poly->p[0])))\n\t\t|| (*s != '\\0'))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t  errmsg(\"invalid input syntax for type polygon: \\\"%s\\\"\", str)));\n\n\tmake_bound_box(poly);\n\n\tPG_RETURN_POLYGON_P(poly);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 160,
        "critical_vars": [
            "slen"
        ],
        "function": "bit_in",
        "filename": "postgres/CVE-2014-2669/CVE-2014-2669_CWE-189_31400a673325147e1205326008e32135a78b4d8a_varbit.c.diff",
        "label": "True",
        "function_code": "Datum\nbit_in(PG_FUNCTION_ARGS)\n{\n\tchar\t   *input_string = PG_GETARG_CSTRING(0);\n\n#ifdef NOT_USED\n\tOid\t\t\ttypelem = PG_GETARG_OID(1);\n#endif\n\tint32\t\tatttypmod = PG_GETARG_INT32(2);\n\tVarBit\t   *result;\t\t\t/* The resulting bit string\t\t\t  */\n\tchar\t   *sp;\t\t\t\t/* pointer into the character string  */\n\tbits8\t   *r;\t\t\t\t/* pointer into the result */\n\tint\t\t\tlen,\t\t\t/* Length of the whole data structure */\n\t\t\t\tbitlen,\t\t\t/* Number of bits in the bit string   */\n\t\t\t\tslen;\t\t\t/* Length of the input string\t\t  */\n\tbool\t\tbit_not_hex;\t/* false = hex string  true = bit string */\n\tint\t\t\tbc;\n\tbits8\t\tx = 0;\n\n\t/* Check that the first character is a b or an x */\n\tif (input_string[0] == 'b' || input_string[0] == 'B')\n\t{\n\t\tbit_not_hex = true;\n\t\tsp = input_string + 1;\n\t}\n\telse if (input_string[0] == 'x' || input_string[0] == 'X')\n\t{\n\t\tbit_not_hex = false;\n\t\tsp = input_string + 1;\n\t}\n\telse\n\t{\n\t\t/*\n\t\t * Otherwise it's binary.  This allows things like cast('1001' as bit)\n\t\t * to work transparently.\n\t\t */\n\t\tbit_not_hex = true;\n\t\tsp = input_string;\n\t}\n\n\t/*\n\t * Determine bitlength from input string.  MaxAllocSize ensures a regular\n\t * input is small enough, but we must check hex input.\n\t */\n\tslen = strlen(sp);\n\tif (bit_not_hex)\n\t\tbitlen = slen;\n\telse\n\t{\n\t\tif (slen > VARBITMAXLEN / 4)\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t\t errmsg(\"bit string length exceeds the maximum allowed (%d)\",\n\t\t\t\t\t\tVARBITMAXLEN)));\n\t\tbitlen = slen * 4;\n\t}\n\n\t/*\n\t * Sometimes atttypmod is not supplied. If it is supplied we need to make\n\t * sure that the bitstring fits.\n\t */\n\tif (atttypmod <= 0)\n\t\tatttypmod = bitlen;\n\telse if (bitlen != atttypmod)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_STRING_DATA_LENGTH_MISMATCH),\n\t\t\t\t errmsg(\"bit string length %d does not match type bit(%d)\",\n\t\t\t\t\t\tbitlen, atttypmod)));\n\n\tlen = VARBITTOTALLEN(atttypmod);\n\t/* set to 0 so that *r is always initialised and string is zero-padded */\n\tresult = (VarBit *) palloc0(len);\n\tSET_VARSIZE(result, len);\n\tVARBITLEN(result) = atttypmod;\n\n\tr = VARBITS(result);\n\tif (bit_not_hex)\n\t{\n\t\t/* Parse the bit representation of the string */\n\t\t/* We know it fits, as bitlen was compared to atttypmod */\n\t\tx = HIGHBIT;\n\t\tfor (; *sp; sp++)\n\t\t{\n\t\t\tif (*sp == '1')\n\t\t\t\t*r |= x;\n\t\t\telse if (*sp != '0')\n\t\t\t\tereport(ERROR,\n\t\t\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t\t\t\t errmsg(\"\\\"%c\\\" is not a valid binary digit\",\n\t\t\t\t\t\t\t\t*sp)));\n\n\t\t\tx >>= 1;\n\t\t\tif (x == 0)\n\t\t\t{\n\t\t\t\tx = HIGHBIT;\n\t\t\t\tr++;\n\t\t\t}\n\t\t}\n\t}\n\telse\n\t{\n\t\t/* Parse the hex representation of the string */\n\t\tfor (bc = 0; *sp; sp++)\n\t\t{\n\t\t\tif (*sp >= '0' && *sp <= '9')\n\t\t\t\tx = (bits8) (*sp - '0');\n\t\t\telse if (*sp >= 'A' && *sp <= 'F')\n\t\t\t\tx = (bits8) (*sp - 'A') + 10;\n\t\t\telse if (*sp >= 'a' && *sp <= 'f')\n\t\t\t\tx = (bits8) (*sp - 'a') + 10;\n\t\t\telse\n\t\t\t\tereport(ERROR,\n\t\t\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t\t\t\t errmsg(\"\\\"%c\\\" is not a valid hexadecimal digit\",\n\t\t\t\t\t\t\t\t*sp)));\n\n\t\t\tif (bc)\n\t\t\t{\n\t\t\t\t*r++ |= x;\n\t\t\t\tbc = 0;\n\t\t\t}\n\t\t\telse\n\t\t\t{\n\t\t\t\t*r = x << 4;\n\t\t\t\tbc = 1;\n\t\t\t}\n\t\t}\n\t}\n\n\tPG_RETURN_VARBIT_P(result);\n}\n\n\nDatum\nbit_out(PG_FUNCTION_ARGS)\n{\n#if 1\n\t/* same as varbit output */\n\treturn varbit_out(fcinfo);\n#else\n\n\t/*\n\t * This is how one would print a hex string, in case someone wants to\n\t * write a formatting function.\n\t */\n\tVarBit\t   *s = PG_GETARG_VARBIT_P(0);\n\tchar\t   *result,\n\t\t\t   *r;\n\tbits8\t   *sp;\n\tint\t\t\ti,\n\t\t\t\tlen,\n\t\t\t\tbitlen;\n\n\tbitlen = VARBITLEN(s);\n\tlen = (bitlen + 3) / 4;\n\tresult = (char *) palloc(len + 2);\n\tsp = VARBITS(s);\n\tr = result;\n\t*r++ = 'X';\n\t/* we cheat by knowing that we store full bytes zero padded */\n\tfor (i = 0; i < len; i += 2, sp++)\n\t{\n\t\t*r++ = HEXDIG((*sp) >> 4);\n\t\t*r++ = HEXDIG((*sp) & 0xF);\n\t}\n\n\t/*\n\t * Go back one step if we printed a hex number that was not part of the\n\t * bitstring anymore\n\t */\n\tif (i > len)\n\t\tr--;\n\t*r = '\\0';\n\n\tPG_RETURN_CSTRING(result);\n#endif\n}\n\n/*\n *\t\tbit_recv\t\t\t- converts external binary format to bit\n */\nDatum\nbit_recv(PG_FUNCTION_ARGS)\n{\n\tStringInfo\tbuf = (StringInfo) PG_GETARG_POINTER(0);\n\n#ifdef NOT_USED\n\tOid\t\t\ttypelem = PG_GETARG_OID(1);\n#endif\n\tint32\t\tatttypmod = PG_GETARG_INT32(2);\n\tVarBit\t   *result;\n\tint\t\t\tlen,\n\t\t\t\tbitlen;\n\tint\t\t\tipad;\n\tbits8\t\tmask;\n\n\tbitlen = pq_getmsgint(buf, sizeof(int32));\n\tif (bitlen < 0)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_BINARY_REPRESENTATION),\n\t\t\t\t errmsg(\"invalid length in external bit string\")));\n\n\t/*\n\t * Sometimes atttypmod is not supplied. If it is supplied we need to make\n\t * sure that the bitstring fits.\n\t */\n\tif (atttypmod > 0 && bitlen != atttypmod)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_STRING_DATA_LENGTH_MISMATCH),\n\t\t\t\t errmsg(\"bit string length %d does not match type bit(%d)\",\n\t\t\t\t\t\tbitlen, atttypmod)));\n\n\tlen = VARBITTOTALLEN(bitlen);\n\tresult = (VarBit *) palloc(len);\n\tSET_VARSIZE(result, len);\n\tVARBITLEN(result) = bitlen;\n\n\tpq_copymsgbytes(buf, (char *) VARBITS(result), VARBITBYTES(result));\n\n\t/* Make sure last byte is zero-padded if needed */\n\tipad = VARBITPAD(result);\n\tif (ipad > 0)\n\t{\n\t\tmask = BITMASK << ipad;\n\t\t*(VARBITS(result) + VARBITBYTES(result) - 1) &= mask;\n\t}\n\n\tPG_RETURN_VARBIT_P(result);\n}\n\n/*\n *\t\tbit_send\t\t\t- converts bit to binary format\n */\nDatum\nbit_send(PG_FUNCTION_ARGS)\n{\n\t/* Exactly the same as varbit_send, so share code */\n\treturn varbit_send(fcinfo);\n}\n\n/*\n * bit()\n * Converts a bit() type to a specific internal length.\n * len is the bitlength specified in the column definition.\n *\n * If doing implicit cast, raise error when source data is wrong length.\n * If doing explicit cast, silently truncate or zero-pad to specified length.\n */\nDatum\nbit(PG_FUNCTION_ARGS)\n{\n\tVarBit\t   *arg = PG_GETARG_VARBIT_P(0);\n\tint32\t\tlen = PG_GETARG_INT32(1);\n\tbool\t\tisExplicit = PG_GETARG_BOOL(2);\n\tVarBit\t   *result;\n\tint\t\t\trlen;\n\tint\t\t\tipad;\n\tbits8\t\tmask;\n\n\t/* No work if typmod is invalid or supplied data matches it already */\n\tif (len <= 0 || len == VARBITLEN(arg))\n\t\tPG_RETURN_VARBIT_P(arg);\n\n\tif (!isExplicit)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_STRING_DATA_LENGTH_MISMATCH),\n\t\t\t\t errmsg(\"bit string length %d does not match type bit(%d)\",\n\t\t\t\t\t\tVARBITLEN(arg), len)));\n\n\trlen = VARBITTOTALLEN(len);\n\t/* set to 0 so that string is zero-padded */\n\tresult = (VarBit *) palloc0(rlen);\n\tSET_VARSIZE(result, rlen);\n\tVARBITLEN(result) = len;\n\n\tmemcpy(VARBITS(result), VARBITS(arg),\n\t\t   Min(VARBITBYTES(result), VARBITBYTES(arg)));\n\n\t/*\n\t * Make sure last byte is zero-padded if needed.  This is useless but safe\n\t * if source data was shorter than target length (we assume the last byte\n\t * of the source data was itself correctly zero-padded).\n\t */\n\tipad = VARBITPAD(result);\n\tif (ipad > 0)\n\t{\n\t\tmask = BITMASK << ipad;\n\t\t*(VARBITS(result) + VARBITBYTES(result) - 1) &= mask;\n\t}\n\n\tPG_RETURN_VARBIT_P(result);\n}\n\nDatum\nbittypmodin(PG_FUNCTION_ARGS)\n{\n\tArrayType  *ta = PG_GETARG_ARRAYTYPE_P(0);\n\n\tPG_RETURN_INT32(anybit_typmodin(ta, \"bit\"));\n}\n\nDatum\nbittypmodout(PG_FUNCTION_ARGS)\n{\n\tint32\t\ttypmod = PG_GETARG_INT32(0);\n\n\tPG_RETURN_CSTRING(anybit_typmodout(typmod));\n}\n\n\n/*\n * varbit_in -\n *\t  converts a string to the internal representation of a bitstring.\n *\t\tThis is the same as bit_in except that atttypmod is taken as\n *\t\tthe maximum length, not the exact length to force the bitstring to.\n */\nDatum\nvarbit_in(PG_FUNCTION_ARGS)\n{\n\tchar\t   *input_string = PG_GETARG_CSTRING(0);\n\n#ifdef NOT_USED\n\tOid\t\t\ttypelem = PG_GETARG_OID(1);\n#endif\n\tint32\t\tatttypmod = PG_GETARG_INT32(2);\n\tVarBit\t   *result;\t\t\t/* The resulting bit string\t\t\t  */\n\tchar\t   *sp;\t\t\t\t/* pointer into the character string  */\n\tbits8\t   *r;\t\t\t\t/* pointer into the result */\n\tint\t\t\tlen,\t\t\t/* Length of the whole data structure */\n\t\t\t\tbitlen,\t\t\t/* Number of bits in the bit string   */\n\t\t\t\tslen;\t\t\t/* Length of the input string\t\t  */\n\tbool\t\tbit_not_hex;\t/* false = hex string  true = bit string */\n\tint\t\t\tbc;\n\tbits8\t\tx = 0;\n\n\t/* Check that the first character is a b or an x */\n\tif (input_string[0] == 'b' || input_string[0] == 'B')\n\t{\n\t\tbit_not_hex = true;\n\t\tsp = input_string + 1;\n\t}\n\telse if (input_string[0] == 'x' || input_string[0] == 'X')\n\t{\n\t\tbit_not_hex = false;\n\t\tsp = input_string + 1;\n\t}\n\telse\n\t{\n\t\tbit_not_hex = true;\n\t\tsp = input_string;\n\t}\n\n\t/*\n\t * Determine bitlength from input string.  MaxAllocSize ensures a regular\n\t * input is small enough, but we must check hex input.\n\t */\n\tslen = strlen(sp);\n\tif (bit_not_hex)\n\t\tbitlen = slen;\n\telse\n\t{\n\t\tif (slen > VARBITMAXLEN / 4)\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t\t errmsg(\"bit string length exceeds the maximum allowed (%d)\",\n\t\t\t\t\t\tVARBITMAXLEN)));\n\t\tbitlen = slen * 4;\n\t}\n\n\t/*\n\t * Sometimes atttypmod is not supplied. If it is supplied we need to make\n\t * sure that the bitstring fits.\n\t */\n\tif (atttypmod <= 0)\n\t\tatttypmod = bitlen;\n\telse if (bitlen > atttypmod)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_STRING_DATA_RIGHT_TRUNCATION),\n\t\t\t\t errmsg(\"bit string too long for type bit varying(%d)\",\n\t\t\t\t\t\tatttypmod)));\n\n\tlen = VARBITTOTALLEN(bitlen);\n\t/* set to 0 so that *r is always initialised and string is zero-padded */\n\tresult = (VarBit *) palloc0(len);\n\tSET_VARSIZE(result, len);\n\tVARBITLEN(result) = Min(bitlen, atttypmod);\n\n\tr = VARBITS(result);\n\tif (bit_not_hex)\n\t{\n\t\t/* Parse the bit representation of the string */\n\t\t/* We know it fits, as bitlen was compared to atttypmod */\n\t\tx = HIGHBIT;\n\t\tfor (; *sp; sp++)\n\t\t{\n\t\t\tif (*sp == '1')\n\t\t\t\t*r |= x;\n\t\t\telse if (*sp != '0')\n\t\t\t\tereport(ERROR,\n\t\t\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t\t\t\t errmsg(\"\\\"%c\\\" is not a valid binary digit\",\n\t\t\t\t\t\t\t\t*sp)));\n\n\t\t\tx >>= 1;\n\t\t\tif (x == 0)\n\t\t\t{\n\t\t\t\tx = HIGHBIT;\n\t\t\t\tr++;\n\t\t\t}\n\t\t}\n\t}\n\telse\n\t{\n\t\t/* Parse the hex representation of the string */\n\t\tfor (bc = 0; *sp; sp++)\n\t\t{\n\t\t\tif (*sp >= '0' && *sp <= '9')\n\t\t\t\tx = (bits8) (*sp - '0');\n\t\t\telse if (*sp >= 'A' && *sp <= 'F')\n\t\t\t\tx = (bits8) (*sp - 'A') + 10;\n\t\t\telse if (*sp >= 'a' && *sp <= 'f')\n\t\t\t\tx = (bits8) (*sp - 'a') + 10;\n\t\t\telse\n\t\t\t\tereport(ERROR,\n\t\t\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t\t\t\t errmsg(\"\\\"%c\\\" is not a valid hexadecimal digit\",\n\t\t\t\t\t\t\t\t*sp)));\n\n\t\t\tif (bc)\n\t\t\t{\n\t\t\t\t*r++ |= x;\n\t\t\t\tbc = 0;\n\t\t\t}\n\t\t\telse\n\t\t\t{\n\t\t\t\t*r = x << 4;\n\t\t\t\tbc = 1;\n\t\t\t}\n\t\t}\n\t}\n\n\tPG_RETURN_VARBIT_P(result);\n}\n\n/*\n * varbit_out -\n *\t  Prints the string as bits to preserve length accurately\n *\n * XXX varbit_recv() and hex input to varbit_in() can load a value that this\n * cannot emit.  Consider using hex output for such values.\n */\nDatum\nvarbit_out(PG_FUNCTION_ARGS)\n{\n\tVarBit\t   *s = PG_GETARG_VARBIT_P(0);\n\tchar\t   *result,\n\t\t\t   *r;\n\tbits8\t   *sp;\n\tbits8\t\tx;\n\tint\t\t\ti,\n\t\t\t\tk,\n\t\t\t\tlen;\n\n\tlen = VARBITLEN(s);\n\tresult = (char *) palloc(len + 1);\n\tsp = VARBITS(s);\n\tr = result;\n\tfor (i = 0; i <= len - BITS_PER_BYTE; i += BITS_PER_BYTE, sp++)\n\t{\n\t\t/* print full bytes */\n\t\tx = *sp;\n\t\tfor (k = 0; k < BITS_PER_BYTE; k++)\n\t\t{\n\t\t\t*r++ = IS_HIGHBIT_SET(x) ? '1' : '0';\n\t\t\tx <<= 1;\n\t\t}\n\t}\n\tif (i < len)\n\t{\n\t\t/* print the last partial byte */\n\t\tx = *sp;\n\t\tfor (k = i; k < len; k++)\n\t\t{\n\t\t\t*r++ = IS_HIGHBIT_SET(x) ? '1' : '0';\n\t\t\tx <<= 1;\n\t\t}\n\t}\n\t*r = '\\0';\n\n\tPG_RETURN_CSTRING(result);\n}\n\n/*\n *\t\tvarbit_recv\t\t\t- converts external binary format to varbit\n *\n * External format is the bitlen as an int32, then the byte array.\n */\nDatum\nvarbit_recv(PG_FUNCTION_ARGS)\n{\n\tStringInfo\tbuf = (StringInfo) PG_GETARG_POINTER(0);\n\n#ifdef NOT_USED\n\tOid\t\t\ttypelem = PG_GETARG_OID(1);\n#endif\n\tint32\t\tatttypmod = PG_GETARG_INT32(2);\n\tVarBit\t   *result;\n\tint\t\t\tlen,\n\t\t\t\tbitlen;\n\tint\t\t\tipad;\n\tbits8\t\tmask;\n\n\tbitlen = pq_getmsgint(buf, sizeof(int32));\n\tif (bitlen < 0)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_BINARY_REPRESENTATION),\n\t\t\t\t errmsg(\"invalid length in external bit string\")));\n\n\t/*\n\t * Sometimes atttypmod is not supplied. If it is supplied we need to make\n\t * sure that the bitstring fits.\n\t */\n\tif (atttypmod > 0 && bitlen > atttypmod)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_STRING_DATA_RIGHT_TRUNCATION),\n\t\t\t\t errmsg(\"bit string too long for type bit varying(%d)\",\n\t\t\t\t\t\tatttypmod)));\n\n\tlen = VARBITTOTALLEN(bitlen);\n\tresult = (VarBit *) palloc(len);\n\tSET_VARSIZE(result, len);\n\tVARBITLEN(result) = bitlen;\n\n\tpq_copymsgbytes(buf, (char *) VARBITS(result), VARBITBYTES(result));\n\n\t/* Make sure last byte is zero-padded if needed */\n\tipad = VARBITPAD(result);\n\tif (ipad > 0)\n\t{\n\t\tmask = BITMASK << ipad;\n\t\t*(VARBITS(result) + VARBITBYTES(result) - 1) &= mask;\n\t}\n\n\tPG_RETURN_VARBIT_P(result);\n}\n\n/*\n *\t\tvarbit_send\t\t\t- converts varbit to binary format\n */\nDatum\nvarbit_send(PG_FUNCTION_ARGS)\n{\n\tVarBit\t   *s = PG_GETARG_VARBIT_P(0);\n\tStringInfoData buf;\n\n\tpq_begintypsend(&buf);\n\tpq_sendint(&buf, VARBITLEN(s), sizeof(int32));\n\tpq_sendbytes(&buf, (char *) VARBITS(s), VARBITBYTES(s));\n\tPG_RETURN_BYTEA_P(pq_endtypsend(&buf));\n}\n\n/*\n * varbit_transform()\n * Flatten calls to varbit's length coercion function that set the new maximum\n * length >= the previous maximum length.  We can ignore the isExplicit\n * argument, since that only affects truncation cases.\n */\nDatum\nvarbit_transform(PG_FUNCTION_ARGS)\n{\n\tFuncExpr   *expr = (FuncExpr *) PG_GETARG_POINTER(0);\n\tNode\t   *ret = NULL;\n\tNode\t   *typmod;\n\n\tAssert(IsA(expr, FuncExpr));\n\tAssert(list_length(expr->args) >= 2);\n\n\ttypmod = (Node *) lsecond(expr->args);\n\n\tif (IsA(typmod, Const) &&!((Const *) typmod)->constisnull)\n\t{\n\t\tNode\t   *source = (Node *) linitial(expr->args);\n\t\tint32\t\tnew_typmod = DatumGetInt32(((Const *) typmod)->constvalue);\n\t\tint32\t\told_max = exprTypmod(source);\n\t\tint32\t\tnew_max = new_typmod;\n\n\t\t/* Note: varbit() treats typmod 0 as invalid, so we do too */\n\t\tif (new_max <= 0 || (old_max > 0 && old_max <= new_max))\n\t\t\tret = relabel_to_typmod(source, new_typmod);\n\t}\n\n\tPG_RETURN_POINTER(ret);\n}\n\n/*\n * varbit()\n * Converts a varbit() type to a specific internal length.\n * len is the maximum bitlength specified in the column definition.\n *\n * If doing implicit cast, raise error when source data is too long.\n * If doing explicit cast, silently truncate to max length.\n */\nDatum\nvarbit(PG_FUNCTION_ARGS)\n{\n\tVarBit\t   *arg = PG_GETARG_VARBIT_P(0);\n\tint32\t\tlen = PG_GETARG_INT32(1);\n\tbool\t\tisExplicit = PG_GETARG_BOOL(2);\n\tVarBit\t   *result;\n\tint\t\t\trlen;\n\tint\t\t\tipad;\n\tbits8\t\tmask;\n\n\t/* No work if typmod is invalid or supplied data matches it already */\n\tif (len <= 0 || len >= VARBITLEN(arg))\n\t\tPG_RETURN_VARBIT_P(arg);\n\n\tif (!isExplicit)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_STRING_DATA_RIGHT_TRUNCATION),\n\t\t\t\t errmsg(\"bit string too long for type bit varying(%d)\",\n\t\t\t\t\t\tlen)));\n\n\trlen = VARBITTOTALLEN(len);\n\tresult = (VarBit *) palloc(rlen);\n\tSET_VARSIZE(result, rlen);\n\tVARBITLEN(result) = len;\n\n\tmemcpy(VARBITS(result), VARBITS(arg), VARBITBYTES(result));\n\n\t/* Make sure last byte is zero-padded if needed */\n\tipad = VARBITPAD(result);\n\tif (ipad > 0)\n\t{\n\t\tmask = BITMASK << ipad;\n\t\t*(VARBITS(result) + VARBITBYTES(result) - 1) &= mask;\n\t}\n\n\tPG_RETURN_VARBIT_P(result);\n}\n\nDatum\nvarbittypmodin(PG_FUNCTION_ARGS)\n{\n\tArrayType  *ta = PG_GETARG_ARRAYTYPE_P(0);\n\n\tPG_RETURN_INT32(anybit_typmodin(ta, \"varbit\"));\n}\n\nDatum\nvarbittypmodout(PG_FUNCTION_ARGS)\n{\n\tint32\t\ttypmod = PG_GETARG_INT32(0);\n\n\tPG_RETURN_CSTRING(anybit_typmodout(typmod));\n}\n\n\n/*\n * Comparison operators\n *\n * We only need one set of comparison operators for bitstrings, as the lengths\n * are stored in the same way for zero-padded and varying bit strings.\n *\n * Note that the standard is not unambiguous about the comparison between\n * zero-padded bit strings and varying bitstrings. If the same value is written\n * into a zero padded bitstring as into a varying bitstring, but the zero\n * padded bitstring has greater length, it will be bigger.\n *\n * Zeros from the beginning of a bitstring cannot simply be ignored, as they\n * may be part of a bit string and may be significant.\n *\n * Note: btree indexes need these routines not to leak memory; therefore,\n * be careful to free working copies of toasted datums.  Most places don't\n * need to be so careful.\n */\n\n/*\n * bit_cmp\n *\n * Compares two bitstrings and returns <0, 0, >0 depending on whether the first\n * string is smaller, equal, or bigger than the second. All bits are considered\n * and additional zero bits may make one string smaller/larger than the other,\n * even if their zero-padded values would be the same.\n */\nstatic int32\nbit_cmp(VarBit *arg1, VarBit *arg2)\n{\n\tint\t\t\tbitlen1,\n\t\t\t\tbytelen1,\n\t\t\t\tbitlen2,\n\t\t\t\tbytelen2;\n\tint32\t\tcmp;\n\n\tbytelen1 = VARBITBYTES(arg1);\n\tbytelen2 = VARBITBYTES(arg2);\n\n\tcmp = memcmp(VARBITS(arg1), VARBITS(arg2), Min(bytelen1, bytelen2));\n\tif (cmp == 0)\n\t{\n\t\tbitlen1 = VARBITLEN(arg1);\n\t\tbitlen2 = VARBITLEN(arg2);\n\t\tif (bitlen1 != bitlen2)\n\t\t\tcmp = (bitlen1 < bitlen2) ? -1 : 1;\n\t}\n\treturn cmp;\n}\n\nDatum\nbiteq(PG_FUNCTION_ARGS)\n{\n\tVarBit\t   *arg1 = PG_GETARG_VARBIT_P(0);\n\tVarBit\t   *arg2 = PG_GETARG_VARBIT_P(1);\n\tbool\t\tresult;\n\tint\t\t\tbitlen1,\n\t\t\t\tbitlen2;\n\n\tbitlen1 = VARBITLEN(arg1);\n\tbitlen2 = VARBITLEN(arg2);\n\n\t/* fast path for different-length inputs */\n\tif (bitlen1 != bitlen2)\n\t\tresult = false;\n\telse\n\t\tresult = (bit_cmp(arg1, arg2) == 0);\n\n\tPG_FREE_IF_COPY(arg1, 0);\n\tPG_FREE_IF_COPY(arg2, 1);\n\n\tPG_RETURN_BOOL(result);\n}\n\nDatum\nbitne(PG_FUNCTION_ARGS)\n{\n\tVarBit\t   *arg1 = PG_GETARG_VARBIT_P(0);\n\tVarBit\t   *arg2 = PG_GETARG_VARBIT_P(1);\n\tbool\t\tresult;\n\tint\t\t\tbitlen1,\n\t\t\t\tbitlen2;\n\n\tbitlen1 = VARBITLEN(arg1);\n\tbitlen2 = VARBITLEN(arg2);\n\n\t/* fast path for different-length inputs */\n\tif (bitlen1 != bitlen2)\n\t\tresult = true;\n\telse\n\t\tresult = (bit_cmp(arg1, arg2) != 0);\n\n\tPG_FREE_IF_COPY(arg1, 0);\n\tPG_FREE_IF_COPY(arg2, 1);\n\n\tPG_RETURN_BOOL(result);\n}\n\nDatum\nbitlt(PG_FUNCTION_ARGS)\n{\n\tVarBit\t   *arg1 = PG_GETARG_VARBIT_P(0);\n\tVarBit\t   *arg2 = PG_GETARG_VARBIT_P(1);\n\tbool\t\tresult;\n\n\tresult = (bit_cmp(arg1, arg2) < 0);\n\n\tPG_FREE_IF_COPY(arg1, 0);\n\tPG_FREE_IF_COPY(arg2, 1);\n\n\tPG_RETURN_BOOL(result);\n}\n\nDatum\nbitle(PG_FUNCTION_ARGS)\n{\n\tVarBit\t   *arg1 = PG_GETARG_VARBIT_P(0);\n\tVarBit\t   *arg2 = PG_GETARG_VARBIT_P(1);\n\tbool\t\tresult;\n\n\tresult = (bit_cmp(arg1, arg2) <= 0);\n\n\tPG_FREE_IF_COPY(arg1, 0);\n\tPG_FREE_IF_COPY(arg2, 1);\n\n\tPG_RETURN_BOOL(result);\n}\n\nDatum\nbitgt(PG_FUNCTION_ARGS)\n{\n\tVarBit\t   *arg1 = PG_GETARG_VARBIT_P(0);\n\tVarBit\t   *arg2 = PG_GETARG_VARBIT_P(1);\n\tbool\t\tresult;\n\n\tresult = (bit_cmp(arg1, arg2) > 0);\n\n\tPG_FREE_IF_COPY(arg1, 0);\n\tPG_FREE_IF_COPY(arg2, 1);\n\n\tPG_RETURN_BOOL(result);\n}\n\nDatum\nbitge(PG_FUNCTION_ARGS)\n{\n\tVarBit\t   *arg1 = PG_GETARG_VARBIT_P(0);\n\tVarBit\t   *arg2 = PG_GETARG_VARBIT_P(1);\n\tbool\t\tresult;\n\n\tresult = (bit_cmp(arg1, arg2) >= 0);\n\n\tPG_FREE_IF_COPY(arg1, 0);\n\tPG_FREE_IF_COPY(arg2, 1);\n\n\tPG_RETURN_BOOL(result);\n}\n\nDatum\nbitcmp(PG_FUNCTION_ARGS)\n{\n\tVarBit\t   *arg1 = PG_GETARG_VARBIT_P(0);\n\tVarBit\t   *arg2 = PG_GETARG_VARBIT_P(1);\n\tint32\t\tresult;\n\n\tresult = bit_cmp(arg1, arg2);\n\n\tPG_FREE_IF_COPY(arg1, 0);\n\tPG_FREE_IF_COPY(arg2, 1);\n\n\tPG_RETURN_INT32(result);\n}\n\n/*\n * bitcat\n * Concatenation of bit strings\n */\nDatum\nbitcat(PG_FUNCTION_ARGS)\n{\n\tVarBit\t   *arg1 = PG_GETARG_VARBIT_P(0);\n\tVarBit\t   *arg2 = PG_GETARG_VARBIT_P(1);\n\n\tPG_RETURN_VARBIT_P(bit_catenate(arg1, arg2));\n}\n\nstatic VarBit *\nbit_catenate(VarBit *arg1, VarBit *arg2)\n{\n\tVarBit\t   *result;\n\tint\t\t\tbitlen1,\n\t\t\t\tbitlen2,\n\t\t\t\tbytelen,\n\t\t\t\tbit1pad,\n\t\t\t\tbit2shift;\n\tbits8\t   *pr,\n\t\t\t   *pa;\n\n\tbitlen1 = VARBITLEN(arg1);\n\tbitlen2 = VARBITLEN(arg2);\n\n\tif (bitlen1 > VARBITMAXLEN - bitlen2)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t\t errmsg(\"bit string length exceeds the maximum allowed (%d)\",\n\t\t\t\t\t\tVARBITMAXLEN)));\n\tbytelen = VARBITTOTALLEN(bitlen1 + bitlen2);\n\n\tresult = (VarBit *) palloc(bytelen);\n\tSET_VARSIZE(result, bytelen);\n\tVARBITLEN(result) = bitlen1 + bitlen2;\n\n\t/* Copy the first bitstring in */\n\tmemcpy(VARBITS(result), VARBITS(arg1), VARBITBYTES(arg1));\n\n\t/* Copy the second bit string */\n\tbit1pad = VARBITPAD(arg1);\n\tif (bit1pad == 0)\n\t{\n\t\tmemcpy(VARBITS(result) + VARBITBYTES(arg1), VARBITS(arg2),\n\t\t\t   VARBITBYTES(arg2));\n\t}\n\telse if (bitlen2 > 0)\n\t{\n\t\t/* We need to shift all the bits to fit */\n\t\tbit2shift = BITS_PER_BYTE - bit1pad;\n\t\tpr = VARBITS(result) + VARBITBYTES(arg1) - 1;\n\t\tfor (pa = VARBITS(arg2); pa < VARBITEND(arg2); pa++)\n\t\t{\n\t\t\t*pr |= ((*pa >> bit2shift) & BITMASK);\n\t\t\tpr++;\n\t\t\tif (pr < VARBITEND(result))\n\t\t\t\t*pr = (*pa << bit1pad) & BITMASK;\n\t\t}\n\t}\n\n\treturn result;\n}\n\n/*\n * bitsubstr\n * retrieve a substring from the bit string.\n * Note, s is 1-based.\n * SQL draft 6.10 9)\n */\nDatum\nbitsubstr(PG_FUNCTION_ARGS)\n{\n\tPG_RETURN_VARBIT_P(bitsubstring(PG_GETARG_VARBIT_P(0),\n\t\t\t\t\t\t\t\t\tPG_GETARG_INT32(1),\n\t\t\t\t\t\t\t\t\tPG_GETARG_INT32(2),\n\t\t\t\t\t\t\t\t\tfalse));\n}\n\nDatum\nbitsubstr_no_len(PG_FUNCTION_ARGS)\n{\n\tPG_RETURN_VARBIT_P(bitsubstring(PG_GETARG_VARBIT_P(0),\n\t\t\t\t\t\t\t\t\tPG_GETARG_INT32(1),\n\t\t\t\t\t\t\t\t\t-1, true));\n}\n\nstatic VarBit *\nbitsubstring(VarBit *arg, int32 s, int32 l, bool length_not_specified)\n{\n\tVarBit\t   *result;\n\tint\t\t\tbitlen,\n\t\t\t\trbitlen,\n\t\t\t\tlen,\n\t\t\t\tipad = 0,\n\t\t\t\tishift,\n\t\t\t\ti;\n\tint\t\t\te,\n\t\t\t\ts1,\n\t\t\t\te1;\n\tbits8\t\tmask,\n\t\t\t   *r,\n\t\t\t   *ps;\n\n\tbitlen = VARBITLEN(arg);\n\ts1 = Max(s, 1);\n\t/* If we do not have an upper bound, use end of string */\n\tif (length_not_specified)\n\t{\n\t\te1 = bitlen + 1;\n\t}\n\telse\n\t{\n\t\te = s + l;\n\n\t\t/*\n\t\t * A negative value for L is the only way for the end position to be\n\t\t * before the start. SQL99 says to throw an error.\n\t\t */\n\t\tif (e < s)\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_SUBSTRING_ERROR),\n\t\t\t\t\t errmsg(\"negative substring length not allowed\")));\n\t\te1 = Min(e, bitlen + 1);\n\t}\n\tif (s1 > bitlen || e1 <= s1)\n\t{\n\t\t/* Need to return a zero-length bitstring */\n\t\tlen = VARBITTOTALLEN(0);\n\t\tresult = (VarBit *) palloc(len);\n\t\tSET_VARSIZE(result, len);\n\t\tVARBITLEN(result) = 0;\n\t}\n\telse\n\t{\n\t\t/*\n\t\t * OK, we've got a true substring starting at position s1-1 and ending\n\t\t * at position e1-1\n\t\t */\n\t\trbitlen = e1 - s1;\n\t\tlen = VARBITTOTALLEN(rbitlen);\n\t\tresult = (VarBit *) palloc(len);\n\t\tSET_VARSIZE(result, len);\n\t\tVARBITLEN(result) = rbitlen;\n\t\tlen -= VARHDRSZ + VARBITHDRSZ;\n\t\t/* Are we copying from a byte boundary? */\n\t\tif ((s1 - 1) % BITS_PER_BYTE == 0)\n\t\t{\n\t\t\t/* Yep, we are copying bytes */\n\t\t\tmemcpy(VARBITS(result), VARBITS(arg) + (s1 - 1) / BITS_PER_BYTE,\n\t\t\t\t   len);\n\t\t}\n\t\telse\n\t\t{\n\t\t\t/* Figure out how much we need to shift the sequence by */\n\t\t\tishift = (s1 - 1) % BITS_PER_BYTE;\n\t\t\tr = VARBITS(result);\n\t\t\tps = VARBITS(arg) + (s1 - 1) / BITS_PER_BYTE;\n\t\t\tfor (i = 0; i < len; i++)\n\t\t\t{\n\t\t\t\t*r = (*ps << ishift) & BITMASK;\n\t\t\t\tif ((++ps) < VARBITEND(arg))\n\t\t\t\t\t*r |= *ps >> (BITS_PER_BYTE - ishift);\n\t\t\t\tr++;\n\t\t\t}\n\t\t}\n\t\t/* Do we need to pad at the end? */\n\t\tipad = VARBITPAD(result);\n\t\tif (ipad > 0)\n\t\t{\n\t\t\tmask = BITMASK << ipad;\n\t\t\t*(VARBITS(result) + len - 1) &= mask;\n\t\t}\n\t}\n\n\treturn result;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 472,
        "critical_vars": [
            "slen"
        ],
        "function": "varbit_in",
        "filename": "postgres/CVE-2014-2669/CVE-2014-2669_CWE-189_31400a673325147e1205326008e32135a78b4d8a_varbit.c.diff",
        "label": "True",
        "function_code": "Datum\nvarbit_in(PG_FUNCTION_ARGS)\n{\n\tchar\t   *input_string = PG_GETARG_CSTRING(0);\n\n#ifdef NOT_USED\n\tOid\t\t\ttypelem = PG_GETARG_OID(1);\n#endif\n\tint32\t\tatttypmod = PG_GETARG_INT32(2);\n\tVarBit\t   *result;\t\t\t/* The resulting bit string\t\t\t  */\n\tchar\t   *sp;\t\t\t\t/* pointer into the character string  */\n\tbits8\t   *r;\t\t\t\t/* pointer into the result */\n\tint\t\t\tlen,\t\t\t/* Length of the whole data structure */\n\t\t\t\tbitlen,\t\t\t/* Number of bits in the bit string   */\n\t\t\t\tslen;\t\t\t/* Length of the input string\t\t  */\n\tbool\t\tbit_not_hex;\t/* false = hex string  true = bit string */\n\tint\t\t\tbc;\n\tbits8\t\tx = 0;\n\n\t/* Check that the first character is a b or an x */\n\tif (input_string[0] == 'b' || input_string[0] == 'B')\n\t{\n\t\tbit_not_hex = true;\n\t\tsp = input_string + 1;\n\t}\n\telse if (input_string[0] == 'x' || input_string[0] == 'X')\n\t{\n\t\tbit_not_hex = false;\n\t\tsp = input_string + 1;\n\t}\n\telse\n\t{\n\t\tbit_not_hex = true;\n\t\tsp = input_string;\n\t}\n\n\t/*\n\t * Determine bitlength from input string.  MaxAllocSize ensures a regular\n\t * input is small enough, but we must check hex input.\n\t */\n\tslen = strlen(sp);\n\tif (bit_not_hex)\n\t\tbitlen = slen;\n\telse\n\t{\n\t\tif (slen > VARBITMAXLEN / 4)\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t\t errmsg(\"bit string length exceeds the maximum allowed (%d)\",\n\t\t\t\t\t\tVARBITMAXLEN)));\n\t\tbitlen = slen * 4;\n\t}\n\n\t/*\n\t * Sometimes atttypmod is not supplied. If it is supplied we need to make\n\t * sure that the bitstring fits.\n\t */\n\tif (atttypmod <= 0)\n\t\tatttypmod = bitlen;\n\telse if (bitlen > atttypmod)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_STRING_DATA_RIGHT_TRUNCATION),\n\t\t\t\t errmsg(\"bit string too long for type bit varying(%d)\",\n\t\t\t\t\t\tatttypmod)));\n\n\tlen = VARBITTOTALLEN(bitlen);\n\t/* set to 0 so that *r is always initialised and string is zero-padded */\n\tresult = (VarBit *) palloc0(len);\n\tSET_VARSIZE(result, len);\n\tVARBITLEN(result) = Min(bitlen, atttypmod);\n\n\tr = VARBITS(result);\n\tif (bit_not_hex)\n\t{\n\t\t/* Parse the bit representation of the string */\n\t\t/* We know it fits, as bitlen was compared to atttypmod */\n\t\tx = HIGHBIT;\n\t\tfor (; *sp; sp++)\n\t\t{\n\t\t\tif (*sp == '1')\n\t\t\t\t*r |= x;\n\t\t\telse if (*sp != '0')\n\t\t\t\tereport(ERROR,\n\t\t\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t\t\t\t errmsg(\"\\\"%c\\\" is not a valid binary digit\",\n\t\t\t\t\t\t\t\t*sp)));\n\n\t\t\tx >>= 1;\n\t\t\tif (x == 0)\n\t\t\t{\n\t\t\t\tx = HIGHBIT;\n\t\t\t\tr++;\n\t\t\t}\n\t\t}\n\t}\n\telse\n\t{\n\t\t/* Parse the hex representation of the string */\n\t\tfor (bc = 0; *sp; sp++)\n\t\t{\n\t\t\tif (*sp >= '0' && *sp <= '9')\n\t\t\t\tx = (bits8) (*sp - '0');\n\t\t\telse if (*sp >= 'A' && *sp <= 'F')\n\t\t\t\tx = (bits8) (*sp - 'A') + 10;\n\t\t\telse if (*sp >= 'a' && *sp <= 'f')\n\t\t\t\tx = (bits8) (*sp - 'a') + 10;\n\t\t\telse\n\t\t\t\tereport(ERROR,\n\t\t\t\t\t\t(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),\n\t\t\t\t\t\t errmsg(\"\\\"%c\\\" is not a valid hexadecimal digit\",\n\t\t\t\t\t\t\t\t*sp)));\n\n\t\t\tif (bc)\n\t\t\t{\n\t\t\t\t*r++ |= x;\n\t\t\t\tbc = 0;\n\t\t\t}\n\t\t\telse\n\t\t\t{\n\t\t\t\t*r = x << 4;\n\t\t\t\tbc = 1;\n\t\t\t}\n\t\t}\n\t}\n\n\tPG_RETURN_VARBIT_P(result);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 970,
        "critical_vars": [
            "bitlen1"
        ],
        "function": "bit_catenate",
        "filename": "postgres/CVE-2014-2669/CVE-2014-2669_CWE-189_31400a673325147e1205326008e32135a78b4d8a_varbit.c.diff",
        "label": "True",
        "function_code": "\nstatic VarBit *bit_catenate(VarBit *arg1, VarBit *arg2);\nstatic VarBit *bitsubstring(VarBit *arg, int32 s, int32 l,\n\t\t\t bool length_not_specified);\nstatic VarBit *bit_overlay(VarBit *t1, VarBit *t2, int sp, int sl);\n\n\n/*\n * common code for bittypmodin and varbittypmodin\n */\nstatic int32\nanybit_typmodin(ArrayType *ta, const char *typename)\n{\n\tint32\t\ttypmod;\n\tint32\t   *tl;\n\tint\t\t\tn;\n\n\ttl = ArrayGetIntegerTypmods(ta, &n);\n\n\t/*\n\t * we're not too tense about good error message here because grammar\n\t * shouldn't allow wrong number of modifiers for BIT\n\t */\n\tif (n != 1)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_PARAMETER_VALUE),\n\t\t\t\t errmsg(\"invalid type modifier\")));\n\n\tif (*tl < 1)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_PARAMETER_VALUE),\n\t\t\t\t errmsg(\"length for type %s must be at least 1\",\n\t\t\t\t\t\ttypename)));\n\tif (*tl > (MaxAttrSize * BITS_PER_BYTE))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INVALID_PARAMETER_VALUE),\n\t\t\t\t errmsg(\"length for type %s cannot exceed %d\",\n\t\t\t\t\t\ttypename, MaxAttrSize * BITS_PER_BYTE)));\n\n\ttypmod = *tl;\n\n\treturn typmod;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 443,
        "critical_vars": [
            "pcount"
        ],
        "function": "hstore_recv",
        "filename": "postgres/CVE-2014-2669/CVE-2014-2669_CWE-189_31400a673325147e1205326008e32135a78b4d8a_hstore_io.c.diff",
        "label": "True",
        "function_code": "Datum\t\thstore_recv(PG_FUNCTION_ARGS);\nDatum\nhstore_recv(PG_FUNCTION_ARGS)\n{\n\tint32\t\tbuflen;\n\tHStore\t   *out;\n\tPairs\t   *pairs;\n\tint32\t\ti;\n\tint32\t\tpcount;\n\tStringInfo\tbuf = (StringInfo) PG_GETARG_POINTER(0);\n\n\tpcount = pq_getmsgint(buf, 4);\n\n\tif (pcount == 0)\n\t{\n\t\tout = hstorePairs(NULL, 0, 0);\n\t\tPG_RETURN_POINTER(out);\n\t}\n\n\tif (pcount < 0 || pcount > MaxAllocSize / sizeof(Pairs))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t  errmsg(\"number of pairs (%d) exceeds the maximum allowed (%d)\",\n\t\t\t\t\t pcount, (int) (MaxAllocSize / sizeof(Pairs)))));\n\tpairs = palloc(pcount * sizeof(Pairs));\n\n\tfor (i = 0; i < pcount; ++i)\n\t{\n\t\tint\t\t\trawlen = pq_getmsgint(buf, 4);\n\t\tint\t\t\tlen;\n\n\t\tif (rawlen < 0)\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_NULL_VALUE_NOT_ALLOWED),\n\t\t\t\t\t errmsg(\"null value not allowed for hstore key\")));\n\n\t\tpairs[i].key = pq_getmsgtext(buf, rawlen, &len);\n\t\tpairs[i].keylen = hstoreCheckKeyLen(len);\n\t\tpairs[i].needfree = true;\n\n\t\trawlen = pq_getmsgint(buf, 4);\n\t\tif (rawlen < 0)\n\t\t{\n\t\t\tpairs[i].val = NULL;\n\t\t\tpairs[i].vallen = 0;\n\t\t\tpairs[i].isnull = true;\n\t\t}\n\t\telse\n\t\t{\n\t\t\tpairs[i].val = pq_getmsgtext(buf, rawlen, &len);\n\t\t\tpairs[i].vallen = hstoreCheckValLen(len);\n\t\t\tpairs[i].isnull = false;\n\t\t}\n\t}\n\n\tpcount = hstoreUniquePairs(pairs, pcount, &buflen);\n\n\tout = hstorePairs(pairs, pcount, buflen);\n\n\tPG_RETURN_POINTER(out);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 444,
        "critical_vars": [
            "Pairs",
            "pcount"
        ],
        "function": "hstore_recv",
        "filename": "postgres/CVE-2014-2669/CVE-2014-2669_CWE-189_31400a673325147e1205326008e32135a78b4d8a_hstore_io.c.diff",
        "label": "True",
        "function_code": "Datum\t\thstore_recv(PG_FUNCTION_ARGS);\nDatum\nhstore_recv(PG_FUNCTION_ARGS)\n{\n\tint32\t\tbuflen;\n\tHStore\t   *out;\n\tPairs\t   *pairs;\n\tint32\t\ti;\n\tint32\t\tpcount;\n\tStringInfo\tbuf = (StringInfo) PG_GETARG_POINTER(0);\n\n\tpcount = pq_getmsgint(buf, 4);\n\n\tif (pcount == 0)\n\t{\n\t\tout = hstorePairs(NULL, 0, 0);\n\t\tPG_RETURN_POINTER(out);\n\t}\n\n\tif (pcount < 0 || pcount > MaxAllocSize / sizeof(Pairs))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t  errmsg(\"number of pairs (%d) exceeds the maximum allowed (%d)\",\n\t\t\t\t\t pcount, (int) (MaxAllocSize / sizeof(Pairs)))));\n\tpairs = palloc(pcount * sizeof(Pairs));\n\n\tfor (i = 0; i < pcount; ++i)\n\t{\n\t\tint\t\t\trawlen = pq_getmsgint(buf, 4);\n\t\tint\t\t\tlen;\n\n\t\tif (rawlen < 0)\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_NULL_VALUE_NOT_ALLOWED),\n\t\t\t\t\t errmsg(\"null value not allowed for hstore key\")));\n\n\t\tpairs[i].key = pq_getmsgtext(buf, rawlen, &len);\n\t\tpairs[i].keylen = hstoreCheckKeyLen(len);\n\t\tpairs[i].needfree = true;\n\n\t\trawlen = pq_getmsgint(buf, 4);\n\t\tif (rawlen < 0)\n\t\t{\n\t\t\tpairs[i].val = NULL;\n\t\t\tpairs[i].vallen = 0;\n\t\t\tpairs[i].isnull = true;\n\t\t}\n\t\telse\n\t\t{\n\t\t\tpairs[i].val = pq_getmsgtext(buf, rawlen, &len);\n\t\t\tpairs[i].vallen = hstoreCheckValLen(len);\n\t\t\tpairs[i].isnull = false;\n\t\t}\n\t}\n\n\tpcount = hstoreUniquePairs(pairs, pcount, &buflen);\n\n\tout = hstorePairs(pairs, pcount, buflen);\n\n\tPG_RETURN_POINTER(out);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 564,
        "critical_vars": [
            "key_count"
        ],
        "function": "hstore_from_arrays",
        "filename": "postgres/CVE-2014-2669/CVE-2014-2669_CWE-189_31400a673325147e1205326008e32135a78b4d8a_hstore_io.c.diff",
        "label": "True",
        "function_code": "Datum\t\thstore_from_arrays(PG_FUNCTION_ARGS);\nDatum\nhstore_from_arrays(PG_FUNCTION_ARGS)\n{\n\tint32\t\tbuflen;\n\tHStore\t   *out;\n\tPairs\t   *pairs;\n\tDatum\t   *key_datums;\n\tbool\t   *key_nulls;\n\tint\t\t\tkey_count;\n\tDatum\t   *value_datums;\n\tbool\t   *value_nulls;\n\tint\t\t\tvalue_count;\n\tArrayType  *key_array;\n\tArrayType  *value_array;\n\tint\t\t\ti;\n\n\tif (PG_ARGISNULL(0))\n\t\tPG_RETURN_NULL();\n\n\tkey_array = PG_GETARG_ARRAYTYPE_P(0);\n\n\tAssert(ARR_ELEMTYPE(key_array) == TEXTOID);\n\n\t/*\n\t * must check >1 rather than != 1 because empty arrays have 0 dimensions,\n\t * not 1\n\t */\n\n\tif (ARR_NDIM(key_array) > 1)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_ARRAY_SUBSCRIPT_ERROR),\n\t\t\t\t errmsg(\"wrong number of array subscripts\")));\n\n\tdeconstruct_array(key_array,\n\t\t\t\t\t  TEXTOID, -1, false, 'i',\n\t\t\t\t\t  &key_datums, &key_nulls, &key_count);\n\n\t/* see discussion in hstoreArrayToPairs() */\n\tif (key_count > MaxAllocSize / sizeof(Pairs))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t  errmsg(\"number of pairs (%d) exceeds the maximum allowed (%d)\",\n\t\t\t\t\t key_count, (int) (MaxAllocSize / sizeof(Pairs)))));\n\n\t/* value_array might be NULL */\n\n\tif (PG_ARGISNULL(1))\n\t{\n\t\tvalue_array = NULL;\n\t\tvalue_count = key_count;\n\t\tvalue_datums = NULL;\n\t\tvalue_nulls = NULL;\n\t}\n\telse\n\t{\n\t\tvalue_array = PG_GETARG_ARRAYTYPE_P(1);\n\n\t\tAssert(ARR_ELEMTYPE(value_array) == TEXTOID);\n\n\t\tif (ARR_NDIM(value_array) > 1)\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_ARRAY_SUBSCRIPT_ERROR),\n\t\t\t\t\t errmsg(\"wrong number of array subscripts\")));\n\n\t\tif ((ARR_NDIM(key_array) > 0 || ARR_NDIM(value_array) > 0) &&\n\t\t\t(ARR_NDIM(key_array) != ARR_NDIM(value_array) ||\n\t\t\t ARR_DIMS(key_array)[0] != ARR_DIMS(value_array)[0] ||\n\t\t\t ARR_LBOUND(key_array)[0] != ARR_LBOUND(value_array)[0]))\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_ARRAY_SUBSCRIPT_ERROR),\n\t\t\t\t\t errmsg(\"arrays must have same bounds\")));\n\n\t\tdeconstruct_array(value_array,\n\t\t\t\t\t\t  TEXTOID, -1, false, 'i',\n\t\t\t\t\t\t  &value_datums, &value_nulls, &value_count);\n\n\t\tAssert(key_count == value_count);\n\t}\n\n\tpairs = palloc(key_count * sizeof(Pairs));\n\n\tfor (i = 0; i < key_count; ++i)\n\t{\n\t\tif (key_nulls[i])\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_NULL_VALUE_NOT_ALLOWED),\n\t\t\t\t\t errmsg(\"null value not allowed for hstore key\")));\n\n\t\tif (!value_nulls || value_nulls[i])\n\t\t{\n\t\t\tpairs[i].key = VARDATA_ANY(key_datums[i]);\n\t\t\tpairs[i].val = NULL;\n\t\t\tpairs[i].keylen = hstoreCheckKeyLen(VARSIZE_ANY_EXHDR(key_datums[i]));\n\t\t\tpairs[i].vallen = 4;\n\t\t\tpairs[i].isnull = true;\n\t\t\tpairs[i].needfree = false;\n\t\t}\n\t\telse\n\t\t{\n\t\t\tpairs[i].key = VARDATA_ANY(key_datums[i]);\n\t\t\tpairs[i].val = VARDATA_ANY(value_datums[i]);\n\t\t\tpairs[i].keylen = hstoreCheckKeyLen(VARSIZE_ANY_EXHDR(key_datums[i]));\n\t\t\tpairs[i].vallen = hstoreCheckValLen(VARSIZE_ANY_EXHDR(value_datums[i]));\n\t\t\tpairs[i].isnull = false;\n\t\t\tpairs[i].needfree = false;\n\t\t}\n\t}\n\n\tkey_count = hstoreUniquePairs(pairs, key_count, &buflen);\n\n\tout = hstorePairs(pairs, key_count, buflen);\n\n\tPG_RETURN_POINTER(out);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 565,
        "critical_vars": [
            "Pairs",
            "key_count"
        ],
        "function": "hstore_from_arrays",
        "filename": "postgres/CVE-2014-2669/CVE-2014-2669_CWE-189_31400a673325147e1205326008e32135a78b4d8a_hstore_io.c.diff",
        "label": "True",
        "function_code": "Datum\t\thstore_from_arrays(PG_FUNCTION_ARGS);\nDatum\nhstore_from_arrays(PG_FUNCTION_ARGS)\n{\n\tint32\t\tbuflen;\n\tHStore\t   *out;\n\tPairs\t   *pairs;\n\tDatum\t   *key_datums;\n\tbool\t   *key_nulls;\n\tint\t\t\tkey_count;\n\tDatum\t   *value_datums;\n\tbool\t   *value_nulls;\n\tint\t\t\tvalue_count;\n\tArrayType  *key_array;\n\tArrayType  *value_array;\n\tint\t\t\ti;\n\n\tif (PG_ARGISNULL(0))\n\t\tPG_RETURN_NULL();\n\n\tkey_array = PG_GETARG_ARRAYTYPE_P(0);\n\n\tAssert(ARR_ELEMTYPE(key_array) == TEXTOID);\n\n\t/*\n\t * must check >1 rather than != 1 because empty arrays have 0 dimensions,\n\t * not 1\n\t */\n\n\tif (ARR_NDIM(key_array) > 1)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_ARRAY_SUBSCRIPT_ERROR),\n\t\t\t\t errmsg(\"wrong number of array subscripts\")));\n\n\tdeconstruct_array(key_array,\n\t\t\t\t\t  TEXTOID, -1, false, 'i',\n\t\t\t\t\t  &key_datums, &key_nulls, &key_count);\n\n\t/* see discussion in hstoreArrayToPairs() */\n\tif (key_count > MaxAllocSize / sizeof(Pairs))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t  errmsg(\"number of pairs (%d) exceeds the maximum allowed (%d)\",\n\t\t\t\t\t key_count, (int) (MaxAllocSize / sizeof(Pairs)))));\n\n\t/* value_array might be NULL */\n\n\tif (PG_ARGISNULL(1))\n\t{\n\t\tvalue_array = NULL;\n\t\tvalue_count = key_count;\n\t\tvalue_datums = NULL;\n\t\tvalue_nulls = NULL;\n\t}\n\telse\n\t{\n\t\tvalue_array = PG_GETARG_ARRAYTYPE_P(1);\n\n\t\tAssert(ARR_ELEMTYPE(value_array) == TEXTOID);\n\n\t\tif (ARR_NDIM(value_array) > 1)\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_ARRAY_SUBSCRIPT_ERROR),\n\t\t\t\t\t errmsg(\"wrong number of array subscripts\")));\n\n\t\tif ((ARR_NDIM(key_array) > 0 || ARR_NDIM(value_array) > 0) &&\n\t\t\t(ARR_NDIM(key_array) != ARR_NDIM(value_array) ||\n\t\t\t ARR_DIMS(key_array)[0] != ARR_DIMS(value_array)[0] ||\n\t\t\t ARR_LBOUND(key_array)[0] != ARR_LBOUND(value_array)[0]))\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_ARRAY_SUBSCRIPT_ERROR),\n\t\t\t\t\t errmsg(\"arrays must have same bounds\")));\n\n\t\tdeconstruct_array(value_array,\n\t\t\t\t\t\t  TEXTOID, -1, false, 'i',\n\t\t\t\t\t\t  &value_datums, &value_nulls, &value_count);\n\n\t\tAssert(key_count == value_count);\n\t}\n\n\tpairs = palloc(key_count * sizeof(Pairs));\n\n\tfor (i = 0; i < key_count; ++i)\n\t{\n\t\tif (key_nulls[i])\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_NULL_VALUE_NOT_ALLOWED),\n\t\t\t\t\t errmsg(\"null value not allowed for hstore key\")));\n\n\t\tif (!value_nulls || value_nulls[i])\n\t\t{\n\t\t\tpairs[i].key = VARDATA_ANY(key_datums[i]);\n\t\t\tpairs[i].val = NULL;\n\t\t\tpairs[i].keylen = hstoreCheckKeyLen(VARSIZE_ANY_EXHDR(key_datums[i]));\n\t\t\tpairs[i].vallen = 4;\n\t\t\tpairs[i].isnull = true;\n\t\t\tpairs[i].needfree = false;\n\t\t}\n\t\telse\n\t\t{\n\t\t\tpairs[i].key = VARDATA_ANY(key_datums[i]);\n\t\t\tpairs[i].val = VARDATA_ANY(value_datums[i]);\n\t\t\tpairs[i].keylen = hstoreCheckKeyLen(VARSIZE_ANY_EXHDR(key_datums[i]));\n\t\t\tpairs[i].vallen = hstoreCheckValLen(VARSIZE_ANY_EXHDR(value_datums[i]));\n\t\t\tpairs[i].isnull = false;\n\t\t\tpairs[i].needfree = false;\n\t\t}\n\t}\n\n\tkey_count = hstoreUniquePairs(pairs, key_count, &buflen);\n\n\tout = hstorePairs(pairs, key_count, buflen);\n\n\tPG_RETURN_POINTER(out);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 693,
        "critical_vars": [
            "count"
        ],
        "function": "hstore_from_array",
        "filename": "postgres/CVE-2014-2669/CVE-2014-2669_CWE-189_31400a673325147e1205326008e32135a78b4d8a_hstore_io.c.diff",
        "label": "True",
        "function_code": "Datum\t\thstore_from_array(PG_FUNCTION_ARGS);\nDatum\nhstore_from_array(PG_FUNCTION_ARGS)\n{\n\tArrayType  *in_array = PG_GETARG_ARRAYTYPE_P(0);\n\tint\t\t\tndims = ARR_NDIM(in_array);\n\tint\t\t\tcount;\n\tint32\t\tbuflen;\n\tHStore\t   *out;\n\tPairs\t   *pairs;\n\tDatum\t   *in_datums;\n\tbool\t   *in_nulls;\n\tint\t\t\tin_count;\n\tint\t\t\ti;\n\n\tAssert(ARR_ELEMTYPE(in_array) == TEXTOID);\n\n\tswitch (ndims)\n\t{\n\t\tcase 0:\n\t\t\tout = hstorePairs(NULL, 0, 0);\n\t\t\tPG_RETURN_POINTER(out);\n\n\t\tcase 1:\n\t\t\tif ((ARR_DIMS(in_array)[0]) % 2)\n\t\t\t\tereport(ERROR,\n\t\t\t\t\t\t(errcode(ERRCODE_ARRAY_SUBSCRIPT_ERROR),\n\t\t\t\t\t\t errmsg(\"array must have even number of elements\")));\n\t\t\tbreak;\n\n\t\tcase 2:\n\t\t\tif ((ARR_DIMS(in_array)[1]) != 2)\n\t\t\t\tereport(ERROR,\n\t\t\t\t\t\t(errcode(ERRCODE_ARRAY_SUBSCRIPT_ERROR),\n\t\t\t\t\t\t errmsg(\"array must have two columns\")));\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_ARRAY_SUBSCRIPT_ERROR),\n\t\t\t\t\t errmsg(\"wrong number of array subscripts\")));\n\t}\n\n\tdeconstruct_array(in_array,\n\t\t\t\t\t  TEXTOID, -1, false, 'i',\n\t\t\t\t\t  &in_datums, &in_nulls, &in_count);\n\n\tcount = in_count / 2;\n\n\t/* see discussion in hstoreArrayToPairs() */\n\tif (count > MaxAllocSize / sizeof(Pairs))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t  errmsg(\"number of pairs (%d) exceeds the maximum allowed (%d)\",\n\t\t\t\t\t count, (int) (MaxAllocSize / sizeof(Pairs)))));\n\n\tpairs = palloc(count * sizeof(Pairs));\n\n\tfor (i = 0; i < count; ++i)\n\t{\n\t\tif (in_nulls[i * 2])\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_NULL_VALUE_NOT_ALLOWED),\n\t\t\t\t\t errmsg(\"null value not allowed for hstore key\")));\n\n\t\tif (in_nulls[i * 2 + 1])\n\t\t{\n\t\t\tpairs[i].key = VARDATA_ANY(in_datums[i * 2]);\n\t\t\tpairs[i].val = NULL;\n\t\t\tpairs[i].keylen = hstoreCheckKeyLen(VARSIZE_ANY_EXHDR(in_datums[i * 2]));\n\t\t\tpairs[i].vallen = 4;\n\t\t\tpairs[i].isnull = true;\n\t\t\tpairs[i].needfree = false;\n\t\t}\n\t\telse\n\t\t{\n\t\t\tpairs[i].key = VARDATA_ANY(in_datums[i * 2]);\n\t\t\tpairs[i].val = VARDATA_ANY(in_datums[i * 2 + 1]);\n\t\t\tpairs[i].keylen = hstoreCheckKeyLen(VARSIZE_ANY_EXHDR(in_datums[i * 2]));\n\t\t\tpairs[i].vallen = hstoreCheckValLen(VARSIZE_ANY_EXHDR(in_datums[i * 2 + 1]));\n\t\t\tpairs[i].isnull = false;\n\t\t\tpairs[i].needfree = false;\n\t\t}\n\t}\n\n\tcount = hstoreUniquePairs(pairs, count, &buflen);\n\n\tout = hstorePairs(pairs, count, buflen);\n\n\tPG_RETURN_POINTER(out);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 694,
        "critical_vars": [
            "Pairs",
            "count"
        ],
        "function": "hstore_from_array",
        "filename": "postgres/CVE-2014-2669/CVE-2014-2669_CWE-189_31400a673325147e1205326008e32135a78b4d8a_hstore_io.c.diff",
        "label": "True",
        "function_code": "Datum\t\thstore_from_array(PG_FUNCTION_ARGS);\nDatum\nhstore_from_array(PG_FUNCTION_ARGS)\n{\n\tArrayType  *in_array = PG_GETARG_ARRAYTYPE_P(0);\n\tint\t\t\tndims = ARR_NDIM(in_array);\n\tint\t\t\tcount;\n\tint32\t\tbuflen;\n\tHStore\t   *out;\n\tPairs\t   *pairs;\n\tDatum\t   *in_datums;\n\tbool\t   *in_nulls;\n\tint\t\t\tin_count;\n\tint\t\t\ti;\n\n\tAssert(ARR_ELEMTYPE(in_array) == TEXTOID);\n\n\tswitch (ndims)\n\t{\n\t\tcase 0:\n\t\t\tout = hstorePairs(NULL, 0, 0);\n\t\t\tPG_RETURN_POINTER(out);\n\n\t\tcase 1:\n\t\t\tif ((ARR_DIMS(in_array)[0]) % 2)\n\t\t\t\tereport(ERROR,\n\t\t\t\t\t\t(errcode(ERRCODE_ARRAY_SUBSCRIPT_ERROR),\n\t\t\t\t\t\t errmsg(\"array must have even number of elements\")));\n\t\t\tbreak;\n\n\t\tcase 2:\n\t\t\tif ((ARR_DIMS(in_array)[1]) != 2)\n\t\t\t\tereport(ERROR,\n\t\t\t\t\t\t(errcode(ERRCODE_ARRAY_SUBSCRIPT_ERROR),\n\t\t\t\t\t\t errmsg(\"array must have two columns\")));\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_ARRAY_SUBSCRIPT_ERROR),\n\t\t\t\t\t errmsg(\"wrong number of array subscripts\")));\n\t}\n\n\tdeconstruct_array(in_array,\n\t\t\t\t\t  TEXTOID, -1, false, 'i',\n\t\t\t\t\t  &in_datums, &in_nulls, &in_count);\n\n\tcount = in_count / 2;\n\n\t/* see discussion in hstoreArrayToPairs() */\n\tif (count > MaxAllocSize / sizeof(Pairs))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t  errmsg(\"number of pairs (%d) exceeds the maximum allowed (%d)\",\n\t\t\t\t\t count, (int) (MaxAllocSize / sizeof(Pairs)))));\n\n\tpairs = palloc(count * sizeof(Pairs));\n\n\tfor (i = 0; i < count; ++i)\n\t{\n\t\tif (in_nulls[i * 2])\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_NULL_VALUE_NOT_ALLOWED),\n\t\t\t\t\t errmsg(\"null value not allowed for hstore key\")));\n\n\t\tif (in_nulls[i * 2 + 1])\n\t\t{\n\t\t\tpairs[i].key = VARDATA_ANY(in_datums[i * 2]);\n\t\t\tpairs[i].val = NULL;\n\t\t\tpairs[i].keylen = hstoreCheckKeyLen(VARSIZE_ANY_EXHDR(in_datums[i * 2]));\n\t\t\tpairs[i].vallen = 4;\n\t\t\tpairs[i].isnull = true;\n\t\t\tpairs[i].needfree = false;\n\t\t}\n\t\telse\n\t\t{\n\t\t\tpairs[i].key = VARDATA_ANY(in_datums[i * 2]);\n\t\t\tpairs[i].val = VARDATA_ANY(in_datums[i * 2 + 1]);\n\t\t\tpairs[i].keylen = hstoreCheckKeyLen(VARSIZE_ANY_EXHDR(in_datums[i * 2]));\n\t\t\tpairs[i].vallen = hstoreCheckValLen(VARSIZE_ANY_EXHDR(in_datums[i * 2 + 1]));\n\t\t\tpairs[i].isnull = false;\n\t\t\tpairs[i].needfree = false;\n\t\t}\n\t}\n\n\tcount = hstoreUniquePairs(pairs, count, &buflen);\n\n\tout = hstorePairs(pairs, count, buflen);\n\n\tPG_RETURN_POINTER(out);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 830,
        "critical_vars": [
            "ncolumns"
        ],
        "function": "hstore_from_record",
        "filename": "postgres/CVE-2014-2669/CVE-2014-2669_CWE-189_31400a673325147e1205326008e32135a78b4d8a_hstore_io.c.diff",
        "label": "True",
        "function_code": "Datum\t\thstore_from_record(PG_FUNCTION_ARGS);\nDatum\nhstore_from_record(PG_FUNCTION_ARGS)\n{\n\tHeapTupleHeader rec;\n\tint32\t\tbuflen;\n\tHStore\t   *out;\n\tPairs\t   *pairs;\n\tOid\t\t\ttupType;\n\tint32\t\ttupTypmod;\n\tTupleDesc\ttupdesc;\n\tHeapTupleData tuple;\n\tRecordIOData *my_extra;\n\tint\t\t\tncolumns;\n\tint\t\t\ti,\n\t\t\t\tj;\n\tDatum\t   *values;\n\tbool\t   *nulls;\n\n\tif (PG_ARGISNULL(0))\n\t{\n\t\tOid\t\t\targtype = get_fn_expr_argtype(fcinfo->flinfo, 0);\n\n\t\t/*\n\t\t * have no tuple to look at, so the only source of type info is the\n\t\t * argtype. The lookup_rowtype_tupdesc call below will error out if we\n\t\t * don't have a known composite type oid here.\n\t\t */\n\t\ttupType = argtype;\n\t\ttupTypmod = -1;\n\n\t\trec = NULL;\n\t}\n\telse\n\t{\n\t\trec = PG_GETARG_HEAPTUPLEHEADER(0);\n\n\t\t/* Extract type info from the tuple itself */\n\t\ttupType = HeapTupleHeaderGetTypeId(rec);\n\t\ttupTypmod = HeapTupleHeaderGetTypMod(rec);\n\t}\n\n\ttupdesc = lookup_rowtype_tupdesc(tupType, tupTypmod);\n\tncolumns = tupdesc->natts;\n\n\t/*\n\t * We arrange to look up the needed I/O info just once per series of\n\t * calls, assuming the record type doesn't change underneath us.\n\t */\n\tmy_extra = (RecordIOData *) fcinfo->flinfo->fn_extra;\n\tif (my_extra == NULL ||\n\t\tmy_extra->ncolumns != ncolumns)\n\t{\n\t\tfcinfo->flinfo->fn_extra =\n\t\t\tMemoryContextAlloc(fcinfo->flinfo->fn_mcxt,\n\t\t\t\t\t\t\t   sizeof(RecordIOData) - sizeof(ColumnIOData)\n\t\t\t\t\t\t\t   + ncolumns * sizeof(ColumnIOData));\n\t\tmy_extra = (RecordIOData *) fcinfo->flinfo->fn_extra;\n\t\tmy_extra->record_type = InvalidOid;\n\t\tmy_extra->record_typmod = 0;\n\t}\n\n\tif (my_extra->record_type != tupType ||\n\t\tmy_extra->record_typmod != tupTypmod)\n\t{\n\t\tMemSet(my_extra, 0,\n\t\t\t   sizeof(RecordIOData) - sizeof(ColumnIOData)\n\t\t\t   + ncolumns * sizeof(ColumnIOData));\n\t\tmy_extra->record_type = tupType;\n\t\tmy_extra->record_typmod = tupTypmod;\n\t\tmy_extra->ncolumns = ncolumns;\n\t}\n\n\tAssert(ncolumns <= MaxTupleAttributeNumber);\t\t/* thus, no overflow */\n\tpairs = palloc(ncolumns * sizeof(Pairs));\n\n\tif (rec)\n\t{\n\t\t/* Build a temporary HeapTuple control str\n... (function end not found)"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 350,
        "critical_vars": [
            "state.num",
            "state.sumlen"
        ],
        "function": "queryin",
        "filename": "postgres/CVE-2014-2669/CVE-2014-2669_CWE-189_31400a673325147e1205326008e32135a78b4d8a_ltxtquery_io.c.diff",
        "label": "True",
        "function_code": "static ltxtquery *\nqueryin(char *buf)\n{\n\tQPRS_STATE\tstate;\n\tint32\t\ti;\n\tltxtquery  *query;\n\tint32\t\tcommonlen;\n\tITEM\t   *ptr;\n\tNODE\t   *tmp;\n\tint32\t\tpos = 0;\n\n#ifdef BS_DEBUG\n\tchar\t\tpbuf[16384],\n\t\t\t   *cur;\n#endif\n\n\t/* init state */\n\tstate.buf = buf;\n\tstate.state = WAITOPERAND;\n\tstate.count = 0;\n\tstate.num = 0;\n\tstate.str = NULL;\n\n\t/* init list of operand */\n\tstate.sumlen = 0;\n\tstate.lenop = 64;\n\tstate.curop = state.op = (char *) palloc(state.lenop);\n\t*(state.curop) = '\\0';\n\n\t/* parse query & make polish notation (postfix, but in reverse order) */\n\tmakepol(&state);\n\tif (!state.num)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_SYNTAX_ERROR),\n\t\t\t\t errmsg(\"syntax error\"),\n\t\t\t\t errdetail(\"Empty query.\")));\n\n\tif (LTXTQUERY_TOO_BIG(state.num, state.sumlen))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t\t errmsg(\"ltxtquery is too large\")));\n\tcommonlen = COMPUTESIZE(state.num, state.sumlen);\n\n\tquery = (ltxtquery *) palloc(commonlen);\n\tSET_VARSIZE(query, commonlen);\n\tquery->size = state.num;\n\tptr = GETQUERY(query);\n\n\t/* set item in polish notation */\n\tfor (i = 0; i < state.num; i++)\n\t{\n\t\tptr[i].type = state.str->type;\n\t\tptr[i].val = state.str->val;\n\t\tptr[i].distance = state.str->distance;\n\t\tptr[i].length = state.str->length;\n\t\tptr[i].flag = state.str->flag;\n\t\ttmp = state.str->next;\n\t\tpfree(state.str);\n\t\tstate.str = tmp;\n\t}\n\n\t/* set user friendly-operand view */\n\tmemcpy((void *) GETOPERAND(query), (void *) state.op, state.sumlen);\n\tpfree(state.op);\n\n\t/* set left operand's position for every operator */\n\tpos = 0;\n\tfindoprnd(ptr, &pos);\n\n\treturn query;\n}\n\n/*\n * in without morphology\n */\nDatum\nltxtq_in(PG_FUNCTION_ARGS)\n{\n\tPG_RETURN_POINTER(queryin((char *) PG_GETARG_POINTER(0)));\n}\n\n/*\n * out function\n */\ntypedef struct\n{\n\tITEM\t   *curpol;\n\tchar\t   *buf;\n\tchar\t   *cur;\n\tchar\t   *op;\n\tint32\t\tbuflen;\n} INFIX;\n\n#define RESIZEBUF(inf,addsize) \\\nwhile( ( (inf)->cur - (inf)->buf ) + (addsize) + 1 >= (inf)->buflen ) \\\n{ \\\n\tint32 len = (inf)->cur - (inf)->buf; \\\n\t(inf)->bufl\n... (function end not found)"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 101,
        "critical_vars": [
            "key_count"
        ],
        "function": "hstoreArrayToPairs",
        "filename": "postgres/CVE-2014-2669/CVE-2014-2669_CWE-189_31400a673325147e1205326008e32135a78b4d8a_hstore_op.c.diff",
        "label": "True",
        "function_code": "\nPairs *\nhstoreArrayToPairs(ArrayType *a, int *npairs)\n{\n\tDatum\t   *key_datums;\n\tbool\t   *key_nulls;\n\tint\t\t\tkey_count;\n\tPairs\t   *key_pairs;\n\tint\t\t\tbufsiz;\n\tint\t\t\ti,\n\t\t\t\tj;\n\n\tdeconstruct_array(a,\n\t\t\t\t\t  TEXTOID, -1, false, 'i',\n\t\t\t\t\t  &key_datums, &key_nulls, &key_count);\n\n\tif (key_count == 0)\n\t{\n\t\t*npairs = 0;\n\t\treturn NULL;\n\t}\n\n\t/*\n\t * A text array uses at least eight bytes per element, so any overflow in\n\t * \"key_count * sizeof(Pairs)\" is small enough for palloc() to catch.\n\t * However, credible improvements to the array format could invalidate\n\t * that assumption.  Therefore, use an explicit check rather than relying\n\t * on palloc() to complain.\n\t */\n\tif (key_count > MaxAllocSize / sizeof(Pairs))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t  errmsg(\"number of pairs (%d) exceeds the maximum allowed (%d)\",\n\t\t\t\t\t key_count, (int) (MaxAllocSize / sizeof(Pairs)))));\n\n\tkey_pairs = palloc(sizeof(Pairs) * key_count);\n\n\tfor (i = 0, j = 0; i < key_count; i++)\n\t{\n\t\tif (!key_nulls[i])\n\t\t{\n\t\t\tkey_pairs[j].key = VARDATA(key_datums[i]);\n\t\t\tkey_pairs[j].keylen = VARSIZE(key_datums[i]) - VARHDRSZ;\n\t\t\tkey_pairs[j].val = NULL;\n\t\t\tkey_pairs[j].vallen = 0;\n\t\t\tkey_pairs[j].needfree = 0;\n\t\t\tkey_pairs[j].isnull = 1;\n\t\t\tj++;\n\t\t}\n\t}\n\n\t*npairs = hstoreUniquePairs(key_pairs, j, &bufsiz);\n\n\treturn key_pairs;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 102,
        "critical_vars": [
            "Pairs",
            "key_count"
        ],
        "function": "hstoreArrayToPairs",
        "filename": "postgres/CVE-2014-2669/CVE-2014-2669_CWE-189_31400a673325147e1205326008e32135a78b4d8a_hstore_op.c.diff",
        "label": "True",
        "function_code": "\nPairs *\nhstoreArrayToPairs(ArrayType *a, int *npairs)\n{\n\tDatum\t   *key_datums;\n\tbool\t   *key_nulls;\n\tint\t\t\tkey_count;\n\tPairs\t   *key_pairs;\n\tint\t\t\tbufsiz;\n\tint\t\t\ti,\n\t\t\t\tj;\n\n\tdeconstruct_array(a,\n\t\t\t\t\t  TEXTOID, -1, false, 'i',\n\t\t\t\t\t  &key_datums, &key_nulls, &key_count);\n\n\tif (key_count == 0)\n\t{\n\t\t*npairs = 0;\n\t\treturn NULL;\n\t}\n\n\t/*\n\t * A text array uses at least eight bytes per element, so any overflow in\n\t * \"key_count * sizeof(Pairs)\" is small enough for palloc() to catch.\n\t * However, credible improvements to the array format could invalidate\n\t * that assumption.  Therefore, use an explicit check rather than relying\n\t * on palloc() to complain.\n\t */\n\tif (key_count > MaxAllocSize / sizeof(Pairs))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t  errmsg(\"number of pairs (%d) exceeds the maximum allowed (%d)\",\n\t\t\t\t\t key_count, (int) (MaxAllocSize / sizeof(Pairs)))));\n\n\tkey_pairs = palloc(sizeof(Pairs) * key_count);\n\n\tfor (i = 0, j = 0; i < key_count; i++)\n\t{\n\t\tif (!key_nulls[i])\n\t\t{\n\t\t\tkey_pairs[j].key = VARDATA(key_datums[i]);\n\t\t\tkey_pairs[j].keylen = VARSIZE(key_datums[i]) - VARHDRSZ;\n\t\t\tkey_pairs[j].val = NULL;\n\t\t\tkey_pairs[j].vallen = 0;\n\t\t\tkey_pairs[j].needfree = 0;\n\t\t\tkey_pairs[j].isnull = 1;\n\t\t\tj++;\n\t\t}\n\t}\n\n\t*npairs = hstoreUniquePairs(key_pairs, j, &bufsiz);\n\n\treturn key_pairs;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 68,
        "critical_vars": [
            "num"
        ],
        "function": "ltree_in",
        "filename": "postgres/CVE-2014-2669/CVE-2014-2669_CWE-189_31400a673325147e1205326008e32135a78b4d8a_ltree_io.c.diff",
        "label": "True",
        "function_code": "\nDatum\nltree_in(PG_FUNCTION_ARGS)\n{\n\tchar\t   *buf = (char *) PG_GETARG_POINTER(0);\n\tchar\t   *ptr;\n\tnodeitem   *list,\n\t\t\t   *lptr;\n\tint\t\t\tnum = 0,\n\t\t\t\ttotallen = 0;\n\tint\t\t\tstate = LTPRS_WAITNAME;\n\tltree\t   *result;\n\tltree_level *curlevel;\n\tint\t\t\tcharlen;\n\tint\t\t\tpos = 0;\n\n\tptr = buf;\n\twhile (*ptr)\n\t{\n\t\tcharlen = pg_mblen(ptr);\n\t\tif (charlen == 1 && t_iseq(ptr, '.'))\n\t\t\tnum++;\n\t\tptr += charlen;\n\t}\n\n\tif (num + 1 > MaxAllocSize / sizeof(nodeitem))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t errmsg(\"number of levels (%d) exceeds the maximum allowed (%d)\",\n\t\t\t\t\tnum + 1, (int) (MaxAllocSize / sizeof(nodeitem)))));\n\tlist = lptr = (nodeitem *) palloc(sizeof(nodeitem) * (num + 1));\n\tptr = buf;\n\twhile (*ptr)\n\t{\n\t\tcharlen = pg_mblen(ptr);\n\n\t\tif (state == LTPRS_WAITNAME)\n\t\t{\n\t\t\tif (ISALNUM(ptr))\n\t\t\t{\n\t\t\t\tlptr->start = ptr;\n\t\t\t\tlptr->wlen = 0;\n\t\t\t\tstate = LTPRS_WAITDELIM;\n\t\t\t}\n\t\t\telse\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse if (state == LTPRS_WAITDELIM)\n\t\t{\n\t\t\tif (charlen == 1 && t_iseq(ptr, '.'))\n\t\t\t{\n\t\t\t\tlptr->len = ptr - lptr->start;\n\t\t\t\tif (lptr->wlen > 255)\n\t\t\t\t\tereport(ERROR,\n\t\t\t\t\t\t\t(errcode(ERRCODE_NAME_TOO_LONG),\n\t\t\t\t\t\t\t errmsg(\"name of level is too long\"),\n\t\t\t\t\t\t\t errdetail(\"Name length is %d, must \"\n\t\t\t\t\t\t\t\t\t   \"be < 256, in position %d.\",\n\t\t\t\t\t\t\t\t\t   lptr->wlen, pos)));\n\n\t\t\t\ttotallen += MAXALIGN(lptr->len + LEVEL_HDRSIZE);\n\t\t\t\tlptr++;\n\t\t\t\tstate = LTPRS_WAITNAME;\n\t\t\t}\n\t\t\telse if (!ISALNUM(ptr))\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse\n\t\t\t/* internal error */\n\t\t\telog(ERROR, \"internal error in parser\");\n\n\t\tptr += charlen;\n\t\tlptr->wlen++;\n\t\tpos++;\n\t}\n\n\tif (state == LTPRS_WAITDELIM)\n\t{\n\t\tlptr->len = ptr - lptr->start;\n\t\tif (lptr->wlen > 255)\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_NAME_TOO_LONG),\n\t\t\t\t\t errmsg(\"name of level is too long\"),\n\t\t\t\t\t errdetail(\"Name length is %d, must \"\n\t\t\t\t\t\t\t   \"be < 256, in position %d.\",\n\t\t\t\t\t\t\t   lptr->wlen, pos)));\n\n\t\ttotallen += MAXALIGN(lptr->len + LEVEL_HDRSIZE);\n\t\tlptr++;\n\t}\n\telse if (!(state == LTPRS_WAITNAME && lptr == list))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_SYNTAX_ERROR),\n\t\t\t\t errmsg(\"syntax error\"),\n\t\t\t\t errdetail(\"Unexpected end of line.\")));\n\n\tresult = (ltree *) palloc0(LTREE_HDRSIZE + totallen);\n\tSET_VARSIZE(result, LTREE_HDRSIZE + totallen);\n\tresult->numlevel = lptr - list;\n\tcurlevel = LTREE_FIRST(result);\n\tlptr = list;\n\twhile (lptr - list < result->numlevel)\n\t{\n\t\tcurlevel->len = (uint16) lptr->len;\n\t\tmemcpy(curlevel->name, lptr->start, lptr->len);\n\t\tcurlevel = LEVEL_NEXT(curlevel);\n\t\tlptr++;\n\t}\n\n\tpfree(list);\n\tPG_RETURN_POINTER(result);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 69,
        "critical_vars": [
            "num",
            "nodeitem"
        ],
        "function": "ltree_in",
        "filename": "postgres/CVE-2014-2669/CVE-2014-2669_CWE-189_31400a673325147e1205326008e32135a78b4d8a_ltree_io.c.diff",
        "label": "True",
        "function_code": "\nDatum\nltree_in(PG_FUNCTION_ARGS)\n{\n\tchar\t   *buf = (char *) PG_GETARG_POINTER(0);\n\tchar\t   *ptr;\n\tnodeitem   *list,\n\t\t\t   *lptr;\n\tint\t\t\tnum = 0,\n\t\t\t\ttotallen = 0;\n\tint\t\t\tstate = LTPRS_WAITNAME;\n\tltree\t   *result;\n\tltree_level *curlevel;\n\tint\t\t\tcharlen;\n\tint\t\t\tpos = 0;\n\n\tptr = buf;\n\twhile (*ptr)\n\t{\n\t\tcharlen = pg_mblen(ptr);\n\t\tif (charlen == 1 && t_iseq(ptr, '.'))\n\t\t\tnum++;\n\t\tptr += charlen;\n\t}\n\n\tif (num + 1 > MaxAllocSize / sizeof(nodeitem))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t errmsg(\"number of levels (%d) exceeds the maximum allowed (%d)\",\n\t\t\t\t\tnum + 1, (int) (MaxAllocSize / sizeof(nodeitem)))));\n\tlist = lptr = (nodeitem *) palloc(sizeof(nodeitem) * (num + 1));\n\tptr = buf;\n\twhile (*ptr)\n\t{\n\t\tcharlen = pg_mblen(ptr);\n\n\t\tif (state == LTPRS_WAITNAME)\n\t\t{\n\t\t\tif (ISALNUM(ptr))\n\t\t\t{\n\t\t\t\tlptr->start = ptr;\n\t\t\t\tlptr->wlen = 0;\n\t\t\t\tstate = LTPRS_WAITDELIM;\n\t\t\t}\n\t\t\telse\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse if (state == LTPRS_WAITDELIM)\n\t\t{\n\t\t\tif (charlen == 1 && t_iseq(ptr, '.'))\n\t\t\t{\n\t\t\t\tlptr->len = ptr - lptr->start;\n\t\t\t\tif (lptr->wlen > 255)\n\t\t\t\t\tereport(ERROR,\n\t\t\t\t\t\t\t(errcode(ERRCODE_NAME_TOO_LONG),\n\t\t\t\t\t\t\t errmsg(\"name of level is too long\"),\n\t\t\t\t\t\t\t errdetail(\"Name length is %d, must \"\n\t\t\t\t\t\t\t\t\t   \"be < 256, in position %d.\",\n\t\t\t\t\t\t\t\t\t   lptr->wlen, pos)));\n\n\t\t\t\ttotallen += MAXALIGN(lptr->len + LEVEL_HDRSIZE);\n\t\t\t\tlptr++;\n\t\t\t\tstate = LTPRS_WAITNAME;\n\t\t\t}\n\t\t\telse if (!ISALNUM(ptr))\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse\n\t\t\t/* internal error */\n\t\t\telog(ERROR, \"internal error in parser\");\n\n\t\tptr += charlen;\n\t\tlptr->wlen++;\n\t\tpos++;\n\t}\n\n\tif (state == LTPRS_WAITDELIM)\n\t{\n\t\tlptr->len = ptr - lptr->start;\n\t\tif (lptr->wlen > 255)\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_NAME_TOO_LONG),\n\t\t\t\t\t errmsg(\"name of level is too long\"),\n\t\t\t\t\t errdetail(\"Name length is %d, must \"\n\t\t\t\t\t\t\t   \"be < 256, in position %d.\",\n\t\t\t\t\t\t\t   lptr->wlen, pos)));\n\n\t\ttotallen += MAXALIGN(lptr->len + LEVEL_HDRSIZE);\n\t\tlptr++;\n\t}\n\telse if (!(state == LTPRS_WAITNAME && lptr == list))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_SYNTAX_ERROR),\n\t\t\t\t errmsg(\"syntax error\"),\n\t\t\t\t errdetail(\"Unexpected end of line.\")));\n\n\tresult = (ltree *) palloc0(LTREE_HDRSIZE + totallen);\n\tSET_VARSIZE(result, LTREE_HDRSIZE + totallen);\n\tresult->numlevel = lptr - list;\n\tcurlevel = LTREE_FIRST(result);\n\tlptr = list;\n\twhile (lptr - list < result->numlevel)\n\t{\n\t\tcurlevel->len = (uint16) lptr->len;\n\t\tmemcpy(curlevel->name, lptr->start, lptr->len);\n\t\tcurlevel = LEVEL_NEXT(curlevel);\n\t\tlptr++;\n\t}\n\n\tpfree(list);\n\tPG_RETURN_POINTER(result);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 237,
        "critical_vars": [
            "num"
        ],
        "function": "lquery_in",
        "filename": "postgres/CVE-2014-2669/CVE-2014-2669_CWE-189_31400a673325147e1205326008e32135a78b4d8a_ltree_io.c.diff",
        "label": "True",
        "function_code": "\nDatum\nlquery_in(PG_FUNCTION_ARGS)\n{\n\tchar\t   *buf = (char *) PG_GETARG_POINTER(0);\n\tchar\t   *ptr;\n\tint\t\t\tnum = 0,\n\t\t\t\ttotallen = 0,\n\t\t\t\tnumOR = 0;\n\tint\t\t\tstate = LQPRS_WAITLEVEL;\n\tlquery\t   *result;\n\tnodeitem   *lptr = NULL;\n\tlquery_level *cur,\n\t\t\t   *curqlevel,\n\t\t\t   *tmpql;\n\tlquery_variant *lrptr = NULL;\n\tbool\t\thasnot = false;\n\tbool\t\twasbad = false;\n\tint\t\t\tcharlen;\n\tint\t\t\tpos = 0;\n\n\tptr = buf;\n\twhile (*ptr)\n\t{\n\t\tcharlen = pg_mblen(ptr);\n\n\t\tif (charlen == 1)\n\t\t{\n\t\t\tif (t_iseq(ptr, '.'))\n\t\t\t\tnum++;\n\t\t\telse if (t_iseq(ptr, '|'))\n\t\t\t\tnumOR++;\n\t\t}\n\n\t\tptr += charlen;\n\t}\n\n\tnum++;\n\tif (num > MaxAllocSize / ITEMSIZE)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t errmsg(\"number of levels (%d) exceeds the maximum allowed (%d)\",\n\t\t\t\t\tnum, (int) (MaxAllocSize / ITEMSIZE))));\n\tcurqlevel = tmpql = (lquery_level *) palloc0(ITEMSIZE * num);\n\tptr = buf;\n\twhile (*ptr)\n\t{\n\t\tcharlen = pg_mblen(ptr);\n\n\t\tif (state == LQPRS_WAITLEVEL)\n\t\t{\n\t\t\tif (ISALNUM(ptr))\n\t\t\t{\n\t\t\t\tGETVAR(curqlevel) = lptr = (nodeitem *) palloc0(sizeof(nodeitem) * (numOR + 1));\n\t\t\t\tlptr->start = ptr;\n\t\t\t\tstate = LQPRS_WAITDELIM;\n\t\t\t\tcurqlevel->numvar = 1;\n\t\t\t}\n\t\t\telse if (charlen == 1 && t_iseq(ptr, '!'))\n\t\t\t{\n\t\t\t\tGETVAR(curqlevel) = lptr = (nodeitem *) palloc0(sizeof(nodeitem) * (numOR + 1));\n\t\t\t\tlptr->start = ptr + 1;\n\t\t\t\tstate = LQPRS_WAITDELIM;\n\t\t\t\tcurqlevel->numvar = 1;\n\t\t\t\tcurqlevel->flag |= LQL_NOT;\n\t\t\t\thasnot = true;\n\t\t\t}\n\t\t\telse if (charlen == 1 && t_iseq(ptr, '*'))\n\t\t\t\tstate = LQPRS_WAITOPEN;\n\t\t\telse\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse if (state == LQPRS_WAITVAR)\n\t\t{\n\t\t\tif (ISALNUM(ptr))\n\t\t\t{\n\t\t\t\tlptr++;\n\t\t\t\tlptr->start = ptr;\n\t\t\t\tstate = LQPRS_WAITDELIM;\n\t\t\t\tcurqlevel->numvar++;\n\t\t\t}\n\t\t\telse\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse if (state == LQPRS_WAITDELIM)\n\t\t{\n\t\t\tif (charlen == 1 && t_iseq(ptr, '@'))\n\t\t\t{\n\t\t\t\tif (lptr->start == ptr)\n\t\t\t\t\tUNCHAR;\n\t\t\t\tlptr->flag |= LVAR_INCASE;\n\t\t\t\tcurqlevel->flag |= LVAR_INCASE;\n\t\t\t}\n\t\t\telse if (charlen == 1 && t_iseq(ptr, '*'))\n\t\t\t{\n\t\t\t\tif (lptr->start == ptr)\n\t\t\t\t\tUNCHAR;\n\t\t\t\tlptr->flag |= LVAR_ANYEND;\n\t\t\t\tcurqlevel->flag |= LVAR_ANYEND;\n\t\t\t}\n\t\t\telse if (charlen == 1 && t_iseq(ptr, '%'))\n\t\t\t{\n\t\t\t\tif (lptr->start == ptr)\n\t\t\t\t\tUNCHAR;\n\t\t\t\tlptr->flag |= LVAR_SUBLEXEME;\n\t\t\t\tcurqlevel->flag |= LVAR_SUBLEXEME;\n\t\t\t}\n\t\t\telse if (charlen == 1 && t_iseq(ptr, '|'))\n\t\t\t{\n\t\t\t\tlptr->len = ptr - lptr->start -\n\t\t\t\t\t((lptr->flag & LVAR_SUBLEXEME) ? 1 : 0) -\n\t\t\t\t\t((lptr->flag & LVAR_INCASE) ? 1 : 0) -\n\t\t\t\t\t((lptr->flag & LVAR_ANYEND) ? 1 : 0);\n\t\t\t\tif (lptr->wlen > 255)\n\t\t\t\t\tereport(ERROR,\n\t\t\t\t\t\t\t(errcode(ERRCODE_NAME_TOO_LONG),\n\t\t\t\t\t\t\t errmsg(\"name of level is too long\"),\n\t\t\t\t\t\t\t errdetail(\"Name length is %d, must \"\n\t\t\t\t\t\t\t\t\t   \"be < 256, in position %d.\",\n\t\t\t\t\t\t\t\t\t   lptr->wlen, pos)));\n\n\t\t\t\tstate = LQPRS_WAITVAR;\n\t\t\t}\n\t\t\telse if (charlen == 1 && t_iseq(ptr, '.'))\n\t\t\t{\n\t\t\t\tlptr->len = ptr - lptr->start -\n\t\t\t\t\t((lptr->flag & LVAR_SUBLEXEME) ? 1 : 0) -\n\t\t\t\t\t((lptr->flag & LVAR_INCASE) ? 1 : 0) -\n\t\t\t\t\t((lptr->flag & LVAR_ANYEND) ? 1 : 0);\n\t\t\t\tif (lptr->wlen > 255)\n\t\t\t\t\tereport(ERROR,\n\t\t\t\t\t\t\t(errcode(ERRCODE_NAME_TOO_LONG),\n\t\t\t\t\t\t\t errmsg(\"name of level is too long\"),\n\t\t\t\t\t\t\t errdetail(\"Name length is %d, must \"\n\t\t\t\t\t\t\t\t\t   \"be < 256, in position %d.\",\n\t\t\t\t\t\t\t\t\t   lptr->wlen, pos)));\n\n\t\t\t\tstate = LQPRS_WAITLEVEL;\n\t\t\t\tcurqlevel = NEXTLEV(curqlevel);\n\t\t\t}\n\t\t\telse if (ISALNUM(ptr))\n\t\t\t{\n\t\t\t\tif (lptr->flag)\n\t\t\t\t\tUNCHAR;\n\t\t\t}\n\t\t\telse\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse if (state == LQPRS_WAITOPEN)\n\t\t{\n\t\t\tif (charlen == 1 && t_iseq(ptr, '{'))\n\t\t\t\tstate = LQPRS_WAITFNUM;\n\t\t\telse if (charlen == 1 && t_iseq(ptr, '.'))\n\t\t\t{\n\t\t\t\tcurqlevel->low = 0;\n\t\t\t\tcurqlevel->high = 0xffff;\n\t\t\t\tcurqlevel = NEXTLEV(curqlevel);\n\t\t\t\tstate = LQPRS_WAITLEVEL;\n\t\t\t}\n\t\t\telse\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse if (state == LQPRS_WAITFNUM)\n\t\t{\n\t\t\tif (charlen == 1 && t_iseq(ptr, ','))\n\t\t\t\tstate = LQPRS_WAITSNUM;\n\t\t\telse if (t_isdigit(ptr))\n\t\t\t{\n\t\t\t\tcurqlevel->low = atoi(ptr);\n\t\t\t\tstate = LQPRS_WAITND;\n\t\t\t}\n\t\t\telse\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse if (state == LQPRS_WAITSNUM)\n\t\t{\n\t\t\tif (t_isdigit(ptr))\n\t\t\t{\n\t\t\t\tcurqlevel->high = atoi(ptr);\n\t\t\t\tstate = LQPRS_WAITCLOSE;\n\t\t\t}\n\t\t\telse if (charlen == 1 && t_iseq(ptr, '}'))\n\t\t\t{\n\t\t\t\tcurqlevel->high = 0xffff;\n\t\t\t\tstate = LQPRS_WAITEND;\n\t\t\t}\n\t\t\telse\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse if (state == LQPRS_WAITCLOSE)\n\t\t{\n\t\t\tif (charlen == 1 && t_iseq(ptr, '}'))\n\t\t\t\tstate = LQPRS_WAITEND;\n\t\t\telse if (!t_isdigit(ptr))\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse if (state == LQPRS_WAITND)\n\t\t{\n\t\t\tif (charlen == 1 && t_iseq(ptr, '}'))\n\t\t\t{\n\t\t\t\tcurqlevel->high = curqlevel->low;\n\t\t\t\tstate = LQPRS_WAITEND;\n\t\t\t}\n\t\t\telse if (charlen == 1 && t_iseq(ptr, ','))\n\t\t\t\tstate = LQPRS_WAITSNUM;\n\t\t\telse if (!t_isdigit(ptr))\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse if (state == LQPRS_WAITEND)\n\t\t{\n\t\t\tif (charlen == 1 && t_iseq(ptr, '.'))\n\t\t\t{\n\t\t\t\tstate = LQPRS_WAITLEVEL;\n\t\t\t\tcurqlevel = NEXTLEV(curqlevel);\n\t\t\t}\n\t\t\telse\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse\n\t\t\t/* internal error */\n\t\t\telog(ERROR, \"internal error in parser\");\n\n\t\tptr += charlen;\n\t\tif (state == LQPRS_WAITDELIM)\n\t\t\tlptr->wlen++;\n\t\tpos++;\n\t}\n\n\tif (state == LQPRS_WAITDELIM)\n\t{\n\t\tif (lptr->start == ptr)\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_SYNTAX_ERROR),\n\t\t\t\t\t errmsg(\"syntax error\"),\n\t\t\t\t\t errdetail(\"Unexpected end of line.\")));\n\n\t\tlptr->len = ptr - lptr->start -\n\t\t\t((lptr->flag & LVAR_SUBLEXEME) ? 1 : 0) -\n\t\t\t((lptr->flag & LVAR_INCASE) ? 1 : 0) -\n\t\t\t((lptr->flag & LVAR_ANYEND) ? 1 : 0);\n\t\tif (lptr->len == 0)\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_SYNTAX_ERROR),\n\t\t\t\t\t errmsg(\"syntax error\"),\n\t\t\t\t\t errdetail(\"Unexpected end of line.\")));\n\n\t\tif (lptr->wlen > 255)\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_NAME_TOO_LONG),\n\t\t\t\t\t errmsg(\"name of level is too long\"),\n\t\t\t\t\t errdetail(\"Name length is %d, must \"\n\t\t\t\t\t\t\t   \"be < 256, in position %d.\",\n\t\t\t\t\t\t\t   lptr->wlen, pos)));\n\t}\n\telse if (state == LQPRS_WAITOPEN)\n\t\tcurqlevel->high = 0xffff;\n\telse if (state != LQPRS_WAITEND)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_SYNTAX_ERROR),\n\t\t\t\t errmsg(\"syntax error\"),\n\t\t\t\t errdetail(\"Unexpected end of line.\")));\n\n\tcurqlevel = tmpql;\n\ttotallen = LQUERY_HDRSIZE;\n\twhile ((char *) curqlevel - (char *) tmpql < num * ITEMSIZE)\n\t{\n\t\ttotallen += LQL_HDRSIZE;\n\t\tif (curqlevel->numvar)\n\t\t{\n\t\t\tlptr = GETVAR(curqlevel);\n\t\t\twhile (lptr - GETVAR(curqlevel) < curqlevel->numvar)\n\t\t\t{\n\t\t\t\ttotallen += MAXALIGN(LVAR_HDRSIZE + lptr->len);\n\t\t\t\tlptr++;\n\t\t\t}\n\t\t}\n\t\telse if (curqlevel->low > curqlevel->high)\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_SYNTAX_ERROR),\n\t\t\t\t\t errmsg(\"syntax error\"),\n\t\t\t\t\t errdetail(\"Low limit(%d) is greater than upper(%d).\",\n\t\t\t\t\t\t\t   curqlevel->low, curqlevel->high)));\n\n\t\tcurqlevel = NEXTLEV(curqlevel);\n\t}\n\n\tresult = (lquery *) palloc0(totallen);\n\tSET_VARSIZE(result, totallen);\n\tresult->numlevel = num;\n\tresult->firstgood = 0;\n\tresult->flag = 0;\n\tif (hasnot)\n\t\tresult->flag |= LQUERY_HASNOT;\n\tcur = LQUERY_FIRST(result);\n\tcurqlevel = tmpql;\n\twhile ((char *) curqlevel - (char *) tmpql < num * ITEMSIZE)\n\t{\n\t\tmemcpy(cur, curqlevel, LQL_HDRSIZE);\n\t\tcur->totallen = LQL_HDRSIZE;\n\t\tif (curqlevel->numvar)\n\t\t{\n\t\t\tlrptr = LQL_FIRST(cur);\n\t\t\tlptr = GETVAR(curqlevel);\n\t\t\twhile (lptr - GETVAR(curqlevel) < curqlevel->numvar)\n\t\t\t{\n\t\t\t\tcur->totallen += MAXALIGN(LVAR_HDRSIZE + lptr->len);\n\t\t\t\tlrptr->len = lptr->len;\n\t\t\t\tlrptr->flag = lptr->flag;\n\t\t\t\tlrptr->val = ltree_crc32_sz(lptr->start, lptr->len);\n\t\t\t\tmemcpy(lrptr->name, lptr->start, lptr->len);\n\t\t\t\tlptr++;\n\t\t\t\tlrptr = LVAR_NEXT(lrptr);\n\t\t\t}\n\t\t\tpfree(GETVAR(curqlevel));\n\t\t\tif (cur->numvar > 1 || cur->flag != 0)\n\t\t\t\twasbad = true;\n\t\t\telse if (wasbad == false)\n\t\t\t\t(result->firstgood)++;\n\t\t}\n\t\telse\n\t\t\twasbad = true;\n\t\tcurqlevel = NEXTLEV(curqlevel);\n\t\tcur = LQL_NEXT(cur);\n\t}\n\n\tpfree(tmpql);\n\tPG_RETURN_POINTER(result);\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 238,
        "critical_vars": [
            "num",
            "MaxAllocSize"
        ],
        "function": "lquery_in",
        "filename": "postgres/CVE-2014-2669/CVE-2014-2669_CWE-189_31400a673325147e1205326008e32135a78b4d8a_ltree_io.c.diff",
        "label": "True",
        "function_code": "\nDatum\nlquery_in(PG_FUNCTION_ARGS)\n{\n\tchar\t   *buf = (char *) PG_GETARG_POINTER(0);\n\tchar\t   *ptr;\n\tint\t\t\tnum = 0,\n\t\t\t\ttotallen = 0,\n\t\t\t\tnumOR = 0;\n\tint\t\t\tstate = LQPRS_WAITLEVEL;\n\tlquery\t   *result;\n\tnodeitem   *lptr = NULL;\n\tlquery_level *cur,\n\t\t\t   *curqlevel,\n\t\t\t   *tmpql;\n\tlquery_variant *lrptr = NULL;\n\tbool\t\thasnot = false;\n\tbool\t\twasbad = false;\n\tint\t\t\tcharlen;\n\tint\t\t\tpos = 0;\n\n\tptr = buf;\n\twhile (*ptr)\n\t{\n\t\tcharlen = pg_mblen(ptr);\n\n\t\tif (charlen == 1)\n\t\t{\n\t\t\tif (t_iseq(ptr, '.'))\n\t\t\t\tnum++;\n\t\t\telse if (t_iseq(ptr, '|'))\n\t\t\t\tnumOR++;\n\t\t}\n\n\t\tptr += charlen;\n\t}\n\n\tnum++;\n\tif (num > MaxAllocSize / ITEMSIZE)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),\n\t\t\t errmsg(\"number of levels (%d) exceeds the maximum allowed (%d)\",\n\t\t\t\t\tnum, (int) (MaxAllocSize / ITEMSIZE))));\n\tcurqlevel = tmpql = (lquery_level *) palloc0(ITEMSIZE * num);\n\tptr = buf;\n\twhile (*ptr)\n\t{\n\t\tcharlen = pg_mblen(ptr);\n\n\t\tif (state == LQPRS_WAITLEVEL)\n\t\t{\n\t\t\tif (ISALNUM(ptr))\n\t\t\t{\n\t\t\t\tGETVAR(curqlevel) = lptr = (nodeitem *) palloc0(sizeof(nodeitem) * (numOR + 1));\n\t\t\t\tlptr->start = ptr;\n\t\t\t\tstate = LQPRS_WAITDELIM;\n\t\t\t\tcurqlevel->numvar = 1;\n\t\t\t}\n\t\t\telse if (charlen == 1 && t_iseq(ptr, '!'))\n\t\t\t{\n\t\t\t\tGETVAR(curqlevel) = lptr = (nodeitem *) palloc0(sizeof(nodeitem) * (numOR + 1));\n\t\t\t\tlptr->start = ptr + 1;\n\t\t\t\tstate = LQPRS_WAITDELIM;\n\t\t\t\tcurqlevel->numvar = 1;\n\t\t\t\tcurqlevel->flag |= LQL_NOT;\n\t\t\t\thasnot = true;\n\t\t\t}\n\t\t\telse if (charlen == 1 && t_iseq(ptr, '*'))\n\t\t\t\tstate = LQPRS_WAITOPEN;\n\t\t\telse\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse if (state == LQPRS_WAITVAR)\n\t\t{\n\t\t\tif (ISALNUM(ptr))\n\t\t\t{\n\t\t\t\tlptr++;\n\t\t\t\tlptr->start = ptr;\n\t\t\t\tstate = LQPRS_WAITDELIM;\n\t\t\t\tcurqlevel->numvar++;\n\t\t\t}\n\t\t\telse\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse if (state == LQPRS_WAITDELIM)\n\t\t{\n\t\t\tif (charlen == 1 && t_iseq(ptr, '@'))\n\t\t\t{\n\t\t\t\tif (lptr->start == ptr)\n\t\t\t\t\tUNCHAR;\n\t\t\t\tlptr->flag |= LVAR_INCASE;\n\t\t\t\tcurqlevel->flag |= LVAR_INCASE;\n\t\t\t}\n\t\t\telse if (charlen == 1 && t_iseq(ptr, '*'))\n\t\t\t{\n\t\t\t\tif (lptr->start == ptr)\n\t\t\t\t\tUNCHAR;\n\t\t\t\tlptr->flag |= LVAR_ANYEND;\n\t\t\t\tcurqlevel->flag |= LVAR_ANYEND;\n\t\t\t}\n\t\t\telse if (charlen == 1 && t_iseq(ptr, '%'))\n\t\t\t{\n\t\t\t\tif (lptr->start == ptr)\n\t\t\t\t\tUNCHAR;\n\t\t\t\tlptr->flag |= LVAR_SUBLEXEME;\n\t\t\t\tcurqlevel->flag |= LVAR_SUBLEXEME;\n\t\t\t}\n\t\t\telse if (charlen == 1 && t_iseq(ptr, '|'))\n\t\t\t{\n\t\t\t\tlptr->len = ptr - lptr->start -\n\t\t\t\t\t((lptr->flag & LVAR_SUBLEXEME) ? 1 : 0) -\n\t\t\t\t\t((lptr->flag & LVAR_INCASE) ? 1 : 0) -\n\t\t\t\t\t((lptr->flag & LVAR_ANYEND) ? 1 : 0);\n\t\t\t\tif (lptr->wlen > 255)\n\t\t\t\t\tereport(ERROR,\n\t\t\t\t\t\t\t(errcode(ERRCODE_NAME_TOO_LONG),\n\t\t\t\t\t\t\t errmsg(\"name of level is too long\"),\n\t\t\t\t\t\t\t errdetail(\"Name length is %d, must \"\n\t\t\t\t\t\t\t\t\t   \"be < 256, in position %d.\",\n\t\t\t\t\t\t\t\t\t   lptr->wlen, pos)));\n\n\t\t\t\tstate = LQPRS_WAITVAR;\n\t\t\t}\n\t\t\telse if (charlen == 1 && t_iseq(ptr, '.'))\n\t\t\t{\n\t\t\t\tlptr->len = ptr - lptr->start -\n\t\t\t\t\t((lptr->flag & LVAR_SUBLEXEME) ? 1 : 0) -\n\t\t\t\t\t((lptr->flag & LVAR_INCASE) ? 1 : 0) -\n\t\t\t\t\t((lptr->flag & LVAR_ANYEND) ? 1 : 0);\n\t\t\t\tif (lptr->wlen > 255)\n\t\t\t\t\tereport(ERROR,\n\t\t\t\t\t\t\t(errcode(ERRCODE_NAME_TOO_LONG),\n\t\t\t\t\t\t\t errmsg(\"name of level is too long\"),\n\t\t\t\t\t\t\t errdetail(\"Name length is %d, must \"\n\t\t\t\t\t\t\t\t\t   \"be < 256, in position %d.\",\n\t\t\t\t\t\t\t\t\t   lptr->wlen, pos)));\n\n\t\t\t\tstate = LQPRS_WAITLEVEL;\n\t\t\t\tcurqlevel = NEXTLEV(curqlevel);\n\t\t\t}\n\t\t\telse if (ISALNUM(ptr))\n\t\t\t{\n\t\t\t\tif (lptr->flag)\n\t\t\t\t\tUNCHAR;\n\t\t\t}\n\t\t\telse\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse if (state == LQPRS_WAITOPEN)\n\t\t{\n\t\t\tif (charlen == 1 && t_iseq(ptr, '{'))\n\t\t\t\tstate = LQPRS_WAITFNUM;\n\t\t\telse if (charlen == 1 && t_iseq(ptr, '.'))\n\t\t\t{\n\t\t\t\tcurqlevel->low = 0;\n\t\t\t\tcurqlevel->high = 0xffff;\n\t\t\t\tcurqlevel = NEXTLEV(curqlevel);\n\t\t\t\tstate = LQPRS_WAITLEVEL;\n\t\t\t}\n\t\t\telse\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse if (state == LQPRS_WAITFNUM)\n\t\t{\n\t\t\tif (charlen == 1 && t_iseq(ptr, ','))\n\t\t\t\tstate = LQPRS_WAITSNUM;\n\t\t\telse if (t_isdigit(ptr))\n\t\t\t{\n\t\t\t\tcurqlevel->low = atoi(ptr);\n\t\t\t\tstate = LQPRS_WAITND;\n\t\t\t}\n\t\t\telse\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse if (state == LQPRS_WAITSNUM)\n\t\t{\n\t\t\tif (t_isdigit(ptr))\n\t\t\t{\n\t\t\t\tcurqlevel->high = atoi(ptr);\n\t\t\t\tstate = LQPRS_WAITCLOSE;\n\t\t\t}\n\t\t\telse if (charlen == 1 && t_iseq(ptr, '}'))\n\t\t\t{\n\t\t\t\tcurqlevel->high = 0xffff;\n\t\t\t\tstate = LQPRS_WAITEND;\n\t\t\t}\n\t\t\telse\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse if (state == LQPRS_WAITCLOSE)\n\t\t{\n\t\t\tif (charlen == 1 && t_iseq(ptr, '}'))\n\t\t\t\tstate = LQPRS_WAITEND;\n\t\t\telse if (!t_isdigit(ptr))\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse if (state == LQPRS_WAITND)\n\t\t{\n\t\t\tif (charlen == 1 && t_iseq(ptr, '}'))\n\t\t\t{\n\t\t\t\tcurqlevel->high = curqlevel->low;\n\t\t\t\tstate = LQPRS_WAITEND;\n\t\t\t}\n\t\t\telse if (charlen == 1 && t_iseq(ptr, ','))\n\t\t\t\tstate = LQPRS_WAITSNUM;\n\t\t\telse if (!t_isdigit(ptr))\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse if (state == LQPRS_WAITEND)\n\t\t{\n\t\t\tif (charlen == 1 && t_iseq(ptr, '.'))\n\t\t\t{\n\t\t\t\tstate = LQPRS_WAITLEVEL;\n\t\t\t\tcurqlevel = NEXTLEV(curqlevel);\n\t\t\t}\n\t\t\telse\n\t\t\t\tUNCHAR;\n\t\t}\n\t\telse\n\t\t\t/* internal error */\n\t\t\telog(ERROR, \"internal error in parser\");\n\n\t\tptr += charlen;\n\t\tif (state == LQPRS_WAITDELIM)\n\t\t\tlptr->wlen++;\n\t\tpos++;\n\t}\n\n\tif (state == LQPRS_WAITDELIM)\n\t{\n\t\tif (lptr->start == ptr)\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_SYNTAX_ERROR),\n\t\t\t\t\t errmsg(\"syntax error\"),\n\t\t\t\t\t errdetail(\"Unexpected end of line.\")));\n\n\t\tlptr->len = ptr - lptr->start -\n\t\t\t((lptr->flag & LVAR_SUBLEXEME) ? 1 : 0) -\n\t\t\t((lptr->flag & LVAR_INCASE) ? 1 : 0) -\n\t\t\t((lptr->flag & LVAR_ANYEND) ? 1 : 0);\n\t\tif (lptr->len == 0)\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_SYNTAX_ERROR),\n\t\t\t\t\t errmsg(\"syntax error\"),\n\t\t\t\t\t errdetail(\"Unexpected end of line.\")));\n\n\t\tif (lptr->wlen > 255)\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_NAME_TOO_LONG),\n\t\t\t\t\t errmsg(\"name of level is too long\"),\n\t\t\t\t\t errdetail(\"Name length is %d, must \"\n\t\t\t\t\t\t\t   \"be < 256, in position %d.\",\n\t\t\t\t\t\t\t   lptr->wlen, pos)));\n\t}\n\telse if (state == LQPRS_WAITOPEN)\n\t\tcurqlevel->high = 0xffff;\n\telse if (state != LQPRS_WAITEND)\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_SYNTAX_ERROR),\n\t\t\t\t errmsg(\"syntax error\"),\n\t\t\t\t errdetail(\"Unexpected end of line.\")));\n\n\tcurqlevel = tmpql;\n\ttotallen = LQUERY_HDRSIZE;\n\twhile ((char *) curqlevel - (char *) tmpql < num * ITEMSIZE)\n\t{\n\t\ttotallen += LQL_HDRSIZE;\n\t\tif (curqlevel->numvar)\n\t\t{\n\t\t\tlptr = GETVAR(curqlevel);\n\t\t\twhile (lptr - GETVAR(curqlevel) < curqlevel->numvar)\n\t\t\t{\n\t\t\t\ttotallen += MAXALIGN(LVAR_HDRSIZE + lptr->len);\n\t\t\t\tlptr++;\n\t\t\t}\n\t\t}\n\t\telse if (curqlevel->low > curqlevel->high)\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_SYNTAX_ERROR),\n\t\t\t\t\t errmsg(\"syntax error\"),\n\t\t\t\t\t errdetail(\"Low limit(%d) is greater than upper(%d).\",\n\t\t\t\t\t\t\t   curqlevel->low, curqlevel->high)));\n\n\t\tcurqlevel = NEXTLEV(curqlevel);\n\t}\n\n\tresult = (lquery *) palloc0(totallen);\n\tSET_VARSIZE(result, totallen);\n\tresult->numlevel = num;\n\tresult->firstgood = 0;\n\tresult->flag = 0;\n\tif (hasnot)\n\t\tresult->flag |= LQUERY_HASNOT;\n\tcur = LQUERY_FIRST(result);\n\tcurqlevel = tmpql;\n\twhile ((char *) curqlevel - (char *) tmpql < num * ITEMSIZE)\n\t{\n\t\tmemcpy(cur, curqlevel, LQL_HDRSIZE);\n\t\tcur->totallen = LQL_HDRSIZE;\n\t\tif (curqlevel->numvar)\n\t\t{\n\t\t\tlrptr = LQL_FIRST(cur);\n\t\t\tlptr = GETVAR(curqlevel);\n\t\t\twhile (lptr - GETVAR(curqlevel) < curqlevel->numvar)\n\t\t\t{\n\t\t\t\tcur->totallen += MAXALIGN(LVAR_HDRSIZE + lptr->len);\n\t\t\t\tlrptr->len = lptr->len;\n\t\t\t\tlrptr->flag = lptr->flag;\n\t\t\t\tlrptr->val = ltree_crc32_sz(lptr->start, lptr->len);\n\t\t\t\tmemcpy(lrptr->name, lptr->start, lptr->len);\n\t\t\t\tlptr++;\n\t\t\t\tlrptr = LVAR_NEXT(lrptr);\n\t\t\t}\n\t\t\tpfree(GETVAR(curqlevel));\n\t\t\tif (cur->numvar > 1 || cur->flag != 0)\n\t\t\t\twasbad = true;\n\t\t\telse if (wasbad == false)\n\t\t\t\t(result->firstgood)++;\n\t\t}\n\t\telse\n\t\t\twasbad = true;\n\t\tcurqlevel = NEXTLEV(curqlevel);\n\t\tcur = LQL_NEXT(cur);\n\t}\n\n\tpfree(tmpql);\n\tPG_RETURN_POINTER(result);\n}"
    },
    {
        "patch_model": "Replace",
        "change_type": "Fun-Call",
        "line_old": 99,
        "line_new": 99,
        "critical_vars": [
            "buffer",
            "hasread",
            "putsize",
            "&header"
        ],
        "function": "fcgid_header_bucket_read",
        "filename": "httpd-mod_fcgid/CVE-2010-3872/CVE-2010-3872_CWE-189_b1afa70840b4ab4e6fbc12ac8798b2f3ccc336b2_fcgid_bucket.c.diff",
        "label": "True",
        "function_code": "\nstatic apr_status_t fcgid_header_bucket_read(apr_bucket * b,\n                                             const char **str,\n                                             apr_size_t * len,\n                                             apr_read_type_e block)\n{\n    fcgid_bucket_ctx *ctx = (fcgid_bucket_ctx *) b->data;\n    apr_status_t rv;\n    apr_size_t hasread, bodysize;\n    FCGI_Header header;\n    apr_bucket *curbucket = b;\n\n    /* Keep reading until I get a fastcgi header */\n    hasread = 0;\n    while (hasread < sizeof(header)) {\n        char *buffer;\n        apr_size_t bufferlen, putsize;\n\n        /* Feed some data if necessary */\n        if ((rv =\n             fcgid_feed_data(ctx, b->list, &buffer,\n                             &bufferlen)) != APR_SUCCESS)\n            return rv;\n\n        /* Initialize header */\n        putsize = fcgid_min(bufferlen, sizeof(header) - hasread);\n        memcpy((apr_byte_t *)&header + hasread, buffer, putsize);\n        hasread += putsize;\n\n        /* Ignore the bytes that have read */\n        fcgid_ignore_bytes(ctx, putsize);\n    }\n\n    /* Get the body size */\n    bodysize = header.contentLengthB1;\n    bodysize <<= 8;\n    bodysize += header.contentLengthB0;\n\n    /* Handle FCGI_STDERR body, write the content to log file */\n    if (header.type == FCGI_STDERR) {\n        char *logbuf = apr_bucket_alloc(APR_BUCKET_BUFF_SIZE, b->list);\n        char *line;\n\n        memset(logbuf, 0, APR_BUCKET_BUFF_SIZE);\n\n        hasread = 0;\n        while (hasread < bodysize) {\n            char *buffer;\n            apr_size_t bufferlen, canput, willput;\n\n            /* Feed some data if necessary */\n            if ((rv =\n                 fcgid_feed_data(ctx, b->list, &buffer,\n                                 &bufferlen)) != APR_SUCCESS) {\n                apr_bucket_free(logbuf);\n                return rv;\n            }\n\n            canput = fcgid_min(bufferlen, bodysize - hasread);\n            willput =\n                fcgid_min(canput, APR_BUCKET_BUFF_SIZE \n... (function end not found)"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 629,
        "critical_vars": [
            "out_len"
        ],
        "function": "string_chunk_split",
        "filename": "hhvm/CVE-2014-6228/CVE-2014-6228_CWE-189_1f91e076a585118495b976a413c1df40f6fd3d41_zend-string.cpp.diff",
        "label": "False",
        "function_code": "\nString string_chunk_split(const char *src, int srclen, const char *end,\n                          int endlen, int chunklen) {\n  int chunks = srclen / chunklen; // complete chunks!\n  int restlen = srclen - chunks * chunklen; /* srclen % chunklen */\n\n  int out_len = (chunks + 1) * endlen + srclen;\n  String ret(out_len, ReserveString);\n  char *dest = ret.bufferSlice().ptr;\n\n  const char *p; char *q;\n  const char *pMax = src + srclen - chunklen + 1;\n  for (p = src, q = dest; p < pMax; ) {\n    memcpy(q, p, chunklen);\n    q += chunklen;\n    memcpy(q, end, endlen);\n    q += endlen;\n    p += chunklen;\n  }\n\n  if (restlen) {\n    memcpy(q, p, restlen);\n    q += restlen;\n    memcpy(q, end, endlen);\n    q += endlen;\n  }\n\n  ret.setSize(q - dest);\n  return ret;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 629,
        "critical_vars": [
            "ReserveString",
            "endlen",
            "chunks",
            "srclen"
        ],
        "function": "string_chunk_split",
        "filename": "hhvm/CVE-2014-6228/CVE-2014-6228_CWE-189_1f91e076a585118495b976a413c1df40f6fd3d41_zend-string.cpp.diff",
        "label": "True",
        "function_code": "\nString string_chunk_split(const char *src, int srclen, const char *end,\n                          int endlen, int chunklen) {\n  int chunks = srclen / chunklen; // complete chunks!\n  int restlen = srclen - chunks * chunklen; /* srclen % chunklen */\n\n  String ret(\n    safe_address(\n      chunks + 1,\n      endlen,\n      srclen\n    ),\n    ReserveString\n  );\n  char *dest = ret.bufferSlice().ptr;\n\n  const char *p; char *q;\n  const char *pMax = src + srclen - chunklen + 1;\n  for (p = src, q = dest; p < pMax; ) {\n    memcpy(q, p, chunklen);\n    q += chunklen;\n    memcpy(q, end, endlen);\n    q += endlen;\n    p += chunklen;\n  }\n\n  if (restlen) {\n    memcpy(q, p, restlen);\n    q += restlen;\n    memcpy(q, end, endlen);\n    q += endlen;\n  }\n\n  ret.setSize(q - dest);\n  return ret;\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Fun-Call",
        "line_old": 630,
        "critical_vars": [
            "out_len",
            "ReserveString"
        ],
        "function": "string_chunk_split",
        "filename": "hhvm/CVE-2014-6228/CVE-2014-6228_CWE-189_1f91e076a585118495b976a413c1df40f6fd3d41_zend-string.cpp.diff",
        "label": "False",
        "function_code": "\nString string_chunk_split(const char *src, int srclen, const char *end,\n                          int endlen, int chunklen) {\n  int chunks = srclen / chunklen; // complete chunks!\n  int restlen = srclen - chunks * chunklen; /* srclen % chunklen */\n\n  int out_len = (chunks + 1) * endlen + srclen;\n  String ret(out_len, ReserveString);\n  char *dest = ret.bufferSlice().ptr;\n\n  const char *p; char *q;\n  const char *pMax = src + srclen - chunklen + 1;\n  for (p = src, q = dest; p < pMax; ) {\n    memcpy(q, p, chunklen);\n    q += chunklen;\n    memcpy(q, end, endlen);\n    q += endlen;\n    p += chunklen;\n  }\n\n  if (restlen) {\n    memcpy(q, p, restlen);\n    q += restlen;\n    memcpy(q, end, endlen);\n    q += endlen;\n  }\n\n  ret.setSize(q - dest);\n  return ret;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 673,
        "critical_vars": [
            "s"
        ],
        "function": "license_read_scope_list",
        "filename": "FreeRDP/CVE-2014-0791/CVE-2014-0791_CWE-189_e2745807c4c3e0a590c0f69a9b655dc74ebaa03e_license.c.diff",
        "label": "True",
        "function_code": "\nBOOL license_read_scope_list(wStream* s, SCOPE_LIST* scopeList)\n{\n\tUINT32 i;\n\tUINT32 scopeCount;\n\n\tif (Stream_GetRemainingLength(s) < 4)\n\t\treturn FALSE;\n\n\tStream_Read_UINT32(s, scopeCount); /* ScopeCount (4 bytes) */\n\n        if (Stream_GetRemainingLength(s) / sizeof(LICENSE_BLOB) < scopeCount)\n                return FALSE;  /* Avoid overflow in malloc */\n\n\tscopeList->count = scopeCount;\n\tscopeList->array = (LICENSE_BLOB*) malloc(sizeof(LICENSE_BLOB) * scopeCount);\n\n\t/* ScopeArray */\n\tfor (i = 0; i < scopeCount; i++)\n\t{\n\t\tscopeList->array[i].type = BB_SCOPE_BLOB;\n\n\t\tif (!license_read_binary_blob(s, &scopeList->array[i]))\n\t\t\treturn FALSE;\n\t}\n\n\treturn TRUE;\n}"
    },
    {
        "patch_model": "Replace",
        "change_type": "if-Condition",
        "line_old": 208,
        "line_new": 208,
        "critical_vars": [
            "cluster"
        ],
        "function": "set_fat",
        "filename": "dosfstools/CVE-2015-8872/CVE-2015-8872_CWE-189_07908124838afcc99c577d1d3e84cef2dbd39cb7_fat.c.diff",
        "label": "True",
        "function_code": "\t    set_fat(fs, i, -1);\n\t}\n\tif (curEntry.value >= fs->clusters + 2 &&\n\t    (curEntry.value < FAT_MIN_BAD(fs))) {\n\t    printf(\"Cluster %ld out of range (%ld > %ld). Setting to EOF.\\n\",\n\t\t   (long)(i - 2), (long)curEntry.value, (long)(fs->clusters + 2 - 1));\n\t    set_fat(fs, i, -1);\n\t}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Fun-Call",
        "line_old": 29,
        "critical_vars": [
            "offset",
            "length",
            "int32_t"
        ],
        "function": "FontData::Size",
        "filename": "sfntly/CVE-2015-6781/CVE-2015-6781_CWE-189_de776d4ef06ca29c240de3444348894f032b03ff_font_data.cc.diff",
        "label": "False",
        "function_code": "Function not found"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 31,
        "critical_vars": [
            "offset",
            "length",
            "int32_t"
        ],
        "function": "FontData::Size",
        "filename": "sfntly/CVE-2015-6781/CVE-2015-6781_CWE-189_de776d4ef06ca29c240de3444348894f032b03ff_font_data.cc.diff",
        "label": "True",
        "function_code": "Function not found"
    },
    {
        "patch_model": "Delete",
        "change_type": "if-Condition",
        "line_old": 30,
        "critical_vars": [
            "offset",
            "length"
        ],
        "function": "FontData::Size",
        "filename": "sfntly/CVE-2015-6781/CVE-2015-6781_CWE-189_de776d4ef06ca29c240de3444348894f032b03ff_font_data.cc.diff",
        "label": "False",
        "function_code": "Function not found"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 33,
        "critical_vars": [
            "offset"
        ],
        "function": "FontData::Size",
        "filename": "sfntly/CVE-2015-6781/CVE-2015-6781_CWE-189_de776d4ef06ca29c240de3444348894f032b03ff_font_data.cc.diff",
        "label": "True",
        "function_code": "Function not found"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 34,
        "critical_vars": [
            "length"
        ],
        "function": "FontData::Size",
        "filename": "sfntly/CVE-2015-6781/CVE-2015-6781_CWE-189_de776d4ef06ca29c240de3444348894f032b03ff_font_data.cc.diff",
        "label": "True",
        "function_code": "Function not found"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 33,
        "critical_vars": [
            "bound_offset_"
        ],
        "function": "FontData::Size",
        "filename": "sfntly/CVE-2015-6781/CVE-2015-6781_CWE-189_de776d4ef06ca29c240de3444348894f032b03ff_font_data.cc.diff",
        "label": "False",
        "function_code": "Function not found"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 37,
        "critical_vars": [
            "offset"
        ],
        "function": "FontData::Size",
        "filename": "sfntly/CVE-2015-6781/CVE-2015-6781_CWE-189_de776d4ef06ca29c240de3444348894f032b03ff_font_data.cc.diff",
        "label": "True",
        "function_code": "Function not found"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 38,
        "critical_vars": [
            "new_offset"
        ],
        "function": "FontData::Size",
        "filename": "sfntly/CVE-2015-6781/CVE-2015-6781_CWE-189_de776d4ef06ca29c240de3444348894f032b03ff_font_data.cc.diff",
        "label": "True",
        "function_code": "Function not found"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 40,
        "critical_vars": [
            "length"
        ],
        "function": "FontData::Size",
        "filename": "sfntly/CVE-2015-6781/CVE-2015-6781_CWE-189_de776d4ef06ca29c240de3444348894f032b03ff_font_data.cc.diff",
        "label": "True",
        "function_code": "Function not found"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 44,
        "critical_vars": [
            "new_offset"
        ],
        "function": "FontData::Size",
        "filename": "sfntly/CVE-2015-6781/CVE-2015-6781_CWE-189_de776d4ef06ca29c240de3444348894f032b03ff_font_data.cc.diff",
        "label": "True",
        "function_code": "Function not found"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 49,
        "critical_vars": [
            "length"
        ],
        "function": "FontData::Size",
        "filename": "sfntly/CVE-2015-6781/CVE-2015-6781_CWE-189_de776d4ef06ca29c240de3444348894f032b03ff_font_data.cc.diff",
        "label": "True",
        "function_code": "Function not found"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 52,
        "critical_vars": [
            "bound_offset_"
        ],
        "function": "FontData::Size",
        "filename": "sfntly/CVE-2015-6781/CVE-2015-6781_CWE-189_de776d4ef06ca29c240de3444348894f032b03ff_font_data.cc.diff",
        "label": "True",
        "function_code": "Function not found"
    },
    {
        "patch_model": "Delete",
        "change_type": "Fun-Call",
        "line_old": 38,
        "critical_vars": [
            "offset",
            "int32_t"
        ],
        "function": "FontData::Size",
        "filename": "sfntly/CVE-2015-6781/CVE-2015-6781_CWE-189_de776d4ef06ca29c240de3444348894f032b03ff_font_data.cc.diff",
        "label": "False",
        "function_code": "Function not found"
    },
    {
        "patch_model": "Delete",
        "change_type": "if-Condition",
        "line_old": 39,
        "critical_vars": [
            "offset"
        ],
        "function": "FontData::Size",
        "filename": "sfntly/CVE-2015-6781/CVE-2015-6781_CWE-189_de776d4ef06ca29c240de3444348894f032b03ff_font_data.cc.diff",
        "label": "False",
        "function_code": "Function not found"
    },
    {
        "patch_model": "Delete",
        "change_type": "Assignment",
        "line_old": 42,
        "critical_vars": [
            "bound_offset_"
        ],
        "function": "FontData::Size",
        "filename": "sfntly/CVE-2015-6781/CVE-2015-6781_CWE-189_de776d4ef06ca29c240de3444348894f032b03ff_font_data.cc.diff",
        "label": "False",
        "function_code": "Function not found"
    },
    {
        "patch_model": "Replace",
        "change_type": "for-Condition",
        "line_old": 1934,
        "line_new": 1934,
        "critical_vars": [
            "i"
        ],
        "function": "ff_set_cmp",
        "filename": "FFmpeg/CVE-2013-7010/CVE-2013-7010_CWE-189_454a11a1c9c686c78aa97954306fb63453299760_dsputil.c.diff",
        "label": "True",
        "function_code": "\nvoid ff_set_cmp(DSPContext* c, me_cmp_func *cmp, int type){\n    int i;\n\n    memset(cmp, 0, sizeof(void*)*6);\n\n    for(i=0; i<6; i++){\n        switch(type&0xFF){\n        case FF_CMP_SAD:\n            cmp[i]= c->sad[i];\n            break;\n        case FF_CMP_SATD:\n            cmp[i]= c->hadamard8_diff[i];\n            break;\n        case FF_CMP_SSE:\n            cmp[i]= c->sse[i];\n            break;\n        case FF_CMP_DCT:\n            cmp[i]= c->dct_sad[i];\n            break;\n        case FF_CMP_DCT264:\n            cmp[i]= c->dct264_sad[i];\n            break;\n        case FF_CMP_DCTMAX:\n            cmp[i]= c->dct_max[i];\n            break;\n        case FF_CMP_PSNR:\n            cmp[i]= c->quant_psnr[i];\n            break;\n        case FF_CMP_BIT:\n            cmp[i]= c->bit[i];\n            break;\n        case FF_CMP_RD:\n            cmp[i]= c->rd[i];\n            break;\n        case FF_CMP_VSAD:\n            cmp[i]= c->vsad[i];\n            break;\n        case FF_CMP_VSSE:\n            cmp[i]= c->vsse[i];\n            break;\n        case FF_CMP_ZERO:\n            cmp[i]= zero_cmp;\n            break;\n        case FF_CMP_NSSE:\n            cmp[i]= c->nsse[i];\n            break;\n#if CONFIG_DWT\n        case FF_CMP_W53:\n            cmp[i]= c->w53[i];\n            break;\n        case FF_CMP_W97:\n            cmp[i]= c->w97[i];\n            break;\n#endif\n        default:\n            av_log(NULL, AV_LOG_ERROR,\"internal error in cmp function selection\\n\");\n        }\n    }\n}"
    },
    {
        "patch_model": "Replace",
        "change_type": "for-Condition",
        "line_old": 1959,
        "line_new": 1959,
        "critical_vars": [
            "i"
        ],
        "function": "diff_bytes_c",
        "filename": "FFmpeg/CVE-2013-7010/CVE-2013-7010_CWE-189_454a11a1c9c686c78aa97954306fb63453299760_dsputil.c.diff",
        "label": "True",
        "function_code": "\nstatic void diff_bytes_c(uint8_t *dst, const uint8_t *src1, const uint8_t *src2, int w){\n    long i;\n#if !HAVE_FAST_UNALIGNED\n    if((long)src2 & (sizeof(long)-1)){\n        for(i=0; i+7<w; i+=8){\n            dst[i+0] = src1[i+0]-src2[i+0];\n            dst[i+1] = src1[i+1]-src2[i+1];\n            dst[i+2] = src1[i+2]-src2[i+2];\n            dst[i+3] = src1[i+3]-src2[i+3];\n            dst[i+4] = src1[i+4]-src2[i+4];\n            dst[i+5] = src1[i+5]-src2[i+5];\n            dst[i+6] = src1[i+6]-src2[i+6];\n            dst[i+7] = src1[i+7]-src2[i+7];\n        }\n    }else\n#endif\n    for(i=0; i<=w-(int)sizeof(long); i+=sizeof(long)){\n        long a = *(long*)(src1+i);\n        long b = *(long*)(src2+i);\n        *(long*)(dst+i) = ((a|pb_80) - (b&pb_7f)) ^ ((a^b^pb_80)&pb_80);\n    }\n    for(; i<w; i++)\n        dst[i+0] = src1[i+0]-src2[i+0];\n}"
    },
    {
        "patch_model": "Replace",
        "change_type": "Assignment",
        "line_old": 456,
        "line_new": 456,
        "critical_vars": [
            "c->tile_stride"
        ],
        "function": "g2m_init_buffers",
        "filename": "FFmpeg/CVE-2013-7013/CVE-2013-7013_CWE-189_821a5938d100458f4d09d634041b05c860554ce0_g2meet.c.diff",
        "label": "True",
        "function_code": "\nstatic int g2m_init_buffers(G2MContext *c)\n{\n    int aligned_height;\n\n    if (!c->framebuf || c->old_width < c->width || c->old_height < c->height) {\n        c->framebuf_stride = FFALIGN(c->width * 3, 16);\n        aligned_height     = FFALIGN(c->height,    16);\n        av_free(c->framebuf);\n        c->framebuf = av_mallocz(c->framebuf_stride * aligned_height);\n        if (!c->framebuf)\n            return AVERROR(ENOMEM);\n    }\n    if (!c->synth_tile || !c->jpeg_tile ||\n        c->old_tile_w < c->tile_width ||\n        c->old_tile_h < c->tile_height) {\n        c->tile_stride = FFALIGN(c->tile_width, 16) * 3;\n        aligned_height = FFALIGN(c->tile_height,    16);\n        av_free(c->synth_tile);\n        av_free(c->jpeg_tile);\n        av_free(c->kempf_buf);\n        av_free(c->kempf_flags);\n        c->synth_tile  = av_mallocz(c->tile_stride      * aligned_height);\n        c->jpeg_tile   = av_mallocz(c->tile_stride      * aligned_height);\n        c->kempf_buf   = av_mallocz((c->tile_width + 1) * aligned_height\n                                    + FF_INPUT_BUFFER_PADDING_SIZE);\n        c->kempf_flags = av_mallocz( c->tile_width      * aligned_height);\n        if (!c->synth_tile || !c->jpeg_tile ||\n            !c->kempf_buf || !c->kempf_flags)\n            return AVERROR(ENOMEM);\n    }\n\n    return 0;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Var-Declaration",
        "line_new": 303,
        "critical_vars": [
            "blen"
        ],
        "function": "lsr_read_extension",
        "filename": "gpac/CVE-2022-4202/CVE-2022-4202_CWE-189_b3d821c4ae9ba62b3a194d9dcb5e99f17bd56908_lsr_dec.c.diff",
        "label": "True",
        "function_code": "\nstatic void lsr_read_extension(GF_LASeRCodec *lsr, const char *name)\n{\n\tu32 len = lsr_read_vluimsbf5(lsr, name);\n#if 0\n\t*out_data = gf_malloc(sizeof(char)*len);\n\tgf_bs_read_data(lsr->bs, *out_data, len);\n\t*out_len = len;\n#else\n\twhile (len && gf_bs_available(lsr->bs) ) {\n\t\tgf_bs_read_int(lsr->bs, 8);\n\t\tlen--;\n\t}\n\tif (len) lsr->last_error = GF_NON_COMPLIANT_BITSTREAM;\n#endif\n}"
    },
    {
        "patch_model": "Delete",
        "change_type": "Fun-Call",
        "line_old": 307,
        "critical_vars": [
            "lsr->bs",
            "len"
        ],
        "function": "lsr_read_extension",
        "filename": "gpac/CVE-2022-4202/CVE-2022-4202_CWE-189_b3d821c4ae9ba62b3a194d9dcb5e99f17bd56908_lsr_dec.c.diff",
        "label": "False",
        "function_code": "\nstatic void lsr_read_extension(GF_LASeRCodec *lsr, const char *name)\n{\n\tu32 len = lsr_read_vluimsbf5(lsr, name);\n#if 0\n\t*out_data = gf_malloc(sizeof(char)*len);\n\tgf_bs_read_data(lsr->bs, *out_data, len);\n\t*out_len = len;\n#else\n\twhile (len && gf_bs_available(lsr->bs) ) {\n\t\tgf_bs_read_int(lsr->bs, 8);\n\t\tlen--;\n\t}\n\tif (len) lsr->last_error = GF_NON_COMPLIANT_BITSTREAM;\n#endif\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "while-Condition",
        "line_new": 306,
        "critical_vars": [
            "lsr->bs",
            "len"
        ],
        "function": "lsr_read_extension",
        "filename": "gpac/CVE-2022-4202/CVE-2022-4202_CWE-189_b3d821c4ae9ba62b3a194d9dcb5e99f17bd56908_lsr_dec.c.diff",
        "label": "True",
        "function_code": "\nstatic void lsr_read_extension(GF_LASeRCodec *lsr, const char *name)\n{\n\tu32 len = lsr_read_vluimsbf5(lsr, name);\n#if 0\n\t*out_data = gf_malloc(sizeof(char)*len);\n\tgf_bs_read_data(lsr->bs, *out_data, len);\n\t*out_len = len;\n#else\n\twhile (len && gf_bs_available(lsr->bs) ) {\n\t\tgf_bs_read_int(lsr->bs, 8);\n\t\tlen--;\n\t}\n\tif (len) lsr->last_error = GF_NON_COMPLIANT_BITSTREAM;\n#endif\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 307,
        "critical_vars": [
            "lsr->bs",
            "len"
        ],
        "function": "lsr_read_extension",
        "filename": "gpac/CVE-2022-4202/CVE-2022-4202_CWE-189_b3d821c4ae9ba62b3a194d9dcb5e99f17bd56908_lsr_dec.c.diff",
        "label": "True",
        "function_code": "\nstatic void lsr_read_extension(GF_LASeRCodec *lsr, const char *name)\n{\n\tu32 len = lsr_read_vluimsbf5(lsr, name);\n#if 0\n\t*out_data = gf_malloc(sizeof(char)*len);\n\tgf_bs_read_data(lsr->bs, *out_data, len);\n\t*out_len = len;\n#else\n\twhile (len && gf_bs_available(lsr->bs) ) {\n\t\tgf_bs_read_int(lsr->bs, 8);\n\t\tlen--;\n\t}\n\tif (len) lsr->last_error = GF_NON_COMPLIANT_BITSTREAM;\n#endif\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 308,
        "critical_vars": [
            "len"
        ],
        "function": "lsr_read_extension",
        "filename": "gpac/CVE-2022-4202/CVE-2022-4202_CWE-189_b3d821c4ae9ba62b3a194d9dcb5e99f17bd56908_lsr_dec.c.diff",
        "label": "True",
        "function_code": "\nstatic void lsr_read_extension(GF_LASeRCodec *lsr, const char *name)\n{\n\tu32 len = lsr_read_vluimsbf5(lsr, name);\n#if 0\n\t*out_data = gf_malloc(sizeof(char)*len);\n\tgf_bs_read_data(lsr->bs, *out_data, len);\n\t*out_len = len;\n#else\n\twhile (len && gf_bs_available(lsr->bs) ) {\n\t\tgf_bs_read_int(lsr->bs, 8);\n\t\tlen--;\n\t}\n\tif (len) lsr->last_error = GF_NON_COMPLIANT_BITSTREAM;\n#endif\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 310,
        "critical_vars": [
            "blen"
        ],
        "function": "lsr_read_extension",
        "filename": "gpac/CVE-2022-4202/CVE-2022-4202_CWE-189_b3d821c4ae9ba62b3a194d9dcb5e99f17bd56908_lsr_dec.c.diff",
        "label": "True",
        "function_code": "\nstatic void lsr_read_extension(GF_LASeRCodec *lsr, const char *name)\n{\n\tu32 len = lsr_read_vluimsbf5(lsr, name);\n#if 0\n\t*out_data = gf_malloc(sizeof(char)*len);\n\tgf_bs_read_data(lsr->bs, *out_data, len);\n\t*out_len = len;\n#else\n\twhile (len && gf_bs_available(lsr->bs) ) {\n\t\tgf_bs_read_int(lsr->bs, 8);\n\t\tlen--;\n\t}\n\tif (len) lsr->last_error = GF_NON_COMPLIANT_BITSTREAM;\n#endif\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 311,
        "critical_vars": [
            "lsr->bs",
            "blen"
        ],
        "function": "lsr_read_extension",
        "filename": "gpac/CVE-2022-4202/CVE-2022-4202_CWE-189_b3d821c4ae9ba62b3a194d9dcb5e99f17bd56908_lsr_dec.c.diff",
        "label": "True",
        "function_code": "\nstatic void lsr_read_extension(GF_LASeRCodec *lsr, const char *name)\n{\n\tu32 len = lsr_read_vluimsbf5(lsr, name);\n#if 0\n\t*out_data = gf_malloc(sizeof(char)*len);\n\tgf_bs_read_data(lsr->bs, *out_data, len);\n\t*out_len = len;\n#else\n\twhile (len && gf_bs_available(lsr->bs) ) {\n\t\tgf_bs_read_int(lsr->bs, 8);\n\t\tlen--;\n\t}\n\tif (len) lsr->last_error = GF_NON_COMPLIANT_BITSTREAM;\n#endif\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 312,
        "critical_vars": [
            "len"
        ],
        "function": "lsr_read_extension",
        "filename": "gpac/CVE-2022-4202/CVE-2022-4202_CWE-189_b3d821c4ae9ba62b3a194d9dcb5e99f17bd56908_lsr_dec.c.diff",
        "label": "True",
        "function_code": "\nstatic void lsr_read_extension(GF_LASeRCodec *lsr, const char *name)\n{\n\tu32 len = lsr_read_vluimsbf5(lsr, name);\n#if 0\n\t*out_data = gf_malloc(sizeof(char)*len);\n\tgf_bs_read_data(lsr->bs, *out_data, len);\n\t*out_len = len;\n#else\n\twhile (len && gf_bs_available(lsr->bs) ) {\n\t\tgf_bs_read_int(lsr->bs, 8);\n\t\tlen--;\n\t}\n\tif (len) lsr->last_error = GF_NON_COMPLIANT_BITSTREAM;\n#endif\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "while-Condition",
        "line_new": 314,
        "critical_vars": [
            "len"
        ],
        "function": "lsr_read_extension",
        "filename": "gpac/CVE-2022-4202/CVE-2022-4202_CWE-189_b3d821c4ae9ba62b3a194d9dcb5e99f17bd56908_lsr_dec.c.diff",
        "label": "True",
        "function_code": "\nstatic void lsr_read_extension(GF_LASeRCodec *lsr, const char *name)\n{\n\tu32 len = lsr_read_vluimsbf5(lsr, name);\n#if 0\n\t*out_data = gf_malloc(sizeof(char)*len);\n\tgf_bs_read_data(lsr->bs, *out_data, len);\n\t*out_len = len;\n#else\n\twhile (len && gf_bs_available(lsr->bs) ) {\n\t\tgf_bs_read_int(lsr->bs, 8);\n\t\tlen--;\n\t}\n\tif (len) lsr->last_error = GF_NON_COMPLIANT_BITSTREAM;\n#endif\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 315,
        "critical_vars": [
            "lsr->bs"
        ],
        "function": "lsr_read_extension",
        "filename": "gpac/CVE-2022-4202/CVE-2022-4202_CWE-189_b3d821c4ae9ba62b3a194d9dcb5e99f17bd56908_lsr_dec.c.diff",
        "label": "True",
        "function_code": "\nstatic void lsr_read_extension(GF_LASeRCodec *lsr, const char *name)\n{\n\tu32 len = lsr_read_vluimsbf5(lsr, name);\n#if 0\n\t*out_data = gf_malloc(sizeof(char)*len);\n\tgf_bs_read_data(lsr->bs, *out_data, len);\n\t*out_len = len;\n#else\n\twhile (len && gf_bs_available(lsr->bs) ) {\n\t\tgf_bs_read_int(lsr->bs, 8);\n\t\tlen--;\n\t}\n\tif (len) lsr->last_error = GF_NON_COMPLIANT_BITSTREAM;\n#endif\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Assignment",
        "line_new": 316,
        "critical_vars": [
            "len"
        ],
        "function": "lsr_read_extension",
        "filename": "gpac/CVE-2022-4202/CVE-2022-4202_CWE-189_b3d821c4ae9ba62b3a194d9dcb5e99f17bd56908_lsr_dec.c.diff",
        "label": "True",
        "function_code": "\nstatic void lsr_read_extension(GF_LASeRCodec *lsr, const char *name)\n{\n\tu32 len = lsr_read_vluimsbf5(lsr, name);\n#if 0\n\t*out_data = gf_malloc(sizeof(char)*len);\n\tgf_bs_read_data(lsr->bs, *out_data, len);\n\t*out_len = len;\n#else\n\twhile (len && gf_bs_available(lsr->bs) ) {\n\t\tgf_bs_read_int(lsr->bs, 8);\n\t\tlen--;\n\t}\n\tif (len) lsr->last_error = GF_NON_COMPLIANT_BITSTREAM;\n#endif\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 852,
        "critical_vars": [
            "nb_bits"
        ],
        "function": "lsr_read_id",
        "filename": "gpac/CVE-2022-4202/CVE-2022-4202_CWE-189_b3d821c4ae9ba62b3a194d9dcb5e99f17bd56908_lsr_dec.c.diff",
        "label": "True",
        "function_code": "static void lsr_read_id(GF_LASeRCodec *lsr, GF_Node *n)\n{\n\tGF_FieldInfo info;\n\tu32 val, id, i, count;\n\tchar *name;\n\tGF_LSR_READ_INT(lsr, val, 1, \"has_id\");\n\tif (!val) return;\n\n\tname = NULL;\n\tid = 1+lsr_read_vluimsbf5(lsr, \"ID\");\n\tgf_node_set_id(n, id, name);\n\n\tGF_LSR_READ_INT(lsr, val, 1, \"reserved\");\n\t/*currently not used*/\n\tif (val) {\n\t\tu32 len = lsr_read_vluimsbf5(lsr, \"len\");\n\t\tGF_LSR_READ_INT(lsr, val, len, \"reserved\");\n\t}\n\n\t/*update all pending HREFs*/\n\tcount = gf_list_count(lsr->deferred_hrefs);\n\tfor (i=0; i<count; i++) {\n\t\tXMLRI *href = (XMLRI *)gf_list_get(lsr->deferred_hrefs, i);\n\t\tchar *str_id = href ? href->string : NULL;\n\t\tif (!str_id) return;\n\t\t\n\t\tif (str_id[0] == '#') str_id++;\n\t\t/*skip 'N'*/\n\t\tstr_id++;\n\t\tif (id == (1 + (u32) atoi(str_id))) {\n\t\t\thref->target = (SVG_Element*) n;\n\t\t\tgf_free(href->string);\n\t\t\thref->string = NULL;\n\t\t\tgf_list_rem(lsr->deferred_hrefs, i);\n\t\t\ti--;\n\t\t\tcount--;\n\t\t}\n\t}\n\n\t/*update unresolved listeners*/\n\tcount = gf_list_count(lsr->deferred_listeners);\n\tfor (i=0; i<count; i++) {\n\t\tGF_Node *par;\n\t\tXMLRI *observer = NULL;\n\t\tGF_Node *listener = (GF_Node *)gf_list_get(lsr->deferred_listeners, i);\n\n\t\tpar = NULL;\n\t\tif (gf_node_get_attribute_by_tag(listener, TAG_XMLEV_ATT_observer, GF_FALSE, GF_FALSE, &info) == GF_OK) {\n\t\t\tobserver = (XMLRI*)info.far_ptr;\n\t\t\tif (observer->type == XMLRI_ELEMENTID) {\n\t\t\t\tif (!observer->target) continue;\n\t\t\t\telse par = (GF_Node*)observer->target;\n\t\t\t}\n\t\t}\n\t\tif (gf_node_get_attribute_by_tag(listener, TAG_XMLEV_ATT_target, GF_FALSE, GF_FALSE, &info) == GF_OK) {\n\t\t\tif (((XMLRI*)info.far_ptr)->type == XMLRI_ELEMENTID) {\n\t\t\t\tif (!((XMLRI*)info.far_ptr)->target) continue;\n\t\t\t\telse if (!par) par = (GF_Node*)((XMLRI*)info.far_ptr)->target;\n\t\t\t}\n\t\t}\n\t\t/*FIXME - double check with XML events*/\n\t\tif (!par && !observer) {\n\t\t\tif (gf_node_get_attribute_by_tag(listener, TAG_XMLEV_ATT_event, GF_FALSE, GF_FALSE, &info) == GF_OK) {\n\t\t\t\tXMLEV_Event *ev = (XMLEV_Event *)info.far_ptr;\n\t\t\t\t/*all non-UI get attched to root*/\n\t\t\t\tif (ev && (ev->type>GF_EVENT_MOUSEWHEEL)) {\n\t\t\t\t\tpar = (GF_Node*) lsr->current_root;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tassert(par);\n\t\tgf_node_dom_listener_add(par, listener);\n\t\tgf_list_rem(lsr->deferred_listeners, i);\n\t\ti--;\n\t\tcount--;\n\t}\n\n\t/*update all pending animations*/\n\tcount = gf_list_count(lsr->deferred_anims);\n\tfor (i=0; i<count; i++) {\n\t\tSVG_Element *elt = (SVG_Element *)gf_list_get(lsr->deferred_anims, i);\n\t\tif (lsr_setup_smil_anim(lsr, elt, NULL)) {\n\t\t\tgf_list_rem(lsr->deferred_anims, i);\n\t\t\ti--;\n\t\t\tcount--;\n\t\t\tgf_node_init((GF_Node*)elt);\n\t\t}\n\t}\n}"
    },
    {
        "patch_model": "Replace",
        "change_type": "Assignment",
        "line_old": 845,
        "line_new": 856,
        "critical_vars": [
            "neg"
        ],
        "function": "lsr_read_id",
        "filename": "gpac/CVE-2022-4202/CVE-2022-4202_CWE-189_b3d821c4ae9ba62b3a194d9dcb5e99f17bd56908_lsr_dec.c.diff",
        "label": "True",
        "function_code": "static void lsr_read_id(GF_LASeRCodec *lsr, GF_Node *n)\n{\n\tGF_FieldInfo info;\n\tu32 val, id, i, count;\n\tchar *name;\n\tGF_LSR_READ_INT(lsr, val, 1, \"has_id\");\n\tif (!val) return;\n\n\tname = NULL;\n\tid = 1+lsr_read_vluimsbf5(lsr, \"ID\");\n\tgf_node_set_id(n, id, name);\n\n\tGF_LSR_READ_INT(lsr, val, 1, \"reserved\");\n\t/*currently not used*/\n\tif (val) {\n\t\tu32 len = lsr_read_vluimsbf5(lsr, \"len\");\n\t\tGF_LSR_READ_INT(lsr, val, len, \"reserved\");\n\t}\n\n\t/*update all pending HREFs*/\n\tcount = gf_list_count(lsr->deferred_hrefs);\n\tfor (i=0; i<count; i++) {\n\t\tXMLRI *href = (XMLRI *)gf_list_get(lsr->deferred_hrefs, i);\n\t\tchar *str_id = href ? href->string : NULL;\n\t\tif (!str_id) return;\n\t\t\n\t\tif (str_id[0] == '#') str_id++;\n\t\t/*skip 'N'*/\n\t\tstr_id++;\n\t\tif (id == (1 + (u32) atoi(str_id))) {\n\t\t\thref->target = (SVG_Element*) n;\n\t\t\tgf_free(href->string);\n\t\t\thref->string = NULL;\n\t\t\tgf_list_rem(lsr->deferred_hrefs, i);\n\t\t\ti--;\n\t\t\tcount--;\n\t\t}\n\t}\n\n\t/*update unresolved listeners*/\n\tcount = gf_list_count(lsr->deferred_listeners);\n\tfor (i=0; i<count; i++) {\n\t\tGF_Node *par;\n\t\tXMLRI *observer = NULL;\n\t\tGF_Node *listener = (GF_Node *)gf_list_get(lsr->deferred_listeners, i);\n\n\t\tpar = NULL;\n\t\tif (gf_node_get_attribute_by_tag(listener, TAG_XMLEV_ATT_observer, GF_FALSE, GF_FALSE, &info) == GF_OK) {\n\t\t\tobserver = (XMLRI*)info.far_ptr;\n\t\t\tif (observer->type == XMLRI_ELEMENTID) {\n\t\t\t\tif (!observer->target) continue;\n\t\t\t\telse par = (GF_Node*)observer->target;\n\t\t\t}\n\t\t}\n\t\tif (gf_node_get_attribute_by_tag(listener, TAG_XMLEV_ATT_target, GF_FALSE, GF_FALSE, &info) == GF_OK) {\n\t\t\tif (((XMLRI*)info.far_ptr)->type == XMLRI_ELEMENTID) {\n\t\t\t\tif (!((XMLRI*)info.far_ptr)->target) continue;\n\t\t\t\telse if (!par) par = (GF_Node*)((XMLRI*)info.far_ptr)->target;\n\t\t\t}\n\t\t}\n\t\t/*FIXME - double check with XML events*/\n\t\tif (!par && !observer) {\n\t\t\tif (gf_node_get_attribute_by_tag(listener, TAG_XMLEV_ATT_event, GF_FALSE, GF_FALSE, &info) == GF_OK) {\n\t\t\t\tXMLEV_Event *ev = (XMLEV_Event *)info.far_ptr;\n\t\t\t\t/*all non-UI get attched to root*/\n\t\t\t\tif (ev && (ev->type>GF_EVENT_MOUSEWHEEL)) {\n\t\t\t\t\tpar = (GF_Node*) lsr->current_root;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tassert(par);\n\t\tgf_node_dom_listener_add(par, listener);\n\t\tgf_list_rem(lsr->deferred_listeners, i);\n\t\ti--;\n\t\tcount--;\n\t}\n\n\t/*update all pending animations*/\n\tcount = gf_list_count(lsr->deferred_anims);\n\tfor (i=0; i<count; i++) {\n\t\tSVG_Element *elt = (SVG_Element *)gf_list_get(lsr->deferred_anims, i);\n\t\tif (lsr_setup_smil_anim(lsr, elt, NULL)) {\n\t\t\tgf_list_rem(lsr->deferred_anims, i);\n\t\t\ti--;\n\t\t\tcount--;\n\t\t\tgf_node_init((GF_Node*)elt);\n\t\t}\n\t}\n}"
    },
    {
        "patch_model": "Replace",
        "change_type": "Assignment",
        "line_old": 856,
        "line_new": 867,
        "critical_vars": [
            "neg"
        ],
        "function": "lsr_translate_coords",
        "filename": "gpac/CVE-2022-4202/CVE-2022-4202_CWE-189_b3d821c4ae9ba62b3a194d9dcb5e99f17bd56908_lsr_dec.c.diff",
        "label": "True",
        "function_code": "\nstatic Fixed lsr_translate_coords(GF_LASeRCodec *lsr, u32 val, u32 nb_bits)\n{\n\tif (!nb_bits) return 0;\n\tif (nb_bits>=32) return 0;\n\n#ifdef GPAC_FIXED_POINT\n\tif (val >> (nb_bits-1) ) {\n\t\ts64 neg = (s64) val - (0x00000001UL << nb_bits);\n\t\tif (neg < -FIX_ONE / 2)\n\t\t\treturn 2 * gf_divfix(INT2FIX(neg/2), lsr->res_factor);\n\t\treturn gf_divfix(INT2FIX(neg), lsr->res_factor);\n\t} else {\n\t\tif (val > FIX_ONE / 2)\n\t\t\treturn 2 * gf_divfix(INT2FIX(val/2), lsr->res_factor);\n\t\treturn gf_divfix(INT2FIX(val), lsr->res_factor);\n\t}\n#else\n\tif (val >> (nb_bits-1) ) {\n\t\ts64 neg = (s64) val - (0x00000001UL << nb_bits);\n\t\treturn ((Fixed)neg) / lsr->res_factor;\n\t} else {\n\t\treturn ((Fixed)val) / lsr->res_factor;\n\t}\n#endif\n}"
    },
    {
        "patch_model": "Replace",
        "change_type": "Assignment",
        "line_old": 867,
        "line_new": 878,
        "critical_vars": [
            "v"
        ],
        "function": "lsr_translate_coords",
        "filename": "gpac/CVE-2022-4202/CVE-2022-4202_CWE-189_b3d821c4ae9ba62b3a194d9dcb5e99f17bd56908_lsr_dec.c.diff",
        "label": "True",
        "function_code": "\nstatic Fixed lsr_translate_coords(GF_LASeRCodec *lsr, u32 val, u32 nb_bits)\n{\n\tif (!nb_bits) return 0;\n\tif (nb_bits>=32) return 0;\n\n#ifdef GPAC_FIXED_POINT\n\tif (val >> (nb_bits-1) ) {\n\t\ts64 neg = (s64) val - (0x00000001UL << nb_bits);\n\t\tif (neg < -FIX_ONE / 2)\n\t\t\treturn 2 * gf_divfix(INT2FIX(neg/2), lsr->res_factor);\n\t\treturn gf_divfix(INT2FIX(neg), lsr->res_factor);\n\t} else {\n\t\tif (val > FIX_ONE / 2)\n\t\t\treturn 2 * gf_divfix(INT2FIX(val/2), lsr->res_factor);\n\t\treturn gf_divfix(INT2FIX(val), lsr->res_factor);\n\t}\n#else\n\tif (val >> (nb_bits-1) ) {\n\t\ts64 neg = (s64) val - (0x00000001UL << nb_bits);\n\t\treturn ((Fixed)neg) / lsr->res_factor;\n\t} else {\n\t\treturn ((Fixed)val) / lsr->res_factor;\n\t}\n#endif\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "if-Condition",
        "line_new": 838,
        "critical_vars": [
            "q"
        ],
        "function": "cdf_read_property_info",
        "filename": "file/CVE-2014-3587/CVE-2014-3587_CWE-189_0641e56be1af003aa02c7c6b0184466540637233_cdf.c.diff",
        "label": "True",
        "function_code": "\nint\ncdf_read_property_info(const cdf_stream_t *sst, const cdf_header_t *h,\n    uint32_t offs, cdf_property_info_t **info, size_t *count, size_t *maxcount)\n{\n\tconst cdf_section_header_t *shp;\n\tcdf_section_header_t sh;\n\tconst uint8_t *p, *q, *e;\n\tint16_t s16;\n\tint32_t s32;\n\tuint32_t u32;\n\tint64_t s64;\n\tuint64_t u64;\n\tcdf_timestamp_t tp;\n\tsize_t i, o, o4, nelements, j;\n\tcdf_property_info_t *inp;\n\n\tif (offs > UINT32_MAX / 4) {\n\t\terrno = EFTYPE;\n\t\tgoto out;\n\t}\n\tshp = CAST(const cdf_section_header_t *, (const void *)\n\t    ((const char *)sst->sst_tab + offs));\n\tif (cdf_check_stream_offset(sst, h, shp, sizeof(*shp), __LINE__) == -1)\n\t\tgoto out;\n\tsh.sh_len = CDF_TOLE4(shp->sh_len);\n#define CDF_SHLEN_LIMIT (UINT32_MAX / 8)\n\tif (sh.sh_len > CDF_SHLEN_LIMIT) {\n\t\terrno = EFTYPE;\n\t\tgoto out;\n\t}\n\tsh.sh_properties = CDF_TOLE4(shp->sh_properties);\n#define CDF_PROP_LIMIT (UINT32_MAX / (4 * sizeof(*inp)))\n\tif (sh.sh_properties > CDF_PROP_LIMIT)\n\t\tgoto out;\n\tDPRINTF((\"section len: %u properties %u\\n\", sh.sh_len,\n\t    sh.sh_properties));\n\tif (*maxcount) {\n\t\tif (*maxcount > CDF_PROP_LIMIT)\n\t\t\tgoto out;\n\t\t*maxcount += sh.sh_properties;\n\t\tinp = CAST(cdf_property_info_t *,\n\t\t    realloc(*info, *maxcount * sizeof(*inp)));\n\t} else {\n\t\t*maxcount = sh.sh_properties;\n\t\tinp = CAST(cdf_property_info_t *,\n\t\t    malloc(*maxcount * sizeof(*inp)));\n\t}\n\tif (inp == NULL)\n\t\tgoto out;\n\t*info = inp;\n\tinp += *count;\n\t*count += sh.sh_properties;\n\tp = CAST(const uint8_t *, (const void *)\n\t    ((const char *)(const void *)sst->sst_tab +\n\t    offs + sizeof(sh)));\n\te = CAST(const uint8_t *, (const void *)\n\t    (((const char *)(const void *)shp) + sh.sh_len));\n\tif (cdf_check_stream_offset(sst, h, e, 0, __LINE__) == -1)\n\t\tgoto out;\n\tfor (i = 0; i < sh.sh_properties; i++) {\n\t\tsize_t tail = (i << 1) + 1;\n\t\tif (cdf_check_stream_offset(sst, h, p, tail * sizeof(uint32_t),\n\t\t    __LINE__) == -1)\n\t\t\tgoto out;\n\t\tsize_t ofs = CDF_GETUINT32(p, tail);\n\t\tq = (const uint8_t *)(const void *)\n\t\t    ((const char *)(const void *)p + ofs\n\t\t    - 2 * sizeof(uint32_t));\n\t\tif (q < p) {\n\t\t\tDPRINTF((\"Wrapped around %p < %p\\n\", q, p));\n\t\t\tgoto out;\n\t\t}\n\t\tif (q > e) {\n\t\t\tDPRINTF((\"Ran of the end %p > %p\\n\", q, e));\n\t\t\tgoto out;\n\t\t}\n\t\tinp[i].pi_id = CDF_GETUINT32(p, i << 1);\n\t\tinp[i].pi_type = CDF_GETUINT32(q, 0);\n\t\tDPRINTF((\"%\" SIZE_T_FORMAT \"u) id=%x type=%x offs=0x%tx,0x%x\\n\",\n\t\t    i, inp[i].pi_id, inp[i].pi_type, q - p, offs));\n\t\tif (inp[i].pi_type & CDF_VECTOR) {\n\t\t\tnelements = CDF_GETUINT32(q, 1);\n\t\t\tif (nelements == 0) {\n\t\t\t\tDPRINTF((\"CDF_VECTOR with nelements == 0\\n\"));\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\to = 2;\n\t\t} else {\n\t\t\tnelements = 1;\n\t\t\to = 1;\n\t\t}\n\t\to4 = o * sizeof(uint32_t);\n\t\tif (inp[i].pi_type & (CDF_ARRAY|CDF_BYREF|CDF_RESERVED))\n\t\t\tgoto unknown;\n\t\tswitch (inp[i].pi_type & CDF_TYPEMASK) {\n\t\tcase CDF_NULL:\n\t\tcase CDF_EMPTY:\n\t\t\tbreak;\n\t\tcase CDF_SIGNED16:\n\t\t\tif (inp[i].pi_type & CDF_VECTOR)\n\t\t\t\tgoto unknown;\n\t\t\t(void)memcpy(&s16, &q[o4], sizeof(s16));\n\t\t\tinp[i].pi_s16 = CDF_TOLE2(s16);\n\t\t\tbreak;\n\t\tcase CDF_SIGNED32:\n\t\t\tif (inp[i].pi_type & CDF_VECTOR)\n\t\t\t\tgoto unknown;\n\t\t\t(void)memcpy(&s32, &q[o4], sizeof(s32));\n\t\t\tinp[i].pi_s32 = CDF_TOLE4((uint32_t)s32);\n\t\t\tbreak;\n\t\tcase CDF_BOOL:\n\t\tcase CDF_UNSIGNED32:\n\t\t\tif (inp[i].pi_type & CDF_VECTOR)\n\t\t\t\tgoto unknown;\n\t\t\t(void)memcpy(&u32, &q[o4], sizeof(u32));\n\t\t\tinp[i].pi_u32 = CDF_TOLE4(u32);\n\t\t\tbreak;\n\t\tcase CDF_SIGNED64:\n\t\t\tif (inp[i].pi_type & CDF_VECTOR)\n\t\t\t\tgoto unknown;\n\t\t\t(void)memcpy(&s64, &q[o4], sizeof(s64));\n\t\t\tinp[i].pi_s64 = CDF_TOLE8((uint64_t)s64);\n\t\t\tbreak;\n\t\tcase CDF_UNSIGNED64:\n\t\t\tif (inp[i].pi_type & CDF_VECTOR)\n\t\t\t\tgoto unknown;\n\t\t\t(void)memcpy(&u64, &q[o4], sizeof(u64));\n\t\t\tinp[i].pi_u64 = CDF_TOLE8((uint64_t)u64);\n\t\t\tbreak;\n\t\tcase CDF_FLOAT:\n\t\t\tif (inp[i].pi_type & CDF_VECTOR)\n\t\t\t\tgoto unknown;\n\t\t\t(void)memcpy(&u32, &q[o4], sizeof(u32));\n\t\t\tu32 = CDF_TOLE4(u32);\n\t\t\tmemcpy(&inp[i].pi_f, &u32, sizeof(inp[i].pi_f));\n\t\t\tbreak;\n\t\tcase CDF_DOUBLE:\n\t\t\tif (inp[i].pi_type & CDF_VECTOR)\n\t\t\t\tgoto unknown;\n\t\t\t(void)memcpy(&u64, &q[o4], sizeof(u64));\n\t\t\tu64 = CDF_TOLE8((uint64_t)u64);\n\t\t\tmemcpy(&inp[i].pi_d, &u64, sizeof(inp[i].pi_d));\n\t\t\tbreak;\n\t\tcase CDF_LENGTH32_STRING:\n\t\tcase CDF_LENGTH32_WSTRING:\n\t\t\tif (nelements > 1) {\n\t\t\t\tsize_t nelem = inp - *info;\n\t\t\t\tif (*maxcount > CDF_PROP_LIMIT\n\t\t\t\t    || nelements > CDF_PROP_LIMIT)\n\t\t\t\t\tgoto out;\n\t\t\t\t*maxcount += nelements;\n\t\t\t\tinp = CAST(cdf_property_info_t *,\n\t\t\t\t    realloc(*info, *maxcount * sizeof(*inp)));\n\t\t\t\tif (inp == NULL)\n\t\t\t\t\tgoto out;\n\t\t\t\t*info = inp;\n\t\t\t\tinp = *info + nelem;\n\t\t\t}\n\t\t\tDPRINTF((\"nelements = %\" SIZE_T_FORMAT \"u\\n\",\n\t\t\t    nelements));\n\t\t\tfor (j = 0; j < nelements && i < sh.sh_properties;\n\t\t\t    j++, i++)\n\t\t\t{\n\t\t\t\tuint32_t l = CDF_GETUINT32(q, o);\n\t\t\t\tinp[i].pi_str.s_len = l;\n\t\t\t\tinp[i].pi_str.s_buf = (const char *)\n\t\t\t\t    (const void *)(&q[o4 + sizeof(l)]);\n\t\t\t\tDPRINTF((\"l = %d, r = %\" SIZE_T_FORMAT\n\t\t\t\t    \"u, s = %s\\n\", l,\n\t\t\t\t    CDF_ROUND(l, sizeof(l)),\n\t\t\t\t    inp[i].pi_str.s_buf));\n\t\t\t\tif (l & 1)\n\t\t\t\t\tl++;\n\t\t\t\to += l >> 1;\n\t\t\t\tif (q + o >= e)\n\t\t\t\t\tgoto out;\n\t\t\t\to4 = o * sizeof(uint32_t);\n\t\t\t}\n\t\t\ti--;\n\t\t\tbreak;\n\t\tcase CDF_FILETIME:\n\t\t\tif (inp[i].pi_type & CDF_VECTOR)\n\t\t\t\tgoto unknown;\n\t\t\t(void)memcpy(&tp, &q[o4], sizeof(tp));\n\t\t\tinp[i].pi_tp = CDF_TOLE8((uint64_t)tp);\n\t\t\tbreak;\n\t\tcase CDF_CLIPBOARD:\n\t\t\tif (inp[i].pi_type & CDF_VECTOR)\n\t\t\t\tgoto unknown;\n\t\t\tbreak;\n\t\tdefault:\n\t\tunknown:\n\t\t\tDPRINTF((\"Don't know how to deal with %x\\n\",\n\t\t\t    inp[i].pi_type));\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn 0;\nout:\n\tfree(*info);\n\treturn -1;\n}"
    },
    {
        "patch_model": "Add",
        "change_type": "Fun-Call",
        "line_new": 839,
        "critical_vars": [
            "p",
            "q"
        ],
        "function": "cdf_read_property_info",
        "filename": "file/CVE-2014-3587/CVE-2014-3587_CWE-189_0641e56be1af003aa02c7c6b0184466540637233_cdf.c.diff",
        "label": "True",
        "function_code": "\nint\ncdf_read_property_info(const cdf_stream_t *sst, const cdf_header_t *h,\n    uint32_t offs, cdf_property_info_t **info, size_t *count, size_t *maxcount)\n{\n\tconst cdf_section_header_t *shp;\n\tcdf_section_header_t sh;\n\tconst uint8_t *p, *q, *e;\n\tint16_t s16;\n\tint32_t s32;\n\tuint32_t u32;\n\tint64_t s64;\n\tuint64_t u64;\n\tcdf_timestamp_t tp;\n\tsize_t i, o, o4, nelements, j;\n\tcdf_property_info_t *inp;\n\n\tif (offs > UINT32_MAX / 4) {\n\t\terrno = EFTYPE;\n\t\tgoto out;\n\t}\n\tshp = CAST(const cdf_section_header_t *, (const void *)\n\t    ((const char *)sst->sst_tab + offs));\n\tif (cdf_check_stream_offset(sst, h, shp, sizeof(*shp), __LINE__) == -1)\n\t\tgoto out;\n\tsh.sh_len = CDF_TOLE4(shp->sh_len);\n#define CDF_SHLEN_LIMIT (UINT32_MAX / 8)\n\tif (sh.sh_len > CDF_SHLEN_LIMIT) {\n\t\terrno = EFTYPE;\n\t\tgoto out;\n\t}\n\tsh.sh_properties = CDF_TOLE4(shp->sh_properties);\n#define CDF_PROP_LIMIT (UINT32_MAX / (4 * sizeof(*inp)))\n\tif (sh.sh_properties > CDF_PROP_LIMIT)\n\t\tgoto out;\n\tDPRINTF((\"section len: %u properties %u\\n\", sh.sh_len,\n\t    sh.sh_properties));\n\tif (*maxcount) {\n\t\tif (*maxcount > CDF_PROP_LIMIT)\n\t\t\tgoto out;\n\t\t*maxcount += sh.sh_properties;\n\t\tinp = CAST(cdf_property_info_t *,\n\t\t    realloc(*info, *maxcount * sizeof(*inp)));\n\t} else {\n\t\t*maxcount = sh.sh_properties;\n\t\tinp = CAST(cdf_property_info_t *,\n\t\t    malloc(*maxcount * sizeof(*inp)));\n\t}\n\tif (inp == NULL)\n\t\tgoto out;\n\t*info = inp;\n\tinp += *count;\n\t*count += sh.sh_properties;\n\tp = CAST(const uint8_t *, (const void *)\n\t    ((const char *)(const void *)sst->sst_tab +\n\t    offs + sizeof(sh)));\n\te = CAST(const uint8_t *, (const void *)\n\t    (((const char *)(const void *)shp) + sh.sh_len));\n\tif (cdf_check_stream_offset(sst, h, e, 0, __LINE__) == -1)\n\t\tgoto out;\n\tfor (i = 0; i < sh.sh_properties; i++) {\n\t\tsize_t tail = (i << 1) + 1;\n\t\tif (cdf_check_stream_offset(sst, h, p, tail * sizeof(uint32_t),\n\t\t    __LINE__) == -1)\n\t\t\tgoto out;\n\t\tsize_t ofs = CDF_GETUINT32(p, tail);\n\t\tq = (const uint8_t *)(const void *)\n\t\t    ((const char *)(const void *)p + ofs\n\t\t    - 2 * sizeof(uint32_t));\n\t\tif (q < p) {\n\t\t\tDPRINTF((\"Wrapped around %p < %p\\n\", q, p));\n\t\t\tgoto out;\n\t\t}\n\t\tif (q > e) {\n\t\t\tDPRINTF((\"Ran of the end %p > %p\\n\", q, e));\n\t\t\tgoto out;\n\t\t}\n\t\tinp[i].pi_id = CDF_GETUINT32(p, i << 1);\n\t\tinp[i].pi_type = CDF_GETUINT32(q, 0);\n\t\tDPRINTF((\"%\" SIZE_T_FORMAT \"u) id=%x type=%x offs=0x%tx,0x%x\\n\",\n\t\t    i, inp[i].pi_id, inp[i].pi_type, q - p, offs));\n\t\tif (inp[i].pi_type & CDF_VECTOR) {\n\t\t\tnelements = CDF_GETUINT32(q, 1);\n\t\t\tif (nelements == 0) {\n\t\t\t\tDPRINTF((\"CDF_VECTOR with nelements == 0\\n\"));\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\to = 2;\n\t\t} else {\n\t\t\tnelements = 1;\n\t\t\to = 1;\n\t\t}\n\t\to4 = o * sizeof(uint32_t);\n\t\tif (inp[i].pi_type & (CDF_ARRAY|CDF_BYREF|CDF_RESERVED))\n\t\t\tgoto unknown;\n\t\tswitch (inp[i].pi_type & CDF_TYPEMASK) {\n\t\tcase CDF_NULL:\n\t\tcase CDF_EMPTY:\n\t\t\tbreak;\n\t\tcase CDF_SIGNED16:\n\t\t\tif (inp[i].pi_type & CDF_VECTOR)\n\t\t\t\tgoto unknown;\n\t\t\t(void)memcpy(&s16, &q[o4], sizeof(s16));\n\t\t\tinp[i].pi_s16 = CDF_TOLE2(s16);\n\t\t\tbreak;\n\t\tcase CDF_SIGNED32:\n\t\t\tif (inp[i].pi_type & CDF_VECTOR)\n\t\t\t\tgoto unknown;\n\t\t\t(void)memcpy(&s32, &q[o4], sizeof(s32));\n\t\t\tinp[i].pi_s32 = CDF_TOLE4((uint32_t)s32);\n\t\t\tbreak;\n\t\tcase CDF_BOOL:\n\t\tcase CDF_UNSIGNED32:\n\t\t\tif (inp[i].pi_type & CDF_VECTOR)\n\t\t\t\tgoto unknown;\n\t\t\t(void)memcpy(&u32, &q[o4], sizeof(u32));\n\t\t\tinp[i].pi_u32 = CDF_TOLE4(u32);\n\t\t\tbreak;\n\t\tcase CDF_SIGNED64:\n\t\t\tif (inp[i].pi_type & CDF_VECTOR)\n\t\t\t\tgoto unknown;\n\t\t\t(void)memcpy(&s64, &q[o4], sizeof(s64));\n\t\t\tinp[i].pi_s64 = CDF_TOLE8((uint64_t)s64);\n\t\t\tbreak;\n\t\tcase CDF_UNSIGNED64:\n\t\t\tif (inp[i].pi_type & CDF_VECTOR)\n\t\t\t\tgoto unknown;\n\t\t\t(void)memcpy(&u64, &q[o4], sizeof(u64));\n\t\t\tinp[i].pi_u64 = CDF_TOLE8((uint64_t)u64);\n\t\t\tbreak;\n\t\tcase CDF_FLOAT:\n\t\t\tif (inp[i].pi_type & CDF_VECTOR)\n\t\t\t\tgoto unknown;\n\t\t\t(void)memcpy(&u32, &q[o4], sizeof(u32));\n\t\t\tu32 = CDF_TOLE4(u32);\n\t\t\tmemcpy(&inp[i].pi_f, &u32, sizeof(inp[i].pi_f));\n\t\t\tbreak;\n\t\tcase CDF_DOUBLE:\n\t\t\tif (inp[i].pi_type & CDF_VECTOR)\n\t\t\t\tgoto unknown;\n\t\t\t(void)memcpy(&u64, &q[o4], sizeof(u64));\n\t\t\tu64 = CDF_TOLE8((uint64_t)u64);\n\t\t\tmemcpy(&inp[i].pi_d, &u64, sizeof(inp[i].pi_d));\n\t\t\tbreak;\n\t\tcase CDF_LENGTH32_STRING:\n\t\tcase CDF_LENGTH32_WSTRING:\n\t\t\tif (nelements > 1) {\n\t\t\t\tsize_t nelem = inp - *info;\n\t\t\t\tif (*maxcount > CDF_PROP_LIMIT\n\t\t\t\t    || nelements > CDF_PROP_LIMIT)\n\t\t\t\t\tgoto out;\n\t\t\t\t*maxcount += nelements;\n\t\t\t\tinp = CAST(cdf_property_info_t *,\n\t\t\t\t    realloc(*info, *maxcount * sizeof(*inp)));\n\t\t\t\tif (inp == NULL)\n\t\t\t\t\tgoto out;\n\t\t\t\t*info = inp;\n\t\t\t\tinp = *info + nelem;\n\t\t\t}\n\t\t\tDPRINTF((\"nelements = %\" SIZE_T_FORMAT \"u\\n\",\n\t\t\t    nelements));\n\t\t\tfor (j = 0; j < nelements && i < sh.sh_properties;\n\t\t\t    j++, i++)\n\t\t\t{\n\t\t\t\tuint32_t l = CDF_GETUINT32(q, o);\n\t\t\t\tinp[i].pi_str.s_len = l;\n\t\t\t\tinp[i].pi_str.s_buf = (const char *)\n\t\t\t\t    (const void *)(&q[o4 + sizeof(l)]);\n\t\t\t\tDPRINTF((\"l = %d, r = %\" SIZE_T_FORMAT\n\t\t\t\t    \"u, s = %s\\n\", l,\n\t\t\t\t    CDF_ROUND(l, sizeof(l)),\n\t\t\t\t    inp[i].pi_str.s_buf));\n\t\t\t\tif (l & 1)\n\t\t\t\t\tl++;\n\t\t\t\to += l >> 1;\n\t\t\t\tif (q + o >= e)\n\t\t\t\t\tgoto out;\n\t\t\t\to4 = o * sizeof(uint32_t);\n\t\t\t}\n\t\t\ti--;\n\t\t\tbreak;\n\t\tcase CDF_FILETIME:\n\t\t\tif (inp[i].pi_type & CDF_VECTOR)\n\t\t\t\tgoto unknown;\n\t\t\t(void)memcpy(&tp, &q[o4], sizeof(tp));\n\t\t\tinp[i].pi_tp = CDF_TOLE8((uint64_t)tp);\n\t\t\tbreak;\n\t\tcase CDF_CLIPBOARD:\n\t\t\tif (inp[i].pi_type & CDF_VECTOR)\n\t\t\t\tgoto unknown;\n\t\t\tbreak;\n\t\tdefault:\n\t\tunknown:\n\t\t\tDPRINTF((\"Don't know how to deal with %x\\n\",\n\t\t\t    inp[i].pi_type));\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn 0;\nout:\n\tfree(*info);\n\treturn -1;\n}"
    }
]